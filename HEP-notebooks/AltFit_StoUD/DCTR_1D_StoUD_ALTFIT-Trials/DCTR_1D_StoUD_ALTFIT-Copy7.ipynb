{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTR Alternative Fitting Algorithm for probStoUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:33:33.336439Z",
     "start_time": "2020-07-25T21:33:33.326191Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:33:41.066149Z",
     "start_time": "2020-07-25T21:33:33.784333Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:33:41.087744Z",
     "start_time": "2020-07-25T21:33:41.069869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22)\n",
    "rc('xtick', labelsize=15)\n",
    "rc('ytick', labelsize=15)\n",
    "rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:33:41.099030Z",
     "start_time": "2020-07-25T21:33:41.092789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:33:41.115796Z",
     "start_time": "2020-07-25T21:33:41.104750Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:33:41.137242Z",
     "start_time": "2020-07-25T21:33:41.120453Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:33:41.145481Z",
     "start_time": "2020-07-25T21:33:41.140668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/data0/users/aandreassen/zenodo/'\n",
    "data_dir1 = '/data1/users/asuresh/DCTRFitting/StoUDFitting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:34:06.201662Z",
     "start_time": "2020-07-25T21:33:41.150119Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "#unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')\n",
    "unknown_dataset =  np.load(data_dir1 + 'test1D_strange200.npz', allow_pickle=True)['dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:41:06.273401Z",
     "start_time": "2020-07-25T21:34:06.212771Z"
    }
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:41:32.444056Z",
     "start_time": "2020-07-25T21:41:06.286501Z"
    }
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:43:09.730393Z",
     "start_time": "2020-07-25T21:41:32.464605Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.5, shuffle=True)\n",
    "X_train_theta, X_test_theta, Y_train_theta, Y_test_theta = data_split(X_fit, Y_fit, test=0., shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:43:11.638150Z",
     "start_time": "2020-07-25T21:43:09.738297Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100, 100, 128)\n",
    "F_sizes = (100, 100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights(\n",
    "    './saved_models/DCTR_ee_dijets_1D_probStoUD_Copy7.h5')  #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:43:11.664121Z",
     "start_time": "2020-07-25T21:43:11.646314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "def reweight(events, param):  #from NN (DCTR)\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, param]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(events[...,0])>0,\n",
    "                                   K.ones_like(events[...,0]),\n",
    "                                   K.zeros_like(events[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "\n",
    "    model_inputs = K.concatenate([events, concat_input_and_params], -1)\n",
    "    # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    \n",
    "    f = dctr.model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:43:13.017871Z",
     "start_time": "2020-07-25T21:43:11.670788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = PFN(input_dim=4,\n",
    "            Phi_sizes=Phi_sizes,\n",
    "            F_sizes=F_sizes,\n",
    "            latent_dropout= 0.2,\n",
    "            F_dropouts= 0.2,\n",
    "            output_dim=1,\n",
    "            output_act='sigmoid',\n",
    "            summary=False)\n",
    "reinitialize_weights = model.model.get_weights()\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0., MSE_loss = False):\n",
    "    x = inputs  #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(\n",
    "        x, np.arange(51),\n",
    "        axis=1)  # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    weights = reweight(events = x, param = val)  # NN reweight\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-25T21:33:49.152Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-13-2bac8f3c1b35>:9: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 622s 692us/step - loss: 0.7084 - acc: 0.4984 - val_loss: 0.6925 - val_acc: 0.4974\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 616s 684us/step - loss: 0.6917 - acc: 0.4972 - val_loss: 0.6904 - val_acc: 0.4956\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 614s 683us/step - loss: 0.6869 - acc: 0.4921 - val_loss: 0.6840 - val_acc: 0.4917\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 618s 687us/step - loss: 0.6835 - acc: 0.4900 - val_loss: 0.6828 - val_acc: 0.4910\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6831 - acc: 0.4901 - val_loss: 0.6831 - val_acc: 0.4918\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6828 - acc: 0.4899 - val_loss: 0.6828 - val_acc: 0.4913\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 611s 679us/step - loss: 0.6826 - acc: 0.4896 - val_loss: 0.6826 - val_acc: 0.4907\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6825 - acc: 0.4894 - val_loss: 0.6827 - val_acc: 0.4908\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6824 - acc: 0.4894 - val_loss: 0.6828 - val_acc: 0.4909\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6824 - acc: 0.4900 - val_loss: 0.6826 - val_acc: 0.4908\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6823 - acc: 0.4892 - val_loss: 0.6823 - val_acc: 0.4910\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6822 - acc: 0.4894 - val_loss: 0.6829 - val_acc: 0.4913\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6822 - acc: 0.4891 - val_loss: 0.6822 - val_acc: 0.4908\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6822 - acc: 0.4893 - val_loss: 0.6834 - val_acc: 0.4911\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 607s 675us/step - loss: 0.6822 - acc: 0.4891 - val_loss: 0.6824 - val_acc: 0.4909\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6821 - acc: 0.4894 - val_loss: 0.6827 - val_acc: 0.4913\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 607s 674us/step - loss: 0.6820 - acc: 0.4896 - val_loss: 0.6834 - val_acc: 0.4910\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 607s 675us/step - loss: 0.6820 - acc: 0.4895 - val_loss: 0.6819 - val_acc: 0.4909\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 609s 676us/step - loss: 0.6820 - acc: 0.4895 - val_loss: 0.6831 - val_acc: 0.4910\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 607s 675us/step - loss: 0.6820 - acc: 0.4896 - val_loss: 0.6824 - val_acc: 0.4913\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6820 - acc: 0.4894 - val_loss: 0.6825 - val_acc: 0.4910\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6820 - acc: 0.4894 - val_loss: 0.6826 - val_acc: 0.4908\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6819 - acc: 0.4896 - val_loss: 0.6831 - val_acc: 0.4910\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6819 - acc: 0.4895 - val_loss: 0.6832 - val_acc: 0.4911\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 602s 668us/step - loss: 0.6819 - acc: 0.4892 - val_loss: 0.6830 - val_acc: 0.4907\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6819 - acc: 0.4894 - val_loss: 0.6826 - val_acc: 0.4909\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6819 - acc: 0.4894 - val_loss: 0.6824 - val_acc: 0.4908\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 601s 667us/step - loss: 0.6820 - acc: 0.4893 - val_loss: 0.6828 - val_acc: 0.4908\n",
      "trainnig theta = : 0.10833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 618s 687us/step - loss: 0.6839 - acc: 0.4894 - val_loss: 0.6839 - val_acc: 0.4909\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 597s 664us/step - loss: 0.6839 - acc: 0.4893 - val_loss: 0.6843 - val_acc: 0.4909\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 596s 663us/step - loss: 0.6839 - acc: 0.4892 - val_loss: 0.6858 - val_acc: 0.4912\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 598s 664us/step - loss: 0.6839 - acc: 0.4895 - val_loss: 0.6847 - val_acc: 0.4910\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6838 - acc: 0.4893 - val_loss: 0.6847 - val_acc: 0.4908\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6838 - acc: 0.4894 - val_loss: 0.6851 - val_acc: 0.4911\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6838 - acc: 0.4892 - val_loss: 0.6845 - val_acc: 0.4911\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6837 - acc: 0.4894 - val_loss: 0.6842 - val_acc: 0.4909\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6837 - acc: 0.4893 - val_loss: 0.6848 - val_acc: 0.4909\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 591s 657us/step - loss: 0.6837 - acc: 0.4897 - val_loss: 0.6846 - val_acc: 0.4907\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 593s 658us/step - loss: 0.6838 - acc: 0.4893 - val_loss: 0.6845 - val_acc: 0.4909\n",
      "trainnig theta = : 0.11666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6856 - acc: 0.4892 - val_loss: 0.6859 - val_acc: 0.4911\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 590s 655us/step - loss: 0.6855 - acc: 0.4895 - val_loss: 0.6863 - val_acc: 0.4911\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6856 - acc: 0.4894 - val_loss: 0.6856 - val_acc: 0.4909\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 590s 656us/step - loss: 0.6856 - acc: 0.4893 - val_loss: 0.6857 - val_acc: 0.4910\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6855 - acc: 0.4894 - val_loss: 0.6860 - val_acc: 0.4911\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 591s 656us/step - loss: 0.6855 - acc: 0.4898 - val_loss: 0.6860 - val_acc: 0.4911\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 590s 655us/step - loss: 0.6855 - acc: 0.4895 - val_loss: 0.6860 - val_acc: 0.4913\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 590s 656us/step - loss: 0.6855 - acc: 0.4895 - val_loss: 0.6860 - val_acc: 0.4908\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 588s 654us/step - loss: 0.6855 - acc: 0.4895 - val_loss: 0.6861 - val_acc: 0.4912\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 587s 653us/step - loss: 0.6855 - acc: 0.4895 - val_loss: 0.6862 - val_acc: 0.4910\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 586s 652us/step - loss: 0.6854 - acc: 0.4899 - val_loss: 0.6864 - val_acc: 0.4910\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 589s 654us/step - loss: 0.6855 - acc: 0.4897 - val_loss: 0.6864 - val_acc: 0.4911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 586s 651us/step - loss: 0.6855 - acc: 0.4895 - val_loss: 0.6860 - val_acc: 0.4909\n",
      "trainnig theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 589s 654us/step - loss: 0.6872 - acc: 0.4896 - val_loss: 0.6874 - val_acc: 0.4911\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 583s 648us/step - loss: 0.6871 - acc: 0.4899 - val_loss: 0.6872 - val_acc: 0.4909\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 581s 646us/step - loss: 0.6871 - acc: 0.4895 - val_loss: 0.6875 - val_acc: 0.4911\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 583s 647us/step - loss: 0.6871 - acc: 0.4894 - val_loss: 0.6874 - val_acc: 0.4909\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 580s 645us/step - loss: 0.6871 - acc: 0.4898 - val_loss: 0.6882 - val_acc: 0.4908\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 650s 723us/step - loss: 0.6871 - acc: 0.4897 - val_loss: 0.6876 - val_acc: 0.4911\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 671s 745us/step - loss: 0.6871 - acc: 0.4895 - val_loss: 0.6876 - val_acc: 0.4911\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 672s 747us/step - loss: 0.6871 - acc: 0.4893 - val_loss: 0.6874 - val_acc: 0.4910\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 697s 774us/step - loss: 0.6871 - acc: 0.4896 - val_loss: 0.6876 - val_acc: 0.4909\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 709s 788us/step - loss: 0.6871 - acc: 0.4896 - val_loss: 0.6877 - val_acc: 0.4910\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 709s 787us/step - loss: 0.6870 - acc: 0.4901 - val_loss: 0.6880 - val_acc: 0.4911\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6871 - acc: 0.4897 - val_loss: 0.6873 - val_acc: 0.4909\n",
      "trainnig theta = : 0.13333333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 716s 795us/step - loss: 0.6886 - acc: 0.4892 - val_loss: 0.6886 - val_acc: 0.4911\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6885 - acc: 0.4899 - val_loss: 0.6892 - val_acc: 0.4913\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6885 - acc: 0.4894 - val_loss: 0.6895 - val_acc: 0.4912\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6885 - acc: 0.4897 - val_loss: 0.6892 - val_acc: 0.4911\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6886 - acc: 0.4896 - val_loss: 0.6893 - val_acc: 0.4910\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6885 - acc: 0.4897 - val_loss: 0.6887 - val_acc: 0.4911\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 712s 791us/step - loss: 0.6885 - acc: 0.4898 - val_loss: 0.6887 - val_acc: 0.4910\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 716s 795us/step - loss: 0.6885 - acc: 0.4898 - val_loss: 0.6893 - val_acc: 0.4910\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 714s 794us/step - loss: 0.6885 - acc: 0.4899 - val_loss: 0.6891 - val_acc: 0.4909\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6885 - acc: 0.4903 - val_loss: 0.6888 - val_acc: 0.4908\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6885 - acc: 0.4894 - val_loss: 0.6886 - val_acc: 0.4909\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 680s 756us/step - loss: 0.6885 - acc: 0.4898 - val_loss: 0.6893 - val_acc: 0.4914\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6884 - acc: 0.4898 - val_loss: 0.6892 - val_acc: 0.4909\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6884 - acc: 0.4900 - val_loss: 0.6893 - val_acc: 0.4909\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6884 - acc: 0.4901 - val_loss: 0.6890 - val_acc: 0.4909\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 704s 783us/step - loss: 0.6884 - acc: 0.4899 - val_loss: 0.6889 - val_acc: 0.4907\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6884 - acc: 0.4899 - val_loss: 0.6890 - val_acc: 0.4909\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 672s 746us/step - loss: 0.6884 - acc: 0.4898 - val_loss: 0.6892 - val_acc: 0.4909\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 674s 749us/step - loss: 0.6884 - acc: 0.4901 - val_loss: 0.6889 - val_acc: 0.4909\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6885 - acc: 0.4900 - val_loss: 0.6890 - val_acc: 0.4910\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6884 - acc: 0.4902 - val_loss: 0.6888 - val_acc: 0.4908\n",
      "trainnig theta = : 0.14166666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 712s 792us/step - loss: 0.6897 - acc: 0.4900 - val_loss: 0.6902 - val_acc: 0.4911\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6897 - acc: 0.4898 - val_loss: 0.6901 - val_acc: 0.4911\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6897 - acc: 0.4901 - val_loss: 0.6901 - val_acc: 0.4911\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6897 - acc: 0.4901 - val_loss: 0.6900 - val_acc: 0.4912\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 667s 742us/step - loss: 0.6897 - acc: 0.4902 - val_loss: 0.6902 - val_acc: 0.4909\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6897 - acc: 0.4898 - val_loss: 0.6901 - val_acc: 0.4911\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6897 - acc: 0.4898 - val_loss: 0.6900 - val_acc: 0.4909\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6897 - acc: 0.4901 - val_loss: 0.6899 - val_acc: 0.4913\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6897 - acc: 0.4900 - val_loss: 0.6901 - val_acc: 0.4911\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6896 - acc: 0.4903 - val_loss: 0.6902 - val_acc: 0.4910\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 712s 791us/step - loss: 0.6897 - acc: 0.4901 - val_loss: 0.6901 - val_acc: 0.4916\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6896 - acc: 0.4905 - val_loss: 0.6901 - val_acc: 0.4908\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 669s 744us/step - loss: 0.6896 - acc: 0.4904 - val_loss: 0.6901 - val_acc: 0.4909\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 678s 753us/step - loss: 0.6896 - acc: 0.4901 - val_loss: 0.6899 - val_acc: 0.4911\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 678s 753us/step - loss: 0.6896 - acc: 0.4901 - val_loss: 0.6900 - val_acc: 0.4911\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 696s 774us/step - loss: 0.6897 - acc: 0.4903 - val_loss: 0.6904 - val_acc: 0.4914\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 705s 784us/step - loss: 0.6897 - acc: 0.4905 - val_loss: 0.6902 - val_acc: 0.4907\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6897 - acc: 0.4907 - val_loss: 0.6903 - val_acc: 0.4911\n",
      "trainnig theta = : 0.15000000000000002\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 716s 795us/step - loss: 0.6908 - acc: 0.4901 - val_loss: 0.6913 - val_acc: 0.4912\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 709s 788us/step - loss: 0.6908 - acc: 0.4903 - val_loss: 0.6911 - val_acc: 0.4914\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 668s 743us/step - loss: 0.6908 - acc: 0.4904 - val_loss: 0.6911 - val_acc: 0.4913\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 682s 758us/step - loss: 0.6908 - acc: 0.4903 - val_loss: 0.6914 - val_acc: 0.4913\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6908 - acc: 0.4906 - val_loss: 0.6911 - val_acc: 0.4910\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6908 - acc: 0.4904 - val_loss: 0.6910 - val_acc: 0.4914\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 717s 796us/step - loss: 0.6908 - acc: 0.4901 - val_loss: 0.6912 - val_acc: 0.4910\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 714s 794us/step - loss: 0.6908 - acc: 0.4906 - val_loss: 0.6912 - val_acc: 0.4907\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 713s 792us/step - loss: 0.6908 - acc: 0.4905 - val_loss: 0.6910 - val_acc: 0.4913\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 714s 793us/step - loss: 0.6908 - acc: 0.4901 - val_loss: 0.6912 - val_acc: 0.4912\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 670s 745us/step - loss: 0.6908 - acc: 0.4902 - val_loss: 0.6910 - val_acc: 0.4909\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6907 - acc: 0.4903 - val_loss: 0.6912 - val_acc: 0.4910\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 680s 755us/step - loss: 0.6907 - acc: 0.4905 - val_loss: 0.6912 - val_acc: 0.4913\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6907 - acc: 0.4906 - val_loss: 0.6913 - val_acc: 0.4912\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 713s 792us/step - loss: 0.6908 - acc: 0.4905 - val_loss: 0.6911 - val_acc: 0.4910\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 710s 789us/step - loss: 0.6907 - acc: 0.4901 - val_loss: 0.6910 - val_acc: 0.4911\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 710s 789us/step - loss: 0.6908 - acc: 0.4903 - val_loss: 0.6912 - val_acc: 0.4912\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6907 - acc: 0.4907 - val_loss: 0.6911 - val_acc: 0.4911\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6907 - acc: 0.4908 - val_loss: 0.6913 - val_acc: 0.4910\n",
      "trainnig theta = : 0.15833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 676s 751us/step - loss: 0.6917 - acc: 0.4906 - val_loss: 0.6920 - val_acc: 0.4911\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 671s 746us/step - loss: 0.6917 - acc: 0.4905 - val_loss: 0.6920 - val_acc: 0.4911\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6917 - acc: 0.4906 - val_loss: 0.6920 - val_acc: 0.4909\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 724s 804us/step - loss: 0.6917 - acc: 0.4909 - val_loss: 0.6920 - val_acc: 0.4912\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 712s 791us/step - loss: 0.6918 - acc: 0.4907 - val_loss: 0.6920 - val_acc: 0.4911\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 711s 790us/step - loss: 0.6917 - acc: 0.4905 - val_loss: 0.6921 - val_acc: 0.4912\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6917 - acc: 0.4910 - val_loss: 0.6921 - val_acc: 0.4909\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 665s 739us/step - loss: 0.6917 - acc: 0.4908 - val_loss: 0.6921 - val_acc: 0.4910\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 696s 773us/step - loss: 0.6918 - acc: 0.4904 - val_loss: 0.6922 - val_acc: 0.4909\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 699s 776us/step - loss: 0.6918 - acc: 0.4902 - val_loss: 0.6920 - val_acc: 0.4911\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6917 - acc: 0.4910 - val_loss: 0.6921 - val_acc: 0.4911\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 713s 793us/step - loss: 0.6917 - acc: 0.4908 - val_loss: 0.6921 - val_acc: 0.4909\n",
      "trainnig theta = : 0.16666666666666669\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 719s 799us/step - loss: 0.6926 - acc: 0.4910 - val_loss: 0.6929 - val_acc: 0.4912\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6927 - acc: 0.4906 - val_loss: 0.6929 - val_acc: 0.4910\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6927 - acc: 0.4909 - val_loss: 0.6931 - val_acc: 0.4918\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6927 - acc: 0.4914 - val_loss: 0.6929 - val_acc: 0.4913\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6926 - acc: 0.4912 - val_loss: 0.6930 - val_acc: 0.4913\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6926 - acc: 0.4911 - val_loss: 0.6929 - val_acc: 0.4913\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 718s 798us/step - loss: 0.6927 - acc: 0.4911 - val_loss: 0.6930 - val_acc: 0.4914\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 719s 799us/step - loss: 0.6926 - acc: 0.4913 - val_loss: 0.6928 - val_acc: 0.4908\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 716s 796us/step - loss: 0.6926 - acc: 0.4917 - val_loss: 0.6929 - val_acc: 0.4911\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 719s 799us/step - loss: 0.6926 - acc: 0.4915 - val_loss: 0.6930 - val_acc: 0.4911\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 717s 797us/step - loss: 0.6927 - acc: 0.4916 - val_loss: 0.6929 - val_acc: 0.4911\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 717s 796us/step - loss: 0.6926 - acc: 0.4914 - val_loss: 0.6929 - val_acc: 0.4913\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 717s 797us/step - loss: 0.6926 - acc: 0.4908 - val_loss: 0.6929 - val_acc: 0.4911\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 720s 800us/step - loss: 0.6926 - acc: 0.4921 - val_loss: 0.6929 - val_acc: 0.4909\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 718s 798us/step - loss: 0.6926 - acc: 0.4913 - val_loss: 0.6931 - val_acc: 0.4916\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 718s 798us/step - loss: 0.6926 - acc: 0.4913 - val_loss: 0.6929 - val_acc: 0.4913\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 718s 798us/step - loss: 0.6926 - acc: 0.4917 - val_loss: 0.6930 - val_acc: 0.4913\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 717s 796us/step - loss: 0.6926 - acc: 0.4919 - val_loss: 0.6930 - val_acc: 0.4916\n",
      "trainnig theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 725s 806us/step - loss: 0.6931 - acc: 0.4915 - val_loss: 0.6934 - val_acc: 0.4911\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 718s 798us/step - loss: 0.6931 - acc: 0.4915 - val_loss: 0.6933 - val_acc: 0.4914\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 719s 799us/step - loss: 0.6931 - acc: 0.4911 - val_loss: 0.6933 - val_acc: 0.4913\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 716s 795us/step - loss: 0.6931 - acc: 0.4919 - val_loss: 0.6933 - val_acc: 0.4909\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 717s 797us/step - loss: 0.6931 - acc: 0.4920 - val_loss: 0.6933 - val_acc: 0.4925\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 725s 805us/step - loss: 0.6931 - acc: 0.4920 - val_loss: 0.6933 - val_acc: 0.4912\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 722s 802us/step - loss: 0.6931 - acc: 0.4920 - val_loss: 0.6933 - val_acc: 0.4913\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 721s 801us/step - loss: 0.6930 - acc: 0.4921 - val_loss: 0.6934 - val_acc: 0.4924\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 717s 797us/step - loss: 0.6931 - acc: 0.4923 - val_loss: 0.6933 - val_acc: 0.4921\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 717s 797us/step - loss: 0.6930 - acc: 0.4929 - val_loss: 0.6934 - val_acc: 0.4917\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 717s 796us/step - loss: 0.6930 - acc: 0.4926 - val_loss: 0.6934 - val_acc: 0.4924\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 716s 795us/step - loss: 0.6930 - acc: 0.4925 - val_loss: 0.6933 - val_acc: 0.4913\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 716s 795us/step - loss: 0.6931 - acc: 0.4927 - val_loss: 0.6933 - val_acc: 0.4918\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 715s 795us/step - loss: 0.6930 - acc: 0.4924 - val_loss: 0.6933 - val_acc: 0.4914\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 717s 796us/step - loss: 0.6931 - acc: 0.4931 - val_loss: 0.6933 - val_acc: 0.4912\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 713s 792us/step - loss: 0.6930 - acc: 0.4929 - val_loss: 0.6934 - val_acc: 0.4919\n",
      "trainnig theta = : 0.18333333333333335\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 714s 793us/step - loss: 0.6928 - acc: 0.4935 - val_loss: 0.6930 - val_acc: 0.4916\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 708s 786us/step - loss: 0.6928 - acc: 0.4939 - val_loss: 0.6930 - val_acc: 0.4920\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 709s 788us/step - loss: 0.6928 - acc: 0.4928 - val_loss: 0.6930 - val_acc: 0.4918\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6928 - acc: 0.4939 - val_loss: 0.6930 - val_acc: 0.4919\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6928 - acc: 0.4935 - val_loss: 0.6930 - val_acc: 0.4924\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 707s 785us/step - loss: 0.6928 - acc: 0.4942 - val_loss: 0.6930 - val_acc: 0.4922\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6928 - acc: 0.4941 - val_loss: 0.6930 - val_acc: 0.4935\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6928 - acc: 0.4940 - val_loss: 0.6930 - val_acc: 0.4930\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6928 - acc: 0.4943 - val_loss: 0.6930 - val_acc: 0.4925\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6928 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.4939\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 705s 784us/step - loss: 0.6928 - acc: 0.4953 - val_loss: 0.6930 - val_acc: 0.4931\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6927 - acc: 0.4949 - val_loss: 0.6930 - val_acc: 0.4930\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 704s 783us/step - loss: 0.6928 - acc: 0.4948 - val_loss: 0.6930 - val_acc: 0.4923\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6928 - acc: 0.4946 - val_loss: 0.6930 - val_acc: 0.4923\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6927 - acc: 0.4949 - val_loss: 0.6930 - val_acc: 0.4925\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6927 - acc: 0.4946 - val_loss: 0.6930 - val_acc: 0.4930\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6927 - acc: 0.4946 - val_loss: 0.6930 - val_acc: 0.4918\n",
      "trainnig theta = : 0.19166666666666665\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 711s 790us/step - loss: 0.6919 - acc: 0.4952 - val_loss: 0.6920 - val_acc: 0.4927\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6919 - acc: 0.4958 - val_loss: 0.6921 - val_acc: 0.4926\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6919 - acc: 0.4953 - val_loss: 0.6920 - val_acc: 0.4923\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6919 - acc: 0.4959 - val_loss: 0.6921 - val_acc: 0.4958\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 696s 773us/step - loss: 0.6919 - acc: 0.4968 - val_loss: 0.6921 - val_acc: 0.4963\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 695s 773us/step - loss: 0.6919 - acc: 0.4964 - val_loss: 0.6921 - val_acc: 0.4927\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6919 - acc: 0.4977 - val_loss: 0.6921 - val_acc: 0.4941\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6919 - acc: 0.4968 - val_loss: 0.6921 - val_acc: 0.4929\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 696s 773us/step - loss: 0.6919 - acc: 0.4965 - val_loss: 0.6921 - val_acc: 0.4926\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6918 - acc: 0.4972 - val_loss: 0.6921 - val_acc: 0.4940\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 694s 772us/step - loss: 0.6919 - acc: 0.4967 - val_loss: 0.6921 - val_acc: 0.4942\n",
      "trainnig theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6909 - acc: 0.4976 - val_loss: 0.6910 - val_acc: 0.4946\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6909 - acc: 0.4990 - val_loss: 0.6910 - val_acc: 0.4985\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6908 - acc: 0.4990 - val_loss: 0.6910 - val_acc: 0.4985\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6909 - acc: 0.4991 - val_loss: 0.6910 - val_acc: 0.4970\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6909 - acc: 0.4991 - val_loss: 0.6910 - val_acc: 0.4978\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6909 - acc: 0.4995 - val_loss: 0.6910 - val_acc: 0.4968\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6908 - acc: 0.4999 - val_loss: 0.6910 - val_acc: 0.4983\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6908 - acc: 0.5000 - val_loss: 0.6910 - val_acc: 0.4970\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6908 - acc: 0.5008 - val_loss: 0.6910 - val_acc: 0.4982\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6908 - acc: 0.5020 - val_loss: 0.6910 - val_acc: 0.4988\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6908 - acc: 0.5018 - val_loss: 0.6910 - val_acc: 0.4982\n",
      "trainnig theta = : 0.20833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6907 - acc: 0.5027 - val_loss: 0.6907 - val_acc: 0.5039\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6907 - acc: 0.5026 - val_loss: 0.6907 - val_acc: 0.5033\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 691s 767us/step - loss: 0.6907 - acc: 0.5042 - val_loss: 0.6907 - val_acc: 0.5048\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6907 - acc: 0.5045 - val_loss: 0.6908 - val_acc: 0.5005\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6907 - acc: 0.5046 - val_loss: 0.6907 - val_acc: 0.5075\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6907 - acc: 0.5055 - val_loss: 0.6908 - val_acc: 0.5058\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6907 - acc: 0.5046 - val_loss: 0.6907 - val_acc: 0.4987\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6907 - acc: 0.5035 - val_loss: 0.6907 - val_acc: 0.5011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6907 - acc: 0.5049 - val_loss: 0.6907 - val_acc: 0.5057\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6907 - acc: 0.5043 - val_loss: 0.6907 - val_acc: 0.5003\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6907 - acc: 0.5058 - val_loss: 0.6907 - val_acc: 0.5058\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6907 - acc: 0.5067 - val_loss: 0.6908 - val_acc: 0.5002\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 681s 757us/step - loss: 0.6906 - acc: 0.5053 - val_loss: 0.6908 - val_acc: 0.5030\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6907 - acc: 0.5047 - val_loss: 0.6907 - val_acc: 0.5003\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6907 - acc: 0.5044 - val_loss: 0.6908 - val_acc: 0.5026\n",
      "trainnig theta = : 0.21666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6911 - acc: 0.5083 - val_loss: 0.6910 - val_acc: 0.5077\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6911 - acc: 0.5075 - val_loss: 0.6911 - val_acc: 0.5076\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6910 - acc: 0.5077 - val_loss: 0.6911 - val_acc: 0.5077\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 712s 791us/step - loss: 0.6910 - acc: 0.5087 - val_loss: 0.6911 - val_acc: 0.5076\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 719s 799us/step - loss: 0.6910 - acc: 0.5087 - val_loss: 0.6911 - val_acc: 0.5072\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 718s 798us/step - loss: 0.6910 - acc: 0.5081 - val_loss: 0.6911 - val_acc: 0.5073\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 734s 816us/step - loss: 0.6910 - acc: 0.5088 - val_loss: 0.6911 - val_acc: 0.5078\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 741s 823us/step - loss: 0.6910 - acc: 0.5087 - val_loss: 0.6911 - val_acc: 0.5078\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6910 - acc: 0.5092 - val_loss: 0.6911 - val_acc: 0.5071\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 564s 627us/step - loss: 0.6910 - acc: 0.5098 - val_loss: 0.6910 - val_acc: 0.5071\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 570s 634us/step - loss: 0.6910 - acc: 0.5094 - val_loss: 0.6911 - val_acc: 0.5065\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 582s 647us/step - loss: 0.6910 - acc: 0.5100 - val_loss: 0.6911 - val_acc: 0.5078\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6910 - acc: 0.5096 - val_loss: 0.6911 - val_acc: 0.5074\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 573s 637us/step - loss: 0.6910 - acc: 0.5097 - val_loss: 0.6911 - val_acc: 0.5069\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6910 - acc: 0.5095 - val_loss: 0.6911 - val_acc: 0.5077\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 626s 696us/step - loss: 0.6910 - acc: 0.5097 - val_loss: 0.6910 - val_acc: 0.5078\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 625s 694us/step - loss: 0.6910 - acc: 0.5098 - val_loss: 0.6911 - val_acc: 0.5064\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 649s 722us/step - loss: 0.6910 - acc: 0.5098 - val_loss: 0.6911 - val_acc: 0.5074\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 634s 705us/step - loss: 0.6910 - acc: 0.5098 - val_loss: 0.6910 - val_acc: 0.5079\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 638s 709us/step - loss: 0.6910 - acc: 0.5102 - val_loss: 0.6911 - val_acc: 0.5070\n",
      "trainnig theta = : 0.225\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 645s 717us/step - loss: 0.6911 - acc: 0.5098 - val_loss: 0.6911 - val_acc: 0.5089\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 649s 721us/step - loss: 0.6911 - acc: 0.5101 - val_loss: 0.6911 - val_acc: 0.5080\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 645s 716us/step - loss: 0.6910 - acc: 0.5105 - val_loss: 0.6911 - val_acc: 0.5083\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 646s 717us/step - loss: 0.6911 - acc: 0.5102 - val_loss: 0.6910 - val_acc: 0.5091\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 647s 718us/step - loss: 0.6910 - acc: 0.5103 - val_loss: 0.6912 - val_acc: 0.5084\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 623s 692us/step - loss: 0.6910 - acc: 0.5106 - val_loss: 0.6912 - val_acc: 0.5080\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 615s 684us/step - loss: 0.6910 - acc: 0.5103 - val_loss: 0.6911 - val_acc: 0.5084\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 622s 691us/step - loss: 0.6910 - acc: 0.5108 - val_loss: 0.6911 - val_acc: 0.5085\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 629s 699us/step - loss: 0.6910 - acc: 0.5112 - val_loss: 0.6911 - val_acc: 0.5079\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 621s 690us/step - loss: 0.6910 - acc: 0.5111 - val_loss: 0.6911 - val_acc: 0.5087\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 607s 674us/step - loss: 0.6910 - acc: 0.5102 - val_loss: 0.6911 - val_acc: 0.5084\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 633s 704us/step - loss: 0.6910 - acc: 0.5113 - val_loss: 0.6911 - val_acc: 0.5080\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 633s 704us/step - loss: 0.6910 - acc: 0.5111 - val_loss: 0.6911 - val_acc: 0.5086\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 629s 699us/step - loss: 0.6910 - acc: 0.5111 - val_loss: 0.6911 - val_acc: 0.5083\n",
      "trainnig theta = : 0.23333333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 625s 694us/step - loss: 0.6910 - acc: 0.5105 - val_loss: 0.6911 - val_acc: 0.5089\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 604s 672us/step - loss: 0.6910 - acc: 0.5108 - val_loss: 0.6910 - val_acc: 0.5086\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6910 - acc: 0.5106 - val_loss: 0.6912 - val_acc: 0.5078\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6910 - acc: 0.5104 - val_loss: 0.6911 - val_acc: 0.5091\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 576s 640us/step - loss: 0.6910 - acc: 0.5107 - val_loss: 0.6911 - val_acc: 0.5086\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 604s 671us/step - loss: 0.6910 - acc: 0.5103 - val_loss: 0.6911 - val_acc: 0.5084\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6910 - acc: 0.5107 - val_loss: 0.6910 - val_acc: 0.5088\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6910 - acc: 0.5114 - val_loss: 0.6911 - val_acc: 0.5087\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 601s 668us/step - loss: 0.6910 - acc: 0.5114 - val_loss: 0.6912 - val_acc: 0.5084\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 601s 667us/step - loss: 0.6909 - acc: 0.5109 - val_loss: 0.6911 - val_acc: 0.5089\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6909 - acc: 0.5115 - val_loss: 0.6912 - val_acc: 0.5084\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6909 - acc: 0.5113 - val_loss: 0.6910 - val_acc: 0.5088\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6909 - acc: 0.5115 - val_loss: 0.6910 - val_acc: 0.5090\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6909 - acc: 0.5115 - val_loss: 0.6912 - val_acc: 0.5083\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6909 - acc: 0.5110 - val_loss: 0.6911 - val_acc: 0.5087\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 586s 652us/step - loss: 0.6909 - acc: 0.5112 - val_loss: 0.6911 - val_acc: 0.5088\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6909 - acc: 0.5114 - val_loss: 0.6911 - val_acc: 0.5085\n",
      "trainnig theta = : 0.24166666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 614s 682us/step - loss: 0.6912 - acc: 0.5109 - val_loss: 0.6913 - val_acc: 0.5088\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 605s 672us/step - loss: 0.6912 - acc: 0.5106 - val_loss: 0.6913 - val_acc: 0.5084\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 604s 672us/step - loss: 0.6912 - acc: 0.5107 - val_loss: 0.6913 - val_acc: 0.5082\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 601s 667us/step - loss: 0.6912 - acc: 0.5111 - val_loss: 0.6914 - val_acc: 0.5088\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6912 - acc: 0.5110 - val_loss: 0.6912 - val_acc: 0.5089\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6912 - acc: 0.5112 - val_loss: 0.6912 - val_acc: 0.5089\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6912 - acc: 0.5107 - val_loss: 0.6913 - val_acc: 0.5091\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6911 - acc: 0.5113 - val_loss: 0.6914 - val_acc: 0.5086\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 579s 643us/step - loss: 0.6912 - acc: 0.5110 - val_loss: 0.6913 - val_acc: 0.5083\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 572s 635us/step - loss: 0.6912 - acc: 0.5109 - val_loss: 0.6912 - val_acc: 0.5090\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 570s 634us/step - loss: 0.6911 - acc: 0.5108 - val_loss: 0.6914 - val_acc: 0.5082\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 572s 635us/step - loss: 0.6911 - acc: 0.5113 - val_loss: 0.6913 - val_acc: 0.5090\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 569s 632us/step - loss: 0.6911 - acc: 0.5114 - val_loss: 0.6913 - val_acc: 0.5088\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 569s 632us/step - loss: 0.6911 - acc: 0.5115 - val_loss: 0.6913 - val_acc: 0.5085\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 569s 632us/step - loss: 0.6911 - acc: 0.5113 - val_loss: 0.6912 - val_acc: 0.5089\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 583s 648us/step - loss: 0.6911 - acc: 0.5110 - val_loss: 0.6912 - val_acc: 0.5088\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 582s 647us/step - loss: 0.6911 - acc: 0.5124 - val_loss: 0.6913 - val_acc: 0.5086\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 575s 639us/step - loss: 0.6911 - acc: 0.5115 - val_loss: 0.6912 - val_acc: 0.5089\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 559s 621us/step - loss: 0.6911 - acc: 0.5118 - val_loss: 0.6913 - val_acc: 0.5086\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 567s 630us/step - loss: 0.6911 - acc: 0.5114 - val_loss: 0.6913 - val_acc: 0.5084\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 569s 632us/step - loss: 0.6911 - acc: 0.5118 - val_loss: 0.6913 - val_acc: 0.5088\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 575s 639us/step - loss: 0.6911 - acc: 0.5118 - val_loss: 0.6914 - val_acc: 0.5088\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 575s 639us/step - loss: 0.6911 - acc: 0.5121 - val_loss: 0.6913 - val_acc: 0.5084\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 588s 654us/step - loss: 0.6911 - acc: 0.5117 - val_loss: 0.6913 - val_acc: 0.5087\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 582s 646us/step - loss: 0.6911 - acc: 0.5116 - val_loss: 0.6915 - val_acc: 0.5086\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 578s 642us/step - loss: 0.6911 - acc: 0.5118 - val_loss: 0.6912 - val_acc: 0.5090\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 585s 651us/step - loss: 0.6911 - acc: 0.5117 - val_loss: 0.6912 - val_acc: 0.5086\n",
      "Epoch 28/100\n",
      "897000/900000 [============================>.] - ETA: 1s - loss: 0.6910 - acc: 0.5121"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 25)   #iterating across possible StoUD values\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam',\n",
    "                        loss=my_loss_wrapper(myinputs, theta),\n",
    "                        metrics=['accuracy'])\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystopping])\n",
    "    lvals += [history.history['loss'][np.argmin(history.history['val_loss'])]]\n",
    "    vlvals += [np.min(history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T00:51:44.216795Z",
     "start_time": "2020-07-29T00:51:43.246855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFtCAYAAACJLFTlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXl8VNXZ+L8HQiCsQ1gUUJYJCCiLJEHcBQl13xPQal1aSV6rbd++ryVSu7zdfjTY2moXm2Bdam2FBK1L3RLU1h2TiLtVMoACKkgY9iUk5/fHPTfcTGYmM5Pl3pk838/nfpK559xzn3OX89znOc85R2mtEQRBEIRkpIfbAgiCIAhCoogSEwRBEJIWUWKCIAhC0iJKTBAEQUhaRIkJgiAISYsoMUEQBCFpESUmCIIgJC2ixAQhSVBKLXJbBkHoaJRS+Uopf6LHdxslppTKVkqVO7ZKpVShSfPZ/3fi+RcppWqUUtps283v7DB5Kx35tFKqvI1yKh1bjVKqzvyf15l1cpsw10Kb36Vuy9bRmDqVOX4vMvc50nNQGZKulVL5CZzX53imdKxpMZYdy7Nco5QqVUr54pEtFXC7zeoqtNYVQHHCikxrnfIbUAhUAr4w+0uBEqCki2QpBTSQ3UY+v8nnS6QcIBuoA8rdvv5dcE3LY7mmybqZ5zMv0boDecB2YFGC5/fZz1s8aXGU39azvMikF3bG+b24eanN6qL6+oDKRI5NeUvMaPcSoEBrHXSmaa3LsBqBsG4apVRJJ4oWjJaotQ4AFaEyhzk+bLrWulZrnQX4lFI1iYuZFNSbv1GvaTJint9srXVVhCxt1t0cWwwMSUQG8wxWxpsWB209y0ux5C8N9Vx00Pk9RXvarGTFvo+JuMxTXokBRcCKSMrAvOCRGoiE/bQdRH3bWdqkAMjuZIUsdB6lWA14uzCNXyuXXBJhu1LLo+ZKDdrTZiUt5mOlKJzrOBrdQYll0/YXeqQ+lKTvUzIvwlJgUXs6T4Wux9wvv9a6toOKTFpL1TzHAcAfbyOXhLSnzUp2qoD58RzQHZQYWA9FNFo1EqYTPFVeFvuBb/cXvdClFAEViR4cprHf1j5xXMdu2LvDx1jcbVaKUI713MdMd1BilUBetMgs0/+0HJqjfvJJIbeFqV+QOL9wBNfJxzyXCbI45HdZ2FzJgw+s/l63Belk4mqzUgnjKs2Ox9pO60R5PIHWeqlSajFQrpRaCpSaByA0n/1iLMb6CrJdF85O40rjt23GdDTnYSkJH1bn+XIPvmgBrIfDH67+4TD9aIUctkjLtNZFjnQfsM6kB4GFWusKs7/QnBMg0/zNApZECVbpcmK9f7HUqSPrbcpqrysxXFh6jdnv11orx/5sYBnGytFaD27HeTsc+3rQhgVihpXYVkwWUBf6zjryOp/tLKz3u5XlG899NYpnJlZksA8YorWOywMSb5tl3M7l5nyZWuvBDjnAum5vtPc6OPL7sbw6dRwOFgrb5iV4Paqw3snYvBBuh1Z2UfhmNlaIsTZbHZaLLWzYsj4cyqrbKLeQMGGhWA9Uq3Bgk2aHE/tjkLs0SlpJrOU4ZNLR6hzl2O1ECNU317YmZF+40GB/PPLGKV/M1zTR+xdLnTqy3uYlrku07lHqFi1c3m/qsD1MWn6k9yFaWox1bfNZjpbHPr+5ZnkhadvDvYtY0X2h96om3HMe6301MpaH5CuM5T6GOWdcbZaRx37HC8M8D5URnoeYr4PjWteFKT+fkCESiV4PU8+IbV+r/Ik+eMm2mZfXHnuhHdv20IvvuNg6Snn+NtIjleuWErPPm5/AtSsJ17CZtHzni2VevkgvQGms8sYpX9xKLJ77F0udOrre5vlrc4yfo+6Vjs1u/MKOuyG6QloU7l63cUynKTHz3paYhjPSODJbibUaB2euT+hHlo8w487MPWwxXi3W+4qlQDVhxnUa2eMeo5dom0VkRdfiGsVzHULyt2pDCFF87bkesT779tYd+sQAK7pJa12mtZ6nLTfKPKw+Ah9Qk0DkXinRzd0VWO4Zr2C7QBJx5ZVijTcL56OfqVuPYcqL4NOuoWOGDXQE8d6/WOrUkfX2xXlMkXm252nLFRhX57hHKDGzc9hbOUaBaa2zdNuu1XDpdYQPBGn1HjjKzw1JiuW+lgBVOrzLuIIE7keibVaY99GmDOsaO+sSz3UoAYI6iqsxJG+i16OeOIJ3uo0SC0VrXaWt/p0csyvekNU84I0o6XW0HWHUldgPbkz9YU605Y+vJeTBMy/DtpC8tVgP4TrTEOU50soiPNRuEPP9i6VOnVDvhAYmO89JAvfaZZZorYscW4H5G2tASnUsmcz9GhylXJ8jb6z31e5HD0ckRRoXHdBm2f37uaa8mK+DIY8I11hrnaO1LnDsas/1sPunYyLlAzuUUtnRvuC01rWm8zTmkeKOWQOiNUzBCOd3qxG3O+wTbdiWYHU0+xwv7nzCR7zlYFkxhUChUgqsr6+Fbiox0wA5rZt47l8sderIetvBMu3BKx8MXUIiz5axZmwPQ6Tjo95XR3uQGWE+wyygKuTdiSZTh7dZBvvdzyZksHSM18Efelw4Ouh6ZEbY34qUV2JY1kNbpnwpZjBwtEY+1ocwhNCbYVsufrroS9kR2ZXwmCNtRR2C9SLbUU5Z4a6H2Vdgzp2H5QYpxPqaHeeiIvNjfUnG81WcCbHVyYP1XtLF50saHNGY1VhzEAbM/lbWTVv31ZG1Mg6rMRod1mZFwBlNGfN1SID2XI+YXendwZ2YG8OYg3qIyUqZbxSZ/ZWUFSWvfc5Q89v+konakJovo46a89D+Gmpvo1aGebnMw9/KHaesZRWa3ajGBVJs+mkCtB671JXkOFx/EOP9i6VOnVBvOyQ5YRJUmu1yYyYD5j7VYAVOFbXx4drmfXU8Tx01OUJHtllO7DbHVlQxXwfHcW1+AHbA9fARxwd+d1Bi0PYg31xivGiOhqGW6H1e84BAaENibnCr/qUwFGEFF3QEi7EmE27v2LVSrLFz2VgRUJEsu0jTdS3B3X5Cp1zx3r9Y6tSR9Q4Sh0slgbIj0R1mw7ADFKJaCY5Apljuay2wIEpZ8V7XDmuzHMyDFoEf8V6HWloHezjzhb5fiV6PTOJwhXcXJVbaxkUrxrqhTuyvFefXhPP/hUSIWjL78jAuiDDYk/KGfTnsL7+OcD+ZCK96I2+7MEowQGwKOBz2/HdO+bpkaq8wvvl4718sdYq53jEQoPMUfj1EvPZeCkbqLHIJE6DgsLhsa9T+iIjlvi7ETCYQIW+80YmJtFlAK2Vi77PD9Z1yxHsdFmJFKUdax2ye4//2XI8sxJ3YinlYoaWtLr7x/bb6GjFfKxGnajINehGwKkzyMqA4kuVjzPYCrECJFg+c+epZrNse1e4L+dsCZS2oZ8/OkNOB/TGlWC9DtP61SB26RThePKNgt6swC4PGif2SRboW9hpMzYokgfsXS51iqneMxNt3F/PHgONjJPTZW4R1X31d9XFhsM+VqOWZCRGV8hBaX5sVhHfZ2bNE2Ne9Wa627qvjeaoMLdccG+8UUXG3WQ7CTdu0Cssb4zwmruvg6BssCX1nHe8YJm97rkc2cSyvo7Q1uCxlUUoV2jfOKIgFtHyoyyM9DI5OzyqsgIxWodImzwKTbr8wpbG47szNLcF6CX3m73IdYXoYc8wicz5nhGTo11Qm1pdMaYxjOmLGyFyutZ4XId1uGOs57Hqwp3NqMX2OqctiIKGghxiuRSYtLYuKkDDgmO5fLHWKp95x1K8Oa02pcNP5hNYdLBdOVQwfQM41qwIcDjaqwIpQsxVuGdYXf7mpk91XUaW1tpfMCJsWY/0i3b+IH4Ahx0c8vzo8FZNddgDrPix1nHsejsZSW9M92ccFaBnCHtN9Nc+TXX/7I7IinvufaJtlKxKttVKHp5Kyn+lIU2rFdB0cLsiIz064OiZyPZRS24mjTUh5JSYIyYoya8DFopQEwanE3JYlUczHYInWOqfNzIbu4k4UhGSklMNjdwShO1BAnIO4RYkJgkcxLpdAB/QZCkKykBfv2DJRYoLgbRIJChGEpMO4Q+N+1kWJCYKHMdZYbaThGIKglLLXPSw1v+vs/tRkwQTpFCQyw4cEdghCEmDCqovdnHtSEDqL9jzfosQEIUlQSi2KNvxCEJIRM4ygNtHJyUWJCYIgCEmL9ImlKGbi0u1KKe3Ytht/uXPTIZuEdHsU0/fRlTNpCILnESWWomitK8xM2/YMFQFtLYCXFbIpYDCH1wVLtoUUuxPygeFRzAdGqVJqkdnamvswWln55vhK86FZLsMsItMd1hPr7sw0fyNOP2U6U4tMBJwEDniXsOu3Ce5ilFUljjlKjcVco5QKO21YlLIWYX1wFoXsq1FKFUufaGvEEkt97C+4WCbUDCTauSp0CaLAvEk5IZF15n973smYMNZWMHSOQ6O4yggz8a4gSqw7YI8varXkQhikkfQoKsIipIK7mPuSHW5yXbPPH8cYv6Io46TscV9uLirrSUSJpTCOr7ZWi3NGQKww75LH4VXBBe+wgOgff/ayS7GQZ/rAWgXvODwkYomFIEostbG/AFs1fipk2XWQ2dI9jvSHeZM8on/8BYiyGnIIQaKvIdeZq30nLRLYkdrYa36F6w9bELq2luAtjBvKj7UGk9908M/E6n8Rq9kb2OuYRSOmKEWtdY5SyhfuY8VYZz7EGm+FWGKpjf0F2OLBNxNtJhT+K3Q+Jly7HMD0kVRxeEHHhVir5Uq4vTdoyzIKEt+q25GsbXuF+biWKekOiBJLUUzYr/3yrFJK1diDm7FeBPmiiwEzXid0QHg826I4z5ePFdG20LGabnN/mD0cAiiXgc+R6cL71lX3oAhraqYOXak9FRAllrrY/WEVWuscs9mDm6uILeQeOGwZdMfwXq11kdZatWOLeVyPub62AnN+kYf2h9mRpvOJgty3rrlvnY1RqH5iDxDpVogSS12i9YcFiC3k3rboSoD8eAZtehXTsHu1US/H+ugIvc4tXEwOhRZxCXe5b6mBqfNiYK70g4ZHlFjqYr/w4dyGwVgj3cyLUwokfUNoKOWwleoZHP2US0L2txof5mjMI95DuW9dRlvvkS+GPGEx7uJlWAosVe5jhyNKLAUxD78fWowvaSaBUPp5pE4fmlfHW80DCNNYhZPXDthpa/Cz3LfOp5roQVKZJD7+0p4JRBRYFESJpSYRx4clSD5x9KF5FduCiXMuu64MEAj3xR5ufFgxYaYnCoPct86/b5VEj1D0k8B7qKxFIkscwT32fgnmCUHGiaUm0frD2sS8wEGg3uzy2y+TiZ6zvzwrsBSmD5rneHOWY+cNYI1vWm43RCYtE8iipasoxzn5aQzl2GOp5jnHvZn9xVrreaYRXGDOEVDW0u11sSyFbmQpaitfB1BLDO4yx7Vo1ckv9+0wXXjfqrDmNGw1vssxtiuu99Dcx8pQBWZYjPURI9horWVLsQ2oATTWnG7xHlsJFDp+lwI15n8fVoPiN+UXOvKUhpRTCiwK2bcd8Jn/881fHXK+Eqwv0DbLMdsis68OyAs5riRM3RZFq7+L9yw79J6ZffmO335T91Z1kPvm+vtWGGZ/oX0PQvbbQTf+MGn5znseJr0kUTlTdXNdANk6+IYebqh0Ase2eumcjYrd2JgXrS5KOWHTTSOWbxpnn/m7PSTPIkfj21Y5tjz+0PqGNo5mX0KKvQvvXYmR21YYi5zKw6S1auDkvrl+37Kd983s85l9reQ290ZjWVvhyimNsFWKEmu9iTsxRTCulkIcgy/NwOYAUKVDXD0RKKG1q2I+MBdAH3ZvtBUwsIyQKDuDD6svx+lSCi1nJoc7wtsqxz42H8d6aY7AlmrHvrj7VboarXWxUqoSWKaUCmBZT0NMuPwbONarCkHum4torWuVUvOw3Ip1ZncWlqs0XFBHJdb9CV2mpRyr/oVRThfz0i7dBre1qGze2DhswflD94XJG9YiiFSO2Z9v9ju/VssJccNg3GVxltNCHpMn1DKx+xlcv9Zx3JPSGPLIfZOtW28SnSjY2J38zi/HPMw4I/P17fxajhpxpVt/gRYBS3VLS6LFF719Du0INIixHD8tx0PNw3zNq8NrOc3D0cGuYl/jyRWM9VUTQ1a5b0K3RpSYADS7a4J2CK/5W8Rh144d2ZaHNYdb2AGcpvEKmEYYU9Yik1bs2Oec29GmBBN1F2s5Duodcs/HWs7dDiIAK6LOjtRbpMNHfnmJPGKYVUXum9DdUVprt2UQPIL5yp2H5eapx/pKLsHqjynTWgdN39s2HWVuOdOAFWNZEnY/SFlInkJafmVn4QjBjrUcky8fq0/GljuI1ajW2PlD8lSFsRQ8hVKqVMfWjyn3TejWiBITXEFZS428Ea1R7c7Eo8S6ErlvgtcQd6LgFl6dRsgreHUVZ7lvgqcQS0zocoy7qU5by8IISYLcN8GLiCUmdCmm/6bc/F/isjhCjMh9E7yKWGKCIAhC0iKWmCAIgpC0iBITBEEQkhZRYoIgCELSIkpMEARBSFpEiQmCIAhJiygxQRAEIWmR9cQET2AmiV2AtTAgWDNWOCfAzcSahy+AtTBgXLNGmHWpirBmnAhyeK6+Urssx2zs9u86Dk+gG+Dwell+x35bDls+e3+RY/6/tupm71uuta6gA4i3vmbS3VV2HWRAs5A0uL0WjGyyOTcOr3obdiVfDq9+Wx5jeT6sQbrRVkW2V0CuoeX6VtuBRWGOsdfMaiWDSauLcFzEupnjKs2xrZatj+P6tae+PltGt58D2WSLdRN3ouA1giF/W6C1rtVaZwE+pVTU9baMdVGDtRBjlg5j5WjLWlpuVlTODkmu1+Enuq2PdE5tzbJeDAwJkxyxblrrgNZ6nkmrtJdWiYf21ldby7RUhh4jCF5GlJiQrBQA2W1MgbQKwCiHiGhrGRHn4oy2QkjItWeUR9xKyLAEyypbnMCxCddXEJIVUWJCUmKshqXAIucCjDamH8ruF4qlvGJaWkiZWC65REl0FnpbucS1gnEH1FcQkhJRYkIyU2r+hlsxuAQI6PgCQJx57SCSRNnWjmMTob31FYSkRJSYkLSY/qcg1rL2zZjVgCH+Rrq5P8j0vbWnkW+1knGM2P1UpVFzOeiI+gpCsiIh9kKyE8DqG/Prw0vX231C8TbSKzpKKOPuTITFQIUJwIiVTqmvCcG3lWoW1lpiYVd0VkoVcrgfMAuoDA0sCQnjz9RaD47zHIuwAma2mbzldlnmt087VsM2yn0mllvYBwwxblQhhRAlJiQ7AaxG0E/LcVwQZ59POxRPu3GM6yqNU4FBJ9TXMYZsqWPfdqVUMFQ+o1zKnOUppWqUUgu01gXO8ymlCrDcv4VxnqMcK1q0yPz2AeuAYq11mfmd68hfgjVUocCxr1ApVWeiW4UUQdyJQrJjh7s7owEzQ9K8RpFSapHZSsyg6mVY47cScUN2Rn2zw7hTVxASOGKURwkhLl1gIZBvlHMzxlq2LcZYz5EN5ONwsRqFucKcG611ULccxL3IyOA8t3PwuZAiiBITkh27AXdaFfUhaV6jVGu91GzFxjIoBeqMWy5eOqO+4ULwnTOYOAk37s0+Pjc0LYFz2JGaoYE2NVjjBUPzlwBVESzNCmKM4BSSA3EnCsmObYE5G7hASJrncbjESpVS1Q4lEAudUd/QabHCYhTF4ChZoskU0zliKDvUAs0mcmBNJEUsJCliiQnJjh+a3VQ2trsq6qDfUJRS2XY/jUvYgRDxWgodXt9E+geVUn6Hm7RNizKOc9jXJdSqmwnUhvTF2e7LTNMH1mLDCgCx54oUUgCxxISkxTREfkJm1tBaVyilIM4Bw1iNZIdFKCaAbVFEc8G1wu36GsWxDMuyKrE/KJRSMQ8TiIbWOqCUWorlJswxZfux6psT4bDKBPsXhSRDLDEhmbG/9peESSsG/HFaVlluRig6zp2Iu8uV+hoFVoPVz1cUYhF3JHXAQtvSwwr0GBd6PocbViytboIoMSGZscdUteo/MmHbtcQ4aNg0xm90rHgJE3cD7GJ9S4BWIfFhzpcfLb2NY31YY8BqHQExS6Mo4FqspW8ilSd9YimEKDEhKbHHDRESRh3CXJM36iBg00gWhZv13QVsV1xzaHoc1pUb9c0lTICGQ357Nv/2Rk7OjCPvQswA+AjpEp2YQogSE7yGL+RvC0wwQo1Jz4nmDjNpdh9KXThrwOxbRvj5FyNhN8jxWkxR62aw5XAqrtAlYsLSQfXNNPnCyTiE1rKvAHLD5M/D6qu0FUmrcXyxnsPUK9sEZ2SbzR8pOMNY5kWEWdLGBHcsD3eckJworbXbMghCrKsfZ2JZX6XxWhERVjomnrKM9eenpVKx5SyP5FKLUrewfUimoS3icGNbEW9fU7z1NY19OZZlZU9+XKW1LjIWTblD/oApZ6mjfvNoOffkUsdxASw3Z3U7zpFv0sNRRZhr6bgGYMaUkcC1FLyNKDFBEDyNmUJqG62ntrKnmirCCvQY7GZgjuAOosQEQfAsxgJbrLWOFEpv56skijUspC7SJyYIgteJdU5Ir86VKXQiosQEQfAspv8uGG3SXpMW9Eh0qdDFiDsxRoYOHarHjh3rthiC0C3ZuXMn27dvp3fv3i32HzhwgMGDBzNw4ECXJBOiUVNT86XWelhnnkOmnYqRsWPHUl3d3vlKBUEQug9KqQ2dfQ5xJwqCIAhJiygxQRAEIWkRJSYIgiAkLZ7qEzNRRgGsWRGqoi0MaEb452NWlXWODzGDI+uw1g4qjTArQh7WpKIS0SQIgpCkeEaJmSl9ltiKywxeDLvIn1FgJVrrAvO7xl4N1xxX7CinhvBrDpUQ44zfgiAIgjfxkjsxL8TyCkSZvbuUlgporlFgfiA3pJz60HLMb5k/TRAEIcnxhBKLoFSChLHEzHxpeVrrKnufY760bFqP2g/QehZwX5h8giAIQpLhCSVG+KUpthF+hVs/1gj+PKVUvlnp1ba0goRftyjL/kcplS/9YIIgCDGw7kV43dvTUXqlTyyeBfNsxVZvW2OmT6wAa6mHUIXoxwR/GCtOZrkWBEFoi7rn4e9XQOY4yL4aevVxW6KweMUSC+faGxJmH1hKyBfaf4a1nlAQWGpbZqaPLMhhV+V8pxuyLcwifNVKqeqtW7fGepggCEJy83EV/G0BDMmCax73rAID7yixIOFdiuGCLwK0tqbssHy01sWAzyzhYJdZZxRaXPNGaa3LtNa5WuvcYcM6dfovQRAEb/Cfp+ChK2DYREuB9RvqtkRR8YQ7UWtdpZQKdSn6CRMCr7UOhFmW3F4l1s7T3OdllNcKrBVu/Y7+s1wgUymFrEEkCIIAfPA4lF8LR06Drz0MGYPdlqhNPKHEDFVKqWyHm9Dv6PPKBnCkLVVKOSMUcwF7zNh2YJzWOmisseXGzdgimEMpNROoFAUmCIIAvPswrLweRuXAVRXQZ5DbEsWEl5TYQmCxsZxmmt82C7CsrSKwXIZKqRKTNwtY6JiVoxjIsy07rfXS0BOZmUFsy6xeohUFQejWvL0CHimCo0+EK1dA7wFuSxQzsp5YjOTm5mpZikUQhJRjzd/gH9+EsafCV5dDer8OK1opVaO1zu2wAsPglcAOQRAEoaupuc9SYP7Z8NUVHarAugpRYoIgCN2R1cvg8e/AhHlwxUOQ3rdVlo++2MUTb292QbjY8VKfmCAIcTJ79mwAXnjhBVflEJKM1+6Cp2+BiedCwX2Q1rtVlodrN3LrI++S2S+dvMlH0KdXz66XMwZEiQmCIHQnXr4TKn8Iky+Ay+6BtPQWyfsbGvnJ4+/z99WfMGtcJr+7YoZnFRiIEhMEQeg+/PtX8NzP4LhL4dIy6NmrRfKGbXv45oO1vLd5JzfMzuJ/5x1DWk9v9zqJEhOEtmhsgHdXQo80mHhOUnZ+C92cxgZ45vuwugymzoeL74KeLZv/Z977nJvL36KHUvz5mlzmTj7CJWHjQ5SYIESiqQneexie+zlsX2ft69UPJp0HUwsga06rL1lB8By7t0L5NbDhZTjpJpj3U+hx2D3Y0NjE0qc/ZNmL65h21CD+8NVsjs5sHeThVUSJCUI46p6Dqv+Dz96CI6YcDj9+pxze+we8swL6DoHjLrEU2lEnQA9vu12EbsjmN+Ghq2Dvl3DpMpg2v0Xy5zv2c9PfaqnesJ2rTxrDredNpnead/u/wiFKTBCcbKq1lNe6f4FvNFxSBlML2N+o6aEU6WNPhXOWwtpVlkJ780F4424YNBqm5lsK7Yhj3a6FIMBbD1kh9P2GwdefgZHHt0h+8eOtfOehNexvaOTOK2Zw4fSRLgnaPkSJCQLAtjpY9VN4/x+WhXV2CeRexwHSuPfFdfz+ubUoBedOGcFFx49k1jHn0HPSuXBgF3z4T0uhvXwHvHQ7DD8OphXAlMssRSgIXUnjIaj8Ebz2Bxh7mhVC75iJvrFJ87vnPuaOVR8zYXh//nhlDuOH93dP3nYiSkzo3uz6HP5VAjX3Q1ofOKMYTroJ3XsAT737OUue+oBP6/cxd9JwBvXtxRNvb2Z59accMbA3F0wbycUzRnHctAWo6ZdbfQ/vPWIptKr/s5TixX+C6QvcrqXQXdizDSquhXX/hln/BV/5eYt+2227D/Dfy9fw4sdfcumMUfz8kin0TU9uNZDc0gtCouzfYVlOr90FjQdh5jfg9O9B/+G8s3EHP3viNVavr2fSkQP46zdmceoE60t238WNrPrwCx5ds5n7X13P3S+twz+sHxdNH8VFx49k7KxCmFUI9evg0ZvgsW/BkPFwVI679RVSn8/ehoeuhN1fwEV/hBlXtkh+4T9buGXlO9TvPciSS6dy+cyjUUq5JGzHIRMAx4hMAJxCfPAEPHYT7NsOU/LhzFsh088XO/ez9On/8PCbG8nsm87/fmUiC2YeTc8e4V/0HXsbePLdz3h0zSZeX1eP1jD9aB8XTR/J+dNHMLzHHlg22wpvLnwBBhzZ4VWRGTsEAN6psD6aMgbD5X+1llMxfLZjHz974n2efOdz/MP6ceflM5gyqmuWWemKCYBFicWIKLEUYd2/4a+XWRGHF/wWRkxn38FGlr0Y4K50ny/XAAAgAElEQVQX6mhs0lx36lhunDOegX1iD5//bMc+Hn9rM4+u2cx7m3fSQ8Ep44dy22k9ObL8AjjiOLj2n2Gn92kPosS6OU2Nluv6lTth9Ekw/y/QfzgAhxqbuO+V9fym8iMONWm+PXcC1582rkujD7tCiYk7Ueg+fP6O5W7JzIKvPUxTbx+PvbmJkqc/5LMd+zl36pHccvZkRg+Jf4zMiEEZFJ6eReHpWazdsstyN76ynisf781j5/6Bfo9eB0/8D1z0e0gBF47gAfbWQ8XXIfA85H4Dzv5l8xRSNRvqufWRd/nw812cOWk4P7nwuKQa+xUPosSE7kHwE/hrPqT3h6sqqNkCP3viFdZ8GmTKqIH8dsHxzPIP6ZBTjR8+gP/9ykROHT+Uq/78Ote/MZK/nvo9er50G4yYBrOKOuQ8QjdmY7WlwHZuhgvuhJxrANi+5yC/fOpDlld/yohBffjTVTmcddwRKdH3FQlRYkLqs7feciEe2gfXPc3tq/dy56q3GD6gN78qmM6lM0bRI0K/V3uY5R/C0vxpfHf5W9ziO4+lE99FPb0Yhk+Gcad3+PmEbkBTozWM4/klMHAkXPckHH0CTU2aipqNLHnqA3btP0TR6X6+PXcC/XqnfhOf+jUUujcN++Dvl8P2DfC1R7i/ri93rnqPy7KP4qcXHdfpL/klM45iw7a9/LbqY8afWUzRkACsuAYKn4fBYzv13EKKEfwUHimypo+achmcdztk+Pjw85384JF3qd6wnZljB/Pzi6cy8cgBbkvbZYgSE1KXxkNQ8Q34dDXMv58nd/n5v8drmXfsESzNnxYx6rCj+c7cCWzYtpclz20i6/zfkPfSFVbf3Nefgd7JO8hU6ELefRie+G/LErv4TzD9cvYcbOS3/3yfe15ez8A+aSzNn0Z+9lGd4lXwMqLEhNREa3jyZvjPP+Gc23i9z6n89z2ryR49mN9dMaPLFBiAUopfXjaVTcF9fPOpIE+cewfHVH0d/nGDFU2Wwv0VQjs5sBueKoY1Jmz+srtp9I1jZc1Gbn/2Iz7fuZ8rTjiaRWdNYnC/9LbLS0FkxlIhNfn3bVBzL5z6XT4ccznX/6Wa0Zl9+fM1ua4s8Nc7rSelV+UwanAGC1b1o/7kH8AHj1nrOwlCODbVQOlpsOZBOO1m9HVP8/zW/px7x4ssqnibIwb1YeUNJ7Pk0mndVoGBKDEhFan9Czz/C5h+BZtzFnHtPW/QN70n93/9BHx93XvZB/dL595rZwKQvyabg8cWwPM/t+ZeFASbpkZ48Xb481fg0AG49gnemfhtrry3luvufYP9hxr5w1ez+cc3TyZnzGC3pXUdT7kTlVKLgADgB6q01rVR8vqBfCAIoLUuc6SVAHVAFlCqtQ6Y/T6g0GSbCSyJdg4hCfnoGXj8vyFrLsG8X3PNsjfYc+AQ5TecxChfhtvSMXZoP8quzuXKZa9z3YCreGDEWno8XAjXV1lRi0L3ZscmK3hj/Ytw7MVsPHUJt/37Cx5d8xKZ/dL5yYXHccUJo0lPE/vDxjNKTClVjkOpKKUqgXkR8vqBEq11gfldo5Sq1lrXmuOKHeXUAPYcLCVa6yJHGTVKqRxbyQlJzsZqK/LvyKnsv/ReFj7wFhu27eX+r5/ApCMHui1dMzPHZnJbwTS+89Aafjbl+/xo542ov18BC5+Dvpluiye4xfuPwmPfhsYG9px9B7dvnckDf3iLHj3gpjnjKTrDz4A4ZpHpLnhJneeFWEUBpVRehLylZrOZaxSYH8gNKadeKZVn0ursnUZxBbCsOSHZ+XItPFgAA46g8YoVfHvlx1Rv2M5vFhzPSVkdM4i5I7no+FH877xjuPfdBsqzlsCOjdbg1cZDbosmdDUba2DF1bDiapoGj+Pv2Q9y4tMjuPeV9VwyYxQv3DyHm8+aKAosAp6wxIyyCrWGgliWWFVIXh+Wwmu20rTWQfNvNlAfUk7Asb8EWBqS7r0WToiPXV/AXy8BpdBXruTHz23h2fe/4McXHMt500a4LV1EbjpzPOu37WXR6o2MP/GHZK/5EVT9GM76hduiCZ1NU6PVF/rqH+DT19C9B/LBMTfwX+tn88m6vZw5aTjFZ0/qVuO9EsUTSgzwhdm3DavfKhQ/EDSKz2d+12qtq7AUXzh/TJbWeqlSKnQ9jGygOHGxBdc5sBsezIc9X8I1T/CHtzV/fe0Tis7wc90p49yWLipKKZZcOpVNwb1c/sYxvDjlaxzx6u+tiVwnn++2eEJncGCXtRr4a3+E4Aa0bwwfZ/+A4rqpvPl2I9OO6s8vF0zi5KyhbZclAN5RYvF0BPjN33qjuOw+sQKgmtYK0c/h4I9mN6NSqhAreKSKCJg8hQCjR8sKvZ5Da3j82/DFu3DFclZ8NpxfPfs2l84YRfFZk9yWLibS03pQelUul9z1Mud/dA4vDVtD70dvtJaSH3SU2+IJHcWOjfB6qbX46oEdcPSJrJ1xC99/72hWv7KTMUN6c+cVEzl/6ohuN1i5vXilTyzUBQiR3XxBwBfafwYUGbfiUrsvzfSDBQlxVRqXZIHTJRkOrXWZ1jpXa507bNiwGKsidBmrl8G7K2HOrTzXNJ3Fj7zDaROGUpI/LakagkF9e3HftSfQ2COdr+++Ad3YAA8XWi4nIbnZVGP1df52muU6HD+XdRc/xtd7/py8pwaxfvsBfnHJFKr+5wwunD4yqZ5br+AVJRYkvEsxXNRgwOQP3ecH0FoXAz6lVL6jzLqQ/CVAQcLSCu6zsRqe+T4cczZvjrmOGx98k2NHDOSuq3Lo1dMrj3XsjB7Sl7Kv5fD6jsHcM+gma368f9/mtlhCIuzfAR88DvecDcvOhI8r4cQb2Hj1q3yn8ducuXw31evrueWcSfzre3O4ctaYpHxmvYIn3Ila6yqlVKhL0U/LCEQ7b8BYUk58OBSe1rrC/t9YYyscvxdhhdoHze9sGSuWZOzZZoXSDxzBxjNu5xv31DJsQG/uuXYm/ZN41u7csZncet5kfvK45tTR5zLxXyXWbPdjTnZbNCEc+3fAlg9hq2Pb8iHs2myl+0bDWUvYOr6AO17+nIeW1ZHWU3HDGVkUnZ7FoL4SbdgReOmNrwpRKH5Hn1c2tOjTWqqUynP0Z+ViLCul1HZgnNY6aKyx5Q6FlQ/UYoXd20EhuWafkAw0NcLDC2HPFvZf/STXl1urMd//9RMYNqBjV012g2tPHsubnwTJf/syVg99j4yVC+G/XpTxY26yfyds+QC2fgBb/2P+/89hZQWQlgHDjrE+OoZNhCOnsWPkKZS+uIF77qzmUKPmihNG860zxzN8YB/36pKCeEmJLQQWG8tppvltswDL2ioCy2WolCoxebOAhY4By8VAnm3Zaa2XQrNFVh7mvFH7xQSP8e9fQd0q9Hm/ofjVNP7zxS7uvXYm44b2c1uyDsGOWLz4s50s3H0DD/BD1GPfggV/lYmCu5LdW+HDJ6wByOv+Ddr0T/bqC0OPAf8ZlrIaNtn66xsDPSyX4ObgPh5ds5k//e1Fdu5v4KLpI/nuvGMYMyQ1nlGv4RklZqwlO9y9IiStVRh8uH1mf1mE/QFAWoFkZu0qeGEJTLuce/fP5tE1H/C9syYye+JwtyXrUPr1TuOuq3K46Pf7uG/gNVz34d1Q/WeYeb3boqU2u76wJmV+/1GrT1I3QaYfTvk2HH0iDJ8Eg0Y3KysArTXrvtzD6uqNrF5fz+p19Wzcvg+AuZOGc/NZE5k8wjuzxaQinlFighCVHRth5fUwbBKrj/sBv7j/Hb5y7BHccEaW25J1CuOH9+e2gunc+GADpw57lwlPf98aP3bEcW6Lllrs3GwFYbz/KGx4BdCWpXXazXDsRdb1dljAjU2a/2zeyep124zS2s6Xuw8AMLR/OjPHZvKNU8dxctZQGajcRYgSE7zPoYNQfi00NrDl3GV888EPGTOkL7+ePz2lQ5LPnTqC60/L4ooXr+GlQR/Tp/w6KHwB0vu6LVpyE/z0sMX16evWvuHHwuzFluIafniM4Y69DXy0ZRc1G7azel091evr2bnfmhpslC+D0yYM5YRxmZwwLhP/0H4ocfl2OaLEBO9T+UPY+AYNl91L4ZM72XewkYcKT+wWc8ktOnsSb326gxs3/Rd/PvALeGYxXHCH22IlF42HYOMbsLbK2j5bY+0/ciqc+QP05AvZ0nsMa7fsZu3a3ax95V0+3rKLtVv2NFtZAP5h/Thv2ghOGJfJzLGZHDVYPia8gCgxwdu8uxJe/xOc+E1+9PEE1nz6CX+6Kpvxw7uHq6ZXzx78/qszOO93e/gbl/LVmvvAPxuOu8RlyTzOjo1WH+raKgj8y5olQ/Xk4MhcNkz9X97oexq1uzNZ+85u6latZ9eBtc2HDuidRtbw/syZOIzxw/szfnh/ph3lS4no11RElJjgXbZ+ZC1NcdQJrPBdz99f+JAbZmdx9hTvTurbGQwf2Ic/fDWbry3by0kD3mfsY99GjcqxxiEJFg374ZNXDiuurR8CoAeO4ssxZ/MK2Ty4ZQyr65rM1Ad7GTagkfHD+nPxjFHNymr88P4MH9Bb3IJJhCgxwZsc3GMtT5HWm/dOvZMfPPARp00Yys1fmei2ZK5wwrhMvnfOFK5+soiqvj+k98rr4don3RbLPRob4Iv3rD6ttVWw7kU4tA96pnPwqBP58NgLeGLvsazY0I/glkP07KHIGe1j0dnDmDUuk/HDBzAoI/Xd0d0BUWKC99DaWp1564fsKFjB9Y9sZvjA3tx5+Qx6pnAgR1t849Rx1H6ynUXvX8sdn/7eGm7QHWhqhC8/gs1vwqZa2FwLn78LjVZ/lc7M4stjFvAy0/nbljG88dF+tIah/XuTd+wwZk8cxmnjh8kMGSmKKDHBe1TfA++soPGM71P08gDq9wRZecPJDO6X7rZkrqKUYmn+dC78/S4e2/0eF7z4a473ZbEm2Mn9g7u3wsbV0HTIGjuldYu/TU2N7D7QQHDPAYJ7DrBjr7Ud0D0hvT/0HkiPPv3pkTGQnn0G0itjIL37DSSjd2/6pqeRkd6Tvuk9yejVk4bGRg59uQ4215L2+Rr6bHmLvvXvkXZoDwANPfvy5YBJfD6igE8zJvFW03j+sT6NbZsPohQcf3Qfvps3mjkTh3PcyIEpHb0qWIgSE7zFplp4+hYYP48lu8/jtcAGbp8/nSmjBrktmSfo3zuN0qtyuPwP15LT62NunbyB66s7YdmZpiYIPA+196M/fBLV1BAxaw9goNni6aXbp9PZTQa7dR/qyeAAvchSmxmuLIV1QPfifT2Gt5pO4e0mP29rPwE9kqY91mDjnj0Ug/v24rQJQ5gzaTinTRhGZjf/0OmOKK212zIkBbm5ubq6utptMVKbfdvhT6cDmqdOWc4ND6/nmpPG8JOLprgtmed47K3N/OmhR/hH+g/Z3aDInHUFHHOWFbnYux2W2c7N1qKNb/4Fgp+wL20Qj+rTWbE3h730oQkFSjG4Xx+GDujDsIF9OWJgH4YPzGD4oL4MH5TBkYP6MjCjF6qxAQ7somn/Lg7u3cnBvTto2LuTQ/t20Lh/F3r/TvSB3XBwN+rALnoc2svufmPYlTmFvcOmc2jIJNJ79yajV08yjKWW0asnfcz/MvO791FK1WitczvzHGKJCd5Aa3j0Jti1mXUXPsz/rPyUmWMHc+t5x7otmSe5cPpIajfM5trXirnmwAry3v0HPd98AHqmw5hTLIU24SswJIYZTRoPwdpKqLkf/fEzKN3EO+nHU3bwQp49MJMTxo9g/tQRjBvaj5G+DI4c1CcuBdID6GM2QehoxBKLEbHEOpnX/gRPF7Nvzk85e/U09h1s5IlvnSozfkfh4KEmpt30R/b7xpHGIc7oE2D+oPeZ1fAGvr3rrExDxsMxZ1sKbfRJkOZwt23fAG8+gH7zQdSuzezsmclDDafxYMMZ9ByaRX7OUVwyYxQjBmW4U0Eh6RFLTOgebKqFZ3+APuZsbgycxObglzxUeKIosDZIT+vBkR9WcCh9AP/zy7t4LTCWXwSO55P6izlafcF5vd/h/L3vMPm1Unq++nt0+gBU1hxLma2tRNc9D8CrPWZw/8EFrE6fyTkzRvObnKOYcbRPxkoJSYEoMcFd9u+Aiuug/xE8OGIxzz2ziZ9ceBw5Y2T9rFhJO7iLS7OP4tLsowDYFNzH64FtvBbI4ZuBer7cV88pPd7lLN5i7kcvM/iDx9jaYxgPHrqEisbZZE2YTH7OUdxx7BH06dXT5doIQnyIEhPcQ2t47FsQ/JR1F1Tw05WfkTd5OFefNMZtyZKaUb6MFkptc3Afr6+bxWt19fwu8CWN2zfQd+gYLj1jDCtnjOIIsXiFJEaUmOAe1X+G9x+lYc6PWfhCT3x9e7E0f7q4sTqYkb4MLplxFJfMsJTajn0NDOyTJtdZSAkkRlVwh8/ehqe/D+Pn8dP6uazdspvb5x8v43y6gEEZvUSBCSmDKDGh6zmwy1ofrG8mzx/7Ux54fSOFp/s5dcJQtyUTBCHJEHei0LXY8yJuX8e2/If57sqNTBk1sNtO7CsIQvsQS0zoWmr/Au9W0HTGYr71SgYHGpq44/IZpKfJoygIQvx4yhJTSi0CAoAfqNJa10bJ6wfygSCA1rrMkVaCtWpQFlCqtQ4kcg6hg/niPXhqEfhns4yLeaXuY0oum0rWsP5uSyYIQpLiGSWmlCoHlthKRSlVCcyLkNcPlGitC8zvGqVUtda61hxX7CinBsiJ9xxCB3Ngt9UP1mcQ75/4K26772POnXok83OPdlsyQRCSGC/5cPJCrKKAUiovQt5Ss9nMNQrMD+SGlFPvKCeecwgdyZM3w5cfs++CP3HjY5sYPqA3Sy6ZJlFygiC0C08oMaNIAiG7g4SxkpRSPixlVGXv01oHzb/ZQH3IIQEgO55zCB3Mmw/CW3+HMxbx43eGsH7bHm5fcLwsUigIQrvxijvRF2bfNmBmmP1+IGiUks/8rjVKLQiEm68oi9YKLNo5hI5iy4eWFTb2NP6ZeTUrnnmLm+aM50T/ELclEwQhBfCKEotnojy/+VtvW2OmT6wAqKa1QvQTWblFRSlVCBQCjB4dz3J/AgAH91r9YL368lne71h893scf7SP7+RNcFsyQRBSBE+4E2ntAgSI9KkeBHyhfVtAkXErLrX7uUwfWdCkx3MOwIp41Frnaq1zhw0b1lYdhFCqfgxbP6DxkjK+88TnNGm48/IZspihIAgdhldakyDhXYrhXIABkz90nx9Aa10M+JRS+Y4y6+I8h9BePnsL3rgbZi7kj5+MZvX6en528XGMHtLXbckEQUghPOFO1FpXKaVC3X1+WkYg2nkDJrjDiQ+HMtJaV9j/G2tshdY6GOs5hHbS1AT/vBkyMlkz4SZ+e9/7XHT8yOYJaAVBEDoKr1hiAFVKqWzHb7+jzys7JG1pSGh8LkYZKaW220rOWGPLHdGLEc8hdCBv/R02rmbf7B/xrX8EGDGoDz+7eIrbUgmCkIJ4whIzLAQWG8tppvltswDL2ioCy2WolCoxebOAhY5ZOYqBPNvq0lovjfEcQkewLwiVP4KjZvKj9dPYHPyMFUUnMbCPhNMLgtDxeEaJGWup2PysCEkrDpO/1T6zvyzc/rbOIXQQz/8/2LuN6tPupvzRzdw0Zzw5Ywa7LZUgCCmKl9yJQrLz+TvwxjIOzriWb7/QxITh/fnW3PFuSyUIQgojSkzoGLSGJ78HfXwsPVjA5zv3szR/Gr3TerotmSAIKYwoMaFjeHs5fPIqa6ffzN01Qa4/zc+M0eJGFAShc/FMn5iQxOzfAc/+kMaR2XzjrUmMHdKD7+Yd47ZUgiB0A8QSE9rPC7+EPVu5Z+BNbNi+n5LLppGRLm5EQRA6H1FiQvv44j14vZStx1zO/3urD1efNIZZMrmvIAhdhCgxIXFMMIfuM5CFm89l5KAMFp09yW2pBEHoRogSExLn3ZWw4WWeObKQNV/25JeXTaV/b+lmFQSh6+hwJaaUGqiUGtvR5Qoe48AuePYH7B06lW/9ZxoLco/mtAky078gCF1LwkpMKfVLpdQzSqmblVIDzb5ngBrgFqXUclFmKcy/SmDXZyzefw2Z/fvw/fMmuy2RIAjdkPb4ft4ASrXW68BSalgT6javeKiUuhn4VftEFDzHlg/htbt494iLeHTDSP58zVQGZcjciIIgdD3tcScOthWYIR8oCcmzox3lC15Ea3jqezSm9ePrn57LxcePZO7kI9yWShCEbkp7lFizAlNKjQPGAdUheba1o3zBi7z3CKz7N2W9vkpT3yH8+ILj3JZIEIRuTHvciYMc/+cD67TWa0LyyIChVOLAbnjmVrb2n8RtX57C76+cwuB+6W5LJQhCN6Y9SmyH6fNSWG7EfLCiE4F5wC1AQbslFLzDv2+DXZu58dB/cdaUkZw7dYTbEgmC0M1JWIlprVcppQJAHpDl6B+zF7BcAWQD69srpOABvlyLfvUPPN8nj48OTubZi8SNKAiC+7RrZKpRXMtCdi8HMrXW69tTtuAxnr2VBpXOouBl/HjBsQwf0MdtiQRBEDptnFixjBNLIeqeh4+e5s6Gi5g2aQIXHz/KbYkEQRAAGScmtEVTIzxzK1+mjeAvB87l6YunoJRyWypBEARAxokJbVH7F9jyHj/cO59vnDGJkb4MtyUSBEFoRsaJCZHZvxP9/C94L+043ux3OgtPH+e2RIIgCC3w1DgxpdQiIAD4gSqtdW2UvH5z3iCA1rrM7PcBhWa/D6jVWle1lSaE4cVfo/Zs5ZYD32FRwST6pssM9YIgeAvPjBNTSpUDS2zFpZSqNOWEy+sHSrTWBeZ3jVKq2hxbqLVe6shbYtKCbaQJTravR7/2R57sMRs1aoYEcwiC4EkSdidqrVcBK7Gsmiyt9cMmaQGWJWWPE4uVvBDLK6CUyouQt9RsNnMdx4YqvjojT1tpgpPKH3NI9+Cne/P54fnH0qOHBHMIguA92rWemNZ6ndZ6GbBNKXWmUupMYLnW+jazPdxWGQBGWQVCdgcJY4kZl2Ce0w0YYkllKqWcASbzHAouWppgs+FVeP8f/OnQ+eROPY6ZYzPdlkgQBCEs7erkMK7DuzGuRINWSlVhue42xFiUL8y+bcDMMPv9QNAoPp/57ezbWgisMunLgWLHsdHSBICmJnj6FoJpw7h73/k8cc4ktyUSBEGISHsGOw8CKoBKLHdiD611D2ACsAqosAdBx0A8n/q2+69ea11h+rhKTD8ZxrJagaXgShz5o6aFQylVqJSqVkpVb926NQ4Rk5i3l8Nna/jJ3nyuOHUyR2f2dVsiQRCEiLTHnbgQKNBaL3OOF9NaB4xiWQAsjrGs+jD7IkU2BgFfaP8ZUASglCrFCvrIAsqASqVUdltp4dBal2mtc7XWucOGDYuxKknMwT3oVT9hbdoEXuwzhxvnZLktkSAIQlTao8R2aK0jDmbWWgdo3c8VCTvkPZRwxwdM/tB9fqOQ6sy50VoXYbkMi6KlxShj6vPynahdn1G856t896xJDOgjqzULguBt2qPEdAx5BrWdBUx/VqhL0Y/lqgzNG6C1wvNxeHxZqOIrc5QXKU3YsQn98h081/MUdg/PZUHu0W5LJAiC0CbtmnYqWp+XSRsaR3lVIa49v2OQcnZI2tKQ8PtcrJD7Kiw3ppO8GNKEVT+lsamRH+2dzw/On0xaz3YFrgqCIHQJ7YlOLMMK3rgLWKW13gnNyms+lptubhzlLQQWmwCNmea3jb1GWRGA1rrYDFT2A1nAQttNqJRaYsLo68yxAccA6ohp3ZpNNfD2Q9ynL+aYicdx2oRu0P8nCEJK0J5FMXcopYqwLJmVSimne7EWmG8rthjLC3I45L0iJK1VKHy4fWZ/rTl/XGndFq3h6e+zO20wv9t7ASvPney2RIIgCDHTEYtifsVMAGy7+2pDZrcXvMz7/4BPX+MXDddz8axJjB/e322JBEEQYqZDZnQ1SksUV7LRsB8qf8Qnvfw8RR7P5x3jtkSCIAhx0WHTkiulZmAFWGQBX2KFwdfHOvWU4AKv3wXBT7jl4Pe56ZyJDO6X7rZEgiAIcdFhSkxr/SbwJoBSai5QDgzsyHMIHcjuLeh//5pX007gs/6zuPqksW5LJAiCEDedEkdtZrjP7azyhQ7guZ+jG/Zx654FLD5nEulpcqsEQUg+Oq3lMiHvEgnoRbbVod98gIf0PI4cN4V5xx7htkSCIAgJ0dmuvlinnRK6khdv5xBp/ObABdx3/mSUkrXCBEFITmKyxMw6YYkQbmJfwU22b0C//RB/O3Qmc3KmcNzImGYGEwRB8CSxuhMTnSQ3lvkVha7kpd/QqBV3N13IdySkXhCEJCdWd2KOUuoSIOKs9RHIjTO/0Jns2Ih+868sP3QGc06YzihfhtsSCYIgtItYlZgfWJlA+WKJeYmX76CpqYll+iIemj3ebWkEQRDaTaxKrBYoiLNshbWKsuAFdn2OrrmfisbTmTMrhyMH9XFbIkEQhHYTqxKrSmQ+RKVUdbzHCJ3EK7+jqfEQd3MRD86WFZsFQUgNYgrs0FrfkkjhWuv/SuQ4oYPZvZWmN+7m0caTmH3iLIYPECtMEITUQKaE6g68+ns4dIBlXMYDZ4gVJghC6iBzDaU6e+tpWr2MJxpP5IyTT2Zo/95uSyQIgtBhiCWW6rx2Fz0a9vBndSn3nu53WxpBEIQORZRYKrMvSONrd/Fs40xOO/UMMmWpFUEQUgxxJ6Yyq8voeXAXf+5RwPWnjXNbGkEQhA5HLLFU5cAuDr3yB55vzObk0+fg6ytWmCAIqYenlJhSahHWzPd+rLFpEZdyUUr5gXysFaTRWpeZ/T6g0Oz3AbVa66q2jks53ribtANB/twzn9JTxQoTBCE18YwSU0qVA0tsxaWUqgTmRcjrB0q01gXmd41SqtocW6i1XurIW2LSgv/4GgQAABfZSURBVG0clzoc3MOhl+7kpcbpnDz7KwzK6OW2RIIgCJ2Cl/rE8kKUSUAplRchb6nZbOY6jg1VfHVYll1bx6UO1feStr+ee3oWcN0pY92WRhAEodPwhBIzyip0Ac0gYSwx4y7Mc7oItdZBR5ZMpVSJ4/c8rXVtDMelBg37aHjxt7zceByzzjiHAX3EChMEIXXxijvRF2bfNmBmmP1+IGgUn8/8dvZ7LQRWmfTlQHGMx6UGtQ/Qa99W7ut1I785eazb0giCIHQqXlFimXHktV2D9bYCMn1bBVrrgLG6VgB5QAnWDPyBto4LdyKlVCFWkAijR4+Ou1JdzqEDHPzXr3mzaRK5sy+gf2+v3F5B6Hp27tzJli1baGhocFuUlKRXr14MHz6cgQMHuiqHV1q5+jD7hkTIGwR8of1nWKtPFyulSrGCN4rM/5VKqZy2jgt3IhO5WAaQm5vr/bXR1jxI+t7P+UvaQm4TK0zoxuzcuZMvvviCUaNGkZGRgVLKbZFSCq01+/btY9OmTQCuKjJP9IlxOBw+lHAWUsDkD93nV0plA3W2ZaW1thVUUbTj2iG3d2hs4MDzv+LNpvHMmH0JfdO98n0iCF3Pli1bGDVqFH379hUF1gkopejbty+jRo1iy5YtrsriCSVm3HuhLkU/UBkmb4DWCs/HYYUUqvjKYjgu+XnrIXrv2cT9veZz1Ulj3ZZGEFyloaGBjIwMt8VIeTIyMlx313pCiRmqjCVl43f0XWWHpC0NCb/PxQqdrwIWhJSbx+Gw+kjHJTeNh9j3/FLeaRrL8XMK6NOrp9sSCYLriAXW+XjhGnvJ57QQWGwGJM80v20WYFlNRQBa62IziNkPZAELbReiUmqJCbGvM8cG7H6waMclM/rdCjJ2fcID6cX8dNYYt8URBEHoMjyjxMyYLTvAoiIkrVXgRbh9Zn8tVkRipPOEPS5p0Zq9z/2KT5uOZurcK8QKE4QUoKqqiqKiIvLy8igtTdxZ1FHleBkvuROFRPi4kn47PmZF+iXMn5kEwwAEQWiTvLw8iovb/73dUeV4Gc9YYkJi7Hzu1+zWmYw+42v0ThMrTBCE7oVYYsnMploGfv4aD/U4n/mzUmOkgCAIQjyIEktidj13Ozt1Bn1mXSfjwgQhhamoqGDw4MHk5OQQDFrDXQsKCsjKyqK2tpba2loqKiqoqKigqKioOU84ysrKqKqqas6b7EjLl6zUr6Nf3T+5R5/PFadNcVsaQfA8P3n8Pd7fvNOVcx87ciA/vuC4hI/Pz8+nvr6eyspKfD5ruGtRURG5ubn4fD6ysrIoLy8nOzub+vp6lixZQklJSatyysrK8Pv95OVZI40CgaQPzhZLLFnZ/a87OaQVO6Z/g8H9ZNVmQUh1CgsLqao6PF95IBBoVmiVlZVkZ1tDaXNzc6mtDR+g7ff7KSoqoqysjGAwSGFhYecL3smIJZaM7K0n/Z2/8VjTqVw+d5bb0ghCUtAeS8grzJ8/n7KyMubPn4/ff7gf3O/3U1FRQX19PcFgkPr6cNPRWtGKJSUllJaWUlRURGFhYdKH3oslloTse7mU9Kb91E24jlE+mVpHELoLRUVFlJaWUlVV1ewSBMjJycHv91NYWNhifyhVVVXk5+dTWVnJ9u3bCQQCSe9SFCWWbDTsQ68u5bnG47n4K5EfVkEQUo/s7GyCwWALxVNVVUUwGGx2J9ppwWCwlVuxsrKyeZ/P52s+JpkRd2KScbDmb/Rt2E71yFs588gBbosjCEInUFtbS3l5OYFAgIqKCvLz85vTiouLW1hbeXl5ZGdnNwdt2FtZWRl5eXktysnKymphfWVlZbVwSyYjSmvvL5PlBXJzc3V1dbW7QjQ1suNXx7N+dxoHr6ti5rhIS64J3YXZs2cD8MILL7gqh9f44IMPmDx5sttidAuiXWulVI3WOrczzy/uxCTi0Af/ZNDeT6gavEAUmCAIAuJOTCp2VP2aPU3DOH7e19wWRRAEwROIJZYk6A2vMmT7Gh7rewlzjh3ptjiCIAieQCyxJOHLZ39Fmu7PqDkL6dHD/YXoBEEQvIBYYsnAlx8zZNMqHkk7h/NyxrstjSAIgmcQJZYEbH321xzUafQ+5QbS0+SWCYIg2EiL6HV2b8H3cQWPq9lcfMp0t6URBEHwFKLEPE7987+jZ9Mhds0ool9v6cIUBEFwIkrMyxzYTZ8191JFLhfNPd1taQRBEDyHp5SYUmqRUirf/I06qZdSym/yFSqlCh37fY79i5RSYScYVErlKaXyw6V5hR2v3kvfxl1smHg9Q/r3dlscQRA8QlVVFVlZWe1e1LKjynETzygxpVQ5UKW1rtBaLwVar+h2OK8fKNFaL9ValwFFDqVXaO835cxTSvnCFFMCZHZ0PTqMxkPoV35PddNEzj77QrelEQTBQ+Tl5VFcXOyZctzEM0oMyNNaO6dcDkSyooBSs9nMdRw7LyRvHdBihktTrqfXH9j7ZgW+g5/z5tFf4+jMvm6LIwiC4Ek8ocQiKJUgrRUSxqrK01o3L3GqtQ46smQqpZxW3LwQ5QjgA8KvGucFtGb3C7dT1zSCU8+TKaYEQRAi4QklhqVUQtlGiAVl8ANBu08rTL/XQqBQKVWjlFoEtLCVlVL5WuuKDpO8Ezj48fMM3/0f/jX0ciaPDHdpBEFIZSoqKhg8eDA5OTkEg9Y3ekFBAVlZWa3WCIslf21tLRUVFVRUVFBUVNScJxxlZWVUVVU15/U6XonZjqdvylZs9bY1ZhRWgdY6oLWuVUqtAPKw+r1qMVaeseIi3z2PsPWZ20jXgzjunMK2MwuCkHLk5+dTX19PZWUlPp/1IVtUVERubm7z73jyZ2VlUV5eTnZ2NvX19SxZsoSSktZhB/aaZPZ6Zcmw6rNXlFg4116ktUaCgC+0/wwoAoqVUqVYQR9F5v9KpVSOyT/fBILEhIl6LAQYPXp0rIe1i0Ob32HUtlf4a/9ruHL8iC45pyB0C566BT5/x51zHzkVzvllXIcUFha2CLoIBAItFsOMJ39lZWXz4pe5ubmUl5eHLcPv91NUVERxcTHz58+nsND7H9JecScGCe9SDPcZEKC1NRUA/CZCsU5rHQDQWhdhuROLTERjXKtamgjHXK117rBhw+I5NGE+rbqLA7oXI+fegFIy0a8gdGfmz59PWVkZwWAwphWYI+X3+/1UVFQ0uwrr68OHBOTl5VFSUkJ5eTmDBw9OishFT1hiWusqpVSoS9FPywhEO28gTMi8D6PIaK34yrDcitlYis7+lMnFCgIhHuusU2nYz7B1j/Ji2izmHD/JbWkEIbWI0xLyAkVFRSxcuJDMzEzy89se1hopf05ODsuWLSM/P5/a2lqWL18e9viqqiry8/PJz88nGAxSUFBAIBCISYG6hSeUmKFKKZXtcBP6HX1e2QCOtKVKKWeEYi5QgOWWXAY4AzfygNLQCEWl1Eyg0jMKDNj0ejmj9G72T7mSnrLciiB0e7KzswkGgzH3TYXLX1VVRTAYJDvbGkprp4Urt7KykszMTLKzs/H5fM3HeBkvKbGFwGLj9ptpftsswLK2igC01sVKqRKTNwtYaLsQlVJLTIh9nTk2EEaBLcJSbn6lVL1XohX3vnYfG/VQTpl3qduiCILgEYqLi1v1hdXW1lJeXk4gEKCioqKF1RWaPy8vj+zs7OagDXsrKysjLy+vRTlZWVkEAoFm5ZaVleVpKwxAaa3dliEpyM3N1dXVcXWpxcWuz+vod1cOlcOv5awbf9tp5xFSi9mzZwPwwgsvuCqH1/jggw+YPHmy22J0C6Jda6VUjdY6tzPP75XAjm7P2met7r/RZ17vsiSCIAjJgygxD6CbGhm57mHeSj+eyZOnuC2OIAhC0iBKzAO899LjHKG3cnDqlW6LIgiCkFSIEvMA+16/lx30Z/q8r7otiiAIQlIhSsxlNn++mWm7X+LjI86lT0Y/t8URBEFIKkSJucwHT99Nb3WIo870/vQugiAIXkOUmIscONTIUetXsiF9AkdOnOm2OIIgCEmHKDEXefWl55jIeg5Ok4AOQRCERBAl5iL7Xr+Pg/Qia861bosiCIKQlIgSc4n3P/mCk/c+xydH5NGj32C3xREEQUhKRIm5xFuVf2WQ2suRsxe2nVn4/+3d329bZx3H8fezLYVJkDkuy0CITrIRUAQCxamEYBKgORcgDWmSy9Au0BhqLP4AagVxH8X/AMTccTOljSa4QFzEYRd0Y1ITI6aJiUmxBKOV0IhrmEi2Zu3DxXmOe3LiJD6Of5xz/HlJVurj48ePn9r+nue3yERpNBrMzMxQLpepVqssLCxgjKFSqVCpVFhYWKBQKAz0Nev1Ovl8PhHbrwTFaQHgifGfvQOe/MfL7J77FOc//+1xZ0dEYqbZbLK5udlZRT6Xy7G1tXVoN+ZyuXym16jVaoc2vSwWi1QqFXZ2dk54VvyoJjYGv7/xOl83b3LvK8/DQ/ovEJGjTtsG5Sw1sXa7nbhgdRz9go7Y/fuWD27+mvsYZp/60bizIyIx1Mv2J/Pz0RaHb7fbnb9XrqSnG0NBbMT++Pa/KN7d5N3Zb0DmM+POjojEUC+bUbZaLfL5PNVqlVqtRqFQoN1ud/q2/ObGRqNBoVDoBC5/k8xGo0G1WqVerx9Je319nfX1dcrlcs8bco6L+sRGrPHKy3zT7HKgWpjIyPn7r43aMPZ7KxaLlMtl1tbW2N7eJpvNHjruNxfOzc2xtLTE2toaAKVSiWazye7uLlevXj2Sbr1e7/S9tVotVldXD/XFxY1qYiP0TmuPz93+DXuPPMbUF58Zd3ZEJAX8psdSqUQmkwHo/D1LegDZbLbTDBlXqomN0Ms33uAnD21x90svwiMfGXd2RCZOGnfA7qX/LAq/RudrtVoDTX/QVBMbkfcP7vHBn1/inLnHx772wrizIyICeP1fSaYgNiK/+8ttnrm3yXvZL8MntXuziAxPLpc7VIO6efPmkcfj3kzYKwWxEXntRp2LD72jWpiI9KzZbFKtVlleXqbdblMul6nVaoA3AGNtbY319fXOMV+xWCSbzXZGGebzeRqNRqfWVSqVaLVa1Gq1Tv9Zo9FgdXWVer3O+vr6kftxZay1485DhzHmKtAEckDdWts44dwcUALaANbamjueARbd8QzQsNbWQ48BXAKWT3qNoPn5ebu1tdXP2+KNf7Z545cv8oNzN3jkp2/Do/13uooE+aPt0tjXcxZvvfUWFy9eHHc2JsJJZW2M2bbWRpvQFlFsBnYYY64TCCrGmA1g4Zhzc8CKtfayu79tjNlyz1201lYD5664x9ruOeVAGtvGmIK1dqgTIV569W/87OHXuH/xewpgIiIDFKfmxGKoVtQ0xhSPOXfV3XxPB54bDnw7QM4Frc46Ky5wNfFqc0Nz5393+fDN3/Jxs8+5+R8O86VERCZOLIKYC1bh2lCbLjUx1yRY9JsIAVwty5c1xgRn5i24AJcBus3YO993xntwbesdnuUV7k5fgCefGuZLiYhMnLg0J3ZrY9vF67cKywFtF/gy7n4jENSuAJvu8TWgAmCtbRhjwitmzvmPD8O9+5Y//Ol1yg//FQo/12K/IiIDFpcglj39lA5/Zl8rMGBj2xhz2VrbdMHqGlDEq3k1cLW8YHOlMWYRb/DI0YXDDp+zCHDhwoUo7weA994/4IVHX8W+bzBffT7y80VOowEdMuniUjXoNiX8uGa+NpAJ958B/oCNVbwBHHmgBmwYYw6tpumaJC9ba7sOHPFZa2vW2nlr7fzjjz/e41t5IPPRh/nOh69gPvs0PPbpyM8Xkf7FaeR1WsWhjOMSxPzh8GHdRg023fnhYzkXrHb80YZuJGIFF+ACVoDLZ8pxLz74L+S+BfM/HvpLicgDU1NT7O/vjzsbqbe/v8/U1NRY8xCLIOaa9MJNijlgo8u5TY4GvAwP5peFA9+hWYBuLtqKPxgkXEsbqEdn4NlfwBe+O7SXEJGjZmdnuXXrFnt7e7GoLaSNtZa9vT1u3brF7OzsWPMSlz4xgLoxZi7QTJgL9HnNwaE+raoxJjhCcR6vZtUCfgUEp5cXccPxjTElvD6ylmtSzLnn9jThWUSSYXp6GoDbt29zcHAw5tyk09TUFE888USnrMclTkHsCrDk5nNdcvd9z+HVtsoA1tqKm8ScA/LAFb8J0Riz7IbY+3PC/MEeOeB6l9c9sV9MRJJpenp67D+wMnyxWnYqzs6y7JSIyCQaxbJTsegTExER6YeCmIiIJJaCmIiIJJaCmIiIJJaCmIiIJJZGJ/bIGPMu8Pc+n/4J4N8DzE7aqbyiUXlFo/KK5izl9aS1NvqafREoiI2A25RzqMNM00TlFY3KKxqVVzRxLy81J4qISGIpiImISGIpiI1G7fRTJEDlFY3KKxqVVzSxLi/1iYmISGLFaQHgRHFbuvjbv9RDm3R2O38F2AjvJB01naQaRHm5nbYBruFt3VO21laGlGURSQAFsT4YY64Dy/4PsTFmg2NWwzfGFIE5oERof7Qo6STZoMoLbyeDFbytdZrHpZEGvQZ9t6WQH9wvESjnKOkk3SDKa5Iukvoorzbe9201dGE5/s+XtVa3iDfgTuj+KlA85Tkb4XP6SSeJtwGW1yJeIMuM+z0NubyuA3PBsjjh3NXAv3PAHby9+CKlk+TbAMvrKmDdbcc/nrZbxPJaCZWX9b9/cfl8aWBHRK6mEN492r9KGXk6cTfo92mtbVu3K3eKFe3hK9qmK8dD3B55/r55WG9PvSZeLbbndFJgUOXVBmaAGWtt3j2eRlE+F4v+Y4HyyPWRztAoiEWX6XJslwf/saNOJ+4G+j6NMYvGmJLbFHXubFmLn4hB329eDTuvi6Ro5eX/I+0XSX18LgrWNR+6iwB4EKxi8flSn1h02ZilE3eDfJ/1wNXgujFmxxhTSNmPznFB/1L4oPV2LC+EDs8BlSjpJNygygvo9Iu13PPXbPr6ECN9LkK10TJQsda2XV9Zz+kMk4JYdK0ux853OTaqdOJuYO+zS/NOG/g+MZ/HElGkoG8PD+JYxAv09cAghbQbSHm5Q5NwkRT5otLVwEp4rSfL/aYzLGpOjK5N96uZqO3ng0on7gbyPo0xOWPMnS5p5PvNWEz1FfTdlfFla63fnKOLpBN0Ka+TLpLSJHJ5WWub1toqXo1125VdbD5fCmIRuau28FVIjqPDwUeSTtwN+H2GhztnCHTUp0S/QX8FuDyAdJJmIOU1QRdJkcor2GzognwbWIqazjApiPWnHhpUkAt0fs5FGHBwbDopc+bycl+gzpfGfbly1to0NSX2FfTdXJ0Vv9nLGDOni6Ro5eUeSv1FUpTycoM3woEdvCH2sfl8qU+sP1eAJddWfMnd9z2H9+EvQ+cL8hxQBLLGmDVXNT8tnTQZVHnV3A8QeFfIqRppF1B3gcjvvzkU9OFB344xpgQ0gJYf2IF5d+zYdFLmzOVlra0Fax1pvUhyei2vJkcDey5wLBafL62dKBIz7gd0CbhJaJScW44rY60th+c9BSy4wR3HppMmAy4vf0BMHq+2lrbm157Ly933V9BpAwW8Cc3rp6UzSgpiIiKSWOoTExGRxFIQExGRxFIQExGRxFIQExGRxFIQExGRxFIQExGRxFIQExGRxNKKHSIp4FaiyALXUrbqusiJVBMTSTBjTMYYcx1vKaVrwOaYsyQyUgpiIgnllv3ZBJbddhn+gral8eZMZHQUxESSawVvI8fgenVN0rswssgR6hMTSSC32vgiMBN6KEP3fZ5EUkk1MZFkKgO1LoM45vFWHBeZCKqJiSTTIqFmQ7fVyHFbx4ukkoKYSMIYY/w9r8rGmHLgoZz7uz3iLImMjfYTE0kYY8wGkLXWFkLHrwMlIJ/GzRxFulGfmEjyzAPdtoEvAk0FMJkkCmIiyZMBdoIHAv1hK2PJkciYKIiJJIib4AzefLCgMoC1tjbaHImMl4KYSDKFRyAuAtVxZERknBTERBLEzQs7NA/MGHMVaFlrK+PJlcj4KIiJJE8Nb3CH3xdWRktNyYTSEHuRBHLD6W8CeaCi7VdkUimIiYhIYqk5UUREEktBTEREEktBTEREEktBTEREEktBTEREEktBTEREEktBTEREEktBTEREEktBTEREEktBTEREEktBTEREEuv/1ptlDzt7PMwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"StoUD vs. Loss (Full Phase Space)\\n $F_{dropout} = \\phi_{dropout} = 0.2$\\nDCTR Change\")\n",
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.vlines(0.200, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"probStoUD(200) Vs Loss-FDropoutPhiDropout02-Copy7.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T00:51:44.235290Z",
     "start_time": "2020-07-29T00:51:44.224395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[np.argmax(vlvals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.342771Z",
     "start_time": "2020-07-19T00:03:27.336321Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.590548Z",
     "start_time": "2020-07-19T00:03:27.345029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_21[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = int(len(X_train_theta) / 10)\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1, MSE_loss=False):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        #  when not training theta, fetch as np array\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "\n",
    "    weights = reweight(events = x, param = theta0) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.821775Z",
     "start_time": "2020-07-19T00:03:27.593159Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 151s 167us/step - loss: 0.5987 - acc: 0.4972 - val_loss: 0.5872 - val_acc: 0.4956\n",
      ". theta fit =  0.0\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "1800000/1800000 [==============================] - 225s 125us/step - loss: nan - acc: 0.4957         \n",
      ". theta fit =  nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'argrelmin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f6db20d6f348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     extrema = np.concatenate(\n\u001b[0;32m---> 36\u001b[0;31m         (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mextrema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextrema\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextrema\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mindex_refine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argrelmin' is not defined"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration + 1)\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=1000,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train_theta, Y_train_theta, epochs=1, batch_size=batch_size,verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "\n",
    "        mean_fit = np.array([\n",
    "            np.mean(fit_vals_recent[len(fit_vals_recent) -\n",
    "                                    4:len(fit_vals_recent)])\n",
    "        ])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.823619Z",
     "start_time": "2020-07-17T18:54:03.260Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.200, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
