{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTR Alternative Fitting Algorithm for probStoUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:38:40.697769Z",
     "start_time": "2020-07-23T21:38:40.693844Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:38:43.169882Z",
     "start_time": "2020-07-23T21:38:40.702322Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:38:43.179530Z",
     "start_time": "2020-07-23T21:38:43.173137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22)\n",
    "rc('xtick', labelsize=15)\n",
    "rc('ytick', labelsize=15)\n",
    "rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:38:43.197244Z",
     "start_time": "2020-07-23T21:38:43.182778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:38:43.206351Z",
     "start_time": "2020-07-23T21:38:43.199921Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:38:43.214849Z",
     "start_time": "2020-07-23T21:38:43.208747Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:38:43.223204Z",
     "start_time": "2020-07-23T21:38:43.219193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/data0/users/aandreassen/zenodo/'\n",
    "data_dir1 = '/data1/users/asuresh/DCTRFitting/StoUDFitting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:38:51.234046Z",
     "start_time": "2020-07-23T21:38:43.225912Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "#unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')\n",
    "unknown_dataset =  np.load(data_dir1 + 'test1D_strange200.npz', allow_pickle=True)['dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:42:41.674757Z",
     "start_time": "2020-07-23T21:38:51.236330Z"
    }
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:42:42.937758Z",
     "start_time": "2020-07-23T21:42:41.676871Z"
    }
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:42:45.024933Z",
     "start_time": "2020-07-23T21:42:42.940462Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.5, shuffle=True)\n",
    "X_train_theta, X_test_theta, Y_train_theta, Y_test_theta = data_split(X_fit, Y_fit, test=0., shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:42:45.669900Z",
     "start_time": "2020-07-23T21:42:45.026834Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100, 100, 128)\n",
    "F_sizes = (100, 100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights(\n",
    "    './saved_models/DCTR_ee_dijets_1D_probStoUD_Copy1.h5')  #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:42:45.679491Z",
     "start_time": "2020-07-23T21:42:45.672250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "def reweight(events, param):  #from NN (DCTR)\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, param]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(events[...,0])>0,\n",
    "                                   K.ones_like(events[...,0]),\n",
    "                                   K.zeros_like(events[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "\n",
    "    model_inputs = K.concatenate([events, concat_input_and_params], -1)\n",
    "    # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    \n",
    "    f = dctr.model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:42:46.216874Z",
     "start_time": "2020-07-23T21:42:45.682846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = PFN(input_dim=4,\n",
    "            Phi_sizes=Phi_sizes,\n",
    "            F_sizes=F_sizes,\n",
    "            latent_dropout= 0.2,\n",
    "            F_dropouts= 0.2,\n",
    "            output_dim=1,\n",
    "            output_act='sigmoid',\n",
    "            summary=False)\n",
    "reinitialize_weights = model.model.get_weights()\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0., MSE_loss = False):\n",
    "    x = inputs  #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(\n",
    "        x, np.arange(51),\n",
    "        axis=1)  # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    weights = reweight(events = x, param = val)  # NN reweight\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-23T21:38:40.697Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-13-2bac8f3c1b35>:9: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.7073 - acc: 0.4982 - val_loss: 0.6930 - val_acc: 0.4964\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6926 - acc: 0.4968 - val_loss: 0.6905 - val_acc: 0.4941\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 360s 400us/step - loss: 0.6870 - acc: 0.4927 - val_loss: 0.6826 - val_acc: 0.4912\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6819 - acc: 0.4902 - val_loss: 0.6812 - val_acc: 0.4910\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6817 - acc: 0.4899 - val_loss: 0.6816 - val_acc: 0.4905\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6815 - acc: 0.4896 - val_loss: 0.6814 - val_acc: 0.4906\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 361s 402us/step - loss: 0.6815 - acc: 0.4896 - val_loss: 0.6815 - val_acc: 0.4906\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6814 - acc: 0.4896 - val_loss: 0.6818 - val_acc: 0.4910\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6813 - acc: 0.4898 - val_loss: 0.6815 - val_acc: 0.4908\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6812 - acc: 0.4897 - val_loss: 0.6813 - val_acc: 0.4905\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6812 - acc: 0.4897 - val_loss: 0.6809 - val_acc: 0.4905\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6812 - acc: 0.4895 - val_loss: 0.6812 - val_acc: 0.4905\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6811 - acc: 0.4896 - val_loss: 0.6816 - val_acc: 0.4907\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 361s 401us/step - loss: 0.6810 - acc: 0.4898 - val_loss: 0.6821 - val_acc: 0.4908\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6810 - acc: 0.4895 - val_loss: 0.6811 - val_acc: 0.4905\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6810 - acc: 0.4897 - val_loss: 0.6810 - val_acc: 0.4904\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6810 - acc: 0.4897 - val_loss: 0.6822 - val_acc: 0.4906\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6810 - acc: 0.4895 - val_loss: 0.6826 - val_acc: 0.4909\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6810 - acc: 0.4897 - val_loss: 0.6812 - val_acc: 0.4907\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6810 - acc: 0.4893 - val_loss: 0.6818 - val_acc: 0.4908\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6809 - acc: 0.4897 - val_loss: 0.6810 - val_acc: 0.4907\n",
      "trainnig theta = : 0.10833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6836 - acc: 0.4898 - val_loss: 0.6836 - val_acc: 0.4906\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 367s 408us/step - loss: 0.6835 - acc: 0.4898 - val_loss: 0.6849 - val_acc: 0.4905\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 367s 407us/step - loss: 0.6835 - acc: 0.4898 - val_loss: 0.6834 - val_acc: 0.4905\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6835 - acc: 0.4896 - val_loss: 0.6841 - val_acc: 0.4910\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6835 - acc: 0.4898 - val_loss: 0.6848 - val_acc: 0.4903\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6835 - acc: 0.4896 - val_loss: 0.6838 - val_acc: 0.4907\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6834 - acc: 0.4898 - val_loss: 0.6835 - val_acc: 0.4906\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6834 - acc: 0.4895 - val_loss: 0.6839 - val_acc: 0.4905\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6834 - acc: 0.4898 - val_loss: 0.6840 - val_acc: 0.4906\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6834 - acc: 0.4896 - val_loss: 0.6834 - val_acc: 0.4906\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6834 - acc: 0.4899 - val_loss: 0.6841 - val_acc: 0.4905\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6833 - acc: 0.4900 - val_loss: 0.6837 - val_acc: 0.4904\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6833 - acc: 0.4899 - val_loss: 0.6837 - val_acc: 0.4907\n",
      "trainnig theta = : 0.11666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 367s 408us/step - loss: 0.6856 - acc: 0.4899 - val_loss: 0.6857 - val_acc: 0.4907\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6855 - acc: 0.4901 - val_loss: 0.6873 - val_acc: 0.4906\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6855 - acc: 0.4897 - val_loss: 0.6863 - val_acc: 0.4906\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 343s 382us/step - loss: 0.6855 - acc: 0.4896 - val_loss: 0.6857 - val_acc: 0.4905\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6855 - acc: 0.4895 - val_loss: 0.6858 - val_acc: 0.4906\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6855 - acc: 0.4899 - val_loss: 0.6858 - val_acc: 0.4906\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6855 - acc: 0.4899 - val_loss: 0.6860 - val_acc: 0.4906\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6854 - acc: 0.4896 - val_loss: 0.6856 - val_acc: 0.4908\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6854 - acc: 0.4897 - val_loss: 0.6856 - val_acc: 0.4905\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6854 - acc: 0.4897 - val_loss: 0.6858 - val_acc: 0.4910\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6854 - acc: 0.4899 - val_loss: 0.6859 - val_acc: 0.4907\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6854 - acc: 0.4898 - val_loss: 0.6858 - val_acc: 0.4905\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6854 - acc: 0.4900 - val_loss: 0.6858 - val_acc: 0.4905\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 343s 382us/step - loss: 0.6854 - acc: 0.4898 - val_loss: 0.6855 - val_acc: 0.4906\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 367s 408us/step - loss: 0.6853 - acc: 0.4899 - val_loss: 0.6860 - val_acc: 0.4904\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 374s 416us/step - loss: 0.6854 - acc: 0.4898 - val_loss: 0.6857 - val_acc: 0.4904\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 370s 412us/step - loss: 0.6854 - acc: 0.4897 - val_loss: 0.6858 - val_acc: 0.4905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6853 - acc: 0.4898 - val_loss: 0.6864 - val_acc: 0.4905\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 368s 409us/step - loss: 0.6853 - acc: 0.4898 - val_loss: 0.6859 - val_acc: 0.4908\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6853 - acc: 0.4899 - val_loss: 0.6857 - val_acc: 0.4906\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6853 - acc: 0.4899 - val_loss: 0.6859 - val_acc: 0.4908\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6853 - acc: 0.4897 - val_loss: 0.6858 - val_acc: 0.4908\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6853 - acc: 0.4900 - val_loss: 0.6854 - val_acc: 0.4905\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 367s 408us/step - loss: 0.6853 - acc: 0.4900 - val_loss: 0.6859 - val_acc: 0.4905\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6853 - acc: 0.4897 - val_loss: 0.6863 - val_acc: 0.4906\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6853 - acc: 0.4900 - val_loss: 0.6859 - val_acc: 0.4907\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 361s 401us/step - loss: 0.6852 - acc: 0.4903 - val_loss: 0.6858 - val_acc: 0.4905\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 366s 406us/step - loss: 0.6852 - acc: 0.4899 - val_loss: 0.6857 - val_acc: 0.4906\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 368s 409us/step - loss: 0.6853 - acc: 0.4902 - val_loss: 0.6861 - val_acc: 0.4909\n",
      "Epoch 31/100\n",
      "900000/900000 [==============================] - 368s 408us/step - loss: 0.6852 - acc: 0.4901 - val_loss: 0.6857 - val_acc: 0.4907\n",
      "Epoch 32/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6853 - acc: 0.4898 - val_loss: 0.6861 - val_acc: 0.4908\n",
      "Epoch 33/100\n",
      "900000/900000 [==============================] - 368s 409us/step - loss: 0.6852 - acc: 0.4899 - val_loss: 0.6858 - val_acc: 0.4906\n",
      "Epoch 34/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6852 - acc: 0.4902 - val_loss: 0.6861 - val_acc: 0.4907\n",
      "trainnig theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 372s 413us/step - loss: 0.6872 - acc: 0.4899 - val_loss: 0.6877 - val_acc: 0.4909\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6872 - acc: 0.4903 - val_loss: 0.6877 - val_acc: 0.4911\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6872 - acc: 0.4898 - val_loss: 0.6874 - val_acc: 0.4905\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6872 - acc: 0.4902 - val_loss: 0.6877 - val_acc: 0.4908\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6871 - acc: 0.4901 - val_loss: 0.6877 - val_acc: 0.4904\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6871 - acc: 0.4902 - val_loss: 0.6873 - val_acc: 0.4905\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6872 - acc: 0.4900 - val_loss: 0.6879 - val_acc: 0.4909\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6871 - acc: 0.4899 - val_loss: 0.6876 - val_acc: 0.4907\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 355s 394us/step - loss: 0.6871 - acc: 0.4900 - val_loss: 0.6872 - val_acc: 0.4905\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6871 - acc: 0.4902 - val_loss: 0.6873 - val_acc: 0.4906\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 367s 408us/step - loss: 0.6871 - acc: 0.4902 - val_loss: 0.6874 - val_acc: 0.4905\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 366s 406us/step - loss: 0.6871 - acc: 0.4901 - val_loss: 0.6877 - val_acc: 0.4908\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 365s 405us/step - loss: 0.6871 - acc: 0.4902 - val_loss: 0.6876 - val_acc: 0.4907\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 370s 411us/step - loss: 0.6872 - acc: 0.4902 - val_loss: 0.6875 - val_acc: 0.4906\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6871 - acc: 0.4903 - val_loss: 0.6874 - val_acc: 0.4909\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 368s 409us/step - loss: 0.6871 - acc: 0.4903 - val_loss: 0.6874 - val_acc: 0.4904\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6871 - acc: 0.4904 - val_loss: 0.6876 - val_acc: 0.4907\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6871 - acc: 0.4899 - val_loss: 0.6879 - val_acc: 0.4909\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6871 - acc: 0.4901 - val_loss: 0.6874 - val_acc: 0.4907\n",
      "trainnig theta = : 0.13333333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 370s 411us/step - loss: 0.6887 - acc: 0.4900 - val_loss: 0.6895 - val_acc: 0.4907\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6887 - acc: 0.4904 - val_loss: 0.6891 - val_acc: 0.4909\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 347s 386us/step - loss: 0.6887 - acc: 0.4902 - val_loss: 0.6892 - val_acc: 0.4909\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6887 - acc: 0.4902 - val_loss: 0.6892 - val_acc: 0.4906\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 339s 376us/step - loss: 0.6887 - acc: 0.4899 - val_loss: 0.6893 - val_acc: 0.4906\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6887 - acc: 0.4902 - val_loss: 0.6891 - val_acc: 0.4905\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6887 - acc: 0.4901 - val_loss: 0.6894 - val_acc: 0.4911\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 337s 375us/step - loss: 0.6887 - acc: 0.4902 - val_loss: 0.6889 - val_acc: 0.4905\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 342s 379us/step - loss: 0.6887 - acc: 0.4900 - val_loss: 0.6894 - val_acc: 0.4908\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6887 - acc: 0.4900 - val_loss: 0.6888 - val_acc: 0.4910\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6887 - acc: 0.4900 - val_loss: 0.6888 - val_acc: 0.4905\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6887 - acc: 0.4901 - val_loss: 0.6891 - val_acc: 0.4905\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 339s 377us/step - loss: 0.6887 - acc: 0.4906 - val_loss: 0.6892 - val_acc: 0.4906\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6886 - acc: 0.4904 - val_loss: 0.6891 - val_acc: 0.4906\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6887 - acc: 0.4903 - val_loss: 0.6892 - val_acc: 0.4910\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 341s 378us/step - loss: 0.6887 - acc: 0.4904 - val_loss: 0.6892 - val_acc: 0.4906\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6887 - acc: 0.4902 - val_loss: 0.6894 - val_acc: 0.4905\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6886 - acc: 0.4905 - val_loss: 0.6894 - val_acc: 0.4906\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 342s 379us/step - loss: 0.6887 - acc: 0.4904 - val_loss: 0.6892 - val_acc: 0.4905\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 339s 377us/step - loss: 0.6886 - acc: 0.4906 - val_loss: 0.6891 - val_acc: 0.4905\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6887 - acc: 0.4906 - val_loss: 0.6890 - val_acc: 0.4908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.14166666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 347s 385us/step - loss: 0.6900 - acc: 0.4905 - val_loss: 0.6904 - val_acc: 0.4906\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6900 - acc: 0.4905 - val_loss: 0.6901 - val_acc: 0.4905\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 341s 378us/step - loss: 0.6900 - acc: 0.4904 - val_loss: 0.6904 - val_acc: 0.4905\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6900 - acc: 0.4902 - val_loss: 0.6902 - val_acc: 0.4906\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6900 - acc: 0.4906 - val_loss: 0.6907 - val_acc: 0.4905\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 338s 375us/step - loss: 0.6900 - acc: 0.4907 - val_loss: 0.6907 - val_acc: 0.4911\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6900 - acc: 0.4907 - val_loss: 0.6904 - val_acc: 0.4909\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6900 - acc: 0.4908 - val_loss: 0.6903 - val_acc: 0.4908\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6900 - acc: 0.4904 - val_loss: 0.6907 - val_acc: 0.4908\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6900 - acc: 0.4906 - val_loss: 0.6903 - val_acc: 0.4905\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6900 - acc: 0.4905 - val_loss: 0.6902 - val_acc: 0.4905\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6900 - acc: 0.4908 - val_loss: 0.6905 - val_acc: 0.4909\n",
      "trainnig theta = : 0.15000000000000002\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 370s 411us/step - loss: 0.6911 - acc: 0.4906 - val_loss: 0.6914 - val_acc: 0.4909\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6911 - acc: 0.4907 - val_loss: 0.6913 - val_acc: 0.4905\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6911 - acc: 0.4905 - val_loss: 0.6914 - val_acc: 0.4906\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6911 - acc: 0.4903 - val_loss: 0.6914 - val_acc: 0.4909\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6911 - acc: 0.4907 - val_loss: 0.6919 - val_acc: 0.4914\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6911 - acc: 0.4908 - val_loss: 0.6915 - val_acc: 0.4912\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 360s 400us/step - loss: 0.6911 - acc: 0.4907 - val_loss: 0.6915 - val_acc: 0.4913\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6911 - acc: 0.4906 - val_loss: 0.6916 - val_acc: 0.4909\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 354s 394us/step - loss: 0.6911 - acc: 0.4906 - val_loss: 0.6916 - val_acc: 0.4907\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6911 - acc: 0.4911 - val_loss: 0.6915 - val_acc: 0.4908\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6911 - acc: 0.4911 - val_loss: 0.6913 - val_acc: 0.4908\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6910 - acc: 0.4909 - val_loss: 0.6915 - val_acc: 0.4907\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6910 - acc: 0.4908 - val_loss: 0.6915 - val_acc: 0.4913\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 345s 384us/step - loss: 0.6910 - acc: 0.4908 - val_loss: 0.6915 - val_acc: 0.4905\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6910 - acc: 0.4910 - val_loss: 0.6916 - val_acc: 0.4907\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6911 - acc: 0.4906 - val_loss: 0.6916 - val_acc: 0.4908\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6910 - acc: 0.4910 - val_loss: 0.6914 - val_acc: 0.4910\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6910 - acc: 0.4907 - val_loss: 0.6914 - val_acc: 0.4903\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6910 - acc: 0.4909 - val_loss: 0.6916 - val_acc: 0.4906\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6910 - acc: 0.4909 - val_loss: 0.6916 - val_acc: 0.4906\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 346s 385us/step - loss: 0.6910 - acc: 0.4909 - val_loss: 0.6914 - val_acc: 0.4907\n",
      "trainnig theta = : 0.15833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 350s 389us/step - loss: 0.6919 - acc: 0.4907 - val_loss: 0.6922 - val_acc: 0.4907\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6920 - acc: 0.4908 - val_loss: 0.6923 - val_acc: 0.4907\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6920 - acc: 0.4911 - val_loss: 0.6922 - val_acc: 0.4908\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6920 - acc: 0.4911 - val_loss: 0.6922 - val_acc: 0.4910\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 347s 386us/step - loss: 0.6919 - acc: 0.4911 - val_loss: 0.6924 - val_acc: 0.4912\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 347s 386us/step - loss: 0.6920 - acc: 0.4911 - val_loss: 0.6922 - val_acc: 0.4913\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 346s 385us/step - loss: 0.6919 - acc: 0.4910 - val_loss: 0.6923 - val_acc: 0.4909\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 347s 386us/step - loss: 0.6920 - acc: 0.4914 - val_loss: 0.6923 - val_acc: 0.4907\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6919 - acc: 0.4912 - val_loss: 0.6924 - val_acc: 0.4914\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6920 - acc: 0.4911 - val_loss: 0.6923 - val_acc: 0.4905\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 349s 388us/step - loss: 0.6920 - acc: 0.4912 - val_loss: 0.6922 - val_acc: 0.4908\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 348s 386us/step - loss: 0.6919 - acc: 0.4912 - val_loss: 0.6924 - val_acc: 0.4910\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 344s 383us/step - loss: 0.6919 - acc: 0.4913 - val_loss: 0.6922 - val_acc: 0.4908\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 345s 384us/step - loss: 0.6919 - acc: 0.4912 - val_loss: 0.6924 - val_acc: 0.4908\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 344s 383us/step - loss: 0.6919 - acc: 0.4917 - val_loss: 0.6924 - val_acc: 0.4907\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6919 - acc: 0.4913 - val_loss: 0.6922 - val_acc: 0.4916\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 342s 379us/step - loss: 0.6919 - acc: 0.4920 - val_loss: 0.6924 - val_acc: 0.4913\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6919 - acc: 0.4917 - val_loss: 0.6922 - val_acc: 0.4910\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6919 - acc: 0.4912 - val_loss: 0.6923 - val_acc: 0.4911\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6919 - acc: 0.4911 - val_loss: 0.6924 - val_acc: 0.4913\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6919 - acc: 0.4918 - val_loss: 0.6922 - val_acc: 0.4911\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 339s 377us/step - loss: 0.6919 - acc: 0.4920 - val_loss: 0.6924 - val_acc: 0.4912\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 340s 377us/step - loss: 0.6919 - acc: 0.4920 - val_loss: 0.6924 - val_acc: 0.4914\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 336s 374us/step - loss: 0.6919 - acc: 0.4917 - val_loss: 0.6924 - val_acc: 0.4913\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 338s 375us/step - loss: 0.6919 - acc: 0.4916 - val_loss: 0.6924 - val_acc: 0.4912\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 336s 373us/step - loss: 0.6919 - acc: 0.4917 - val_loss: 0.6923 - val_acc: 0.4908\n",
      "trainnig theta = : 0.16666666666666669\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6927 - acc: 0.4919 - val_loss: 0.6929 - val_acc: 0.4917\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6926 - acc: 0.4923 - val_loss: 0.6931 - val_acc: 0.4914\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6927 - acc: 0.4917 - val_loss: 0.6931 - val_acc: 0.4910\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 338s 375us/step - loss: 0.6927 - acc: 0.4916 - val_loss: 0.6931 - val_acc: 0.4914\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6927 - acc: 0.4918 - val_loss: 0.6931 - val_acc: 0.4910\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 340s 377us/step - loss: 0.6927 - acc: 0.4921 - val_loss: 0.6931 - val_acc: 0.4917\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6927 - acc: 0.4925 - val_loss: 0.6932 - val_acc: 0.4915\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 341s 378us/step - loss: 0.6927 - acc: 0.4919 - val_loss: 0.6930 - val_acc: 0.4915\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6927 - acc: 0.4922 - val_loss: 0.6930 - val_acc: 0.4918\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6927 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.4913\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6927 - acc: 0.4927 - val_loss: 0.6930 - val_acc: 0.4915\n",
      "trainnig theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6932 - acc: 0.4918 - val_loss: 0.6935 - val_acc: 0.4910\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 333s 370us/step - loss: 0.6931 - acc: 0.4920 - val_loss: 0.6935 - val_acc: 0.4913\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6932 - acc: 0.4923 - val_loss: 0.6934 - val_acc: 0.4917\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6932 - acc: 0.4923 - val_loss: 0.6935 - val_acc: 0.4912\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 334s 372us/step - loss: 0.6932 - acc: 0.4928 - val_loss: 0.6934 - val_acc: 0.4912\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6931 - acc: 0.4928 - val_loss: 0.6934 - val_acc: 0.4914\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6931 - acc: 0.4926 - val_loss: 0.6934 - val_acc: 0.4919\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 328s 364us/step - loss: 0.6931 - acc: 0.4933 - val_loss: 0.6935 - val_acc: 0.4916\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6931 - acc: 0.4929 - val_loss: 0.6934 - val_acc: 0.4916\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6931 - acc: 0.4928 - val_loss: 0.6935 - val_acc: 0.4918\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 330s 367us/step - loss: 0.6931 - acc: 0.4932 - val_loss: 0.6935 - val_acc: 0.4911\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6931 - acc: 0.4935 - val_loss: 0.6934 - val_acc: 0.4920\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6931 - acc: 0.4928 - val_loss: 0.6935 - val_acc: 0.4920\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6931 - acc: 0.4937 - val_loss: 0.6935 - val_acc: 0.4920\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 328s 364us/step - loss: 0.6931 - acc: 0.4930 - val_loss: 0.6935 - val_acc: 0.4911\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6931 - acc: 0.4935 - val_loss: 0.6935 - val_acc: 0.4918\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6931 - acc: 0.4930 - val_loss: 0.6935 - val_acc: 0.4913\n",
      "trainnig theta = : 0.18333333333333335\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6927 - acc: 0.4942 - val_loss: 0.6929 - val_acc: 0.4927\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6927 - acc: 0.4948 - val_loss: 0.6930 - val_acc: 0.4920\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6927 - acc: 0.4948 - val_loss: 0.6929 - val_acc: 0.4932\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6927 - acc: 0.4955 - val_loss: 0.6930 - val_acc: 0.4924\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 338s 376us/step - loss: 0.6927 - acc: 0.4952 - val_loss: 0.6930 - val_acc: 0.4928\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 314s 348us/step - loss: 0.6927 - acc: 0.4950 - val_loss: 0.6929 - val_acc: 0.4924\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6927 - acc: 0.4949 - val_loss: 0.6929 - val_acc: 0.4920\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6927 - acc: 0.4953 - val_loss: 0.6930 - val_acc: 0.4934\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 303s 337us/step - loss: 0.6927 - acc: 0.4953 - val_loss: 0.6929 - val_acc: 0.4914\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6927 - acc: 0.4947 - val_loss: 0.6929 - val_acc: 0.4922\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6926 - acc: 0.4955 - val_loss: 0.6930 - val_acc: 0.4925\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 302s 336us/step - loss: 0.6926 - acc: 0.4955 - val_loss: 0.6930 - val_acc: 0.4924\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6927 - acc: 0.4949 - val_loss: 0.6930 - val_acc: 0.4938\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6926 - acc: 0.4956 - val_loss: 0.6929 - val_acc: 0.4925\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6927 - acc: 0.4952 - val_loss: 0.6929 - val_acc: 0.4926\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6926 - acc: 0.4957 - val_loss: 0.6930 - val_acc: 0.4926\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 303s 337us/step - loss: 0.6926 - acc: 0.4958 - val_loss: 0.6930 - val_acc: 0.4930\n",
      "trainnig theta = : 0.19166666666666665\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 311s 346us/step - loss: 0.6918 - acc: 0.4961 - val_loss: 0.6920 - val_acc: 0.4937\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6917 - acc: 0.4973 - val_loss: 0.6920 - val_acc: 0.4933\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6917 - acc: 0.4977 - val_loss: 0.6920 - val_acc: 0.4942\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6918 - acc: 0.4976 - val_loss: 0.6920 - val_acc: 0.4980\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 302s 336us/step - loss: 0.6918 - acc: 0.4984 - val_loss: 0.6921 - val_acc: 0.4968\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 303s 337us/step - loss: 0.6917 - acc: 0.4984 - val_loss: 0.6920 - val_acc: 0.4965\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6917 - acc: 0.4991 - val_loss: 0.6920 - val_acc: 0.4965\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6918 - acc: 0.4978 - val_loss: 0.6920 - val_acc: 0.4958\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 321s 356us/step - loss: 0.6918 - acc: 0.4983 - val_loss: 0.6920 - val_acc: 0.4952\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 313s 347us/step - loss: 0.6918 - acc: 0.4981 - val_loss: 0.6920 - val_acc: 0.4949\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 304s 337us/step - loss: 0.6917 - acc: 0.4989 - val_loss: 0.6920 - val_acc: 0.4968\n",
      "trainnig theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 310s 345us/step - loss: 0.6915 - acc: 0.5001 - val_loss: 0.6916 - val_acc: 0.4985\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6915 - acc: 0.5013 - val_loss: 0.6916 - val_acc: 0.5011\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6915 - acc: 0.5011 - val_loss: 0.6917 - val_acc: 0.4973\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 302s 336us/step - loss: 0.6914 - acc: 0.5007 - val_loss: 0.6916 - val_acc: 0.5018\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6914 - acc: 0.5009 - val_loss: 0.6916 - val_acc: 0.4988\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6914 - acc: 0.5019 - val_loss: 0.6916 - val_acc: 0.5003\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6914 - acc: 0.5019 - val_loss: 0.6916 - val_acc: 0.4999\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6914 - acc: 0.5024 - val_loss: 0.6916 - val_acc: 0.5008\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6914 - acc: 0.5015 - val_loss: 0.6917 - val_acc: 0.4996\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6914 - acc: 0.5020 - val_loss: 0.6916 - val_acc: 0.5004\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6914 - acc: 0.5027 - val_loss: 0.6917 - val_acc: 0.4993\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6914 - acc: 0.5020 - val_loss: 0.6916 - val_acc: 0.5046\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 302s 335us/step - loss: 0.6914 - acc: 0.5035 - val_loss: 0.6916 - val_acc: 0.5015\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6914 - acc: 0.5027 - val_loss: 0.6916 - val_acc: 0.5018\n",
      "trainnig theta = : 0.20833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6920 - acc: 0.5045 - val_loss: 0.6921 - val_acc: 0.5054\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6919 - acc: 0.5069 - val_loss: 0.6921 - val_acc: 0.5055\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 302s 336us/step - loss: 0.6919 - acc: 0.5081 - val_loss: 0.6921 - val_acc: 0.5069\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6919 - acc: 0.5079 - val_loss: 0.6921 - val_acc: 0.5077\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6919 - acc: 0.5089 - val_loss: 0.6921 - val_acc: 0.5082\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 298s 331us/step - loss: 0.6919 - acc: 0.5081 - val_loss: 0.6922 - val_acc: 0.5073\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 299s 333us/step - loss: 0.6919 - acc: 0.5086 - val_loss: 0.6921 - val_acc: 0.5066\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6919 - acc: 0.5085 - val_loss: 0.6921 - val_acc: 0.5072\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6919 - acc: 0.5089 - val_loss: 0.6921 - val_acc: 0.5073\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6919 - acc: 0.5085 - val_loss: 0.6921 - val_acc: 0.5080\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6919 - acc: 0.5089 - val_loss: 0.6921 - val_acc: 0.5080\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6919 - acc: 0.5086 - val_loss: 0.6921 - val_acc: 0.5075\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6919 - acc: 0.5089 - val_loss: 0.6921 - val_acc: 0.5071\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6919 - acc: 0.5101 - val_loss: 0.6921 - val_acc: 0.5068\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6919 - acc: 0.5095 - val_loss: 0.6921 - val_acc: 0.5075\n",
      "trainnig theta = : 0.21666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6928 - acc: 0.5092 - val_loss: 0.6929 - val_acc: 0.5080\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 296s 329us/step - loss: 0.6928 - acc: 0.5088 - val_loss: 0.6929 - val_acc: 0.5085\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6927 - acc: 0.5100 - val_loss: 0.6929 - val_acc: 0.5088\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6927 - acc: 0.5100 - val_loss: 0.6928 - val_acc: 0.5087\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 297s 330us/step - loss: 0.6927 - acc: 0.5096 - val_loss: 0.6928 - val_acc: 0.5092\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 295s 328us/step - loss: 0.6927 - acc: 0.5102 - val_loss: 0.6928 - val_acc: 0.5082\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 299s 333us/step - loss: 0.6927 - acc: 0.5105 - val_loss: 0.6928 - val_acc: 0.5086\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6926 - acc: 0.5111 - val_loss: 0.6928 - val_acc: 0.5082\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6926 - acc: 0.5112 - val_loss: 0.6928 - val_acc: 0.5082\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 297s 331us/step - loss: 0.6926 - acc: 0.5109 - val_loss: 0.6928 - val_acc: 0.5088\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 296s 329us/step - loss: 0.6926 - acc: 0.5105 - val_loss: 0.6929 - val_acc: 0.5086\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6926 - acc: 0.5106 - val_loss: 0.6928 - val_acc: 0.5085\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 296s 329us/step - loss: 0.6926 - acc: 0.5106 - val_loss: 0.6928 - val_acc: 0.5083\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6926 - acc: 0.5110 - val_loss: 0.6929 - val_acc: 0.5085\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6926 - acc: 0.5111 - val_loss: 0.6929 - val_acc: 0.5074\n",
      "trainnig theta = : 0.225\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6928 - acc: 0.5106 - val_loss: 0.6930 - val_acc: 0.5089\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6928 - acc: 0.5105 - val_loss: 0.6929 - val_acc: 0.5088\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 297s 330us/step - loss: 0.6927 - acc: 0.5108 - val_loss: 0.6929 - val_acc: 0.5087\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6927 - acc: 0.5109 - val_loss: 0.6929 - val_acc: 0.5088\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6927 - acc: 0.5108 - val_loss: 0.6932 - val_acc: 0.5086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6927 - acc: 0.5109 - val_loss: 0.6929 - val_acc: 0.5088\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 299s 333us/step - loss: 0.6927 - acc: 0.5111 - val_loss: 0.6929 - val_acc: 0.5091\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6927 - acc: 0.5110 - val_loss: 0.6929 - val_acc: 0.5089\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 301s 335us/step - loss: 0.6927 - acc: 0.5113 - val_loss: 0.6930 - val_acc: 0.5085\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6927 - acc: 0.5107 - val_loss: 0.6929 - val_acc: 0.5086\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6927 - acc: 0.5112 - val_loss: 0.6930 - val_acc: 0.5081\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 298s 331us/step - loss: 0.6927 - acc: 0.5113 - val_loss: 0.6929 - val_acc: 0.5089\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6927 - acc: 0.5110 - val_loss: 0.6930 - val_acc: 0.5089\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 299s 333us/step - loss: 0.6927 - acc: 0.5113 - val_loss: 0.6929 - val_acc: 0.5087\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6926 - acc: 0.5119 - val_loss: 0.6929 - val_acc: 0.5087\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 299s 333us/step - loss: 0.6926 - acc: 0.5117 - val_loss: 0.6929 - val_acc: 0.5088\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 292s 325us/step - loss: 0.6926 - acc: 0.5119 - val_loss: 0.6929 - val_acc: 0.5088\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6926 - acc: 0.5116 - val_loss: 0.6929 - val_acc: 0.5085\n",
      "trainnig theta = : 0.23333333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6923 - acc: 0.5111 - val_loss: 0.6925 - val_acc: 0.5089\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6923 - acc: 0.5110 - val_loss: 0.6926 - val_acc: 0.5083\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6923 - acc: 0.5113 - val_loss: 0.6926 - val_acc: 0.5088\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6923 - acc: 0.5114 - val_loss: 0.6925 - val_acc: 0.5089\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6923 - acc: 0.5114 - val_loss: 0.6925 - val_acc: 0.5092\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 299s 333us/step - loss: 0.6923 - acc: 0.5118 - val_loss: 0.6926 - val_acc: 0.5091\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 298s 331us/step - loss: 0.6923 - acc: 0.5113 - val_loss: 0.6925 - val_acc: 0.5089\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6922 - acc: 0.5119 - val_loss: 0.6925 - val_acc: 0.5089\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6922 - acc: 0.5118 - val_loss: 0.6926 - val_acc: 0.5088\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6922 - acc: 0.5115 - val_loss: 0.6926 - val_acc: 0.5089\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6923 - acc: 0.5114 - val_loss: 0.6926 - val_acc: 0.5089\n",
      "trainnig theta = : 0.24166666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6920 - acc: 0.5109 - val_loss: 0.6920 - val_acc: 0.5088\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 299s 333us/step - loss: 0.6919 - acc: 0.5112 - val_loss: 0.6921 - val_acc: 0.5089\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 297s 330us/step - loss: 0.6919 - acc: 0.5114 - val_loss: 0.6922 - val_acc: 0.5092\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 295s 328us/step - loss: 0.6919 - acc: 0.5113 - val_loss: 0.6921 - val_acc: 0.5087\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6919 - acc: 0.5113 - val_loss: 0.6920 - val_acc: 0.5091\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6919 - acc: 0.5115 - val_loss: 0.6921 - val_acc: 0.5089\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6919 - acc: 0.5116 - val_loss: 0.6923 - val_acc: 0.5089\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 300s 334us/step - loss: 0.6919 - acc: 0.5117 - val_loss: 0.6921 - val_acc: 0.5092\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6919 - acc: 0.5113 - val_loss: 0.6921 - val_acc: 0.5094\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6919 - acc: 0.5117 - val_loss: 0.6921 - val_acc: 0.5089\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 298s 332us/step - loss: 0.6919 - acc: 0.5111 - val_loss: 0.6921 - val_acc: 0.5085\n",
      "trainnig theta = : 0.25\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6916 - acc: 0.5113 - val_loss: 0.6919 - val_acc: 0.5094\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6916 - acc: 0.5114 - val_loss: 0.6919 - val_acc: 0.5093\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6916 - acc: 0.5112 - val_loss: 0.6919 - val_acc: 0.5093\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6916 - acc: 0.5111 - val_loss: 0.6920 - val_acc: 0.5091\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 298s 331us/step - loss: 0.6916 - acc: 0.5116 - val_loss: 0.6919 - val_acc: 0.5091\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 299s 332us/step - loss: 0.6916 - acc: 0.5112 - val_loss: 0.6918 - val_acc: 0.5092\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6916 - acc: 0.5115 - val_loss: 0.6919 - val_acc: 0.5090\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 298s 331us/step - loss: 0.6916 - acc: 0.5113 - val_loss: 0.6919 - val_acc: 0.5085\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 302s 335us/step - loss: 0.6915 - acc: 0.5112 - val_loss: 0.6919 - val_acc: 0.5091\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 301s 334us/step - loss: 0.6915 - acc: 0.5118 - val_loss: 0.6919 - val_acc: 0.5090\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 300s 333us/step - loss: 0.6915 - acc: 0.5116 - val_loss: 0.6920 - val_acc: 0.5090\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 301s 335us/step - loss: 0.6916 - acc: 0.5116 - val_loss: 0.6917 - val_acc: 0.5093\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 302s 336us/step - loss: 0.6915 - acc: 0.5117 - val_loss: 0.6918 - val_acc: 0.5088\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 304s 337us/step - loss: 0.6915 - acc: 0.5118 - val_loss: 0.6920 - val_acc: 0.5089\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6914 - acc: 0.5124 - val_loss: 0.6919 - val_acc: 0.5091\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 303s 336us/step - loss: 0.6915 - acc: 0.5118 - val_loss: 0.6919 - val_acc: 0.5091\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6915 - acc: 0.5116 - val_loss: 0.6919 - val_acc: 0.5090\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6914 - acc: 0.5121 - val_loss: 0.6918 - val_acc: 0.5090\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 303s 336us/step - loss: 0.6915 - acc: 0.5121 - val_loss: 0.6921 - val_acc: 0.5087\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6915 - acc: 0.5115 - val_loss: 0.6918 - val_acc: 0.5090\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 308s 343us/step - loss: 0.6915 - acc: 0.5119 - val_loss: 0.6919 - val_acc: 0.5089\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 312s 346us/step - loss: 0.6915 - acc: 0.5116 - val_loss: 0.6918 - val_acc: 0.5093\n",
      "trainnig theta = : 0.2583333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 320s 356us/step - loss: 0.6914 - acc: 0.5114 - val_loss: 0.6918 - val_acc: 0.5091\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 313s 348us/step - loss: 0.6914 - acc: 0.5117 - val_loss: 0.6918 - val_acc: 0.5093\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6914 - acc: 0.5115 - val_loss: 0.6917 - val_acc: 0.5092\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 317s 353us/step - loss: 0.6913 - acc: 0.5118 - val_loss: 0.6916 - val_acc: 0.5093\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 315s 350us/step - loss: 0.6913 - acc: 0.5120 - val_loss: 0.6917 - val_acc: 0.5088\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 317s 352us/step - loss: 0.6913 - acc: 0.5117 - val_loss: 0.6918 - val_acc: 0.5089\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 315s 350us/step - loss: 0.6913 - acc: 0.5119 - val_loss: 0.6918 - val_acc: 0.5090\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 320s 355us/step - loss: 0.6913 - acc: 0.5122 - val_loss: 0.6917 - val_acc: 0.5093\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 316s 352us/step - loss: 0.6913 - acc: 0.5120 - val_loss: 0.6919 - val_acc: 0.5092\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 318s 353us/step - loss: 0.6913 - acc: 0.5121 - val_loss: 0.6917 - val_acc: 0.5090\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 319s 355us/step - loss: 0.6913 - acc: 0.5120 - val_loss: 0.6919 - val_acc: 0.5089\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 323s 358us/step - loss: 0.6912 - acc: 0.5119 - val_loss: 0.6918 - val_acc: 0.5087\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 322s 357us/step - loss: 0.6912 - acc: 0.5122 - val_loss: 0.6918 - val_acc: 0.5091\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 320s 356us/step - loss: 0.6913 - acc: 0.5121 - val_loss: 0.6919 - val_acc: 0.5089\n",
      "trainnig theta = : 0.26666666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 328s 365us/step - loss: 0.6914 - acc: 0.5114 - val_loss: 0.6915 - val_acc: 0.5092\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 322s 358us/step - loss: 0.6914 - acc: 0.5119 - val_loss: 0.6916 - val_acc: 0.5092\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 322s 358us/step - loss: 0.6913 - acc: 0.5118 - val_loss: 0.6916 - val_acc: 0.5090\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 321s 357us/step - loss: 0.6913 - acc: 0.5120 - val_loss: 0.6918 - val_acc: 0.5088\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 324s 360us/step - loss: 0.6913 - acc: 0.5119 - val_loss: 0.6918 - val_acc: 0.5095\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 322s 358us/step - loss: 0.6913 - acc: 0.5119 - val_loss: 0.6916 - val_acc: 0.5093\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6912 - acc: 0.5118 - val_loss: 0.6915 - val_acc: 0.5091\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 324s 360us/step - loss: 0.6912 - acc: 0.5120 - val_loss: 0.6919 - val_acc: 0.5088\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 323s 358us/step - loss: 0.6913 - acc: 0.5116 - val_loss: 0.6920 - val_acc: 0.5090\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6913 - acc: 0.5120 - val_loss: 0.6919 - val_acc: 0.5091\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 320s 356us/step - loss: 0.6913 - acc: 0.5118 - val_loss: 0.6920 - val_acc: 0.5089\n",
      "trainnig theta = : 0.275\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 335s 372us/step - loss: 0.6914 - acc: 0.5121 - val_loss: 0.6920 - val_acc: 0.5095\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6914 - acc: 0.5120 - val_loss: 0.6920 - val_acc: 0.5091\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 324s 360us/step - loss: 0.6913 - acc: 0.5120 - val_loss: 0.6920 - val_acc: 0.5090\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 323s 359us/step - loss: 0.6913 - acc: 0.5120 - val_loss: 0.6919 - val_acc: 0.5087\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6913 - acc: 0.5118 - val_loss: 0.6918 - val_acc: 0.5088\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 328s 364us/step - loss: 0.6913 - acc: 0.5115 - val_loss: 0.6919 - val_acc: 0.5090\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 324s 359us/step - loss: 0.6913 - acc: 0.5118 - val_loss: 0.6918 - val_acc: 0.5092\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 319s 354us/step - loss: 0.6914 - acc: 0.5119 - val_loss: 0.6920 - val_acc: 0.5087\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6913 - acc: 0.5118 - val_loss: 0.6917 - val_acc: 0.5091\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6913 - acc: 0.5117 - val_loss: 0.6918 - val_acc: 0.5091\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6912 - acc: 0.5121 - val_loss: 0.6917 - val_acc: 0.5090\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 321s 356us/step - loss: 0.6913 - acc: 0.5118 - val_loss: 0.6920 - val_acc: 0.5089\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 324s 360us/step - loss: 0.6912 - acc: 0.5118 - val_loss: 0.6918 - val_acc: 0.5092\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6912 - acc: 0.5120 - val_loss: 0.6917 - val_acc: 0.5087\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 324s 360us/step - loss: 0.6913 - acc: 0.5122 - val_loss: 0.6920 - val_acc: 0.5089\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6912 - acc: 0.5123 - val_loss: 0.6918 - val_acc: 0.5089\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6912 - acc: 0.5120 - val_loss: 0.6919 - val_acc: 0.5089\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 323s 359us/step - loss: 0.6912 - acc: 0.5121 - val_loss: 0.6920 - val_acc: 0.5090\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 323s 359us/step - loss: 0.6912 - acc: 0.5124 - val_loss: 0.6921 - val_acc: 0.5087\n",
      "trainnig theta = : 0.2833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 336s 373us/step - loss: 0.6914 - acc: 0.5119 - val_loss: 0.6919 - val_acc: 0.5093\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 324s 361us/step - loss: 0.6914 - acc: 0.5122 - val_loss: 0.6918 - val_acc: 0.5093\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 322s 358us/step - loss: 0.6914 - acc: 0.5114 - val_loss: 0.6920 - val_acc: 0.5091\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 322s 358us/step - loss: 0.6914 - acc: 0.5115 - val_loss: 0.6920 - val_acc: 0.5094\n",
      "Epoch 5/100\n",
      "899000/900000 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5122"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 25)   #iterating across possible StoUD values\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam',\n",
    "                        loss=my_loss_wrapper(myinputs, theta),\n",
    "                        metrics=['accuracy'])\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystopping])\n",
    "    lvals += [history.history['loss'][np.argmin(history.history['val_loss'])]]\n",
    "    vlvals += [np.min(history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-23T21:38:40.699Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"StoUD vs. Loss (Full Phase Space)\\n $F_{dropout} = \\phi_{dropout} = 0.2$\\nDCTR Change\")\n",
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.vlines(0.200, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"probStoUD(200) Vs Loss-FDropoutPhiDropout02-Copy1.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-23T21:38:40.701Z"
    }
   },
   "outputs": [],
   "source": [
    "thetas[np.argmax(vlvals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.342771Z",
     "start_time": "2020-07-19T00:03:27.336321Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.590548Z",
     "start_time": "2020-07-19T00:03:27.345029Z"
    }
   },
   "outputs": [],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = int(len(X_train_theta) / 10)\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1, MSE_loss=False):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        #  when not training theta, fetch as np array\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "\n",
    "    weights = reweight(events = x, param = theta0) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.821775Z",
     "start_time": "2020-07-19T00:03:27.593159Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration + 1)\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=1000,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train_theta, Y_train_theta, epochs=1, batch_size=batch_size,verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "\n",
    "        mean_fit = np.array([\n",
    "            np.mean(fit_vals_recent[len(fit_vals_recent) -\n",
    "                                    4:len(fit_vals_recent)])\n",
    "        ])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.823619Z",
     "start_time": "2020-07-17T18:54:03.260Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.200, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
