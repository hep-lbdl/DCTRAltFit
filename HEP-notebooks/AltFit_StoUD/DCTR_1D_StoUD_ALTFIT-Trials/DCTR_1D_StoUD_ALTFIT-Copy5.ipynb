{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTR Alternative Fitting Algorithm for probStoUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:52.053412Z",
     "start_time": "2020-07-25T20:56:52.049342Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:54.393386Z",
     "start_time": "2020-07-25T20:56:52.056243Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:54.400187Z",
     "start_time": "2020-07-25T20:56:54.395701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22)\n",
    "rc('xtick', labelsize=15)\n",
    "rc('ytick', labelsize=15)\n",
    "rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:54.416529Z",
     "start_time": "2020-07-25T20:56:54.402389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:54.426055Z",
     "start_time": "2020-07-25T20:56:54.419260Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:54.435598Z",
     "start_time": "2020-07-25T20:56:54.428652Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:54.443648Z",
     "start_time": "2020-07-25T20:56:54.438297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/data0/users/aandreassen/zenodo/'\n",
    "data_dir1 = '/data1/users/asuresh/DCTRFitting/StoUDFitting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:57:01.982051Z",
     "start_time": "2020-07-25T20:56:54.448379Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "#unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')\n",
    "unknown_dataset =  np.load(data_dir1 + 'test1D_strange200.npz', allow_pickle=True)['dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:02:08.126332Z",
     "start_time": "2020-07-25T20:57:01.984371Z"
    }
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:02:18.710207Z",
     "start_time": "2020-07-25T21:02:08.131857Z"
    }
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:02:44.548067Z",
     "start_time": "2020-07-25T21:02:18.713703Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.5, shuffle=True)\n",
    "X_train_theta, X_test_theta, Y_train_theta, Y_test_theta = data_split(X_fit, Y_fit, test=0., shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:02:46.213433Z",
     "start_time": "2020-07-25T21:02:44.560739Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100, 100, 128)\n",
    "F_sizes = (100, 100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights(\n",
    "    './saved_models/DCTR_ee_dijets_1D_probStoUD_Copy5.h5')  #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:02:46.247438Z",
     "start_time": "2020-07-25T21:02:46.228420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "def reweight(events, param):  #from NN (DCTR)\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, param]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(events[...,0])>0,\n",
    "                                   K.ones_like(events[...,0]),\n",
    "                                   K.zeros_like(events[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "\n",
    "    model_inputs = K.concatenate([events, concat_input_and_params], -1)\n",
    "    # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    \n",
    "    f = dctr.model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:02:47.312273Z",
     "start_time": "2020-07-25T21:02:46.251091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = PFN(input_dim=4,\n",
    "            Phi_sizes=Phi_sizes,\n",
    "            F_sizes=F_sizes,\n",
    "            latent_dropout= 0.2,\n",
    "            F_dropouts= 0.2,\n",
    "            output_dim=1,\n",
    "            output_act='sigmoid',\n",
    "            summary=False)\n",
    "reinitialize_weights = model.model.get_weights()\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0., MSE_loss = False):\n",
    "    x = inputs  #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(\n",
    "        x, np.arange(51),\n",
    "        axis=1)  # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    weights = reweight(events = x, param = val)  # NN reweight\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-25T20:56:52.055Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-13-2bac8f3c1b35>:9: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 417s 463us/step - loss: 0.6979 - acc: 0.4982 - val_loss: 0.6845 - val_acc: 0.4971\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 497s 552us/step - loss: 0.6841 - acc: 0.4957 - val_loss: 0.6834 - val_acc: 0.4960\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 529s 588us/step - loss: 0.6838 - acc: 0.4953 - val_loss: 0.6833 - val_acc: 0.4964\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 607s 675us/step - loss: 0.6833 - acc: 0.4948 - val_loss: 0.6807 - val_acc: 0.4933\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 615s 683us/step - loss: 0.6806 - acc: 0.4939 - val_loss: 0.6776 - val_acc: 0.4917\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6775 - acc: 0.4925 - val_loss: 0.6754 - val_acc: 0.4908\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 615s 683us/step - loss: 0.6757 - acc: 0.4911 - val_loss: 0.6741 - val_acc: 0.4902\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 615s 683us/step - loss: 0.6750 - acc: 0.4908 - val_loss: 0.6739 - val_acc: 0.4903\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 608s 675us/step - loss: 0.6747 - acc: 0.4907 - val_loss: 0.6740 - val_acc: 0.4903\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6746 - acc: 0.4905 - val_loss: 0.6742 - val_acc: 0.4908\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 609s 676us/step - loss: 0.6743 - acc: 0.4905 - val_loss: 0.6738 - val_acc: 0.4904\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 607s 675us/step - loss: 0.6743 - acc: 0.4907 - val_loss: 0.6741 - val_acc: 0.4904\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 609s 677us/step - loss: 0.6742 - acc: 0.4909 - val_loss: 0.6737 - val_acc: 0.4898\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 608s 676us/step - loss: 0.6742 - acc: 0.4906 - val_loss: 0.6736 - val_acc: 0.4904\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 607s 674us/step - loss: 0.6740 - acc: 0.4904 - val_loss: 0.6737 - val_acc: 0.4907\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 606s 674us/step - loss: 0.6739 - acc: 0.4908 - val_loss: 0.6737 - val_acc: 0.4898\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 607s 674us/step - loss: 0.6739 - acc: 0.4905 - val_loss: 0.6740 - val_acc: 0.4905\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 605s 672us/step - loss: 0.6739 - acc: 0.4903 - val_loss: 0.6736 - val_acc: 0.4901\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 607s 674us/step - loss: 0.6738 - acc: 0.4906 - val_loss: 0.6744 - val_acc: 0.4908\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6739 - acc: 0.4903 - val_loss: 0.6736 - val_acc: 0.4907\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6738 - acc: 0.4911 - val_loss: 0.6743 - val_acc: 0.4911\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 605s 672us/step - loss: 0.6737 - acc: 0.4908 - val_loss: 0.6738 - val_acc: 0.4903\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 605s 672us/step - loss: 0.6737 - acc: 0.4905 - val_loss: 0.6735 - val_acc: 0.4901\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 606s 674us/step - loss: 0.6737 - acc: 0.4905 - val_loss: 0.6735 - val_acc: 0.4896\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6737 - acc: 0.4905 - val_loss: 0.6734 - val_acc: 0.4900\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 599s 666us/step - loss: 0.6737 - acc: 0.4907 - val_loss: 0.6736 - val_acc: 0.4902\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 597s 663us/step - loss: 0.6736 - acc: 0.4908 - val_loss: 0.6733 - val_acc: 0.4903\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 595s 661us/step - loss: 0.6736 - acc: 0.4908 - val_loss: 0.6735 - val_acc: 0.4902\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6736 - acc: 0.4905 - val_loss: 0.6737 - val_acc: 0.4900\n",
      "Epoch 31/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6736 - acc: 0.4904 - val_loss: 0.6736 - val_acc: 0.4906\n",
      "Epoch 32/100\n",
      "900000/900000 [==============================] - 595s 661us/step - loss: 0.6736 - acc: 0.4906 - val_loss: 0.6740 - val_acc: 0.4902\n",
      "Epoch 33/100\n",
      "900000/900000 [==============================] - 598s 664us/step - loss: 0.6735 - acc: 0.4904 - val_loss: 0.6735 - val_acc: 0.4900\n",
      "Epoch 34/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6736 - acc: 0.4908 - val_loss: 0.6736 - val_acc: 0.4902\n",
      "Epoch 35/100\n",
      "900000/900000 [==============================] - 599s 666us/step - loss: 0.6735 - acc: 0.4905 - val_loss: 0.6734 - val_acc: 0.4897\n",
      "Epoch 36/100\n",
      "900000/900000 [==============================] - 599s 666us/step - loss: 0.6735 - acc: 0.4909 - val_loss: 0.6741 - val_acc: 0.4901\n",
      "Epoch 37/100\n",
      "900000/900000 [==============================] - 599s 666us/step - loss: 0.6735 - acc: 0.4904 - val_loss: 0.6736 - val_acc: 0.4897\n",
      "Epoch 38/100\n",
      "900000/900000 [==============================] - 597s 663us/step - loss: 0.6735 - acc: 0.4911 - val_loss: 0.6738 - val_acc: 0.4901\n",
      "trainnig theta = : 0.10833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 601s 667us/step - loss: 0.6780 - acc: 0.4902 - val_loss: 0.6781 - val_acc: 0.4896\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6780 - acc: 0.4910 - val_loss: 0.6781 - val_acc: 0.4900\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6780 - acc: 0.4907 - val_loss: 0.6779 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6780 - acc: 0.4905 - val_loss: 0.6786 - val_acc: 0.4903\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6780 - acc: 0.4904 - val_loss: 0.6781 - val_acc: 0.4897\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6779 - acc: 0.4906 - val_loss: 0.6779 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 590s 655us/step - loss: 0.6780 - acc: 0.4908 - val_loss: 0.6780 - val_acc: 0.4901\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 588s 653us/step - loss: 0.6780 - acc: 0.4906 - val_loss: 0.6780 - val_acc: 0.4895\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 590s 656us/step - loss: 0.6780 - acc: 0.4907 - val_loss: 0.6780 - val_acc: 0.4899\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 588s 653us/step - loss: 0.6779 - acc: 0.4906 - val_loss: 0.6784 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 588s 654us/step - loss: 0.6779 - acc: 0.4906 - val_loss: 0.6781 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 589s 654us/step - loss: 0.6780 - acc: 0.4909 - val_loss: 0.6780 - val_acc: 0.4902\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 589s 654us/step - loss: 0.6779 - acc: 0.4907 - val_loss: 0.6780 - val_acc: 0.4897\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 588s 653us/step - loss: 0.6779 - acc: 0.4908 - val_loss: 0.6781 - val_acc: 0.4899\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 587s 652us/step - loss: 0.6778 - acc: 0.4907 - val_loss: 0.6778 - val_acc: 0.4897\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 585s 650us/step - loss: 0.6779 - acc: 0.4907 - val_loss: 0.6782 - val_acc: 0.4897\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 585s 650us/step - loss: 0.6778 - acc: 0.4910 - val_loss: 0.6780 - val_acc: 0.4899\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 583s 648us/step - loss: 0.6779 - acc: 0.4909 - val_loss: 0.6781 - val_acc: 0.4899\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 581s 646us/step - loss: 0.6778 - acc: 0.4909 - val_loss: 0.6781 - val_acc: 0.4897\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6779 - acc: 0.4904 - val_loss: 0.6781 - val_acc: 0.4899\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 581s 645us/step - loss: 0.6778 - acc: 0.4910 - val_loss: 0.6785 - val_acc: 0.4897\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 581s 645us/step - loss: 0.6778 - acc: 0.4910 - val_loss: 0.6784 - val_acc: 0.4899\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 580s 645us/step - loss: 0.6778 - acc: 0.4908 - val_loss: 0.6781 - val_acc: 0.4899\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 579s 644us/step - loss: 0.6777 - acc: 0.4910 - val_loss: 0.6778 - val_acc: 0.4898\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 644s 715us/step - loss: 0.6777 - acc: 0.4912 - val_loss: 0.6778 - val_acc: 0.4900\n",
      "trainnig theta = : 0.11666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 674s 749us/step - loss: 0.6817 - acc: 0.4909 - val_loss: 0.6822 - val_acc: 0.4899\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 672s 746us/step - loss: 0.6817 - acc: 0.4910 - val_loss: 0.6820 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 692s 768us/step - loss: 0.6817 - acc: 0.4908 - val_loss: 0.6823 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6817 - acc: 0.4906 - val_loss: 0.6819 - val_acc: 0.4895\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 709s 787us/step - loss: 0.6816 - acc: 0.4907 - val_loss: 0.6817 - val_acc: 0.4901\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6817 - acc: 0.4907 - val_loss: 0.6818 - val_acc: 0.4896\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 711s 790us/step - loss: 0.6816 - acc: 0.4907 - val_loss: 0.6818 - val_acc: 0.4896\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 673s 748us/step - loss: 0.6816 - acc: 0.4909 - val_loss: 0.6817 - val_acc: 0.4900\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 682s 758us/step - loss: 0.6816 - acc: 0.4909 - val_loss: 0.6819 - val_acc: 0.4899\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6816 - acc: 0.4909 - val_loss: 0.6824 - val_acc: 0.4897\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 701s 778us/step - loss: 0.6816 - acc: 0.4909 - val_loss: 0.6819 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 714s 793us/step - loss: 0.6816 - acc: 0.4906 - val_loss: 0.6820 - val_acc: 0.4896\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 716s 795us/step - loss: 0.6817 - acc: 0.4908 - val_loss: 0.6819 - val_acc: 0.4897\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 714s 794us/step - loss: 0.6816 - acc: 0.4910 - val_loss: 0.6820 - val_acc: 0.4898\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6816 - acc: 0.4910 - val_loss: 0.6820 - val_acc: 0.4897\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6815 - acc: 0.4913 - val_loss: 0.6819 - val_acc: 0.4897\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6815 - acc: 0.4911 - val_loss: 0.6818 - val_acc: 0.4897\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6816 - acc: 0.4910 - val_loss: 0.6819 - val_acc: 0.4902\n",
      "trainnig theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6851 - acc: 0.4909 - val_loss: 0.6850 - val_acc: 0.4895\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6851 - acc: 0.4910 - val_loss: 0.6857 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6850 - acc: 0.4909 - val_loss: 0.6853 - val_acc: 0.4901\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6850 - acc: 0.4913 - val_loss: 0.6852 - val_acc: 0.4899\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6850 - acc: 0.4908 - val_loss: 0.6854 - val_acc: 0.4899\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 670s 744us/step - loss: 0.6850 - acc: 0.4908 - val_loss: 0.6851 - val_acc: 0.4897\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6850 - acc: 0.4910 - val_loss: 0.6849 - val_acc: 0.4897\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6850 - acc: 0.4909 - val_loss: 0.6851 - val_acc: 0.4895\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6850 - acc: 0.4910 - val_loss: 0.6851 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 703s 782us/step - loss: 0.6850 - acc: 0.4911 - val_loss: 0.6853 - val_acc: 0.4899\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6850 - acc: 0.4909 - val_loss: 0.6852 - val_acc: 0.4896\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6849 - acc: 0.4912 - val_loss: 0.6852 - val_acc: 0.4899\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6850 - acc: 0.4910 - val_loss: 0.6851 - val_acc: 0.4899\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6850 - acc: 0.4912 - val_loss: 0.6853 - val_acc: 0.4896\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 670s 744us/step - loss: 0.6850 - acc: 0.4913 - val_loss: 0.6854 - val_acc: 0.4898\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6849 - acc: 0.4912 - val_loss: 0.6852 - val_acc: 0.4897\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 680s 756us/step - loss: 0.6849 - acc: 0.4912 - val_loss: 0.6855 - val_acc: 0.4897\n",
      "trainnig theta = : 0.13333333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6882 - acc: 0.4912 - val_loss: 0.6883 - val_acc: 0.4898\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 693s 769us/step - loss: 0.6881 - acc: 0.4909 - val_loss: 0.6891 - val_acc: 0.4900\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6882 - acc: 0.4912 - val_loss: 0.6885 - val_acc: 0.4896\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6882 - acc: 0.4914 - val_loss: 0.6883 - val_acc: 0.4894\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 664s 738us/step - loss: 0.6881 - acc: 0.4914 - val_loss: 0.6885 - val_acc: 0.4893\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 669s 743us/step - loss: 0.6881 - acc: 0.4911 - val_loss: 0.6884 - val_acc: 0.4895\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 671s 746us/step - loss: 0.6881 - acc: 0.4916 - val_loss: 0.6885 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 678s 754us/step - loss: 0.6881 - acc: 0.4914 - val_loss: 0.6883 - val_acc: 0.4895\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6881 - acc: 0.4914 - val_loss: 0.6886 - val_acc: 0.4897\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6881 - acc: 0.4914 - val_loss: 0.6884 - val_acc: 0.4898\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 696s 773us/step - loss: 0.6881 - acc: 0.4913 - val_loss: 0.6883 - val_acc: 0.4899\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 697s 774us/step - loss: 0.6881 - acc: 0.4911 - val_loss: 0.6885 - val_acc: 0.4898\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 671s 745us/step - loss: 0.6881 - acc: 0.4916 - val_loss: 0.6885 - val_acc: 0.4897\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 669s 743us/step - loss: 0.6881 - acc: 0.4913 - val_loss: 0.6886 - val_acc: 0.4899\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 670s 745us/step - loss: 0.6881 - acc: 0.4914 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 676s 751us/step - loss: 0.6881 - acc: 0.4915 - val_loss: 0.6888 - val_acc: 0.4898\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6881 - acc: 0.4915 - val_loss: 0.6883 - val_acc: 0.4897\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6881 - acc: 0.4914 - val_loss: 0.6887 - val_acc: 0.4895\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6880 - acc: 0.4915 - val_loss: 0.6886 - val_acc: 0.4900\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6881 - acc: 0.4916 - val_loss: 0.6883 - val_acc: 0.4898\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 682s 757us/step - loss: 0.6881 - acc: 0.4917 - val_loss: 0.6883 - val_acc: 0.4896\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6881 - acc: 0.4913 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 674s 748us/step - loss: 0.6880 - acc: 0.4918 - val_loss: 0.6884 - val_acc: 0.4898\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 672s 747us/step - loss: 0.6880 - acc: 0.4917 - val_loss: 0.6883 - val_acc: 0.4898\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 697s 774us/step - loss: 0.6881 - acc: 0.4914 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6881 - acc: 0.4916 - val_loss: 0.6883 - val_acc: 0.4899\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6881 - acc: 0.4915 - val_loss: 0.6882 - val_acc: 0.4899\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6881 - acc: 0.4915 - val_loss: 0.6885 - val_acc: 0.4898\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 682s 758us/step - loss: 0.6880 - acc: 0.4915 - val_loss: 0.6885 - val_acc: 0.4897\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 659s 732us/step - loss: 0.6880 - acc: 0.4917 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 31/100\n",
      "900000/900000 [==============================] - 661s 735us/step - loss: 0.6880 - acc: 0.4917 - val_loss: 0.6886 - val_acc: 0.4897\n",
      "Epoch 32/100\n",
      "900000/900000 [==============================] - 664s 737us/step - loss: 0.6880 - acc: 0.4920 - val_loss: 0.6883 - val_acc: 0.4896\n",
      "Epoch 33/100\n",
      "900000/900000 [==============================] - 723s 803us/step - loss: 0.6880 - acc: 0.4916 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 34/100\n",
      "900000/900000 [==============================] - 709s 788us/step - loss: 0.6881 - acc: 0.4915 - val_loss: 0.6881 - val_acc: 0.4898\n",
      "Epoch 35/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6880 - acc: 0.4918 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 36/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6879 - acc: 0.4921 - val_loss: 0.6885 - val_acc: 0.4896\n",
      "Epoch 37/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6880 - acc: 0.4917 - val_loss: 0.6885 - val_acc: 0.4899\n",
      "Epoch 38/100\n",
      "900000/900000 [==============================] - 660s 733us/step - loss: 0.6879 - acc: 0.4919 - val_loss: 0.6886 - val_acc: 0.4895\n",
      "Epoch 39/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6880 - acc: 0.4919 - val_loss: 0.6883 - val_acc: 0.4897\n",
      "Epoch 40/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6880 - acc: 0.4922 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 41/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6879 - acc: 0.4919 - val_loss: 0.6884 - val_acc: 0.4898\n",
      "Epoch 42/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6880 - acc: 0.4917 - val_loss: 0.6883 - val_acc: 0.4899\n",
      "Epoch 43/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6880 - acc: 0.4919 - val_loss: 0.6886 - val_acc: 0.4899\n",
      "Epoch 44/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6879 - acc: 0.4918 - val_loss: 0.6886 - val_acc: 0.4896\n",
      "trainnig theta = : 0.14166666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6908 - acc: 0.4916 - val_loss: 0.6913 - val_acc: 0.4896\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6908 - acc: 0.4919 - val_loss: 0.6913 - val_acc: 0.4895\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 682s 758us/step - loss: 0.6908 - acc: 0.4920 - val_loss: 0.6912 - val_acc: 0.4900\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 712s 791us/step - loss: 0.6908 - acc: 0.4923 - val_loss: 0.6912 - val_acc: 0.4898\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 713s 793us/step - loss: 0.6908 - acc: 0.4920 - val_loss: 0.6911 - val_acc: 0.4895\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 713s 792us/step - loss: 0.6908 - acc: 0.4920 - val_loss: 0.6911 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 713s 793us/step - loss: 0.6908 - acc: 0.4922 - val_loss: 0.6911 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 711s 790us/step - loss: 0.6908 - acc: 0.4920 - val_loss: 0.6911 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 716s 796us/step - loss: 0.6907 - acc: 0.4922 - val_loss: 0.6912 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6907 - acc: 0.4923 - val_loss: 0.6911 - val_acc: 0.4896\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 714s 793us/step - loss: 0.6907 - acc: 0.4924 - val_loss: 0.6912 - val_acc: 0.4898\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6908 - acc: 0.4920 - val_loss: 0.6910 - val_acc: 0.4897\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 716s 796us/step - loss: 0.6907 - acc: 0.4922 - val_loss: 0.6910 - val_acc: 0.4898\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 709s 788us/step - loss: 0.6907 - acc: 0.4924 - val_loss: 0.6914 - val_acc: 0.4898\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6908 - acc: 0.4923 - val_loss: 0.6912 - val_acc: 0.4896\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 714s 793us/step - loss: 0.6907 - acc: 0.4922 - val_loss: 0.6910 - val_acc: 0.4897\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 712s 792us/step - loss: 0.6907 - acc: 0.4925 - val_loss: 0.6912 - val_acc: 0.4899\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6907 - acc: 0.4925 - val_loss: 0.6913 - val_acc: 0.4900\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 712s 791us/step - loss: 0.6907 - acc: 0.4926 - val_loss: 0.6912 - val_acc: 0.4897\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 716s 795us/step - loss: 0.6907 - acc: 0.4927 - val_loss: 0.6911 - val_acc: 0.4897\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 710s 789us/step - loss: 0.6907 - acc: 0.4923 - val_loss: 0.6913 - val_acc: 0.4895\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 723s 804us/step - loss: 0.6907 - acc: 0.4926 - val_loss: 0.6914 - val_acc: 0.4897\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 717s 796us/step - loss: 0.6907 - acc: 0.4926 - val_loss: 0.6911 - val_acc: 0.4900\n",
      "trainnig theta = : 0.15000000000000002\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 715s 795us/step - loss: 0.6930 - acc: 0.4928 - val_loss: 0.6934 - val_acc: 0.4900\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6930 - acc: 0.4925 - val_loss: 0.6935 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 708s 786us/step - loss: 0.6930 - acc: 0.4924 - val_loss: 0.6935 - val_acc: 0.4895\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 709s 788us/step - loss: 0.6930 - acc: 0.4924 - val_loss: 0.6933 - val_acc: 0.4898\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6929 - acc: 0.4926 - val_loss: 0.6933 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 707s 785us/step - loss: 0.6930 - acc: 0.4925 - val_loss: 0.6934 - val_acc: 0.4899\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6930 - acc: 0.4927 - val_loss: 0.6934 - val_acc: 0.4896\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6930 - acc: 0.4927 - val_loss: 0.6934 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6929 - acc: 0.4927 - val_loss: 0.6935 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6930 - acc: 0.4923 - val_loss: 0.6935 - val_acc: 0.4897\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6930 - acc: 0.4928 - val_loss: 0.6936 - val_acc: 0.4900\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6930 - acc: 0.4928 - val_loss: 0.6934 - val_acc: 0.4897\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6929 - acc: 0.4927 - val_loss: 0.6934 - val_acc: 0.4898\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6929 - acc: 0.4926 - val_loss: 0.6933 - val_acc: 0.4899\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 706s 785us/step - loss: 0.6929 - acc: 0.4927 - val_loss: 0.6934 - val_acc: 0.4899\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6930 - acc: 0.4930 - val_loss: 0.6935 - val_acc: 0.4899\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6929 - acc: 0.4932 - val_loss: 0.6934 - val_acc: 0.4895\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6929 - acc: 0.4932 - val_loss: 0.6934 - val_acc: 0.4899\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6929 - acc: 0.4933 - val_loss: 0.6936 - val_acc: 0.4899\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6929 - acc: 0.4930 - val_loss: 0.6934 - val_acc: 0.4897\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6930 - acc: 0.4926 - val_loss: 0.6935 - val_acc: 0.4897\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6929 - acc: 0.4931 - val_loss: 0.6935 - val_acc: 0.4897\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6929 - acc: 0.4930 - val_loss: 0.6934 - val_acc: 0.4896\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6929 - acc: 0.4927 - val_loss: 0.6934 - val_acc: 0.4898\n",
      "trainnig theta = : 0.15833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 708s 786us/step - loss: 0.6947 - acc: 0.4931 - val_loss: 0.6952 - val_acc: 0.4901\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6947 - acc: 0.4928 - val_loss: 0.6950 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 697s 774us/step - loss: 0.6947 - acc: 0.4934 - val_loss: 0.6950 - val_acc: 0.4897\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6947 - acc: 0.4932 - val_loss: 0.6951 - val_acc: 0.4901\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6947 - acc: 0.4930 - val_loss: 0.6952 - val_acc: 0.4898\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6947 - acc: 0.4934 - val_loss: 0.6951 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6946 - acc: 0.4934 - val_loss: 0.6951 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6947 - acc: 0.4933 - val_loss: 0.6951 - val_acc: 0.4901\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6946 - acc: 0.4937 - val_loss: 0.6951 - val_acc: 0.4898\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6946 - acc: 0.4935 - val_loss: 0.6951 - val_acc: 0.4899\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6947 - acc: 0.4932 - val_loss: 0.6953 - val_acc: 0.4901\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6946 - acc: 0.4936 - val_loss: 0.6951 - val_acc: 0.4897\n",
      "trainnig theta = : 0.16666666666666669\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6958 - acc: 0.4932 - val_loss: 0.6962 - val_acc: 0.4900\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6957 - acc: 0.4938 - val_loss: 0.6962 - val_acc: 0.4902\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 689s 765us/step - loss: 0.6957 - acc: 0.4931 - val_loss: 0.6961 - val_acc: 0.4899\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6958 - acc: 0.4936 - val_loss: 0.6961 - val_acc: 0.4901\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 686s 763us/step - loss: 0.6957 - acc: 0.4937 - val_loss: 0.6962 - val_acc: 0.4901\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 685s 762us/step - loss: 0.6957 - acc: 0.4935 - val_loss: 0.6962 - val_acc: 0.4904\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6958 - acc: 0.4938 - val_loss: 0.6962 - val_acc: 0.4897\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6957 - acc: 0.4936 - val_loss: 0.6962 - val_acc: 0.4899\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6957 - acc: 0.4939 - val_loss: 0.6962 - val_acc: 0.4898\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6957 - acc: 0.4938 - val_loss: 0.6962 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6957 - acc: 0.4939 - val_loss: 0.6961 - val_acc: 0.4900\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6957 - acc: 0.4944 - val_loss: 0.6962 - val_acc: 0.4900\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6957 - acc: 0.4941 - val_loss: 0.6962 - val_acc: 0.4902\n",
      "trainnig theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6964 - acc: 0.4932 - val_loss: 0.6967 - val_acc: 0.4901\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6963 - acc: 0.4939 - val_loss: 0.6967 - val_acc: 0.4902\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6963 - acc: 0.4937 - val_loss: 0.6968 - val_acc: 0.4901\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6963 - acc: 0.4937 - val_loss: 0.6967 - val_acc: 0.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6963 - acc: 0.4940 - val_loss: 0.6967 - val_acc: 0.4903\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6963 - acc: 0.4944 - val_loss: 0.6967 - val_acc: 0.4899\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 683s 758us/step - loss: 0.6963 - acc: 0.4942 - val_loss: 0.6967 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6963 - acc: 0.4937 - val_loss: 0.6967 - val_acc: 0.4899\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6963 - acc: 0.4939 - val_loss: 0.6967 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6963 - acc: 0.4945 - val_loss: 0.6967 - val_acc: 0.4904\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6965 - acc: 0.4943 - val_loss: 0.6967 - val_acc: 0.4906\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6964 - acc: 0.4945 - val_loss: 0.6969 - val_acc: 0.4901\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6964 - acc: 0.4939 - val_loss: 0.6968 - val_acc: 0.4902\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 680s 756us/step - loss: 0.6963 - acc: 0.4947 - val_loss: 0.6967 - val_acc: 0.4899\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 677s 753us/step - loss: 0.6963 - acc: 0.4949 - val_loss: 0.6967 - val_acc: 0.4903\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 697s 774us/step - loss: 0.6963 - acc: 0.4949 - val_loss: 0.6968 - val_acc: 0.4901\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 709s 787us/step - loss: 0.6963 - acc: 0.4944 - val_loss: 0.6968 - val_acc: 0.4903\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 710s 789us/step - loss: 0.6963 - acc: 0.4941 - val_loss: 0.6968 - val_acc: 0.4904\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 714s 793us/step - loss: 0.6963 - acc: 0.4947 - val_loss: 0.6967 - val_acc: 0.4904\n",
      "trainnig theta = : 0.18333333333333335\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 729s 810us/step - loss: 0.6963 - acc: 0.4950 - val_loss: 0.6967 - val_acc: 0.4899\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 725s 806us/step - loss: 0.6963 - acc: 0.4950 - val_loss: 0.6967 - val_acc: 0.4900\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 735s 817us/step - loss: 0.6963 - acc: 0.4942 - val_loss: 0.6967 - val_acc: 0.4903\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 737s 819us/step - loss: 0.6963 - acc: 0.4949 - val_loss: 0.6967 - val_acc: 0.4903\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6963 - acc: 0.4948 - val_loss: 0.6967 - val_acc: 0.4902\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 556s 618us/step - loss: 0.6963 - acc: 0.4951 - val_loss: 0.6967 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6963 - acc: 0.4947 - val_loss: 0.6967 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 597s 664us/step - loss: 0.6963 - acc: 0.4946 - val_loss: 0.6967 - val_acc: 0.4899\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 572s 635us/step - loss: 0.6963 - acc: 0.4948 - val_loss: 0.6967 - val_acc: 0.4902\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6962 - acc: 0.4952 - val_loss: 0.6968 - val_acc: 0.4903\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 621s 690us/step - loss: 0.6962 - acc: 0.4957 - val_loss: 0.6968 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 624s 694us/step - loss: 0.6962 - acc: 0.4958 - val_loss: 0.6967 - val_acc: 0.4905\n",
      "trainnig theta = : 0.19166666666666665\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 645s 716us/step - loss: 0.6960 - acc: 0.4953 - val_loss: 0.6962 - val_acc: 0.4907\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 642s 713us/step - loss: 0.6960 - acc: 0.4953 - val_loss: 0.6963 - val_acc: 0.4912\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 630s 700us/step - loss: 0.6959 - acc: 0.4952 - val_loss: 0.6963 - val_acc: 0.4907\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 633s 704us/step - loss: 0.6959 - acc: 0.4950 - val_loss: 0.6962 - val_acc: 0.4906\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 647s 719us/step - loss: 0.6959 - acc: 0.4957 - val_loss: 0.6963 - val_acc: 0.4907\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 645s 717us/step - loss: 0.6959 - acc: 0.4952 - val_loss: 0.6963 - val_acc: 0.4909\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 645s 717us/step - loss: 0.6959 - acc: 0.4965 - val_loss: 0.6962 - val_acc: 0.4907\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 639s 710us/step - loss: 0.6959 - acc: 0.4959 - val_loss: 0.6963 - val_acc: 0.4909\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 611s 678us/step - loss: 0.6959 - acc: 0.4959 - val_loss: 0.6964 - val_acc: 0.4910\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 617s 686us/step - loss: 0.6959 - acc: 0.4965 - val_loss: 0.6963 - val_acc: 0.4907\n",
      "trainnig theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 635s 706us/step - loss: 0.6952 - acc: 0.4956 - val_loss: 0.6954 - val_acc: 0.4918\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 629s 699us/step - loss: 0.6951 - acc: 0.4967 - val_loss: 0.6954 - val_acc: 0.4916\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 611s 679us/step - loss: 0.6951 - acc: 0.4964 - val_loss: 0.6953 - val_acc: 0.4916\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 624s 693us/step - loss: 0.6951 - acc: 0.4966 - val_loss: 0.6954 - val_acc: 0.4921\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 636s 707us/step - loss: 0.6951 - acc: 0.4972 - val_loss: 0.6953 - val_acc: 0.4913\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 631s 701us/step - loss: 0.6951 - acc: 0.4982 - val_loss: 0.6954 - val_acc: 0.4909\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 627s 697us/step - loss: 0.6951 - acc: 0.4979 - val_loss: 0.6954 - val_acc: 0.4920\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6951 - acc: 0.4975 - val_loss: 0.6955 - val_acc: 0.4920\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 607s 674us/step - loss: 0.6951 - acc: 0.4981 - val_loss: 0.6954 - val_acc: 0.4925\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6950 - acc: 0.4976 - val_loss: 0.6954 - val_acc: 0.4928\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 576s 640us/step - loss: 0.6950 - acc: 0.4982 - val_loss: 0.6954 - val_acc: 0.4930\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 590s 656us/step - loss: 0.6950 - acc: 0.4985 - val_loss: 0.6954 - val_acc: 0.4913\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6950 - acc: 0.4987 - val_loss: 0.6955 - val_acc: 0.4936\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 604s 671us/step - loss: 0.6950 - acc: 0.4988 - val_loss: 0.6953 - val_acc: 0.4919\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6950 - acc: 0.4991 - val_loss: 0.6954 - val_acc: 0.4940\n",
      "trainnig theta = : 0.20833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6939 - acc: 0.5003 - val_loss: 0.6941 - val_acc: 0.4929\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6939 - acc: 0.5010 - val_loss: 0.6942 - val_acc: 0.4961\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 601s 668us/step - loss: 0.6939 - acc: 0.5022 - val_loss: 0.6941 - val_acc: 0.4955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 601s 668us/step - loss: 0.6939 - acc: 0.5024 - val_loss: 0.6941 - val_acc: 0.4961\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 595s 661us/step - loss: 0.6939 - acc: 0.5028 - val_loss: 0.6942 - val_acc: 0.4978\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 580s 644us/step - loss: 0.6939 - acc: 0.5022 - val_loss: 0.6941 - val_acc: 0.4989\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6939 - acc: 0.5023 - val_loss: 0.6942 - val_acc: 0.4954\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 582s 647us/step - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6943 - val_acc: 0.4954\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 605s 672us/step - loss: 0.6939 - acc: 0.5021 - val_loss: 0.6941 - val_acc: 0.4965\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6939 - acc: 0.5029 - val_loss: 0.6942 - val_acc: 0.4958\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 604s 671us/step - loss: 0.6938 - acc: 0.5028 - val_loss: 0.6941 - val_acc: 0.4980\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 601s 667us/step - loss: 0.6939 - acc: 0.5026 - val_loss: 0.6942 - val_acc: 0.4977\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6938 - acc: 0.5049 - val_loss: 0.6941 - val_acc: 0.4965\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 601s 667us/step - loss: 0.6938 - acc: 0.5036 - val_loss: 0.6942 - val_acc: 0.4994\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6938 - acc: 0.5040 - val_loss: 0.6941 - val_acc: 0.4989\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 597s 664us/step - loss: 0.6938 - acc: 0.5049 - val_loss: 0.6942 - val_acc: 0.4996\n",
      "trainnig theta = : 0.21666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 599s 666us/step - loss: 0.6929 - acc: 0.5057 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 568s 631us/step - loss: 0.6929 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 569s 632us/step - loss: 0.6929 - acc: 0.5076 - val_loss: 0.6931 - val_acc: 0.5048\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 569s 632us/step - loss: 0.6929 - acc: 0.5080 - val_loss: 0.6931 - val_acc: 0.5043\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 568s 631us/step - loss: 0.6928 - acc: 0.5082 - val_loss: 0.6931 - val_acc: 0.5041\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 566s 629us/step - loss: 0.6929 - acc: 0.5074 - val_loss: 0.6931 - val_acc: 0.5046\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 566s 629us/step - loss: 0.6928 - acc: 0.5090 - val_loss: 0.6931 - val_acc: 0.5039\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 575s 639us/step - loss: 0.6928 - acc: 0.5088 - val_loss: 0.6931 - val_acc: 0.5017\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 573s 636us/step - loss: 0.6928 - acc: 0.5090 - val_loss: 0.6931 - val_acc: 0.5039\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 587s 652us/step - loss: 0.6928 - acc: 0.5092 - val_loss: 0.6931 - val_acc: 0.5035\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 558s 620us/step - loss: 0.6928 - acc: 0.5093 - val_loss: 0.6932 - val_acc: 0.5033\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 563s 625us/step - loss: 0.6928 - acc: 0.5081 - val_loss: 0.6931 - val_acc: 0.5033\n",
      "Epoch 13/100\n",
      "899000/900000 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.5092"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 25)   #iterating across possible StoUD values\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam',\n",
    "                        loss=my_loss_wrapper(myinputs, theta),\n",
    "                        metrics=['accuracy'])\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystopping])\n",
    "    lvals += [history.history['loss'][np.argmin(history.history['val_loss'])]]\n",
    "    vlvals += [np.min(history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T05:11:44.598035Z",
     "start_time": "2020-07-28T05:11:40.388149Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (25,) and (15,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2271f9c056d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"StoUD vs. Loss (Full Phase Space)\\n $F_{dropout} = \\phi_{dropout} = 0.2$\\nDCTR Change\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lvals'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vlvals'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'$\\theta$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2809\u001b[0m     return gca().plot(\n\u001b[1;32m   2810\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2811\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (25,) and (15,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"StoUD vs. Loss (Full Phase Space)\\n $F_{dropout} = \\phi_{dropout} = 0.2$\\nDCTR Change\")\n",
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.vlines(0.200, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"probStoUD(200) Vs Loss-FDropoutPhiDropout02-Copy5.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-25T20:56:52.058Z"
    }
   },
   "outputs": [],
   "source": [
    "thetas[np.argmax(vlvals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.342771Z",
     "start_time": "2020-07-19T00:03:27.336321Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.590548Z",
     "start_time": "2020-07-19T00:03:27.345029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_21[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = int(len(X_train_theta) / 10)\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1, MSE_loss=False):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        #  when not training theta, fetch as np array\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "\n",
    "    weights = reweight(events = x, param = theta0) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.821775Z",
     "start_time": "2020-07-19T00:03:27.593159Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 151s 167us/step - loss: 0.5987 - acc: 0.4972 - val_loss: 0.5872 - val_acc: 0.4956\n",
      ". theta fit =  0.0\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "1800000/1800000 [==============================] - 225s 125us/step - loss: nan - acc: 0.4957         \n",
      ". theta fit =  nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'argrelmin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f6db20d6f348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     extrema = np.concatenate(\n\u001b[0;32m---> 36\u001b[0;31m         (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mextrema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextrema\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextrema\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mindex_refine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argrelmin' is not defined"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration + 1)\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=1000,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train_theta, Y_train_theta, epochs=1, batch_size=batch_size,verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "\n",
    "        mean_fit = np.array([\n",
    "            np.mean(fit_vals_recent[len(fit_vals_recent) -\n",
    "                                    4:len(fit_vals_recent)])\n",
    "        ])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.823619Z",
     "start_time": "2020-07-17T18:54:03.260Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.200, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
