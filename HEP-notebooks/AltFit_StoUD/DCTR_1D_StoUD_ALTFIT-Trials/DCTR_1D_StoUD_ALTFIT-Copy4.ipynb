{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTR Alternative Fitting Algorithm for probStoUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:34.437434Z",
     "start_time": "2020-07-25T20:56:34.432637Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:36.647808Z",
     "start_time": "2020-07-25T20:56:34.440773Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:36.654265Z",
     "start_time": "2020-07-25T20:56:36.650007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22)\n",
    "rc('xtick', labelsize=15)\n",
    "rc('ytick', labelsize=15)\n",
    "rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:36.668368Z",
     "start_time": "2020-07-25T20:56:36.656194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:36.679022Z",
     "start_time": "2020-07-25T20:56:36.673129Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:36.688390Z",
     "start_time": "2020-07-25T20:56:36.680953Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:36.697782Z",
     "start_time": "2020-07-25T20:56:36.693126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/data0/users/aandreassen/zenodo/'\n",
    "data_dir1 = '/data1/users/asuresh/DCTRFitting/StoUDFitting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:44.252282Z",
     "start_time": "2020-07-25T20:56:36.700223Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "#unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')\n",
    "unknown_dataset =  np.load(data_dir1 + 'test1D_strange200.npz', allow_pickle=True)['dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:01:16.967419Z",
     "start_time": "2020-07-25T20:56:44.254253Z"
    }
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:01:18.854417Z",
     "start_time": "2020-07-25T21:01:16.971660Z"
    }
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:01:31.858307Z",
     "start_time": "2020-07-25T21:01:18.857468Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.5, shuffle=True)\n",
    "X_train_theta, X_test_theta, Y_train_theta, Y_test_theta = data_split(X_fit, Y_fit, test=0., shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:01:32.893139Z",
     "start_time": "2020-07-25T21:01:31.866735Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100, 100, 128)\n",
    "F_sizes = (100, 100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights(\n",
    "    './saved_models/DCTR_ee_dijets_1D_probStoUD_Copy4.h5')  #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:01:32.908681Z",
     "start_time": "2020-07-25T21:01:32.897775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "def reweight(events, param):  #from NN (DCTR)\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, param]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(events[...,0])>0,\n",
    "                                   K.ones_like(events[...,0]),\n",
    "                                   K.zeros_like(events[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "\n",
    "    model_inputs = K.concatenate([events, concat_input_and_params], -1)\n",
    "    # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    \n",
    "    f = dctr.model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:01:33.910470Z",
     "start_time": "2020-07-25T21:01:32.916891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = PFN(input_dim=4,\n",
    "            Phi_sizes=Phi_sizes,\n",
    "            F_sizes=F_sizes,\n",
    "            latent_dropout= 0.2,\n",
    "            F_dropouts= 0.2,\n",
    "            output_dim=1,\n",
    "            output_act='sigmoid',\n",
    "            summary=False)\n",
    "reinitialize_weights = model.model.get_weights()\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0., MSE_loss = False):\n",
    "    x = inputs  #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(\n",
    "        x, np.arange(51),\n",
    "        axis=1)  # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    weights = reweight(events = x, param = val)  # NN reweight\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-25T20:56:34.443Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-13-2bac8f3c1b35>:9: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 402s 446us/step - loss: 0.6942 - acc: 0.4971 - val_loss: 0.6825 - val_acc: 0.4963\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 439s 488us/step - loss: 0.6821 - acc: 0.4948 - val_loss: 0.6783 - val_acc: 0.4941\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 481s 535us/step - loss: 0.6785 - acc: 0.4938 - val_loss: 0.6754 - val_acc: 0.4920\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 516s 573us/step - loss: 0.6757 - acc: 0.4905 - val_loss: 0.6744 - val_acc: 0.4911\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 545s 606us/step - loss: 0.6747 - acc: 0.4902 - val_loss: 0.6747 - val_acc: 0.4910\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 661s 734us/step - loss: 0.6742 - acc: 0.4903 - val_loss: 0.6738 - val_acc: 0.4908\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 611s 678us/step - loss: 0.6741 - acc: 0.4900 - val_loss: 0.6735 - val_acc: 0.4911\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 609s 677us/step - loss: 0.6739 - acc: 0.4896 - val_loss: 0.6740 - val_acc: 0.4904\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 613s 681us/step - loss: 0.6738 - acc: 0.4898 - val_loss: 0.6739 - val_acc: 0.4911\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 608s 675us/step - loss: 0.6738 - acc: 0.4897 - val_loss: 0.6737 - val_acc: 0.4906\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 608s 676us/step - loss: 0.6737 - acc: 0.4896 - val_loss: 0.6733 - val_acc: 0.4906\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 609s 677us/step - loss: 0.6736 - acc: 0.4897 - val_loss: 0.6739 - val_acc: 0.4906\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6736 - acc: 0.4897 - val_loss: 0.6737 - val_acc: 0.4902\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6735 - acc: 0.4901 - val_loss: 0.6732 - val_acc: 0.4904\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 609s 676us/step - loss: 0.6735 - acc: 0.4901 - val_loss: 0.6737 - val_acc: 0.4904\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6735 - acc: 0.4899 - val_loss: 0.6733 - val_acc: 0.4902\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6735 - acc: 0.4897 - val_loss: 0.6737 - val_acc: 0.4902\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6734 - acc: 0.4898 - val_loss: 0.6737 - val_acc: 0.4905\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6734 - acc: 0.4898 - val_loss: 0.6730 - val_acc: 0.4903\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6734 - acc: 0.4899 - val_loss: 0.6737 - val_acc: 0.4906\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6734 - acc: 0.4898 - val_loss: 0.6736 - val_acc: 0.4903\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6733 - acc: 0.4901 - val_loss: 0.6736 - val_acc: 0.4900\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6734 - acc: 0.4900 - val_loss: 0.6734 - val_acc: 0.4905\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 604s 671us/step - loss: 0.6734 - acc: 0.4902 - val_loss: 0.6740 - val_acc: 0.4904\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6734 - acc: 0.4897 - val_loss: 0.6733 - val_acc: 0.4903\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 601s 668us/step - loss: 0.6734 - acc: 0.4900 - val_loss: 0.6738 - val_acc: 0.4902\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 598s 664us/step - loss: 0.6733 - acc: 0.4900 - val_loss: 0.6734 - val_acc: 0.4901\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6734 - acc: 0.4898 - val_loss: 0.6740 - val_acc: 0.4901\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 601s 668us/step - loss: 0.6733 - acc: 0.4901 - val_loss: 0.6742 - val_acc: 0.4901\n",
      "trainnig theta = : 0.10833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6765 - acc: 0.4901 - val_loss: 0.6767 - val_acc: 0.4901\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6765 - acc: 0.4901 - val_loss: 0.6767 - val_acc: 0.4902\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 583s 648us/step - loss: 0.6765 - acc: 0.4900 - val_loss: 0.6768 - val_acc: 0.4904\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6764 - acc: 0.4902 - val_loss: 0.6769 - val_acc: 0.4904\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6765 - acc: 0.4898 - val_loss: 0.6765 - val_acc: 0.4902\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 587s 652us/step - loss: 0.6764 - acc: 0.4900 - val_loss: 0.6772 - val_acc: 0.4903\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6764 - acc: 0.4898 - val_loss: 0.6764 - val_acc: 0.4903\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6764 - acc: 0.4902 - val_loss: 0.6763 - val_acc: 0.4904\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 583s 648us/step - loss: 0.6764 - acc: 0.4904 - val_loss: 0.6767 - val_acc: 0.4905\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 581s 646us/step - loss: 0.6764 - acc: 0.4902 - val_loss: 0.6765 - val_acc: 0.4902\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 583s 648us/step - loss: 0.6764 - acc: 0.4901 - val_loss: 0.6766 - val_acc: 0.4901\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 580s 645us/step - loss: 0.6764 - acc: 0.4902 - val_loss: 0.6767 - val_acc: 0.4903\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 580s 644us/step - loss: 0.6763 - acc: 0.4900 - val_loss: 0.6766 - val_acc: 0.4902\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 580s 645us/step - loss: 0.6764 - acc: 0.4900 - val_loss: 0.6762 - val_acc: 0.4903\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 580s 644us/step - loss: 0.6763 - acc: 0.4902 - val_loss: 0.6775 - val_acc: 0.4903\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6763 - acc: 0.4902 - val_loss: 0.6769 - val_acc: 0.4899\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 576s 639us/step - loss: 0.6764 - acc: 0.4902 - val_loss: 0.6766 - val_acc: 0.4903\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 578s 642us/step - loss: 0.6763 - acc: 0.4904 - val_loss: 0.6771 - val_acc: 0.4903\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 575s 638us/step - loss: 0.6763 - acc: 0.4901 - val_loss: 0.6767 - val_acc: 0.4900\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 576s 640us/step - loss: 0.6763 - acc: 0.4902 - val_loss: 0.6765 - val_acc: 0.4901\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 576s 640us/step - loss: 0.6763 - acc: 0.4906 - val_loss: 0.6774 - val_acc: 0.4904\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6763 - acc: 0.4903 - val_loss: 0.6772 - val_acc: 0.4903\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 575s 639us/step - loss: 0.6763 - acc: 0.4905 - val_loss: 0.6768 - val_acc: 0.4902\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 577s 642us/step - loss: 0.6763 - acc: 0.4902 - val_loss: 0.6767 - val_acc: 0.4903\n",
      "trainnig theta = : 0.11666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 581s 646us/step - loss: 0.6795 - acc: 0.4903 - val_loss: 0.6800 - val_acc: 0.4905\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 576s 640us/step - loss: 0.6795 - acc: 0.4902 - val_loss: 0.6797 - val_acc: 0.4899\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 573s 636us/step - loss: 0.6795 - acc: 0.4902 - val_loss: 0.6794 - val_acc: 0.4901\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 574s 638us/step - loss: 0.6795 - acc: 0.4902 - val_loss: 0.6795 - val_acc: 0.4902\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 573s 636us/step - loss: 0.6795 - acc: 0.4902 - val_loss: 0.6799 - val_acc: 0.4902\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 572s 635us/step - loss: 0.6795 - acc: 0.4902 - val_loss: 0.6801 - val_acc: 0.4902\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 567s 630us/step - loss: 0.6795 - acc: 0.4906 - val_loss: 0.6805 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 569s 632us/step - loss: 0.6795 - acc: 0.4906 - val_loss: 0.6799 - val_acc: 0.4905\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 568s 631us/step - loss: 0.6795 - acc: 0.4903 - val_loss: 0.6799 - val_acc: 0.4902\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 639s 710us/step - loss: 0.6795 - acc: 0.4904 - val_loss: 0.6802 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 660s 734us/step - loss: 0.6795 - acc: 0.4903 - val_loss: 0.6803 - val_acc: 0.4901\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 662s 736us/step - loss: 0.6794 - acc: 0.4902 - val_loss: 0.6797 - val_acc: 0.4901\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6794 - acc: 0.4905 - val_loss: 0.6796 - val_acc: 0.4904\n",
      "trainnig theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6827 - acc: 0.4903 - val_loss: 0.6828 - val_acc: 0.4901\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6827 - acc: 0.4905 - val_loss: 0.6830 - val_acc: 0.4903\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6827 - acc: 0.4902 - val_loss: 0.6826 - val_acc: 0.4900\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6827 - acc: 0.4902 - val_loss: 0.6834 - val_acc: 0.4903\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 662s 736us/step - loss: 0.6827 - acc: 0.4902 - val_loss: 0.6830 - val_acc: 0.4902\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 670s 744us/step - loss: 0.6827 - acc: 0.4903 - val_loss: 0.6831 - val_acc: 0.4903\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 673s 748us/step - loss: 0.6827 - acc: 0.4905 - val_loss: 0.6828 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6827 - acc: 0.4906 - val_loss: 0.6830 - val_acc: 0.4902\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6827 - acc: 0.4906 - val_loss: 0.6825 - val_acc: 0.4908\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6827 - acc: 0.4905 - val_loss: 0.6832 - val_acc: 0.4905\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6827 - acc: 0.4904 - val_loss: 0.6833 - val_acc: 0.4904\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6827 - acc: 0.4903 - val_loss: 0.6827 - val_acc: 0.4903\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 669s 743us/step - loss: 0.6827 - acc: 0.4904 - val_loss: 0.6832 - val_acc: 0.4902\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 670s 744us/step - loss: 0.6827 - acc: 0.4904 - val_loss: 0.6833 - val_acc: 0.4903\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 666s 740us/step - loss: 0.6826 - acc: 0.4904 - val_loss: 0.6826 - val_acc: 0.4903\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 679s 755us/step - loss: 0.6826 - acc: 0.4905 - val_loss: 0.6830 - val_acc: 0.4901\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6827 - acc: 0.4904 - val_loss: 0.6827 - val_acc: 0.4904\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6826 - acc: 0.4905 - val_loss: 0.6831 - val_acc: 0.4903\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6826 - acc: 0.4904 - val_loss: 0.6828 - val_acc: 0.4900\n",
      "trainnig theta = : 0.13333333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6858 - acc: 0.4905 - val_loss: 0.6861 - val_acc: 0.4903\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 673s 747us/step - loss: 0.6857 - acc: 0.4907 - val_loss: 0.6858 - val_acc: 0.4901\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 665s 738us/step - loss: 0.6858 - acc: 0.4906 - val_loss: 0.6861 - val_acc: 0.4901\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 665s 739us/step - loss: 0.6858 - acc: 0.4908 - val_loss: 0.6859 - val_acc: 0.4902\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 671s 746us/step - loss: 0.6858 - acc: 0.4903 - val_loss: 0.6861 - val_acc: 0.4904\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6857 - acc: 0.4903 - val_loss: 0.6859 - val_acc: 0.4902\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6858 - acc: 0.4907 - val_loss: 0.6862 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6857 - acc: 0.4906 - val_loss: 0.6860 - val_acc: 0.4903\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6858 - acc: 0.4904 - val_loss: 0.6860 - val_acc: 0.4905\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 676s 751us/step - loss: 0.6858 - acc: 0.4903 - val_loss: 0.6859 - val_acc: 0.4903\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 662s 736us/step - loss: 0.6858 - acc: 0.4906 - val_loss: 0.6858 - val_acc: 0.4903\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 664s 738us/step - loss: 0.6857 - acc: 0.4905 - val_loss: 0.6861 - val_acc: 0.4902\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 664s 738us/step - loss: 0.6857 - acc: 0.4908 - val_loss: 0.6857 - val_acc: 0.4903\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6857 - acc: 0.4908 - val_loss: 0.6861 - val_acc: 0.4902\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6857 - acc: 0.4907 - val_loss: 0.6858 - val_acc: 0.4903\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6857 - acc: 0.4907 - val_loss: 0.6861 - val_acc: 0.4901\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 697s 774us/step - loss: 0.6857 - acc: 0.4907 - val_loss: 0.6858 - val_acc: 0.4902\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6857 - acc: 0.4909 - val_loss: 0.6861 - val_acc: 0.4904\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 658s 731us/step - loss: 0.6857 - acc: 0.4906 - val_loss: 0.6860 - val_acc: 0.4904\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 664s 737us/step - loss: 0.6857 - acc: 0.4911 - val_loss: 0.6857 - val_acc: 0.4901\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 663s 737us/step - loss: 0.6857 - acc: 0.4908 - val_loss: 0.6860 - val_acc: 0.4904\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6856 - acc: 0.4912 - val_loss: 0.6858 - val_acc: 0.4902\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6857 - acc: 0.4909 - val_loss: 0.6860 - val_acc: 0.4905\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6857 - acc: 0.4911 - val_loss: 0.6863 - val_acc: 0.4903\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 696s 774us/step - loss: 0.6857 - acc: 0.4912 - val_loss: 0.6861 - val_acc: 0.4907\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6856 - acc: 0.4911 - val_loss: 0.6867 - val_acc: 0.4903\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 652s 724us/step - loss: 0.6857 - acc: 0.4909 - val_loss: 0.6860 - val_acc: 0.4904\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6857 - acc: 0.4910 - val_loss: 0.6860 - val_acc: 0.4905\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6857 - acc: 0.4909 - val_loss: 0.6859 - val_acc: 0.4901\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6857 - acc: 0.4911 - val_loss: 0.6861 - val_acc: 0.4904\n",
      "trainnig theta = : 0.14166666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6885 - acc: 0.4909 - val_loss: 0.6886 - val_acc: 0.4902\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6886 - acc: 0.4912 - val_loss: 0.6888 - val_acc: 0.4904\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6885 - acc: 0.4909 - val_loss: 0.6886 - val_acc: 0.4902\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6885 - acc: 0.4913 - val_loss: 0.6888 - val_acc: 0.4903\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 659s 732us/step - loss: 0.6886 - acc: 0.4912 - val_loss: 0.6889 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6885 - acc: 0.4910 - val_loss: 0.6888 - val_acc: 0.4902\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 670s 744us/step - loss: 0.6885 - acc: 0.4913 - val_loss: 0.6887 - val_acc: 0.4904\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6885 - acc: 0.4910 - val_loss: 0.6888 - val_acc: 0.4905\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 699s 776us/step - loss: 0.6885 - acc: 0.4915 - val_loss: 0.6890 - val_acc: 0.4905\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 697s 774us/step - loss: 0.6886 - acc: 0.4909 - val_loss: 0.6888 - val_acc: 0.4904\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 695s 773us/step - loss: 0.6885 - acc: 0.4914 - val_loss: 0.6889 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6885 - acc: 0.4912 - val_loss: 0.6887 - val_acc: 0.4902\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 660s 733us/step - loss: 0.6885 - acc: 0.4912 - val_loss: 0.6888 - val_acc: 0.4905\n",
      "trainnig theta = : 0.15000000000000002\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 665s 739us/step - loss: 0.6911 - acc: 0.4910 - val_loss: 0.6913 - val_acc: 0.4902\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 662s 736us/step - loss: 0.6910 - acc: 0.4913 - val_loss: 0.6913 - val_acc: 0.4907\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6910 - acc: 0.4916 - val_loss: 0.6911 - val_acc: 0.4904\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 718s 797us/step - loss: 0.6910 - acc: 0.4914 - val_loss: 0.6911 - val_acc: 0.4905\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 702s 781us/step - loss: 0.6910 - acc: 0.4911 - val_loss: 0.6911 - val_acc: 0.4903\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 696s 774us/step - loss: 0.6911 - acc: 0.4914 - val_loss: 0.6913 - val_acc: 0.4906\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 696s 774us/step - loss: 0.6911 - acc: 0.4912 - val_loss: 0.6911 - val_acc: 0.4903\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6910 - acc: 0.4916 - val_loss: 0.6912 - val_acc: 0.4904\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6910 - acc: 0.4916 - val_loss: 0.6912 - val_acc: 0.4902\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 719s 799us/step - loss: 0.6910 - acc: 0.4914 - val_loss: 0.6911 - val_acc: 0.4906\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 653s 725us/step - loss: 0.6910 - acc: 0.4914 - val_loss: 0.6915 - val_acc: 0.4904\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6911 - acc: 0.4911 - val_loss: 0.6913 - val_acc: 0.4904\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 700s 777us/step - loss: 0.6910 - acc: 0.4917 - val_loss: 0.6912 - val_acc: 0.4903\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6910 - acc: 0.4914 - val_loss: 0.6913 - val_acc: 0.4900\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 696s 774us/step - loss: 0.6910 - acc: 0.4914 - val_loss: 0.6913 - val_acc: 0.4907\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6910 - acc: 0.4915 - val_loss: 0.6912 - val_acc: 0.4903\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6910 - acc: 0.4915 - val_loss: 0.6912 - val_acc: 0.4903\n",
      "trainnig theta = : 0.15833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6930 - acc: 0.4915 - val_loss: 0.6931 - val_acc: 0.4906\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6931 - acc: 0.4917 - val_loss: 0.6931 - val_acc: 0.4905\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6930 - acc: 0.4916 - val_loss: 0.6932 - val_acc: 0.4903\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6930 - acc: 0.4924 - val_loss: 0.6932 - val_acc: 0.4906\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6930 - acc: 0.4919 - val_loss: 0.6932 - val_acc: 0.4907\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6930 - acc: 0.4920 - val_loss: 0.6930 - val_acc: 0.4903\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6930 - acc: 0.4917 - val_loss: 0.6932 - val_acc: 0.4906\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 702s 781us/step - loss: 0.6930 - acc: 0.4918 - val_loss: 0.6933 - val_acc: 0.4904\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6930 - acc: 0.4920 - val_loss: 0.6932 - val_acc: 0.4906\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6929 - acc: 0.4921 - val_loss: 0.6931 - val_acc: 0.4905\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6930 - acc: 0.4919 - val_loss: 0.6932 - val_acc: 0.4906\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6929 - acc: 0.4924 - val_loss: 0.6932 - val_acc: 0.4908\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6930 - acc: 0.4922 - val_loss: 0.6934 - val_acc: 0.4906\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6930 - acc: 0.4924 - val_loss: 0.6932 - val_acc: 0.4904\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6930 - acc: 0.4921 - val_loss: 0.6933 - val_acc: 0.4907\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6930 - acc: 0.4920 - val_loss: 0.6933 - val_acc: 0.4907\n",
      "trainnig theta = : 0.16666666666666669\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6943 - acc: 0.4922 - val_loss: 0.6944 - val_acc: 0.4903\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6943 - acc: 0.4926 - val_loss: 0.6945 - val_acc: 0.4908\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6943 - acc: 0.4922 - val_loss: 0.6944 - val_acc: 0.4905\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6943 - acc: 0.4925 - val_loss: 0.6945 - val_acc: 0.4907\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 703s 782us/step - loss: 0.6943 - acc: 0.4919 - val_loss: 0.6945 - val_acc: 0.4911\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6942 - acc: 0.4926 - val_loss: 0.6945 - val_acc: 0.4904\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 701s 778us/step - loss: 0.6943 - acc: 0.4925 - val_loss: 0.6944 - val_acc: 0.4908\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6943 - acc: 0.4921 - val_loss: 0.6944 - val_acc: 0.4909\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6943 - acc: 0.4920 - val_loss: 0.6945 - val_acc: 0.4910\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6943 - acc: 0.4923 - val_loss: 0.6945 - val_acc: 0.4908\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6943 - acc: 0.4924 - val_loss: 0.6944 - val_acc: 0.4910\n",
      "trainnig theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 707s 785us/step - loss: 0.6948 - acc: 0.4920 - val_loss: 0.6950 - val_acc: 0.4905\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6948 - acc: 0.4931 - val_loss: 0.6950 - val_acc: 0.4908\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 699s 776us/step - loss: 0.6948 - acc: 0.4922 - val_loss: 0.6950 - val_acc: 0.4909\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 695s 773us/step - loss: 0.6948 - acc: 0.4929 - val_loss: 0.6949 - val_acc: 0.4913\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6948 - acc: 0.4934 - val_loss: 0.6949 - val_acc: 0.4908\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6948 - acc: 0.4931 - val_loss: 0.6949 - val_acc: 0.4908\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6948 - acc: 0.4930 - val_loss: 0.6951 - val_acc: 0.4917\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6948 - acc: 0.4935 - val_loss: 0.6950 - val_acc: 0.4912\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6948 - acc: 0.4930 - val_loss: 0.6950 - val_acc: 0.4910\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6948 - acc: 0.4932 - val_loss: 0.6950 - val_acc: 0.4909\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6948 - acc: 0.4928 - val_loss: 0.6951 - val_acc: 0.4915\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 699s 776us/step - loss: 0.6948 - acc: 0.4930 - val_loss: 0.6950 - val_acc: 0.4909\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 696s 774us/step - loss: 0.6948 - acc: 0.4929 - val_loss: 0.6950 - val_acc: 0.4907\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6948 - acc: 0.4931 - val_loss: 0.6950 - val_acc: 0.4910\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6948 - acc: 0.4932 - val_loss: 0.6950 - val_acc: 0.4909\n",
      "trainnig theta = : 0.18333333333333335\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6947 - acc: 0.4934 - val_loss: 0.6948 - val_acc: 0.4917\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6947 - acc: 0.4935 - val_loss: 0.6948 - val_acc: 0.4914\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6947 - acc: 0.4940 - val_loss: 0.6948 - val_acc: 0.4914\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6947 - acc: 0.4941 - val_loss: 0.6947 - val_acc: 0.4912\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6947 - acc: 0.4941 - val_loss: 0.6947 - val_acc: 0.4912\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 689s 765us/step - loss: 0.6946 - acc: 0.4942 - val_loss: 0.6948 - val_acc: 0.4913\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6946 - acc: 0.4939 - val_loss: 0.6948 - val_acc: 0.4909\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 689s 765us/step - loss: 0.6946 - acc: 0.4943 - val_loss: 0.6949 - val_acc: 0.4924\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6946 - acc: 0.4944 - val_loss: 0.6948 - val_acc: 0.4915\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6946 - acc: 0.4951 - val_loss: 0.6948 - val_acc: 0.4910\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6946 - acc: 0.4942 - val_loss: 0.6948 - val_acc: 0.4912\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6946 - acc: 0.4946 - val_loss: 0.6948 - val_acc: 0.4909\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6946 - acc: 0.4950 - val_loss: 0.6948 - val_acc: 0.4918\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6946 - acc: 0.4956 - val_loss: 0.6948 - val_acc: 0.4915\n",
      "trainnig theta = : 0.19166666666666665\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6939 - acc: 0.4953 - val_loss: 0.6939 - val_acc: 0.4921\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6938 - acc: 0.4947 - val_loss: 0.6939 - val_acc: 0.4924\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6938 - acc: 0.4949 - val_loss: 0.6940 - val_acc: 0.4947\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6938 - acc: 0.4968 - val_loss: 0.6939 - val_acc: 0.4921\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6938 - acc: 0.4960 - val_loss: 0.6939 - val_acc: 0.4923\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 682s 758us/step - loss: 0.6938 - acc: 0.4959 - val_loss: 0.6940 - val_acc: 0.4930\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6938 - acc: 0.4967 - val_loss: 0.6939 - val_acc: 0.4949\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6938 - acc: 0.4963 - val_loss: 0.6939 - val_acc: 0.4928\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 684s 759us/step - loss: 0.6938 - acc: 0.4974 - val_loss: 0.6939 - val_acc: 0.4933\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 682s 758us/step - loss: 0.6938 - acc: 0.4975 - val_loss: 0.6940 - val_acc: 0.4935\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 682s 757us/step - loss: 0.6938 - acc: 0.4983 - val_loss: 0.6940 - val_acc: 0.4941\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6938 - acc: 0.4967 - val_loss: 0.6940 - val_acc: 0.4955\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 682s 757us/step - loss: 0.6938 - acc: 0.4981 - val_loss: 0.6939 - val_acc: 0.4926\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 683s 758us/step - loss: 0.6938 - acc: 0.4969 - val_loss: 0.6939 - val_acc: 0.4935\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6938 - acc: 0.4974 - val_loss: 0.6940 - val_acc: 0.4936\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 680s 756us/step - loss: 0.6938 - acc: 0.4985 - val_loss: 0.6939 - val_acc: 0.4944\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 681s 756us/step - loss: 0.6938 - acc: 0.4982 - val_loss: 0.6940 - val_acc: 0.4931\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 681s 756us/step - loss: 0.6938 - acc: 0.4971 - val_loss: 0.6940 - val_acc: 0.4945\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 680s 756us/step - loss: 0.6938 - acc: 0.4985 - val_loss: 0.6940 - val_acc: 0.4937\n",
      "trainnig theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6924 - acc: 0.4986 - val_loss: 0.6926 - val_acc: 0.4968\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 676s 752us/step - loss: 0.6924 - acc: 0.4983 - val_loss: 0.6926 - val_acc: 0.4965\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 680s 755us/step - loss: 0.6925 - acc: 0.5006 - val_loss: 0.6926 - val_acc: 0.4954\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 681s 756us/step - loss: 0.6925 - acc: 0.4994 - val_loss: 0.6926 - val_acc: 0.4954\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 680s 755us/step - loss: 0.6924 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.4953\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6925 - acc: 0.5004 - val_loss: 0.6926 - val_acc: 0.4964\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 674s 748us/step - loss: 0.6924 - acc: 0.5007 - val_loss: 0.6926 - val_acc: 0.4960\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 672s 747us/step - loss: 0.6924 - acc: 0.5003 - val_loss: 0.6926 - val_acc: 0.4972\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 674s 749us/step - loss: 0.6924 - acc: 0.5011 - val_loss: 0.6926 - val_acc: 0.4958\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 674s 748us/step - loss: 0.6924 - acc: 0.5006 - val_loss: 0.6926 - val_acc: 0.4984\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6924 - acc: 0.5013 - val_loss: 0.6926 - val_acc: 0.4980\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6924 - acc: 0.5007 - val_loss: 0.6926 - val_acc: 0.4956\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6924 - acc: 0.5015 - val_loss: 0.6926 - val_acc: 0.4975\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6924 - acc: 0.5015 - val_loss: 0.6926 - val_acc: 0.4973\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 711s 791us/step - loss: 0.6924 - acc: 0.5005 - val_loss: 0.6926 - val_acc: 0.4958\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 717s 797us/step - loss: 0.6924 - acc: 0.5013 - val_loss: 0.6926 - val_acc: 0.4957\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 726s 807us/step - loss: 0.6924 - acc: 0.5021 - val_loss: 0.6926 - val_acc: 0.4975\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 732s 813us/step - loss: 0.6924 - acc: 0.5017 - val_loss: 0.6927 - val_acc: 0.4976\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 775s 862us/step - loss: 0.6924 - acc: 0.5022 - val_loss: 0.6927 - val_acc: 0.4961\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 547s 607us/step - loss: 0.6924 - acc: 0.5031 - val_loss: 0.6926 - val_acc: 0.4973\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 568s 631us/step - loss: 0.6923 - acc: 0.5030 - val_loss: 0.6926 - val_acc: 0.4979\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 582s 647us/step - loss: 0.6924 - acc: 0.5025 - val_loss: 0.6926 - val_acc: 0.4959\n",
      "trainnig theta = : 0.20833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 597s 663us/step - loss: 0.6908 - acc: 0.5035 - val_loss: 0.6909 - val_acc: 0.5013\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6907 - acc: 0.5046 - val_loss: 0.6909 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 609s 676us/step - loss: 0.6907 - acc: 0.5055 - val_loss: 0.6910 - val_acc: 0.5006\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 622s 691us/step - loss: 0.6907 - acc: 0.5054 - val_loss: 0.6909 - val_acc: 0.5043\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 618s 687us/step - loss: 0.6907 - acc: 0.5072 - val_loss: 0.6910 - val_acc: 0.5021\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 645s 717us/step - loss: 0.6907 - acc: 0.5059 - val_loss: 0.6909 - val_acc: 0.5015\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 625s 695us/step - loss: 0.6907 - acc: 0.5063 - val_loss: 0.6909 - val_acc: 0.5013\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 634s 704us/step - loss: 0.6907 - acc: 0.5064 - val_loss: 0.6909 - val_acc: 0.5009\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 632s 702us/step - loss: 0.6907 - acc: 0.5071 - val_loss: 0.6910 - val_acc: 0.5026\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 644s 715us/step - loss: 0.6907 - acc: 0.5066 - val_loss: 0.6909 - val_acc: 0.5024\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 641s 712us/step - loss: 0.6907 - acc: 0.5068 - val_loss: 0.6909 - val_acc: 0.5011\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 641s 712us/step - loss: 0.6907 - acc: 0.5073 - val_loss: 0.6909 - val_acc: 0.5021\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 641s 712us/step - loss: 0.6907 - acc: 0.5075 - val_loss: 0.6909 - val_acc: 0.5026\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 617s 685us/step - loss: 0.6906 - acc: 0.5073 - val_loss: 0.6909 - val_acc: 0.5034\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6906 - acc: 0.5079 - val_loss: 0.6909 - val_acc: 0.5034\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 617s 686us/step - loss: 0.6906 - acc: 0.5076 - val_loss: 0.6910 - val_acc: 0.5025\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 621s 690us/step - loss: 0.6906 - acc: 0.5084 - val_loss: 0.6909 - val_acc: 0.5034\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 620s 689us/step - loss: 0.6906 - acc: 0.5083 - val_loss: 0.6909 - val_acc: 0.5029\n",
      "trainnig theta = : 0.21666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 611s 678us/step - loss: 0.6895 - acc: 0.5087 - val_loss: 0.6896 - val_acc: 0.5068\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 630s 700us/step - loss: 0.6895 - acc: 0.5090 - val_loss: 0.6897 - val_acc: 0.5067\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 625s 695us/step - loss: 0.6895 - acc: 0.5096 - val_loss: 0.6897 - val_acc: 0.5066\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 627s 697us/step - loss: 0.6894 - acc: 0.5100 - val_loss: 0.6896 - val_acc: 0.5072\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6894 - acc: 0.5094 - val_loss: 0.6896 - val_acc: 0.5076\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 600s 666us/step - loss: 0.6894 - acc: 0.5096 - val_loss: 0.6897 - val_acc: 0.5065\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 598s 664us/step - loss: 0.6894 - acc: 0.5096 - val_loss: 0.6897 - val_acc: 0.5070\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6894 - acc: 0.5100 - val_loss: 0.6896 - val_acc: 0.5070\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 573s 636us/step - loss: 0.6894 - acc: 0.5099 - val_loss: 0.6896 - val_acc: 0.5076\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 595s 661us/step - loss: 0.6894 - acc: 0.5109 - val_loss: 0.6897 - val_acc: 0.5070\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 597s 663us/step - loss: 0.6893 - acc: 0.5107 - val_loss: 0.6896 - val_acc: 0.5072\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 597s 663us/step - loss: 0.6894 - acc: 0.5107 - val_loss: 0.6897 - val_acc: 0.5059\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 597s 663us/step - loss: 0.6894 - acc: 0.5107 - val_loss: 0.6897 - val_acc: 0.5069\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 597s 663us/step - loss: 0.6894 - acc: 0.5109 - val_loss: 0.6897 - val_acc: 0.5072\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 595s 661us/step - loss: 0.6893 - acc: 0.5106 - val_loss: 0.6896 - val_acc: 0.5072\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6893 - acc: 0.5111 - val_loss: 0.6897 - val_acc: 0.5064\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 596s 663us/step - loss: 0.6893 - acc: 0.5110 - val_loss: 0.6897 - val_acc: 0.5072\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 582s 647us/step - loss: 0.6893 - acc: 0.5113 - val_loss: 0.6896 - val_acc: 0.5072\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6893 - acc: 0.5118 - val_loss: 0.6897 - val_acc: 0.5061\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 587s 652us/step - loss: 0.6893 - acc: 0.5113 - val_loss: 0.6897 - val_acc: 0.5065\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 586s 651us/step - loss: 0.6893 - acc: 0.5110 - val_loss: 0.6896 - val_acc: 0.5070\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 595s 661us/step - loss: 0.6893 - acc: 0.5114 - val_loss: 0.6896 - val_acc: 0.5076\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6893 - acc: 0.5113 - val_loss: 0.6897 - val_acc: 0.5076\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6893 - acc: 0.5117 - val_loss: 0.6897 - val_acc: 0.5077\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6892 - acc: 0.5127 - val_loss: 0.6897 - val_acc: 0.5067\n",
      "trainnig theta = : 0.225\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6886 - acc: 0.5111 - val_loss: 0.6889 - val_acc: 0.5078\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 590s 655us/step - loss: 0.6886 - acc: 0.5122 - val_loss: 0.6889 - val_acc: 0.5086\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6886 - acc: 0.5114 - val_loss: 0.6890 - val_acc: 0.5077\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 588s 653us/step - loss: 0.6885 - acc: 0.5119 - val_loss: 0.6889 - val_acc: 0.5080\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 582s 646us/step - loss: 0.6886 - acc: 0.5122 - val_loss: 0.6889 - val_acc: 0.5089\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 561s 623us/step - loss: 0.6886 - acc: 0.5119 - val_loss: 0.6891 - val_acc: 0.5079\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 562s 624us/step - loss: 0.6886 - acc: 0.5118 - val_loss: 0.6890 - val_acc: 0.5081\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 561s 623us/step - loss: 0.6886 - acc: 0.5120 - val_loss: 0.6890 - val_acc: 0.5080\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 561s 624us/step - loss: 0.6885 - acc: 0.5123 - val_loss: 0.6890 - val_acc: 0.5077\n",
      "Epoch 10/100\n",
      " 25000/900000 [..............................] - ETA: 5:52 - loss: 0.6884 - acc: 0.5101"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 25)   #iterating across possible StoUD values\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam',\n",
    "                        loss=my_loss_wrapper(myinputs, theta),\n",
    "                        metrics=['accuracy'])\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystopping])\n",
    "    lvals += [history.history['loss'][np.argmin(history.history['val_loss'])]]\n",
    "    vlvals += [np.min(history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T00:46:52.622600Z",
     "start_time": "2020-07-29T00:46:51.660762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"StoUD vs. Loss (Full Phase Space)\\n $F_{dropout} = \\phi_{dropout} = 0.2$\\nDCTR Change\")\n",
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.vlines(0.200, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"probStoUD(200) Vs Loss-FDropoutPhiDropout02-Copy4.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-28T05:14:18.149Z"
    }
   },
   "outputs": [],
   "source": [
    "thetas[np.argmax(vlvals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.342771Z",
     "start_time": "2020-07-19T00:03:27.336321Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.590548Z",
     "start_time": "2020-07-19T00:03:27.345029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_21[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = int(len(X_train_theta) / 10)\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1, MSE_loss=False):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        #  when not training theta, fetch as np array\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "\n",
    "    weights = reweight(events = x, param = theta0) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.821775Z",
     "start_time": "2020-07-19T00:03:27.593159Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 151s 167us/step - loss: 0.5987 - acc: 0.4972 - val_loss: 0.5872 - val_acc: 0.4956\n",
      ". theta fit =  0.0\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "1800000/1800000 [==============================] - 225s 125us/step - loss: nan - acc: 0.4957         \n",
      ". theta fit =  nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'argrelmin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f6db20d6f348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     extrema = np.concatenate(\n\u001b[0;32m---> 36\u001b[0;31m         (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mextrema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextrema\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextrema\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mindex_refine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argrelmin' is not defined"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration + 1)\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=1000,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train_theta, Y_train_theta, epochs=1, batch_size=batch_size,verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "\n",
    "        mean_fit = np.array([\n",
    "            np.mean(fit_vals_recent[len(fit_vals_recent) -\n",
    "                                    4:len(fit_vals_recent)])\n",
    "        ])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.823619Z",
     "start_time": "2020-07-17T18:54:03.260Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.200, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
