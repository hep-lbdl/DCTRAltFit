{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTR Alternative Fitting Algorithm for probStoUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:10:35.225667Z",
     "start_time": "2020-07-25T21:10:35.219299Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:10:42.174225Z",
     "start_time": "2020-07-25T21:10:35.230450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:10:42.194321Z",
     "start_time": "2020-07-25T21:10:42.178670Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22)\n",
    "rc('xtick', labelsize=15)\n",
    "rc('ytick', labelsize=15)\n",
    "rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:10:42.217792Z",
     "start_time": "2020-07-25T21:10:42.203352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:10:42.232627Z",
     "start_time": "2020-07-25T21:10:42.223769Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:10:42.255111Z",
     "start_time": "2020-07-25T21:10:42.235778Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:10:42.271130Z",
     "start_time": "2020-07-25T21:10:42.260694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/data0/users/aandreassen/zenodo/'\n",
    "data_dir1 = '/data1/users/asuresh/DCTRFitting/StoUDFitting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:11:03.326585Z",
     "start_time": "2020-07-25T21:10:42.286604Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "#unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')\n",
    "unknown_dataset =  np.load(data_dir1 + 'test1D_strange200.npz', allow_pickle=True)['dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-25T21:10:35.203Z"
    }
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-25T21:10:35.205Z"
    }
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:19:28.496663Z",
     "start_time": "2020-07-25T21:18:29.762222Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.5, shuffle=True)\n",
    "X_train_theta, X_test_theta, Y_train_theta, Y_test_theta = data_split(X_fit, Y_fit, test=0., shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:19:30.376716Z",
     "start_time": "2020-07-25T21:19:28.505659Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100, 100, 128)\n",
    "F_sizes = (100, 100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights(\n",
    "    './saved_models/DCTR_ee_dijets_1D_probStoUD_Copy6.h5')  #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:19:30.394276Z",
     "start_time": "2020-07-25T21:19:30.381962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "def reweight(events, param):  #from NN (DCTR)\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, param]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(events[...,0])>0,\n",
    "                                   K.ones_like(events[...,0]),\n",
    "                                   K.zeros_like(events[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "\n",
    "    model_inputs = K.concatenate([events, concat_input_and_params], -1)\n",
    "    # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    \n",
    "    f = dctr.model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:19:31.627399Z",
     "start_time": "2020-07-25T21:19:30.403764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = PFN(input_dim=4,\n",
    "            Phi_sizes=Phi_sizes,\n",
    "            F_sizes=F_sizes,\n",
    "            latent_dropout= 0.2,\n",
    "            F_dropouts= 0.2,\n",
    "            output_dim=1,\n",
    "            output_act='sigmoid',\n",
    "            summary=False)\n",
    "reinitialize_weights = model.model.get_weights()\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0., MSE_loss = False):\n",
    "    x = inputs  #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(\n",
    "        x, np.arange(51),\n",
    "        axis=1)  # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    weights = reweight(events = x, param = val)  # NN reweight\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-25T21:10:35.216Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-13-2bac8f3c1b35>:9: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 522s 580us/step - loss: 0.7154 - acc: 0.4990 - val_loss: 0.7018 - val_acc: 0.4965\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 531s 590us/step - loss: 0.7016 - acc: 0.4969 - val_loss: 0.6992 - val_acc: 0.4932\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 630s 700us/step - loss: 0.6965 - acc: 0.4926 - val_loss: 0.6940 - val_acc: 0.4909\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 611s 679us/step - loss: 0.6942 - acc: 0.4911 - val_loss: 0.6938 - val_acc: 0.4912\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 614s 682us/step - loss: 0.6935 - acc: 0.4905 - val_loss: 0.6938 - val_acc: 0.4909\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6934 - acc: 0.4902 - val_loss: 0.6933 - val_acc: 0.4903\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6931 - acc: 0.4904 - val_loss: 0.6933 - val_acc: 0.4901\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6930 - acc: 0.4905 - val_loss: 0.6933 - val_acc: 0.4900\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 609s 677us/step - loss: 0.6930 - acc: 0.4902 - val_loss: 0.6932 - val_acc: 0.4901\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 609s 676us/step - loss: 0.6929 - acc: 0.4900 - val_loss: 0.6932 - val_acc: 0.4902\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 608s 675us/step - loss: 0.6929 - acc: 0.4901 - val_loss: 0.6928 - val_acc: 0.4901\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 607s 675us/step - loss: 0.6929 - acc: 0.4902 - val_loss: 0.6929 - val_acc: 0.4902\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 607s 674us/step - loss: 0.6928 - acc: 0.4899 - val_loss: 0.6928 - val_acc: 0.4900\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6928 - acc: 0.4903 - val_loss: 0.6929 - val_acc: 0.4901\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 607s 675us/step - loss: 0.6928 - acc: 0.4900 - val_loss: 0.6930 - val_acc: 0.4899\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 604s 671us/step - loss: 0.6927 - acc: 0.4903 - val_loss: 0.6933 - val_acc: 0.4900\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 608s 675us/step - loss: 0.6927 - acc: 0.4904 - val_loss: 0.6929 - val_acc: 0.4902\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 604s 672us/step - loss: 0.6928 - acc: 0.4903 - val_loss: 0.6927 - val_acc: 0.4901\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6927 - acc: 0.4903 - val_loss: 0.6933 - val_acc: 0.4900\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 604s 672us/step - loss: 0.6927 - acc: 0.4901 - val_loss: 0.6923 - val_acc: 0.4897\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6926 - acc: 0.4905 - val_loss: 0.6932 - val_acc: 0.4898\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 602s 668us/step - loss: 0.6928 - acc: 0.4899 - val_loss: 0.6929 - val_acc: 0.4898\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6926 - acc: 0.4904 - val_loss: 0.6930 - val_acc: 0.4898\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 603s 671us/step - loss: 0.6925 - acc: 0.4901 - val_loss: 0.6930 - val_acc: 0.4898\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6926 - acc: 0.4904 - val_loss: 0.6932 - val_acc: 0.4900\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 599s 666us/step - loss: 0.6926 - acc: 0.4904 - val_loss: 0.6932 - val_acc: 0.4901\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 596s 663us/step - loss: 0.6926 - acc: 0.4905 - val_loss: 0.6931 - val_acc: 0.4900\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 598s 664us/step - loss: 0.6926 - acc: 0.4903 - val_loss: 0.6926 - val_acc: 0.4899\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6926 - acc: 0.4905 - val_loss: 0.6931 - val_acc: 0.4901\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 597s 664us/step - loss: 0.6926 - acc: 0.4900 - val_loss: 0.6929 - val_acc: 0.4899\n",
      "trainnig theta = : 0.10833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6934 - acc: 0.4902 - val_loss: 0.6939 - val_acc: 0.4900\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 606s 673us/step - loss: 0.6934 - acc: 0.4904 - val_loss: 0.6935 - val_acc: 0.4902\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6933 - acc: 0.4902 - val_loss: 0.6942 - val_acc: 0.4899\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 591s 657us/step - loss: 0.6934 - acc: 0.4903 - val_loss: 0.6941 - val_acc: 0.4899\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6933 - acc: 0.4902 - val_loss: 0.6934 - val_acc: 0.4897\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6934 - acc: 0.4899 - val_loss: 0.6937 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6934 - acc: 0.4903 - val_loss: 0.6935 - val_acc: 0.4900\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6933 - acc: 0.4902 - val_loss: 0.6933 - val_acc: 0.4899\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 590s 655us/step - loss: 0.6933 - acc: 0.4902 - val_loss: 0.6935 - val_acc: 0.4900\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 590s 656us/step - loss: 0.6933 - acc: 0.4903 - val_loss: 0.6942 - val_acc: 0.4898\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 585s 650us/step - loss: 0.6933 - acc: 0.4904 - val_loss: 0.6939 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 586s 651us/step - loss: 0.6933 - acc: 0.4903 - val_loss: 0.6939 - val_acc: 0.4900\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 586s 652us/step - loss: 0.6932 - acc: 0.4906 - val_loss: 0.6936 - val_acc: 0.4901\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 586s 651us/step - loss: 0.6932 - acc: 0.4902 - val_loss: 0.6934 - val_acc: 0.4899\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 587s 652us/step - loss: 0.6933 - acc: 0.4901 - val_loss: 0.6935 - val_acc: 0.4900\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 587s 652us/step - loss: 0.6933 - acc: 0.4904 - val_loss: 0.6935 - val_acc: 0.4899\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 586s 651us/step - loss: 0.6932 - acc: 0.4904 - val_loss: 0.6932 - val_acc: 0.4900\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6933 - acc: 0.4903 - val_loss: 0.6939 - val_acc: 0.4900\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 582s 647us/step - loss: 0.6933 - acc: 0.4901 - val_loss: 0.6939 - val_acc: 0.4900\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 585s 650us/step - loss: 0.6932 - acc: 0.4904 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6932 - acc: 0.4903 - val_loss: 0.6945 - val_acc: 0.4898\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 583s 647us/step - loss: 0.6933 - acc: 0.4906 - val_loss: 0.6936 - val_acc: 0.4900\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 583s 648us/step - loss: 0.6932 - acc: 0.4905 - val_loss: 0.6939 - val_acc: 0.4899\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6932 - acc: 0.4903 - val_loss: 0.6941 - val_acc: 0.4899\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 579s 644us/step - loss: 0.6932 - acc: 0.4904 - val_loss: 0.6937 - val_acc: 0.4900\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 580s 645us/step - loss: 0.6932 - acc: 0.4905 - val_loss: 0.6937 - val_acc: 0.4899\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 579s 644us/step - loss: 0.6932 - acc: 0.4902 - val_loss: 0.6937 - val_acc: 0.4900\n",
      "trainnig theta = : 0.11666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 579s 644us/step - loss: 0.6943 - acc: 0.4902 - val_loss: 0.6955 - val_acc: 0.4901\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6943 - acc: 0.4902 - val_loss: 0.6946 - val_acc: 0.4901\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 576s 640us/step - loss: 0.6942 - acc: 0.4902 - val_loss: 0.6947 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 649s 721us/step - loss: 0.6942 - acc: 0.4904 - val_loss: 0.6945 - val_acc: 0.4902\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6943 - acc: 0.4904 - val_loss: 0.6945 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 671s 746us/step - loss: 0.6942 - acc: 0.4904 - val_loss: 0.6942 - val_acc: 0.4902\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6942 - acc: 0.4904 - val_loss: 0.6951 - val_acc: 0.4904\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 706s 785us/step - loss: 0.6942 - acc: 0.4903 - val_loss: 0.6950 - val_acc: 0.4903\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6942 - acc: 0.4904 - val_loss: 0.6950 - val_acc: 0.4899\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6942 - acc: 0.4905 - val_loss: 0.6945 - val_acc: 0.4901\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 709s 788us/step - loss: 0.6942 - acc: 0.4906 - val_loss: 0.6949 - val_acc: 0.4901\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6942 - acc: 0.4906 - val_loss: 0.6944 - val_acc: 0.4903\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 681s 757us/step - loss: 0.6943 - acc: 0.4904 - val_loss: 0.6957 - val_acc: 0.4906\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 681s 757us/step - loss: 0.6942 - acc: 0.4904 - val_loss: 0.6952 - val_acc: 0.4899\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 699s 776us/step - loss: 0.6942 - acc: 0.4904 - val_loss: 0.6942 - val_acc: 0.4898\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 710s 789us/step - loss: 0.6942 - acc: 0.4905 - val_loss: 0.6951 - val_acc: 0.4902\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 711s 790us/step - loss: 0.6942 - acc: 0.4905 - val_loss: 0.6944 - val_acc: 0.4902\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 715s 794us/step - loss: 0.6942 - acc: 0.4908 - val_loss: 0.6947 - val_acc: 0.4901\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 713s 792us/step - loss: 0.6942 - acc: 0.4905 - val_loss: 0.6946 - val_acc: 0.4901\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 672s 747us/step - loss: 0.6942 - acc: 0.4907 - val_loss: 0.6943 - val_acc: 0.4900\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6942 - acc: 0.4906 - val_loss: 0.6946 - val_acc: 0.4900\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 677s 753us/step - loss: 0.6942 - acc: 0.4908 - val_loss: 0.6948 - val_acc: 0.4900\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6942 - acc: 0.4908 - val_loss: 0.6947 - val_acc: 0.4903\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6941 - acc: 0.4907 - val_loss: 0.6950 - val_acc: 0.4899\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6941 - acc: 0.4905 - val_loss: 0.6944 - val_acc: 0.4900\n",
      "trainnig theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6956 - acc: 0.4907 - val_loss: 0.6959 - val_acc: 0.4900\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 701s 778us/step - loss: 0.6955 - acc: 0.4907 - val_loss: 0.6962 - val_acc: 0.4899\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6956 - acc: 0.4904 - val_loss: 0.6960 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 670s 744us/step - loss: 0.6957 - acc: 0.4906 - val_loss: 0.6962 - val_acc: 0.4905\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 673s 748us/step - loss: 0.6956 - acc: 0.4906 - val_loss: 0.6958 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6956 - acc: 0.4904 - val_loss: 0.6957 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6955 - acc: 0.4905 - val_loss: 0.6962 - val_acc: 0.4900\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6955 - acc: 0.4903 - val_loss: 0.6960 - val_acc: 0.4901\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6955 - acc: 0.4904 - val_loss: 0.6961 - val_acc: 0.4901\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 700s 777us/step - loss: 0.6956 - acc: 0.4907 - val_loss: 0.6957 - val_acc: 0.4901\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 665s 739us/step - loss: 0.6956 - acc: 0.4903 - val_loss: 0.6959 - val_acc: 0.4899\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 670s 745us/step - loss: 0.6955 - acc: 0.4907 - val_loss: 0.6961 - val_acc: 0.4900\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 672s 747us/step - loss: 0.6956 - acc: 0.4904 - val_loss: 0.6960 - val_acc: 0.4900\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 682s 758us/step - loss: 0.6955 - acc: 0.4906 - val_loss: 0.6958 - val_acc: 0.4899\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6955 - acc: 0.4909 - val_loss: 0.6957 - val_acc: 0.4902\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6956 - acc: 0.4905 - val_loss: 0.6959 - val_acc: 0.4897\n",
      "trainnig theta = : 0.13333333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 704s 783us/step - loss: 0.6972 - acc: 0.4905 - val_loss: 0.6974 - val_acc: 0.4902\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6971 - acc: 0.4905 - val_loss: 0.6974 - val_acc: 0.4900\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 664s 738us/step - loss: 0.6971 - acc: 0.4907 - val_loss: 0.6973 - val_acc: 0.4900\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 668s 743us/step - loss: 0.6971 - acc: 0.4906 - val_loss: 0.6973 - val_acc: 0.4904\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 670s 744us/step - loss: 0.6971 - acc: 0.4907 - val_loss: 0.6977 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6972 - acc: 0.4909 - val_loss: 0.6974 - val_acc: 0.4901\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 696s 773us/step - loss: 0.6972 - acc: 0.4908 - val_loss: 0.6976 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6971 - acc: 0.4906 - val_loss: 0.6978 - val_acc: 0.4900\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6971 - acc: 0.4907 - val_loss: 0.6975 - val_acc: 0.4900\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6971 - acc: 0.4905 - val_loss: 0.6975 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6971 - acc: 0.4906 - val_loss: 0.6976 - val_acc: 0.4897\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 670s 745us/step - loss: 0.6971 - acc: 0.4905 - val_loss: 0.6977 - val_acc: 0.4902\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6971 - acc: 0.4909 - val_loss: 0.6977 - val_acc: 0.4902\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6971 - acc: 0.4909 - val_loss: 0.6977 - val_acc: 0.4899\n",
      "trainnig theta = : 0.14166666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 712s 791us/step - loss: 0.6985 - acc: 0.4908 - val_loss: 0.6988 - val_acc: 0.4902\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6984 - acc: 0.4908 - val_loss: 0.6986 - val_acc: 0.4906\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6984 - acc: 0.4904 - val_loss: 0.6986 - val_acc: 0.4903\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 707s 785us/step - loss: 0.6985 - acc: 0.4907 - val_loss: 0.6987 - val_acc: 0.4907\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6984 - acc: 0.4910 - val_loss: 0.6986 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 673s 748us/step - loss: 0.6985 - acc: 0.4907 - val_loss: 0.6990 - val_acc: 0.4901\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6985 - acc: 0.4907 - val_loss: 0.6987 - val_acc: 0.4903\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 680s 756us/step - loss: 0.6985 - acc: 0.4907 - val_loss: 0.6988 - val_acc: 0.4901\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6985 - acc: 0.4910 - val_loss: 0.6986 - val_acc: 0.4901\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6984 - acc: 0.4907 - val_loss: 0.6990 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6984 - acc: 0.4909 - val_loss: 0.6986 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6984 - acc: 0.4908 - val_loss: 0.6988 - val_acc: 0.4899\n",
      "trainnig theta = : 0.15000000000000002\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6992 - acc: 0.4911 - val_loss: 0.6992 - val_acc: 0.4899\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 659s 732us/step - loss: 0.6992 - acc: 0.4906 - val_loss: 0.6993 - val_acc: 0.4899\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 660s 734us/step - loss: 0.6992 - acc: 0.4908 - val_loss: 0.6994 - val_acc: 0.4910\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 668s 742us/step - loss: 0.6991 - acc: 0.4907 - val_loss: 0.6992 - val_acc: 0.4901\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 723s 803us/step - loss: 0.6991 - acc: 0.4909 - val_loss: 0.6992 - val_acc: 0.4903\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 707s 785us/step - loss: 0.6992 - acc: 0.4910 - val_loss: 0.6993 - val_acc: 0.4907\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6991 - acc: 0.4909 - val_loss: 0.6993 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6991 - acc: 0.4910 - val_loss: 0.6993 - val_acc: 0.4902\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 689s 765us/step - loss: 0.6991 - acc: 0.4908 - val_loss: 0.6995 - val_acc: 0.4907\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 659s 732us/step - loss: 0.6991 - acc: 0.4912 - val_loss: 0.6994 - val_acc: 0.4906\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 655s 728us/step - loss: 0.6991 - acc: 0.4911 - val_loss: 0.6994 - val_acc: 0.4901\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6991 - acc: 0.4910 - val_loss: 0.6993 - val_acc: 0.4902\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 700s 777us/step - loss: 0.6991 - acc: 0.4911 - val_loss: 0.6993 - val_acc: 0.4902\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6991 - acc: 0.4913 - val_loss: 0.6992 - val_acc: 0.4902\n",
      "trainnig theta = : 0.15833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6991 - acc: 0.4908 - val_loss: 0.6994 - val_acc: 0.4906\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6992 - acc: 0.4911 - val_loss: 0.6994 - val_acc: 0.4902\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6992 - acc: 0.4908 - val_loss: 0.6992 - val_acc: 0.4902\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6991 - acc: 0.4910 - val_loss: 0.6993 - val_acc: 0.4902\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6992 - acc: 0.4910 - val_loss: 0.6993 - val_acc: 0.4898\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 706s 785us/step - loss: 0.6991 - acc: 0.4909 - val_loss: 0.6994 - val_acc: 0.4905\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6992 - acc: 0.4914 - val_loss: 0.6994 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6991 - acc: 0.4912 - val_loss: 0.6992 - val_acc: 0.4904\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6991 - acc: 0.4912 - val_loss: 0.6992 - val_acc: 0.4903\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6991 - acc: 0.4911 - val_loss: 0.6995 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 708s 786us/step - loss: 0.6991 - acc: 0.4913 - val_loss: 0.6993 - val_acc: 0.4903\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 707s 786us/step - loss: 0.6992 - acc: 0.4917 - val_loss: 0.6993 - val_acc: 0.4907\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 706s 785us/step - loss: 0.6991 - acc: 0.4910 - val_loss: 0.6993 - val_acc: 0.4902\n",
      "trainnig theta = : 0.16666666666666669\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 713s 792us/step - loss: 0.6987 - acc: 0.4912 - val_loss: 0.6988 - val_acc: 0.4906\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 705s 784us/step - loss: 0.6987 - acc: 0.4915 - val_loss: 0.6987 - val_acc: 0.4903\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6987 - acc: 0.4908 - val_loss: 0.6988 - val_acc: 0.4903\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6987 - acc: 0.4914 - val_loss: 0.6987 - val_acc: 0.4906\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6987 - acc: 0.4913 - val_loss: 0.6990 - val_acc: 0.4909\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6987 - acc: 0.4911 - val_loss: 0.6987 - val_acc: 0.4904\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6987 - acc: 0.4915 - val_loss: 0.6988 - val_acc: 0.4901\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 706s 784us/step - loss: 0.6987 - acc: 0.4912 - val_loss: 0.6988 - val_acc: 0.4906\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 705s 784us/step - loss: 0.6987 - acc: 0.4915 - val_loss: 0.6989 - val_acc: 0.4906\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 713s 792us/step - loss: 0.6986 - acc: 0.4916 - val_loss: 0.6988 - val_acc: 0.4906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 709s 787us/step - loss: 0.6986 - acc: 0.4920 - val_loss: 0.6988 - val_acc: 0.4903\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 707s 785us/step - loss: 0.6987 - acc: 0.4914 - val_loss: 0.6989 - val_acc: 0.4905\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6987 - acc: 0.4917 - val_loss: 0.6988 - val_acc: 0.4901\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6987 - acc: 0.4917 - val_loss: 0.6989 - val_acc: 0.4909\n",
      "trainnig theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 712s 791us/step - loss: 0.6980 - acc: 0.4913 - val_loss: 0.6981 - val_acc: 0.4906\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6980 - acc: 0.4914 - val_loss: 0.6981 - val_acc: 0.4904\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 703s 782us/step - loss: 0.6980 - acc: 0.4918 - val_loss: 0.6981 - val_acc: 0.4909\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6980 - acc: 0.4924 - val_loss: 0.6981 - val_acc: 0.4919\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 708s 787us/step - loss: 0.6980 - acc: 0.4923 - val_loss: 0.6981 - val_acc: 0.4910\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 702s 779us/step - loss: 0.6980 - acc: 0.4922 - val_loss: 0.6982 - val_acc: 0.4906\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 700s 777us/step - loss: 0.6980 - acc: 0.4922 - val_loss: 0.6981 - val_acc: 0.4912\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6980 - acc: 0.4921 - val_loss: 0.6981 - val_acc: 0.4906\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6980 - acc: 0.4920 - val_loss: 0.6981 - val_acc: 0.4907\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6980 - acc: 0.4919 - val_loss: 0.6981 - val_acc: 0.4909\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 705s 783us/step - loss: 0.6980 - acc: 0.4920 - val_loss: 0.6981 - val_acc: 0.4909\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6980 - acc: 0.4925 - val_loss: 0.6982 - val_acc: 0.4906\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6980 - acc: 0.4918 - val_loss: 0.6981 - val_acc: 0.4903\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 700s 777us/step - loss: 0.6980 - acc: 0.4927 - val_loss: 0.6982 - val_acc: 0.4909\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6980 - acc: 0.4927 - val_loss: 0.6981 - val_acc: 0.4909\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 700s 778us/step - loss: 0.6980 - acc: 0.4929 - val_loss: 0.6982 - val_acc: 0.4908\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6980 - acc: 0.4928 - val_loss: 0.6982 - val_acc: 0.4908\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6980 - acc: 0.4926 - val_loss: 0.6982 - val_acc: 0.4909\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6980 - acc: 0.4928 - val_loss: 0.6981 - val_acc: 0.4911\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 699s 776us/step - loss: 0.6980 - acc: 0.4930 - val_loss: 0.6982 - val_acc: 0.4917\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6980 - acc: 0.4923 - val_loss: 0.6981 - val_acc: 0.4907\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 695s 773us/step - loss: 0.6980 - acc: 0.4925 - val_loss: 0.6981 - val_acc: 0.4907\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6980 - acc: 0.4932 - val_loss: 0.6982 - val_acc: 0.4909\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 697s 775us/step - loss: 0.6980 - acc: 0.4924 - val_loss: 0.6982 - val_acc: 0.4913\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6980 - acc: 0.4929 - val_loss: 0.6981 - val_acc: 0.4911\n",
      "trainnig theta = : 0.18333333333333335\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 702s 780us/step - loss: 0.6972 - acc: 0.4939 - val_loss: 0.6974 - val_acc: 0.4921\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6972 - acc: 0.4934 - val_loss: 0.6974 - val_acc: 0.4909\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 689s 765us/step - loss: 0.6972 - acc: 0.4933 - val_loss: 0.6973 - val_acc: 0.4912\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6972 - acc: 0.4938 - val_loss: 0.6973 - val_acc: 0.4911\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6972 - acc: 0.4936 - val_loss: 0.6973 - val_acc: 0.4907\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6972 - acc: 0.4936 - val_loss: 0.6974 - val_acc: 0.4917\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6972 - acc: 0.4933 - val_loss: 0.6973 - val_acc: 0.4912\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6972 - acc: 0.4930 - val_loss: 0.6974 - val_acc: 0.4912\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 689s 765us/step - loss: 0.6972 - acc: 0.4940 - val_loss: 0.6974 - val_acc: 0.4921\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6972 - acc: 0.4938 - val_loss: 0.6974 - val_acc: 0.4919\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6972 - acc: 0.4936 - val_loss: 0.6973 - val_acc: 0.4912\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6972 - acc: 0.4936 - val_loss: 0.6974 - val_acc: 0.4911\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 686s 763us/step - loss: 0.6972 - acc: 0.4944 - val_loss: 0.6974 - val_acc: 0.4918\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6972 - acc: 0.4933 - val_loss: 0.6974 - val_acc: 0.4914\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6972 - acc: 0.4941 - val_loss: 0.6973 - val_acc: 0.4910\n",
      "trainnig theta = : 0.19166666666666665\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6967 - acc: 0.4939 - val_loss: 0.6968 - val_acc: 0.4913\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6966 - acc: 0.4953 - val_loss: 0.6968 - val_acc: 0.4923\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6967 - acc: 0.4951 - val_loss: 0.6968 - val_acc: 0.4920\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 689s 765us/step - loss: 0.6967 - acc: 0.4945 - val_loss: 0.6968 - val_acc: 0.4921\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 692s 768us/step - loss: 0.6967 - acc: 0.4939 - val_loss: 0.6968 - val_acc: 0.4922\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6966 - acc: 0.4944 - val_loss: 0.6968 - val_acc: 0.4917\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6967 - acc: 0.4941 - val_loss: 0.6968 - val_acc: 0.4916\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6967 - acc: 0.4945 - val_loss: 0.6968 - val_acc: 0.4912\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6967 - acc: 0.4938 - val_loss: 0.6968 - val_acc: 0.4915\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 689s 765us/step - loss: 0.6966 - acc: 0.4946 - val_loss: 0.6968 - val_acc: 0.4919\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6967 - acc: 0.4953 - val_loss: 0.6968 - val_acc: 0.4936\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 686s 763us/step - loss: 0.6967 - acc: 0.4949 - val_loss: 0.6968 - val_acc: 0.4919\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6966 - acc: 0.4948 - val_loss: 0.6968 - val_acc: 0.4918\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6966 - acc: 0.4951 - val_loss: 0.6968 - val_acc: 0.4926\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6966 - acc: 0.4942 - val_loss: 0.6968 - val_acc: 0.4921\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6966 - acc: 0.4943 - val_loss: 0.6968 - val_acc: 0.4913\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6966 - acc: 0.4943 - val_loss: 0.6968 - val_acc: 0.4921\n",
      "trainnig theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6965 - acc: 0.4951 - val_loss: 0.6966 - val_acc: 0.4925\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 680s 756us/step - loss: 0.6965 - acc: 0.4967 - val_loss: 0.6966 - val_acc: 0.4976\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 678s 754us/step - loss: 0.6965 - acc: 0.4973 - val_loss: 0.6966 - val_acc: 0.4922\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 681s 756us/step - loss: 0.6965 - acc: 0.4954 - val_loss: 0.6966 - val_acc: 0.4945\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6965 - acc: 0.4951 - val_loss: 0.6966 - val_acc: 0.4956\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 706s 785us/step - loss: 0.6965 - acc: 0.4956 - val_loss: 0.6966 - val_acc: 0.4931\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 711s 790us/step - loss: 0.6965 - acc: 0.4967 - val_loss: 0.6966 - val_acc: 0.4940\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 713s 793us/step - loss: 0.6965 - acc: 0.4974 - val_loss: 0.6967 - val_acc: 0.4974\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 722s 803us/step - loss: 0.6965 - acc: 0.4966 - val_loss: 0.6966 - val_acc: 0.4932\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 730s 812us/step - loss: 0.6965 - acc: 0.4956 - val_loss: 0.6966 - val_acc: 0.4921\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 737s 819us/step - loss: 0.6965 - acc: 0.4968 - val_loss: 0.6966 - val_acc: 0.4930\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 742s 825us/step - loss: 0.6965 - acc: 0.4962 - val_loss: 0.6966 - val_acc: 0.4919\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 649s 722us/step - loss: 0.6965 - acc: 0.4967 - val_loss: 0.6966 - val_acc: 0.4928\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 554s 615us/step - loss: 0.6965 - acc: 0.4964 - val_loss: 0.6966 - val_acc: 0.4929\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 580s 644us/step - loss: 0.6964 - acc: 0.4979 - val_loss: 0.6966 - val_acc: 0.4932\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6964 - acc: 0.4975 - val_loss: 0.6966 - val_acc: 0.4964\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 580s 644us/step - loss: 0.6964 - acc: 0.4974 - val_loss: 0.6967 - val_acc: 0.4969\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6964 - acc: 0.4987 - val_loss: 0.6966 - val_acc: 0.4932\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 619s 688us/step - loss: 0.6965 - acc: 0.4981 - val_loss: 0.6967 - val_acc: 0.4983\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 631s 701us/step - loss: 0.6964 - acc: 0.4973 - val_loss: 0.6966 - val_acc: 0.4932\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 635s 705us/step - loss: 0.6964 - acc: 0.4984 - val_loss: 0.6966 - val_acc: 0.4946\n",
      "trainnig theta = : 0.20833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 658s 731us/step - loss: 0.6971 - acc: 0.4988 - val_loss: 0.6972 - val_acc: 0.4979\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 630s 700us/step - loss: 0.6971 - acc: 0.4995 - val_loss: 0.6972 - val_acc: 0.4955\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 636s 707us/step - loss: 0.6971 - acc: 0.4997 - val_loss: 0.6972 - val_acc: 0.4983\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 648s 720us/step - loss: 0.6971 - acc: 0.5016 - val_loss: 0.6972 - val_acc: 0.5009\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 647s 719us/step - loss: 0.6971 - acc: 0.5022 - val_loss: 0.6972 - val_acc: 0.5005\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 646s 718us/step - loss: 0.6971 - acc: 0.5021 - val_loss: 0.6972 - val_acc: 0.5003\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 644s 716us/step - loss: 0.6971 - acc: 0.5021 - val_loss: 0.6972 - val_acc: 0.4973\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6971 - acc: 0.5017 - val_loss: 0.6972 - val_acc: 0.4978\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 620s 689us/step - loss: 0.6971 - acc: 0.5020 - val_loss: 0.6972 - val_acc: 0.4996\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 625s 695us/step - loss: 0.6971 - acc: 0.5013 - val_loss: 0.6973 - val_acc: 0.4988\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 630s 700us/step - loss: 0.6971 - acc: 0.5021 - val_loss: 0.6972 - val_acc: 0.5004\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 614s 682us/step - loss: 0.6971 - acc: 0.5022 - val_loss: 0.6972 - val_acc: 0.4985\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 624s 694us/step - loss: 0.6971 - acc: 0.5029 - val_loss: 0.6972 - val_acc: 0.4990\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 632s 702us/step - loss: 0.6970 - acc: 0.5032 - val_loss: 0.6972 - val_acc: 0.4983\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 632s 702us/step - loss: 0.6971 - acc: 0.5027 - val_loss: 0.6973 - val_acc: 0.5008\n",
      "trainnig theta = : 0.21666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 633s 704us/step - loss: 0.6985 - acc: 0.5035 - val_loss: 0.6986 - val_acc: 0.5016\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6986 - acc: 0.5029 - val_loss: 0.6986 - val_acc: 0.5027\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6985 - acc: 0.5042 - val_loss: 0.6986 - val_acc: 0.5043\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6985 - acc: 0.5052 - val_loss: 0.6986 - val_acc: 0.5028\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 579s 643us/step - loss: 0.6985 - acc: 0.5051 - val_loss: 0.6985 - val_acc: 0.5050\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 585s 650us/step - loss: 0.6985 - acc: 0.5064 - val_loss: 0.6986 - val_acc: 0.5056\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 596s 663us/step - loss: 0.6985 - acc: 0.5059 - val_loss: 0.6986 - val_acc: 0.5046\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 602s 669us/step - loss: 0.6984 - acc: 0.5064 - val_loss: 0.6986 - val_acc: 0.5041\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 599s 666us/step - loss: 0.6985 - acc: 0.5066 - val_loss: 0.6986 - val_acc: 0.5028\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6984 - acc: 0.5067 - val_loss: 0.6986 - val_acc: 0.5052\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 599s 666us/step - loss: 0.6985 - acc: 0.5050 - val_loss: 0.6985 - val_acc: 0.5038\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 597s 664us/step - loss: 0.6984 - acc: 0.5062 - val_loss: 0.6986 - val_acc: 0.5059\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 599s 665us/step - loss: 0.6984 - acc: 0.5063 - val_loss: 0.6986 - val_acc: 0.5039\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6984 - acc: 0.5067 - val_loss: 0.6986 - val_acc: 0.5042\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 570s 633us/step - loss: 0.6984 - acc: 0.5071 - val_loss: 0.6986 - val_acc: 0.5047\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 601s 668us/step - loss: 0.6984 - acc: 0.5074 - val_loss: 0.6986 - val_acc: 0.5052\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 580s 644us/step - loss: 0.6984 - acc: 0.5074 - val_loss: 0.6986 - val_acc: 0.5044\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 598s 664us/step - loss: 0.6984 - acc: 0.5071 - val_loss: 0.6986 - val_acc: 0.5047\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 598s 665us/step - loss: 0.6984 - acc: 0.5066 - val_loss: 0.6986 - val_acc: 0.5051\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6984 - acc: 0.5065 - val_loss: 0.6987 - val_acc: 0.5031\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6984 - acc: 0.5071 - val_loss: 0.6987 - val_acc: 0.5031\n",
      "trainnig theta = : 0.225\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 606s 674us/step - loss: 0.6982 - acc: 0.5073 - val_loss: 0.6981 - val_acc: 0.5089\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6981 - acc: 0.5093 - val_loss: 0.6983 - val_acc: 0.5068\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6981 - acc: 0.5089 - val_loss: 0.6982 - val_acc: 0.5074\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 594s 661us/step - loss: 0.6981 - acc: 0.5089 - val_loss: 0.6983 - val_acc: 0.5077\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 591s 657us/step - loss: 0.6981 - acc: 0.5092 - val_loss: 0.6982 - val_acc: 0.5064\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 568s 631us/step - loss: 0.6981 - acc: 0.5092 - val_loss: 0.6982 - val_acc: 0.5076\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 564s 627us/step - loss: 0.6980 - acc: 0.5097 - val_loss: 0.6982 - val_acc: 0.5066\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 565s 628us/step - loss: 0.6981 - acc: 0.5090 - val_loss: 0.6982 - val_acc: 0.5076\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 569s 632us/step - loss: 0.6980 - acc: 0.5097 - val_loss: 0.6982 - val_acc: 0.5069\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 565s 628us/step - loss: 0.6980 - acc: 0.5091 - val_loss: 0.6982 - val_acc: 0.5064\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 565s 628us/step - loss: 0.6980 - acc: 0.5091 - val_loss: 0.6982 - val_acc: 0.5056\n",
      "trainnig theta = : 0.23333333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 581s 646us/step - loss: 0.6965 - acc: 0.5089 - val_loss: 0.6965 - val_acc: 0.5089\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 573s 637us/step - loss: 0.6965 - acc: 0.5093 - val_loss: 0.6964 - val_acc: 0.5092\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 584s 649us/step - loss: 0.6964 - acc: 0.5095 - val_loss: 0.6966 - val_acc: 0.5089\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 566s 629us/step - loss: 0.6964 - acc: 0.5097 - val_loss: 0.6964 - val_acc: 0.5091\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 560s 622us/step - loss: 0.6964 - acc: 0.5095 - val_loss: 0.6964 - val_acc: 0.5086\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 564s 627us/step - loss: 0.6964 - acc: 0.5099 - val_loss: 0.6965 - val_acc: 0.5088\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 561s 624us/step - loss: 0.6964 - acc: 0.5091 - val_loss: 0.6965 - val_acc: 0.5085\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 573s 637us/step - loss: 0.6964 - acc: 0.5095 - val_loss: 0.6965 - val_acc: 0.5087\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6964 - acc: 0.5096 - val_loss: 0.6965 - val_acc: 0.5091\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 591s 657us/step - loss: 0.6964 - acc: 0.5100 - val_loss: 0.6965 - val_acc: 0.5091\n",
      "Epoch 11/100\n",
      "837000/900000 [==========================>...] - ETA: 25s - loss: 0.6964 - acc: 0.5096"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 25)   #iterating across possible StoUD values\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam',\n",
    "                        loss=my_loss_wrapper(myinputs, theta),\n",
    "                        metrics=['accuracy'])\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystopping])\n",
    "    lvals += [history.history['loss'][np.argmin(history.history['val_loss'])]]\n",
    "    vlvals += [np.min(history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T00:50:16.141176Z",
     "start_time": "2020-07-29T00:50:15.192288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"StoUD vs. Loss (Full Phase Space)\\n $F_{dropout} = \\phi_{dropout} = 0.2$\\nDCTR Change\")\n",
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.vlines(0.200, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"probStoUD(200) Vs Loss-FDropoutPhiDropout02-Copy6.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T00:50:16.165235Z",
     "start_time": "2020-07-29T00:50:16.144984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[np.argmax(vlvals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.342771Z",
     "start_time": "2020-07-19T00:03:27.336321Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.590548Z",
     "start_time": "2020-07-19T00:03:27.345029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_21[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = int(len(X_train_theta) / 10)\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1, MSE_loss=False):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        #  when not training theta, fetch as np array\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "\n",
    "    weights = reweight(events = x, param = theta0) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.821775Z",
     "start_time": "2020-07-19T00:03:27.593159Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 151s 167us/step - loss: 0.5987 - acc: 0.4972 - val_loss: 0.5872 - val_acc: 0.4956\n",
      ". theta fit =  0.0\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "1800000/1800000 [==============================] - 225s 125us/step - loss: nan - acc: 0.4957         \n",
      ". theta fit =  nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'argrelmin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f6db20d6f348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     extrema = np.concatenate(\n\u001b[0;32m---> 36\u001b[0;31m         (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mextrema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextrema\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextrema\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mindex_refine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argrelmin' is not defined"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration + 1)\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=1000,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train_theta, Y_train_theta, epochs=1, batch_size=batch_size,verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "\n",
    "        mean_fit = np.array([\n",
    "            np.mean(fit_vals_recent[len(fit_vals_recent) -\n",
    "                                    4:len(fit_vals_recent)])\n",
    "        ])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.823619Z",
     "start_time": "2020-07-17T18:54:03.260Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.200, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
