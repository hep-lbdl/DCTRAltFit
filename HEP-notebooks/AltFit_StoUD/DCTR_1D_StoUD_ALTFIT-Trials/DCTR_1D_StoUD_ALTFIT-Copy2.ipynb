{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTR Alternative Fitting Algorithm for probStoUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:13.415257Z",
     "start_time": "2020-07-23T21:39:13.410885Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:15.964710Z",
     "start_time": "2020-07-23T21:39:13.418955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:15.972886Z",
     "start_time": "2020-07-23T21:39:15.967105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22)\n",
    "rc('xtick', labelsize=15)\n",
    "rc('ytick', labelsize=15)\n",
    "rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:15.986485Z",
     "start_time": "2020-07-23T21:39:15.976038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:15.996110Z",
     "start_time": "2020-07-23T21:39:15.989372Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:16.008334Z",
     "start_time": "2020-07-23T21:39:15.998769Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:16.016005Z",
     "start_time": "2020-07-23T21:39:16.010882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/data0/users/aandreassen/zenodo/'\n",
    "data_dir1 = '/data1/users/asuresh/DCTRFitting/StoUDFitting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:23.999898Z",
     "start_time": "2020-07-23T21:39:16.020800Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "#unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')\n",
    "unknown_dataset =  np.load(data_dir1 + 'test1D_strange200.npz', allow_pickle=True)['dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:43:35.990925Z",
     "start_time": "2020-07-23T21:39:24.002218Z"
    }
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:43:38.025618Z",
     "start_time": "2020-07-23T21:43:35.996100Z"
    }
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:43:41.472518Z",
     "start_time": "2020-07-23T21:43:38.029554Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.5, shuffle=True)\n",
    "X_train_theta, X_test_theta, Y_train_theta, Y_test_theta = data_split(X_fit, Y_fit, test=0., shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:43:42.671097Z",
     "start_time": "2020-07-23T21:43:41.480031Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100, 100, 128)\n",
    "F_sizes = (100, 100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights(\n",
    "    './saved_models/DCTR_ee_dijets_1D_probStoUD_Copy2.h5')  #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:43:42.689135Z",
     "start_time": "2020-07-23T21:43:42.676508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "def reweight(events, param):  #from NN (DCTR)\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, param]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(events[...,0])>0,\n",
    "                                   K.ones_like(events[...,0]),\n",
    "                                   K.zeros_like(events[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "\n",
    "    model_inputs = K.concatenate([events, concat_input_and_params], -1)\n",
    "    # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    \n",
    "    f = dctr.model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:43:43.737800Z",
     "start_time": "2020-07-23T21:43:42.693730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = PFN(input_dim=4,\n",
    "            Phi_sizes=Phi_sizes,\n",
    "            F_sizes=F_sizes,\n",
    "            latent_dropout= 0.2,\n",
    "            F_dropouts= 0.2,\n",
    "            output_dim=1,\n",
    "            output_act='sigmoid',\n",
    "            summary=False)\n",
    "reinitialize_weights = model.model.get_weights()\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0., MSE_loss = False):\n",
    "    x = inputs  #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(\n",
    "        x, np.arange(51),\n",
    "        axis=1)  # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    weights = reweight(events = x, param = val)  # NN reweight\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T13:25:06.289943Z",
     "start_time": "2020-07-23T21:43:43.740318Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-13-2bac8f3c1b35>:9: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 365s 405us/step - loss: 0.6992 - acc: 0.5002 - val_loss: 0.6826 - val_acc: 0.4992\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 361s 401us/step - loss: 0.6829 - acc: 0.4976 - val_loss: 0.6806 - val_acc: 0.4942\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6802 - acc: 0.4953 - val_loss: 0.6764 - val_acc: 0.4920\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6758 - acc: 0.4927 - val_loss: 0.6737 - val_acc: 0.4913\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6741 - acc: 0.4908 - val_loss: 0.6735 - val_acc: 0.4901\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6735 - acc: 0.4905 - val_loss: 0.6727 - val_acc: 0.4902\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6731 - acc: 0.4904 - val_loss: 0.6724 - val_acc: 0.4900\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6730 - acc: 0.4904 - val_loss: 0.6733 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6728 - acc: 0.4905 - val_loss: 0.6728 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 365s 405us/step - loss: 0.6728 - acc: 0.4905 - val_loss: 0.6724 - val_acc: 0.4898\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6727 - acc: 0.4903 - val_loss: 0.6728 - val_acc: 0.4898\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6726 - acc: 0.4904 - val_loss: 0.6721 - val_acc: 0.4898\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6725 - acc: 0.4906 - val_loss: 0.6725 - val_acc: 0.4899\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6725 - acc: 0.4902 - val_loss: 0.6728 - val_acc: 0.4899\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 361s 401us/step - loss: 0.6725 - acc: 0.4908 - val_loss: 0.6718 - val_acc: 0.4898\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6724 - acc: 0.4907 - val_loss: 0.6725 - val_acc: 0.4897\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6724 - acc: 0.4901 - val_loss: 0.6727 - val_acc: 0.4899\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6724 - acc: 0.4903 - val_loss: 0.6728 - val_acc: 0.4898\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6723 - acc: 0.4904 - val_loss: 0.6722 - val_acc: 0.4896\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6723 - acc: 0.4902 - val_loss: 0.6725 - val_acc: 0.4898\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6724 - acc: 0.4904 - val_loss: 0.6729 - val_acc: 0.4898\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6724 - acc: 0.4905 - val_loss: 0.6729 - val_acc: 0.4896\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6723 - acc: 0.4904 - val_loss: 0.6724 - val_acc: 0.4896\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6723 - acc: 0.4905 - val_loss: 0.6721 - val_acc: 0.4896\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6724 - acc: 0.4904 - val_loss: 0.6725 - val_acc: 0.4897\n",
      "trainnig theta = : 0.10833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 367s 408us/step - loss: 0.6757 - acc: 0.4904 - val_loss: 0.6759 - val_acc: 0.4898\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6757 - acc: 0.4904 - val_loss: 0.6762 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6757 - acc: 0.4902 - val_loss: 0.6755 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6757 - acc: 0.4904 - val_loss: 0.6759 - val_acc: 0.4898\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6756 - acc: 0.4905 - val_loss: 0.6753 - val_acc: 0.4899\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6757 - acc: 0.4901 - val_loss: 0.6756 - val_acc: 0.4896\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6755 - acc: 0.4905 - val_loss: 0.6755 - val_acc: 0.4896\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6756 - acc: 0.4903 - val_loss: 0.6756 - val_acc: 0.4902\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6756 - acc: 0.4904 - val_loss: 0.6755 - val_acc: 0.4898\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6756 - acc: 0.4903 - val_loss: 0.6755 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 361s 401us/step - loss: 0.6755 - acc: 0.4905 - val_loss: 0.6755 - val_acc: 0.4894\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6755 - acc: 0.4905 - val_loss: 0.6752 - val_acc: 0.4896\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6755 - acc: 0.4905 - val_loss: 0.6758 - val_acc: 0.4896\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6756 - acc: 0.4904 - val_loss: 0.6758 - val_acc: 0.4895\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 346s 385us/step - loss: 0.6755 - acc: 0.4905 - val_loss: 0.6755 - val_acc: 0.4899\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6756 - acc: 0.4906 - val_loss: 0.6755 - val_acc: 0.4896\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 344s 383us/step - loss: 0.6755 - acc: 0.4904 - val_loss: 0.6763 - val_acc: 0.4900\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6756 - acc: 0.4905 - val_loss: 0.6759 - val_acc: 0.4896\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6755 - acc: 0.4904 - val_loss: 0.6764 - val_acc: 0.4895\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6755 - acc: 0.4904 - val_loss: 0.6755 - val_acc: 0.4897\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6755 - acc: 0.4907 - val_loss: 0.6761 - val_acc: 0.4897\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 343s 382us/step - loss: 0.6755 - acc: 0.4905 - val_loss: 0.6767 - val_acc: 0.4899\n",
      "trainnig theta = : 0.11666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6784 - acc: 0.4906 - val_loss: 0.6786 - val_acc: 0.4896\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 352s 391us/step - loss: 0.6784 - acc: 0.4907 - val_loss: 0.6790 - val_acc: 0.4896\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 353s 392us/step - loss: 0.6784 - acc: 0.4906 - val_loss: 0.6783 - val_acc: 0.4896\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 377s 419us/step - loss: 0.6784 - acc: 0.4904 - val_loss: 0.6790 - val_acc: 0.4897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 375s 417us/step - loss: 0.6784 - acc: 0.4906 - val_loss: 0.6786 - val_acc: 0.4898\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 371s 412us/step - loss: 0.6784 - acc: 0.4907 - val_loss: 0.6785 - val_acc: 0.4897\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 371s 412us/step - loss: 0.6783 - acc: 0.4906 - val_loss: 0.6791 - val_acc: 0.4897\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 372s 413us/step - loss: 0.6784 - acc: 0.4905 - val_loss: 0.6785 - val_acc: 0.4896\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 371s 413us/step - loss: 0.6784 - acc: 0.4905 - val_loss: 0.6787 - val_acc: 0.4897\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 370s 411us/step - loss: 0.6784 - acc: 0.4904 - val_loss: 0.6788 - val_acc: 0.4898\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 369s 410us/step - loss: 0.6783 - acc: 0.4906 - val_loss: 0.6784 - val_acc: 0.4897\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 370s 411us/step - loss: 0.6784 - acc: 0.4904 - val_loss: 0.6785 - val_acc: 0.4896\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 371s 413us/step - loss: 0.6784 - acc: 0.4902 - val_loss: 0.6785 - val_acc: 0.4896\n",
      "trainnig theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 367s 408us/step - loss: 0.6811 - acc: 0.4902 - val_loss: 0.6812 - val_acc: 0.4896\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 360s 400us/step - loss: 0.6809 - acc: 0.4904 - val_loss: 0.6809 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6810 - acc: 0.4904 - val_loss: 0.6810 - val_acc: 0.4902\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6810 - acc: 0.4905 - val_loss: 0.6808 - val_acc: 0.4895\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6810 - acc: 0.4905 - val_loss: 0.6810 - val_acc: 0.4897\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6810 - acc: 0.4905 - val_loss: 0.6811 - val_acc: 0.4896\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6810 - acc: 0.4905 - val_loss: 0.6812 - val_acc: 0.4900\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 365s 405us/step - loss: 0.6810 - acc: 0.4904 - val_loss: 0.6812 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6810 - acc: 0.4907 - val_loss: 0.6813 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 366s 406us/step - loss: 0.6809 - acc: 0.4906 - val_loss: 0.6811 - val_acc: 0.4896\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6810 - acc: 0.4905 - val_loss: 0.6810 - val_acc: 0.4897\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6810 - acc: 0.4906 - val_loss: 0.6811 - val_acc: 0.4897\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 354s 393us/step - loss: 0.6809 - acc: 0.4902 - val_loss: 0.6810 - val_acc: 0.4897\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6809 - acc: 0.4905 - val_loss: 0.6811 - val_acc: 0.4897\n",
      "trainnig theta = : 0.13333333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 347s 385us/step - loss: 0.6835 - acc: 0.4905 - val_loss: 0.6834 - val_acc: 0.4895\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 340s 377us/step - loss: 0.6836 - acc: 0.4904 - val_loss: 0.6836 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6836 - acc: 0.4904 - val_loss: 0.6837 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 357s 396us/step - loss: 0.6835 - acc: 0.4906 - val_loss: 0.6841 - val_acc: 0.4900\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 361s 402us/step - loss: 0.6835 - acc: 0.4906 - val_loss: 0.6831 - val_acc: 0.4894\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 363s 403us/step - loss: 0.6835 - acc: 0.4903 - val_loss: 0.6837 - val_acc: 0.4897\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 359s 399us/step - loss: 0.6835 - acc: 0.4906 - val_loss: 0.6833 - val_acc: 0.4896\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6835 - acc: 0.4904 - val_loss: 0.6837 - val_acc: 0.4896\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 365s 405us/step - loss: 0.6835 - acc: 0.4907 - val_loss: 0.6838 - val_acc: 0.4895\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6835 - acc: 0.4906 - val_loss: 0.6833 - val_acc: 0.4898\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 363s 404us/step - loss: 0.6835 - acc: 0.4905 - val_loss: 0.6836 - val_acc: 0.4897\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 362s 403us/step - loss: 0.6835 - acc: 0.4906 - val_loss: 0.6836 - val_acc: 0.4895\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 364s 405us/step - loss: 0.6835 - acc: 0.4909 - val_loss: 0.6836 - val_acc: 0.4896\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 362s 402us/step - loss: 0.6834 - acc: 0.4907 - val_loss: 0.6836 - val_acc: 0.4896\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 365s 405us/step - loss: 0.6835 - acc: 0.4905 - val_loss: 0.6835 - val_acc: 0.4896\n",
      "trainnig theta = : 0.14166666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 370s 411us/step - loss: 0.6860 - acc: 0.4906 - val_loss: 0.6859 - val_acc: 0.4895\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6861 - acc: 0.4908 - val_loss: 0.6861 - val_acc: 0.4900\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6860 - acc: 0.4907 - val_loss: 0.6860 - val_acc: 0.4897\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6860 - acc: 0.4906 - val_loss: 0.6862 - val_acc: 0.4897\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6860 - acc: 0.4906 - val_loss: 0.6861 - val_acc: 0.4898\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6860 - acc: 0.4909 - val_loss: 0.6862 - val_acc: 0.4894\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 339s 377us/step - loss: 0.6860 - acc: 0.4903 - val_loss: 0.6860 - val_acc: 0.4895\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6860 - acc: 0.4907 - val_loss: 0.6858 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6860 - acc: 0.4906 - val_loss: 0.6862 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6860 - acc: 0.4911 - val_loss: 0.6860 - val_acc: 0.4896\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6860 - acc: 0.4905 - val_loss: 0.6861 - val_acc: 0.4895\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6860 - acc: 0.4910 - val_loss: 0.6860 - val_acc: 0.4895\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6860 - acc: 0.4907 - val_loss: 0.6861 - val_acc: 0.4893\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6860 - acc: 0.4907 - val_loss: 0.6860 - val_acc: 0.4893\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6860 - acc: 0.4908 - val_loss: 0.6863 - val_acc: 0.4900\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6860 - acc: 0.4909 - val_loss: 0.6859 - val_acc: 0.4897\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6860 - acc: 0.4908 - val_loss: 0.6860 - val_acc: 0.4897\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6860 - acc: 0.4910 - val_loss: 0.6862 - val_acc: 0.4904\n",
      "trainnig theta = : 0.15000000000000002\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 347s 386us/step - loss: 0.6884 - acc: 0.4906 - val_loss: 0.6884 - val_acc: 0.4901\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6884 - acc: 0.4909 - val_loss: 0.6883 - val_acc: 0.4896\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6884 - acc: 0.4906 - val_loss: 0.6885 - val_acc: 0.4897\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6884 - acc: 0.4905 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6884 - acc: 0.4908 - val_loss: 0.6882 - val_acc: 0.4898\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6884 - acc: 0.4907 - val_loss: 0.6882 - val_acc: 0.4896\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6884 - acc: 0.4908 - val_loss: 0.6884 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 346s 385us/step - loss: 0.6884 - acc: 0.4911 - val_loss: 0.6884 - val_acc: 0.4896\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6884 - acc: 0.4910 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 343s 382us/step - loss: 0.6884 - acc: 0.4912 - val_loss: 0.6885 - val_acc: 0.4901\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6883 - acc: 0.4908 - val_loss: 0.6884 - val_acc: 0.4898\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6884 - acc: 0.4910 - val_loss: 0.6883 - val_acc: 0.4896\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 346s 385us/step - loss: 0.6883 - acc: 0.4910 - val_loss: 0.6882 - val_acc: 0.4898\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 354s 393us/step - loss: 0.6883 - acc: 0.4909 - val_loss: 0.6889 - val_acc: 0.4898\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 368s 408us/step - loss: 0.6883 - acc: 0.4910 - val_loss: 0.6883 - val_acc: 0.4897\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 368s 409us/step - loss: 0.6883 - acc: 0.4911 - val_loss: 0.6884 - val_acc: 0.4894\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6883 - acc: 0.4910 - val_loss: 0.6884 - val_acc: 0.4897\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6883 - acc: 0.4912 - val_loss: 0.6885 - val_acc: 0.4897\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6883 - acc: 0.4913 - val_loss: 0.6882 - val_acc: 0.4897\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 366s 407us/step - loss: 0.6884 - acc: 0.4909 - val_loss: 0.6882 - val_acc: 0.4895\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 365s 406us/step - loss: 0.6884 - acc: 0.4909 - val_loss: 0.6884 - val_acc: 0.4905\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 367s 408us/step - loss: 0.6884 - acc: 0.4912 - val_loss: 0.6884 - val_acc: 0.4895\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 364s 404us/step - loss: 0.6883 - acc: 0.4911 - val_loss: 0.6887 - val_acc: 0.4898\n",
      "trainnig theta = : 0.15833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 357s 396us/step - loss: 0.6904 - acc: 0.4911 - val_loss: 0.6904 - val_acc: 0.4898\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 347s 385us/step - loss: 0.6905 - acc: 0.4910 - val_loss: 0.6904 - val_acc: 0.4901\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 347s 385us/step - loss: 0.6905 - acc: 0.4910 - val_loss: 0.6905 - val_acc: 0.4899\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6905 - acc: 0.4909 - val_loss: 0.6903 - val_acc: 0.4898\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 345s 384us/step - loss: 0.6905 - acc: 0.4914 - val_loss: 0.6905 - val_acc: 0.4906\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 346s 384us/step - loss: 0.6905 - acc: 0.4912 - val_loss: 0.6905 - val_acc: 0.4898\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 348s 386us/step - loss: 0.6904 - acc: 0.4915 - val_loss: 0.6903 - val_acc: 0.4901\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 349s 387us/step - loss: 0.6904 - acc: 0.4914 - val_loss: 0.6904 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 346s 385us/step - loss: 0.6905 - acc: 0.4915 - val_loss: 0.6904 - val_acc: 0.4898\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6904 - acc: 0.4914 - val_loss: 0.6905 - val_acc: 0.4902\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 349s 387us/step - loss: 0.6904 - acc: 0.4913 - val_loss: 0.6904 - val_acc: 0.4897\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 347s 386us/step - loss: 0.6904 - acc: 0.4912 - val_loss: 0.6906 - val_acc: 0.4898\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 345s 384us/step - loss: 0.6904 - acc: 0.4915 - val_loss: 0.6904 - val_acc: 0.4900\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6904 - acc: 0.4912 - val_loss: 0.6904 - val_acc: 0.4898\n",
      "trainnig theta = : 0.16666666666666669\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 350s 389us/step - loss: 0.6921 - acc: 0.4914 - val_loss: 0.6920 - val_acc: 0.4900\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 347s 385us/step - loss: 0.6921 - acc: 0.4913 - val_loss: 0.6922 - val_acc: 0.4902\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 346s 385us/step - loss: 0.6921 - acc: 0.4914 - val_loss: 0.6920 - val_acc: 0.4900\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 349s 388us/step - loss: 0.6921 - acc: 0.4913 - val_loss: 0.6920 - val_acc: 0.4898\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6921 - acc: 0.4914 - val_loss: 0.6920 - val_acc: 0.4897\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6921 - acc: 0.4915 - val_loss: 0.6921 - val_acc: 0.4901\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 349s 388us/step - loss: 0.6921 - acc: 0.4913 - val_loss: 0.6920 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 347s 386us/step - loss: 0.6921 - acc: 0.4917 - val_loss: 0.6921 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 350s 388us/step - loss: 0.6921 - acc: 0.4916 - val_loss: 0.6920 - val_acc: 0.4898\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6921 - acc: 0.4912 - val_loss: 0.6920 - val_acc: 0.4898\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 348s 387us/step - loss: 0.6921 - acc: 0.4915 - val_loss: 0.6921 - val_acc: 0.4901\n",
      "trainnig theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 354s 393us/step - loss: 0.6930 - acc: 0.4922 - val_loss: 0.6929 - val_acc: 0.4900\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 348s 386us/step - loss: 0.6930 - acc: 0.4920 - val_loss: 0.6930 - val_acc: 0.4903\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 346s 385us/step - loss: 0.6930 - acc: 0.4916 - val_loss: 0.6930 - val_acc: 0.4899\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6930 - acc: 0.4919 - val_loss: 0.6930 - val_acc: 0.4899\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6930 - acc: 0.4915 - val_loss: 0.6930 - val_acc: 0.4903\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 344s 382us/step - loss: 0.6929 - acc: 0.4921 - val_loss: 0.6930 - val_acc: 0.4905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6930 - acc: 0.4915 - val_loss: 0.6930 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 342s 381us/step - loss: 0.6929 - acc: 0.4922 - val_loss: 0.6930 - val_acc: 0.4904\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6930 - acc: 0.4920 - val_loss: 0.6930 - val_acc: 0.4904\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 339s 377us/step - loss: 0.6930 - acc: 0.4924 - val_loss: 0.6929 - val_acc: 0.4904\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 339s 377us/step - loss: 0.6929 - acc: 0.4922 - val_loss: 0.6930 - val_acc: 0.4899\n",
      "trainnig theta = : 0.18333333333333335\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 345s 383us/step - loss: 0.6932 - acc: 0.4918 - val_loss: 0.6931 - val_acc: 0.4907\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 338s 376us/step - loss: 0.6932 - acc: 0.4921 - val_loss: 0.6931 - val_acc: 0.4902\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 336s 373us/step - loss: 0.6932 - acc: 0.4918 - val_loss: 0.6931 - val_acc: 0.4905\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 338s 376us/step - loss: 0.6931 - acc: 0.4926 - val_loss: 0.6931 - val_acc: 0.4906\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 341s 378us/step - loss: 0.6931 - acc: 0.4920 - val_loss: 0.6931 - val_acc: 0.4902\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 340s 377us/step - loss: 0.6931 - acc: 0.4921 - val_loss: 0.6931 - val_acc: 0.4902\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 339s 376us/step - loss: 0.6931 - acc: 0.4923 - val_loss: 0.6931 - val_acc: 0.4902\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6932 - acc: 0.4921 - val_loss: 0.6931 - val_acc: 0.4913\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 339s 376us/step - loss: 0.6931 - acc: 0.4927 - val_loss: 0.6931 - val_acc: 0.4903\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6931 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.4904\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 339s 377us/step - loss: 0.6931 - acc: 0.4921 - val_loss: 0.6931 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 340s 377us/step - loss: 0.6931 - acc: 0.4936 - val_loss: 0.6931 - val_acc: 0.4902\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 343s 381us/step - loss: 0.6931 - acc: 0.4926 - val_loss: 0.6931 - val_acc: 0.4904\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 340s 378us/step - loss: 0.6931 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.4906\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 342s 380us/step - loss: 0.6931 - acc: 0.4927 - val_loss: 0.6931 - val_acc: 0.4903\n",
      "trainnig theta = : 0.19166666666666665\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 341s 379us/step - loss: 0.6929 - acc: 0.4926 - val_loss: 0.6929 - val_acc: 0.4923\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 335s 372us/step - loss: 0.6929 - acc: 0.4927 - val_loss: 0.6928 - val_acc: 0.4910\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 334s 372us/step - loss: 0.6929 - acc: 0.4935 - val_loss: 0.6929 - val_acc: 0.4930\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6929 - acc: 0.4932 - val_loss: 0.6929 - val_acc: 0.4912\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6929 - acc: 0.4938 - val_loss: 0.6929 - val_acc: 0.4910\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6929 - acc: 0.4937 - val_loss: 0.6928 - val_acc: 0.4916\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6929 - acc: 0.4932 - val_loss: 0.6929 - val_acc: 0.4922\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6929 - acc: 0.4935 - val_loss: 0.6928 - val_acc: 0.4923\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6929 - acc: 0.4939 - val_loss: 0.6928 - val_acc: 0.4913\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6929 - acc: 0.4940 - val_loss: 0.6929 - val_acc: 0.4908\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 333s 370us/step - loss: 0.6929 - acc: 0.4936 - val_loss: 0.6929 - val_acc: 0.4913\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6929 - acc: 0.4938 - val_loss: 0.6929 - val_acc: 0.4933\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 330s 367us/step - loss: 0.6929 - acc: 0.4943 - val_loss: 0.6928 - val_acc: 0.4915\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 329s 365us/step - loss: 0.6929 - acc: 0.4940 - val_loss: 0.6928 - val_acc: 0.4913\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6929 - acc: 0.4939 - val_loss: 0.6928 - val_acc: 0.4907\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 328s 365us/step - loss: 0.6928 - acc: 0.4939 - val_loss: 0.6929 - val_acc: 0.4930\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 329s 365us/step - loss: 0.6929 - acc: 0.4942 - val_loss: 0.6929 - val_acc: 0.4916\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 335s 372us/step - loss: 0.6928 - acc: 0.4949 - val_loss: 0.6928 - val_acc: 0.4921\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 333s 371us/step - loss: 0.6928 - acc: 0.4941 - val_loss: 0.6929 - val_acc: 0.4926\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6928 - acc: 0.4941 - val_loss: 0.6929 - val_acc: 0.4921\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6929 - acc: 0.4946 - val_loss: 0.6928 - val_acc: 0.4918\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 302s 336us/step - loss: 0.6929 - acc: 0.4945 - val_loss: 0.6929 - val_acc: 0.4920\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6929 - acc: 0.4942 - val_loss: 0.6929 - val_acc: 0.4917\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6929 - acc: 0.4943 - val_loss: 0.6929 - val_acc: 0.4913\n",
      "trainnig theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 313s 348us/step - loss: 0.6926 - acc: 0.4958 - val_loss: 0.6926 - val_acc: 0.4925\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6926 - acc: 0.4962 - val_loss: 0.6926 - val_acc: 0.4929\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6926 - acc: 0.4962 - val_loss: 0.6926 - val_acc: 0.4940\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 305s 338us/step - loss: 0.6926 - acc: 0.4955 - val_loss: 0.6926 - val_acc: 0.4924\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6926 - acc: 0.4951 - val_loss: 0.6926 - val_acc: 0.4952\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6926 - acc: 0.4966 - val_loss: 0.6926 - val_acc: 0.4933\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 308s 343us/step - loss: 0.6926 - acc: 0.4968 - val_loss: 0.6926 - val_acc: 0.4929\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6926 - acc: 0.4965 - val_loss: 0.6926 - val_acc: 0.4941\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 303s 337us/step - loss: 0.6926 - acc: 0.4973 - val_loss: 0.6926 - val_acc: 0.4967\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 307s 342us/step - loss: 0.6926 - acc: 0.4961 - val_loss: 0.6926 - val_acc: 0.4951\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6926 - acc: 0.4964 - val_loss: 0.6926 - val_acc: 0.4956\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6926 - acc: 0.4970 - val_loss: 0.6926 - val_acc: 0.4919\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6926 - acc: 0.4962 - val_loss: 0.6926 - val_acc: 0.4941\n",
      "trainnig theta = : 0.20833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 312s 347us/step - loss: 0.6924 - acc: 0.4966 - val_loss: 0.6924 - val_acc: 0.4969\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6924 - acc: 0.5010 - val_loss: 0.6924 - val_acc: 0.4990\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6924 - acc: 0.5011 - val_loss: 0.6924 - val_acc: 0.4989\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 322s 358us/step - loss: 0.6924 - acc: 0.4999 - val_loss: 0.6924 - val_acc: 0.4992\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 320s 355us/step - loss: 0.6924 - acc: 0.5011 - val_loss: 0.6924 - val_acc: 0.4988\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6924 - acc: 0.5017 - val_loss: 0.6924 - val_acc: 0.4981\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6924 - acc: 0.5024 - val_loss: 0.6924 - val_acc: 0.5004\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6924 - acc: 0.5033 - val_loss: 0.6924 - val_acc: 0.5010\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6924 - acc: 0.5021 - val_loss: 0.6924 - val_acc: 0.5002\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6924 - acc: 0.5032 - val_loss: 0.6924 - val_acc: 0.5013\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 301s 335us/step - loss: 0.6924 - acc: 0.5022 - val_loss: 0.6924 - val_acc: 0.5006\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6924 - acc: 0.5034 - val_loss: 0.6924 - val_acc: 0.4997\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6924 - acc: 0.5036 - val_loss: 0.6924 - val_acc: 0.5009\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6924 - acc: 0.5025 - val_loss: 0.6924 - val_acc: 0.5013\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6924 - acc: 0.5026 - val_loss: 0.6924 - val_acc: 0.5015\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6924 - acc: 0.5029 - val_loss: 0.6924 - val_acc: 0.4991\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6923 - acc: 0.5037 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6924 - acc: 0.5043 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6924 - acc: 0.5033 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6923 - acc: 0.5041 - val_loss: 0.6924 - val_acc: 0.5029\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 304s 337us/step - loss: 0.6923 - acc: 0.5042 - val_loss: 0.6924 - val_acc: 0.4991\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6923 - acc: 0.5035 - val_loss: 0.6925 - val_acc: 0.5003\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6924 - acc: 0.5033 - val_loss: 0.6924 - val_acc: 0.5018\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6923 - acc: 0.5032 - val_loss: 0.6924 - val_acc: 0.5009\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6923 - acc: 0.5046 - val_loss: 0.6924 - val_acc: 0.5043\n",
      "trainnig theta = : 0.21666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 312s 347us/step - loss: 0.6922 - acc: 0.5059 - val_loss: 0.6922 - val_acc: 0.5071\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6922 - acc: 0.5064 - val_loss: 0.6923 - val_acc: 0.5031\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6922 - acc: 0.5071 - val_loss: 0.6922 - val_acc: 0.5069\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6922 - acc: 0.5071 - val_loss: 0.6922 - val_acc: 0.5053\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6922 - acc: 0.5068 - val_loss: 0.6922 - val_acc: 0.5052\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6922 - acc: 0.5068 - val_loss: 0.6923 - val_acc: 0.5065\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 306s 339us/step - loss: 0.6922 - acc: 0.5071 - val_loss: 0.6923 - val_acc: 0.5051\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6922 - acc: 0.5068 - val_loss: 0.6922 - val_acc: 0.5060\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6922 - acc: 0.5071 - val_loss: 0.6922 - val_acc: 0.5058\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6922 - acc: 0.5074 - val_loss: 0.6923 - val_acc: 0.5057\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6921 - acc: 0.5070 - val_loss: 0.6923 - val_acc: 0.5051\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6922 - acc: 0.5063 - val_loss: 0.6922 - val_acc: 0.5038\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 306s 341us/step - loss: 0.6921 - acc: 0.5078 - val_loss: 0.6922 - val_acc: 0.5046\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6921 - acc: 0.5078 - val_loss: 0.6922 - val_acc: 0.5054\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 307s 342us/step - loss: 0.6921 - acc: 0.5069 - val_loss: 0.6922 - val_acc: 0.5049\n",
      "trainnig theta = : 0.225\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 314s 348us/step - loss: 0.6915 - acc: 0.5079 - val_loss: 0.6916 - val_acc: 0.5077\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6915 - acc: 0.5082 - val_loss: 0.6915 - val_acc: 0.5083\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 306s 341us/step - loss: 0.6915 - acc: 0.5085 - val_loss: 0.6916 - val_acc: 0.5065\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6915 - acc: 0.5087 - val_loss: 0.6915 - val_acc: 0.5081\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 304s 337us/step - loss: 0.6915 - acc: 0.5094 - val_loss: 0.6915 - val_acc: 0.5081\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6915 - acc: 0.5087 - val_loss: 0.6915 - val_acc: 0.5082\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6914 - acc: 0.5094 - val_loss: 0.6916 - val_acc: 0.5079\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6915 - acc: 0.5088 - val_loss: 0.6916 - val_acc: 0.5081\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 302s 335us/step - loss: 0.6915 - acc: 0.5093 - val_loss: 0.6915 - val_acc: 0.5080\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6914 - acc: 0.5092 - val_loss: 0.6915 - val_acc: 0.5086\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6914 - acc: 0.5093 - val_loss: 0.6915 - val_acc: 0.5090\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6914 - acc: 0.5094 - val_loss: 0.6916 - val_acc: 0.5085\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6914 - acc: 0.5097 - val_loss: 0.6916 - val_acc: 0.5081\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6914 - acc: 0.5095 - val_loss: 0.6916 - val_acc: 0.5077\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6914 - acc: 0.5090 - val_loss: 0.6915 - val_acc: 0.5083\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6914 - acc: 0.5100 - val_loss: 0.6916 - val_acc: 0.5087\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6914 - acc: 0.5100 - val_loss: 0.6915 - val_acc: 0.5090\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6914 - acc: 0.5094 - val_loss: 0.6916 - val_acc: 0.5083\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6914 - acc: 0.5097 - val_loss: 0.6916 - val_acc: 0.5078\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6914 - acc: 0.5100 - val_loss: 0.6915 - val_acc: 0.5080\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 309s 343us/step - loss: 0.6914 - acc: 0.5100 - val_loss: 0.6915 - val_acc: 0.5085\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6914 - acc: 0.5102 - val_loss: 0.6915 - val_acc: 0.5087\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 308s 343us/step - loss: 0.6914 - acc: 0.5098 - val_loss: 0.6915 - val_acc: 0.5076\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 308s 343us/step - loss: 0.6914 - acc: 0.5099 - val_loss: 0.6916 - val_acc: 0.5081\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6914 - acc: 0.5098 - val_loss: 0.6915 - val_acc: 0.5085\n",
      "trainnig theta = : 0.23333333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 313s 348us/step - loss: 0.6902 - acc: 0.5096 - val_loss: 0.6905 - val_acc: 0.5076\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6902 - acc: 0.5100 - val_loss: 0.6904 - val_acc: 0.5090\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6902 - acc: 0.5100 - val_loss: 0.6903 - val_acc: 0.5093\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 307s 342us/step - loss: 0.6903 - acc: 0.5099 - val_loss: 0.6904 - val_acc: 0.5089\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6902 - acc: 0.5103 - val_loss: 0.6903 - val_acc: 0.5092\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6902 - acc: 0.5101 - val_loss: 0.6904 - val_acc: 0.5092\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 307s 342us/step - loss: 0.6902 - acc: 0.5098 - val_loss: 0.6903 - val_acc: 0.5090\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6902 - acc: 0.5105 - val_loss: 0.6903 - val_acc: 0.5092\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6902 - acc: 0.5109 - val_loss: 0.6904 - val_acc: 0.5095\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6902 - acc: 0.5102 - val_loss: 0.6903 - val_acc: 0.5094\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6902 - acc: 0.5104 - val_loss: 0.6903 - val_acc: 0.5091\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 304s 337us/step - loss: 0.6902 - acc: 0.5103 - val_loss: 0.6904 - val_acc: 0.5089\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6902 - acc: 0.5101 - val_loss: 0.6904 - val_acc: 0.5087\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6902 - acc: 0.5102 - val_loss: 0.6903 - val_acc: 0.5092\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 301s 335us/step - loss: 0.6902 - acc: 0.5103 - val_loss: 0.6903 - val_acc: 0.5095\n",
      "trainnig theta = : 0.24166666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 313s 348us/step - loss: 0.6892 - acc: 0.5105 - val_loss: 0.6894 - val_acc: 0.5094\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6892 - acc: 0.5105 - val_loss: 0.6893 - val_acc: 0.5094\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6892 - acc: 0.5103 - val_loss: 0.6893 - val_acc: 0.5096\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6892 - acc: 0.5103 - val_loss: 0.6893 - val_acc: 0.5093\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6891 - acc: 0.5103 - val_loss: 0.6894 - val_acc: 0.5095\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 304s 338us/step - loss: 0.6892 - acc: 0.5103 - val_loss: 0.6893 - val_acc: 0.5089\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6892 - acc: 0.5107 - val_loss: 0.6893 - val_acc: 0.5094\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 302s 335us/step - loss: 0.6892 - acc: 0.5099 - val_loss: 0.6894 - val_acc: 0.5093\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6891 - acc: 0.5102 - val_loss: 0.6893 - val_acc: 0.5096\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6891 - acc: 0.5104 - val_loss: 0.6893 - val_acc: 0.5091\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6892 - acc: 0.5106 - val_loss: 0.6894 - val_acc: 0.5095\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6891 - acc: 0.5107 - val_loss: 0.6894 - val_acc: 0.5091\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 303s 337us/step - loss: 0.6891 - acc: 0.5109 - val_loss: 0.6894 - val_acc: 0.5095\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6891 - acc: 0.5108 - val_loss: 0.6893 - val_acc: 0.5094\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6891 - acc: 0.5103 - val_loss: 0.6893 - val_acc: 0.5093\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6891 - acc: 0.5108 - val_loss: 0.6893 - val_acc: 0.5098\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 305s 339us/step - loss: 0.6891 - acc: 0.5111 - val_loss: 0.6893 - val_acc: 0.5097\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 303s 337us/step - loss: 0.6891 - acc: 0.5104 - val_loss: 0.6893 - val_acc: 0.5091\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 306s 340us/step - loss: 0.6891 - acc: 0.5105 - val_loss: 0.6893 - val_acc: 0.5091\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 307s 341us/step - loss: 0.6891 - acc: 0.5114 - val_loss: 0.6893 - val_acc: 0.5095\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 309s 343us/step - loss: 0.6891 - acc: 0.5107 - val_loss: 0.6893 - val_acc: 0.5092\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 310s 344us/step - loss: 0.6891 - acc: 0.5112 - val_loss: 0.6893 - val_acc: 0.5094\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 310s 344us/step - loss: 0.6891 - acc: 0.5106 - val_loss: 0.6894 - val_acc: 0.5095\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 308s 342us/step - loss: 0.6891 - acc: 0.5107 - val_loss: 0.6893 - val_acc: 0.5093\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 311s 346us/step - loss: 0.6891 - acc: 0.5108 - val_loss: 0.6893 - val_acc: 0.5095\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 309s 344us/step - loss: 0.6891 - acc: 0.5111 - val_loss: 0.6893 - val_acc: 0.5094\n",
      "trainnig theta = : 0.25\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 321s 356us/step - loss: 0.6884 - acc: 0.5108 - val_loss: 0.6887 - val_acc: 0.5097\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 310s 344us/step - loss: 0.6884 - acc: 0.5112 - val_loss: 0.6886 - val_acc: 0.5096\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 315s 350us/step - loss: 0.6884 - acc: 0.5111 - val_loss: 0.6887 - val_acc: 0.5097\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 314s 349us/step - loss: 0.6885 - acc: 0.5104 - val_loss: 0.6887 - val_acc: 0.5095\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 318s 353us/step - loss: 0.6884 - acc: 0.5111 - val_loss: 0.6887 - val_acc: 0.5097\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 333s 370us/step - loss: 0.6885 - acc: 0.5103 - val_loss: 0.6887 - val_acc: 0.5094\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6884 - acc: 0.5109 - val_loss: 0.6887 - val_acc: 0.5095\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 323s 358us/step - loss: 0.6884 - acc: 0.5109 - val_loss: 0.6887 - val_acc: 0.5089\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 322s 358us/step - loss: 0.6884 - acc: 0.5107 - val_loss: 0.6887 - val_acc: 0.5093\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 323s 359us/step - loss: 0.6884 - acc: 0.5112 - val_loss: 0.6887 - val_acc: 0.5095\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 323s 359us/step - loss: 0.6884 - acc: 0.5111 - val_loss: 0.6887 - val_acc: 0.5090\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 324s 360us/step - loss: 0.6884 - acc: 0.5113 - val_loss: 0.6887 - val_acc: 0.5096\n",
      "trainnig theta = : 0.2583333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6879 - acc: 0.5105 - val_loss: 0.6883 - val_acc: 0.5097\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 325s 362us/step - loss: 0.6878 - acc: 0.5106 - val_loss: 0.6881 - val_acc: 0.5092\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6878 - acc: 0.5108 - val_loss: 0.6880 - val_acc: 0.5095\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 328s 365us/step - loss: 0.6878 - acc: 0.5104 - val_loss: 0.6881 - val_acc: 0.5094\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6878 - acc: 0.5107 - val_loss: 0.6880 - val_acc: 0.5098\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 321s 357us/step - loss: 0.6879 - acc: 0.5108 - val_loss: 0.6882 - val_acc: 0.5095\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 326s 363us/step - loss: 0.6878 - acc: 0.5106 - val_loss: 0.6882 - val_acc: 0.5098\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 330s 366us/step - loss: 0.6878 - acc: 0.5108 - val_loss: 0.6880 - val_acc: 0.5096\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 327s 364us/step - loss: 0.6878 - acc: 0.5107 - val_loss: 0.6881 - val_acc: 0.5095\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 331s 367us/step - loss: 0.6878 - acc: 0.5109 - val_loss: 0.6880 - val_acc: 0.5092\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6878 - acc: 0.5111 - val_loss: 0.6881 - val_acc: 0.5097\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6878 - acc: 0.5111 - val_loss: 0.6880 - val_acc: 0.5096\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6878 - acc: 0.5111 - val_loss: 0.6881 - val_acc: 0.5095\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 330s 366us/step - loss: 0.6878 - acc: 0.5111 - val_loss: 0.6881 - val_acc: 0.5096\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 333s 370us/step - loss: 0.6878 - acc: 0.5109 - val_loss: 0.6881 - val_acc: 0.5097\n",
      "trainnig theta = : 0.26666666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6869 - acc: 0.5105 - val_loss: 0.6873 - val_acc: 0.5093\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 327s 363us/step - loss: 0.6869 - acc: 0.5109 - val_loss: 0.6871 - val_acc: 0.5098\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6869 - acc: 0.5111 - val_loss: 0.6872 - val_acc: 0.5096\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6869 - acc: 0.5110 - val_loss: 0.6872 - val_acc: 0.5099\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 327s 363us/step - loss: 0.6869 - acc: 0.5113 - val_loss: 0.6872 - val_acc: 0.5099\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 330s 367us/step - loss: 0.6869 - acc: 0.5108 - val_loss: 0.6871 - val_acc: 0.5097\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 328s 364us/step - loss: 0.6869 - acc: 0.5105 - val_loss: 0.6872 - val_acc: 0.5098\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 327s 364us/step - loss: 0.6869 - acc: 0.5109 - val_loss: 0.6873 - val_acc: 0.5098\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 323s 359us/step - loss: 0.6868 - acc: 0.5112 - val_loss: 0.6872 - val_acc: 0.5100\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6869 - acc: 0.5110 - val_loss: 0.6871 - val_acc: 0.5099\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6869 - acc: 0.5112 - val_loss: 0.6873 - val_acc: 0.5098\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 328s 364us/step - loss: 0.6869 - acc: 0.5114 - val_loss: 0.6873 - val_acc: 0.5093\n",
      "trainnig theta = : 0.275\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 336s 374us/step - loss: 0.6858 - acc: 0.5110 - val_loss: 0.6860 - val_acc: 0.5097\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6857 - acc: 0.5110 - val_loss: 0.6864 - val_acc: 0.5095\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 327s 364us/step - loss: 0.6858 - acc: 0.5106 - val_loss: 0.6862 - val_acc: 0.5096\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 327s 364us/step - loss: 0.6858 - acc: 0.5108 - val_loss: 0.6861 - val_acc: 0.5097\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 327s 363us/step - loss: 0.6858 - acc: 0.5106 - val_loss: 0.6863 - val_acc: 0.5094\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 328s 365us/step - loss: 0.6857 - acc: 0.5110 - val_loss: 0.6862 - val_acc: 0.5099\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6857 - acc: 0.5114 - val_loss: 0.6862 - val_acc: 0.5097\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 323s 359us/step - loss: 0.6857 - acc: 0.5110 - val_loss: 0.6860 - val_acc: 0.5095\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6857 - acc: 0.5110 - val_loss: 0.6860 - val_acc: 0.5100\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 328s 365us/step - loss: 0.6858 - acc: 0.5109 - val_loss: 0.6863 - val_acc: 0.5094\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 323s 359us/step - loss: 0.6857 - acc: 0.5112 - val_loss: 0.6861 - val_acc: 0.5095\n",
      "trainnig theta = : 0.2833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 340s 377us/step - loss: 0.6847 - acc: 0.5107 - val_loss: 0.6852 - val_acc: 0.5101\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6846 - acc: 0.5106 - val_loss: 0.6852 - val_acc: 0.5097\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6846 - acc: 0.5109 - val_loss: 0.6850 - val_acc: 0.5095\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6846 - acc: 0.5105 - val_loss: 0.6851 - val_acc: 0.5096\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 333s 370us/step - loss: 0.6846 - acc: 0.5108 - val_loss: 0.6849 - val_acc: 0.5099\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6846 - acc: 0.5108 - val_loss: 0.6851 - val_acc: 0.5100\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6846 - acc: 0.5107 - val_loss: 0.6849 - val_acc: 0.5098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 327s 364us/step - loss: 0.6847 - acc: 0.5109 - val_loss: 0.6851 - val_acc: 0.5092\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6846 - acc: 0.5111 - val_loss: 0.6849 - val_acc: 0.5100\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6846 - acc: 0.5108 - val_loss: 0.6849 - val_acc: 0.5098\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6845 - acc: 0.5109 - val_loss: 0.6851 - val_acc: 0.5098\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6845 - acc: 0.5115 - val_loss: 0.6849 - val_acc: 0.5098\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6845 - acc: 0.5114 - val_loss: 0.6851 - val_acc: 0.5094\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 330s 367us/step - loss: 0.6845 - acc: 0.5110 - val_loss: 0.6849 - val_acc: 0.5098\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6846 - acc: 0.5112 - val_loss: 0.6852 - val_acc: 0.5099\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6845 - acc: 0.5111 - val_loss: 0.6852 - val_acc: 0.5095\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 326s 362us/step - loss: 0.6845 - acc: 0.5112 - val_loss: 0.6850 - val_acc: 0.5093\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 334s 371us/step - loss: 0.6845 - acc: 0.5110 - val_loss: 0.6848 - val_acc: 0.5098\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 333s 370us/step - loss: 0.6845 - acc: 0.5109 - val_loss: 0.6849 - val_acc: 0.5096\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 327s 363us/step - loss: 0.6845 - acc: 0.5111 - val_loss: 0.6848 - val_acc: 0.5098\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6845 - acc: 0.5117 - val_loss: 0.6850 - val_acc: 0.5095\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6845 - acc: 0.5114 - val_loss: 0.6851 - val_acc: 0.5096\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6845 - acc: 0.5114 - val_loss: 0.6850 - val_acc: 0.5097\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6845 - acc: 0.5116 - val_loss: 0.6851 - val_acc: 0.5097\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 330s 367us/step - loss: 0.6845 - acc: 0.5114 - val_loss: 0.6850 - val_acc: 0.5097\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 332s 369us/step - loss: 0.6845 - acc: 0.5114 - val_loss: 0.6850 - val_acc: 0.5095\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 329s 365us/step - loss: 0.6844 - acc: 0.5121 - val_loss: 0.6848 - val_acc: 0.5096\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 331s 368us/step - loss: 0.6845 - acc: 0.5112 - val_loss: 0.6851 - val_acc: 0.5095\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 330s 367us/step - loss: 0.6845 - acc: 0.5108 - val_loss: 0.6850 - val_acc: 0.5098\n",
      "Epoch 31/100\n",
      "900000/900000 [==============================] - 329s 366us/step - loss: 0.6844 - acc: 0.5113 - val_loss: 0.6850 - val_acc: 0.5094\n",
      "trainnig theta = : 0.29166666666666663\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 337s 374us/step - loss: 0.6833 - acc: 0.5113 - val_loss: 0.6836 - val_acc: 0.5096\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 325s 361us/step - loss: 0.6833 - acc: 0.5112 - val_loss: 0.6835 - val_acc: 0.5100\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 262s 291us/step - loss: 0.6833 - acc: 0.5114 - val_loss: 0.6841 - val_acc: 0.5098\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 226s 251us/step - loss: 0.6834 - acc: 0.5111 - val_loss: 0.6837 - val_acc: 0.5101\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 231s 256us/step - loss: 0.6833 - acc: 0.5110 - val_loss: 0.6838 - val_acc: 0.5097\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 232s 258us/step - loss: 0.6833 - acc: 0.5113 - val_loss: 0.6838 - val_acc: 0.5100\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 233s 259us/step - loss: 0.6833 - acc: 0.5112 - val_loss: 0.6839 - val_acc: 0.5094\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 231s 257us/step - loss: 0.6833 - acc: 0.5110 - val_loss: 0.6838 - val_acc: 0.5095\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 231s 257us/step - loss: 0.6832 - acc: 0.5115 - val_loss: 0.6837 - val_acc: 0.5099\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 230s 256us/step - loss: 0.6833 - acc: 0.5115 - val_loss: 0.6838 - val_acc: 0.5098\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 231s 256us/step - loss: 0.6833 - acc: 0.5115 - val_loss: 0.6838 - val_acc: 0.5095\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 231s 257us/step - loss: 0.6833 - acc: 0.5115 - val_loss: 0.6837 - val_acc: 0.5100\n",
      "trainnig theta = : 0.3\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 239s 265us/step - loss: 0.6821 - acc: 0.5108 - val_loss: 0.6826 - val_acc: 0.5096\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 230s 255us/step - loss: 0.6821 - acc: 0.5110 - val_loss: 0.6827 - val_acc: 0.5092\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 230s 256us/step - loss: 0.6821 - acc: 0.5112 - val_loss: 0.6828 - val_acc: 0.5097\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 230s 256us/step - loss: 0.6821 - acc: 0.5115 - val_loss: 0.6826 - val_acc: 0.5097\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 231s 257us/step - loss: 0.6821 - acc: 0.5111 - val_loss: 0.6825 - val_acc: 0.5101\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 230s 256us/step - loss: 0.6821 - acc: 0.5110 - val_loss: 0.6828 - val_acc: 0.5098\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 225s 250us/step - loss: 0.6820 - acc: 0.5114 - val_loss: 0.6827 - val_acc: 0.5096\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 229s 255us/step - loss: 0.6820 - acc: 0.5115 - val_loss: 0.6827 - val_acc: 0.5095\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 228s 254us/step - loss: 0.6820 - acc: 0.5114 - val_loss: 0.6825 - val_acc: 0.5094\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 230s 255us/step - loss: 0.6820 - acc: 0.5108 - val_loss: 0.6824 - val_acc: 0.5099\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 229s 254us/step - loss: 0.6821 - acc: 0.5111 - val_loss: 0.6825 - val_acc: 0.5100\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 228s 253us/step - loss: 0.6821 - acc: 0.5113 - val_loss: 0.6828 - val_acc: 0.5097\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 229s 255us/step - loss: 0.6820 - acc: 0.5111 - val_loss: 0.6828 - val_acc: 0.5097\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 230s 256us/step - loss: 0.6821 - acc: 0.5118 - val_loss: 0.6825 - val_acc: 0.5102\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 228s 254us/step - loss: 0.6820 - acc: 0.5114 - val_loss: 0.6828 - val_acc: 0.5097\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 230s 255us/step - loss: 0.6820 - acc: 0.5114 - val_loss: 0.6829 - val_acc: 0.5093\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 229s 255us/step - loss: 0.6820 - acc: 0.5118 - val_loss: 0.6825 - val_acc: 0.5098\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 227s 252us/step - loss: 0.6820 - acc: 0.5115 - val_loss: 0.6825 - val_acc: 0.5098\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 227s 253us/step - loss: 0.6820 - acc: 0.5112 - val_loss: 0.6826 - val_acc: 0.5099\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 228s 254us/step - loss: 0.6820 - acc: 0.5112 - val_loss: 0.6827 - val_acc: 0.5097\n",
      "[0.6724823788139556, 0.675545428329044, 0.6783530331982507, 0.6809994108809365, 0.6835048093398413, 0.6859995551904042, 0.6883419800466961, 0.6904595502217611, 0.692084489332305, 0.692971801161766, 0.6931414349211587, 0.6928777006599638, 0.6926197108957503, 0.6923650236262215, 0.6921744169129266, 0.6914407299624549, 0.6902102514770296, 0.6890742939048343, 0.6884157679478328, 0.6878291201591492, 0.6869253764549891, 0.6857678596840965, 0.6845373543103536, 0.6833395557271109, 0.6820406221681171]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 25)   #iterating across possible StoUD values\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam',\n",
    "                        loss=my_loss_wrapper(myinputs, theta),\n",
    "                        metrics=['accuracy'])\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystopping])\n",
    "    lvals += [history.history['loss'][np.argmin(history.history['val_loss'])]]\n",
    "    vlvals += [np.min(history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T13:25:08.776185Z",
     "start_time": "2020-07-25T13:25:06.292292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFtCAYAAACJLFTlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xl8FPX9+PHXJwfhDJtAUE5xUxDBq0lQ8VaCR7UebYDa1rbWklXrt9dPE7G2aq3FUGutWmuCVWu1Con33Swe9WwlEU88YEHlkiMsyB2Sz++P+QxMNrvJbq6Z3byfj8c+IDOzM5/Z2Z33fG6ltUYIIYRIRmluJ0AIIYToKAliQgghkpYEMSGEEElLgpgQQoikJUFMCCFE0pIgJoQQImlJEBNCCJG0JIgJkSSUUmVup0GIrqaUKlFK+Tv6/l4TxJRSBUqpaserVilVatb57P934/HLlFJ1SiltXpvM3wVRtq11bKeVUtXt7KfW8apTSi0z/y/uznNyW5TPQpu/K91OW1cz51Tl+LvMXOdY34PaiPVaKVXSgeP6HN8pHe+6OPcdz3e5TilVqZTyJZK2VOD2PaunaK1rgPIOBzKtdcq/gFKgFvBFWV4JVAAVPZSWSkADBe1s5zfb+TqyH6AAWAZUu/3598BnWh3PZ5qsL/P9LO7ouQPFwCagrIPH99nft0TWJbD/9r7LZWZ9aXcc34svL92zeuh8fUBtR96b8jkxE90rgOla67Bznda6CusmELWYRilV0Y1JC7e1UmsdAmoi0xzl/VHXa63rtdb5gE8pVdfxZCaFBvNvm59pMjLf3wKtdTDGJu2eu3lvOTCkI2kw38HaRNcloL3v8lys9FdGllx00fE9pTP3rGRlX8eOFJmnfBADAsCCWMHA/MBj3SA6XE7bRRra36Rd04GCbg7IovtUYt3AO8Xc/FoVySURuyi1us2tUkNn7llJyzysBKIVHbelNwSxAtp/Qo9Vh5L0dUrmhzAXKOtM5anoeeZ6+bXW9V20y6TNqZrvcQjwJ3qTS0KduWcluyAwI5E39IYgBtaXoi2tbhKmEjxVfiz2F77TT/SiRwWAmo6+OcrNfmPnkuM6+8beGx7GEr5npYhqrO993HpDEKsFittqmWXqn+bD3lY/JaRQsYU5vzAJPuEI15VgvpcdNDvi76qoWyUPH1j1vW4npJsldM9KJaaotCCR3HZGN6bHE7TWc5VSs4FqpdRcoNJ8ASK3s38Ys7GeguyiC2elca0pt93LVDQXYwUJH1bl+XwP/tBCWF8Of7Tzj8bUo5WyL0dapbUOONb7gOVmfRiYpbWuMctLzTEBcs2/+cCcNhqr9Lh4r18859SV52321dmixGjN0uvMcr/WWjmWFwDzMLkcrXVOJ47b5ezPg3ZyIKZbiZ2LyQeWRf5mHds6v9v5WL/vVjnfRK6rCTyTsVoG+4AhWuuESkASvWeZYudqc7xcrXWOIx1gfW5vdfZzcGzvxyrVWca+xkJR73kd/DyCWL/J+Eoh3G5a2UPNNwuwmhhr81qGVcQWtdmy3teUVbez31KiNAvF+kK1ag5s1tnNif1xpLuyjXUV8e7HkSbd1jm38d5NxGiqbz7buohl0ZoG+xNJb4Lpi/sz7ej1i+ecuvK8zY94WUfPvY1za6u5vN+cw6Yo60pi/R7aWhfnubb7XW5rG/v45jMrjli3KdpvEat1X+S1qov2PY/3upo0VkdsVxrPdYxyzITuWSY99m+8NMr3oTbG9yHuz8HxWS+Lsv8SIrpIdPTzMOcZ897XavuOfvGS7WV+vHbfC+14bYr88B0ftm5jf/521sfar1tBzD5uSQc+u4poNzazrsT5wzI/vlg/gMp405tg+hIOYolcv3jOqavP23z/2u3j5zj3WsfLvvlF7XdD2wGpLNq1buc93RbEzO+2wtw4Y/Ujs4NYq35w5vOJfMjyEaXfmbmGLfqrxXtdsQKoJkq/TpP2hPvodfSeRexA1+IzSuRziNi+1T2EiMDXmc8j3u++/eoNdWKA1bpJa12ltZ6mrWKUaVh1BD6grgMt9yppO7u7AKt4xivsIpCOFOVVYvU3i1ZGP1m37sNUHKNMu46u6TbQFRK9fvGcU1eety/B9wTMd3uatooCE6oc94gKMzqH/arGBDCtdb5uv2g12vplRG8I0up34Nh/UcSqeK5rBRDU0YuMa+jA9ejoPSvK79FWhfUZO88lkc+hAgjrNooaI7bt6OfRQAKNd3pNEIuktQ5qq36n0CxKtMlqMfBWG+uX0X4Lo55kf3Hjqg9z0lZ5fD0RXzzzY9gYsW091pdwubkRFTvWVcX4Ursh7usXzzl1w3l3qGOy85h04Fq7bI7WOuB4TTf/xtsgZVE8G5nrldPGfn2ObeO9rnY9ejSxAmlCuuCeZdfvF5n9xf05GMXE+Iy11oVa6+mORZ35POz66bikfMMOpVRBW09wWut6U3kad09xx6gBbd2YwjGO79ZN3K6w7+iNbQ5WRbPP8cOdQfQWb4VYuZhSoFQpBdbT1yw3g5i5ATlzN4lcv3jOqSvP224s0xleeWDoER35bpncjF3CEOv9bV5Xx/0gN8Z4hvlAMOK301aauvyeZdi//QIiOkvH+Tn4I98XTRd9HrkxlreS8kEMK/fQXla+EtMZuK2bfLxfwgiRF8POufjpoSdlR8uuDvc50larQ7B+yHYrp/xon4dZNt0cuxirGKQU62n2QBcDmR/rSTKRp+JciO+cPHjec3r4eEnD0RpzEdYYhCGzvFXupr3r6ti0NoFcY1u67J4Vg7M1ZdyfQwd05vOIuyi9NxQnFsXR56AB4sqlzDCBzH5Kym9jW/uYkdlv+0mmzRupeTLqqjEP7aehzt7UqjA/LvPlb1Ucp6xpFfYWo5oikHJTTxOidd+lnlToKPqDOK9fPOfUDedtN0nusA4GzU4VYyYDc53qsBpOBdp5cG33ujq+T101OEJX3rOc7HuOHaji/hwc72v3AbALPg8fCTzg94YgBu138i0izg/NcWOop+06r2lAKPJGYi5wq/qlKAJYjQu6wmyswYQ723etEqvvXAFWC6hYObtYw3XNwd16Qme6Er1+8ZxTV553mASKVDqw71h6w2gYdgOFNnMJjoZM8VzXemBmG/tK9HPtsnuWwzRo0fAj0c+hntaNPZzbRf6+Ovp55JJAUXhvCWKV7Xxo5VgX1Ml+WnE+TTj/P4sYrZbMsmJMEUQU9qC8UX8c9pNfVxQ/mRZeDSa9nWKCYIj4AnA09vh3zvT1yNBeUcrmE71+8ZxT3OcdhxDdF/AbIOZn76XGSN2liCgNFBw5Ljs3aj9ExHNdZ2EGE4ixbaKtEztyzwJaBRN7md1c35mORD+HWVitlGPNYzbN8f/OfB75SHFiK9Owmpa2+vBN2W+rpxHztBJzqCZzQw8AC6OsngeUx8r5mGz7dKyGEi2+cOapZ7Zuv1e7L+LfFpQ1oZ49OkNhF9bHVGL9GNqqX4tVoRvA8cMzAXaTijIxaILsH1msz8Keg2lvIOnA9YvnnOI67zglWncX98OA42Ek8rtXhnVdfT31cGHYx+pozjMXYgblIbT+bBYQvcjOHiXC/tz3pqu96+r4PtVG7te8N9EhohK+ZzlEG7ZpIVZpjPM9CX0OjrrBisjfrOM3htm2M59HAQlMr6O01bksZSmlSu0LZwLETFp+qatjfRkclZ5BrAYZrZpKm21mmvX2D6YynqI7c3ErsH6EPvPvfB1jeBjznjJzPGcLycinqVysJ5nKOPt0xM2kuVprPS3GevvG2MC+ogd7OKcWw+eYc5kNdKjRQxyfRS4tcxY1Ec2A47p+8ZxTIuedwPktw5pTKtpwPpHnDlYRTjCOByDnnFUh9jU2qsFqoWYH3CqsJ/5qc052XUVQa21PmRF1XZznF+v6xXwAjHh/zOOrfUMx2fsOYV2HuY5jT8Nxs9TWcE/2+0K0bMIe13U13yf7/O2HyJpErn9H71l2INFaK7VvKCn7Ox1rSK24PgdHEWTM7060c+zI56GU2kQC94SUD2JCJCtl5oCLJygJ4Qxibqelo8zDYIXWurDdjY3eUpwoRDKqZF/fHSF6g+kk2IlbgpgQHmWKXEJdUGcoRLIoTrRvmQQxIbytI41ChEg6pjg04e+6BDEhPMzkxupjdccQQillz3tYaf5eZtenJgvTSGd6R0b4kIYdQiQB06y63M2xJ4XoLp35fksQEyJJKKXK2up+IUQyMt0I6js6OLkEMSGEEElL6sRSlBm4dJNSSjtem0x5ufOlI17SpNujTN1HT46kIYTnSRBLUVrrGjPStj1CRUhbE+DlR7wUkMO+ecGSbSLF3kQeMDzKPGBUKqXKzKu9sQ/b2leJeX+tedCslm4WsfWG+cR6u8nm35jDT5nK1IBpAScNB7wr6vxtwl0mWNXiGKPU5JjrlFJRhw1rY19lWA+cgYhldUqpcqkTbU1yYqnPfoKLZ0DNUEcrV0WPkADmTdVEtKwz/7fHnYyLyW2FI8c4NIGriigD7woJYr2B3b+o1ZQLUchN0qNUjElIhbvMdSmINriuWeZPoI9foI1+Una/LzcnlfUkCWIpzPHU1mpyzhgkF+ZdxeybFVx4x0zafvizp12KR7GpA2vVeMdRQiI5sQgSxFKb/QTY6uanIqZdBxkt3eOkPsybimn74S9EG7MhRwjT9hxy3Tnbd9KShh2pzZ7zK1p92MzIubWEt5hiKD/WHEx+U8E/Gav+RXLN3mDPY9aWuFopaq0LlVK+aA8rJnfmQ3LjrUhOLLXZT4AtvvhmoM0ONf8V3c80164GMHUkQfZN6DgLa7ZcaW7vDe3ljMIkNut2rNy2PcN8QtOU9AYSxFKUafZr/3gWKqXq7M7NWD8EeaKLg+mvE9khPJFXWYLHK8Fq0TbLMZvu3vowuzsEUC0dn2PrwevWU9cggDU0U5fO1J4KJIilLrs+rEZrXWhedufmIPE1uQf25Qx6Y/NerXVAa6068Yq7X4/5fO0A5nwij6wPs1uazqANct165rp1NxNQ/cTfQKRXkSCWutqqDwsRX5N7O0dXAZQk0mnTq8yN3as39Wqsh47Iz7lFEZMjoMWcwl2uW2ow5zwbmCr1oNFJEEtd9g8+WrFhON6WbuaHUwkk/Y3QqGRfLtUzHPWUcyKWt+of5riZx7yGct16THu/I18c20RliovnYQWwVLmOXU6CWAoyX34/tOhfslcHmtJPI3Xq0Lza32oaQJSbVbT02g122uv8LNet+y2i7UZSuXS8/6U9EogEsDZIEEtNMfuHdVAJCdSheZWdg0lwLLuebCAQ7Yk9Wv+wcqIMTxSFXLfuv261tN1C0U8HfofKmiSywtG4x14ujXkiSD+x1NRWfVi7zA84DDSYRX77x2Raz9lPnjVYAdMHe8d4c+7H3jaE1b9pvn0jMutygXxaFhUVOgc/jWM/dl+qac5+b2Z5udZ6mrkJzjTHCClr6vZl8UyFbtISaG+7LlBPHMVljs+iVSW/XLd9evC6BbHGNGzVv8vRtyuh36G5jrWRAcyYjfUQI2xaa3ml2AuoAzTWmG6JvrcWKHX8XQnUmf/7sG4ofrP/Usc2lRH7qQTKIpZtAnzm/yXmXx1xvAqsJ9B292NeZWbZMqA44n0VUc6trK3zd/GaFUReM7OsxPG335x7q3OQ6+b67600yvJS+xpELLcb3fijrCtxXvMo6ys6ms5UfbmeAHl18QXdd6PSHXhvqx+d86Zi32zMD21ZG/uJut7cxErMzdln/t0UsU2Z4+bb3n7s9Pgjzzfy5miWdSiw9+C1qzDptgNGmTN4mHWtbnBy3Vy/bgXO62aW+cyyVuk210Zj5bai7acyxqtWgljrlxQnpghT1FKKo/Ol6dgcAoI6oqgnhgpaF1XMAKYC6H3FG+01GJhHRCs7w4dVl+MsUorcz2T2VYS3tx/7vSU45ktzNGxZ5FiWcL1KT9NalyulaoF5SqkQVu5piGku/xaO+aoiyHVzkda6Xik1DatYcZlZnI9VVBqtUUct1vWJnKalGuv8S9s4XNxTu/QabkdReXnjxb4cnD9yWZRto+YIYu3HLC8xy51Pq9VEFMNgissS3E+L9JhtInMmdj2D6591AtekMo5t5LrJq1e/pHWisNmV/M4nx2JMPyPz9O18Wm6zxZVu/QQaAObqljmJFk/09jG0o6FBnPvx07I/1DTM07zaN5fTNBwV7Cr+OZ5cYXJfdXFsKtdN9GoSxASwt7gmbDfhNf8G2Fe0Y7dsK8Yawy1qB05z8wqZmzBmX2VmXbljmXNsR1sFptVdvPtxaHCkewbWdO52IwKwWtTZLfXKdPSWX15STByjqsh1E72d0lq7nQbhEeYpdxpWMU8D1lNyBVZ9TJXWOmzq3jbqNsaWMzewcqychF0PUhWxTSktn7LzcTTBjnc/ZrsSrDoZO91hrJtqnb19xDbBKDkFT1FKVer46jHluoleTYKYcIWyphp5q62bam+WSBDrSXLdhNdIcaJwi1eHEfIKr87iLNdNeIrkxESPM8VNy7Q1LYxIEnLdhBdJTkz0KFN/U23+X+FyckSc5LoJr5KcmBBCiKQlOTEhhBBJS4KYEEKIpCVBTAghRNKSICaEECJpSRATQgiRtCSICSGESFoyn5jwBDNI7EysiQHBGrHCOQBuLtY4fCGsiQETGjXCzEsVwBpxIsy+sfoq7X05RmO3/17GvgF0Q+ybL8vvWG6nw06fvTzgGP+vvXOzl83XWtfQBRI9XzPo7kL7HKRDs0gabs8FIy95OV/sm/U26ky+7Jv9tjrO/fmwOum2NSuyPQNyHS3nt9oElEV5jz1nVqs0mHXLYrwv5rmZ99Wa97aatj6Bz68z5+uz0+j290Be8or3JcWJwmvCEf+2oLWu11rnAz6lVJvzbZncRR3WRIz5OkouR1u5pflmRuWCiNUNOvpAtw2xjqmtUdbLgSFRVsc8N611SGs9zayrtadWSURnz1db07TURr5HCC+TICaS1XSgoJ0hkBYCmOAQk7amEXFOzmgHhA4V7ZngkXAQMuZg5cpmd+C9HT5fIZKVBDGRlEyuYS5Q5pyA0Wbqoex6oXj2V07LHFIuVpFcR3V0FHo7uCQ0g3EXnK8QSUmCmEhmlebfaDMGVwAhnVgDEOe2diOSjtrYifd2RGfPV4ikJEFMJC1T/xTGmtZ+LzMbMCR+k95bH2Tq3jpzk281k3Gc7Hqqyja3cuiK8xUiWUkTe5HsQlh1Y369b+p6u04o0Zv0gq5KlCnu7IjZQI1pgBGvbjlf0wTfDqr5WHOJRZ3RWSlVyr56wHygNrJhSUQz/lytdU6CxyjDajCz0Wxbbe/L/O3TjtmwTXCfjFUs7AOGmGJUkUIkiIlkF8K6Cfpp2Y8LEqzz6UTg6TRHv67KBAMYdMP5OvqQzXUs26SUCkemzwSXKuf+lFJ1SqmZWuvpzuMppaZjFf+WJniMaqzWogHztw9YDpRrravM30WO7SuwuipMdywrVUotM61bRYqQ4kSR7Ozm7s7WgLkR67wmoJQqM68K06l6Hlb/rY4UQ3bH+RZEKU5dQETDERM8Kogo0gVmASUmOO9lcst2jjHeYxQAJTiKWE3AXGCOjdY6rFt24i4zaXAe29n5XKQICWIi2dk3cGeuoiFinddUaq3nmle5yRlUAstMsVyiuuN8ozXBd45g4hSt35v9/qLIdR04ht1SM7KhTR1Wf8HI7SuAYIycZg1xtuAUyUGKE0Wys3NgzhtcKGKd5zmKxCqVUoscQSAe3XG+kcNiRWUCRU4bm7SVpriOEce+I3OgBcRuWBMrEIskJTkxkez8sLeYymYXV7XZ6TeSUqrArqdxid0QItGcQpefb0fqB5VSfkcxabs5ygSOYX8ukbm6yUB9RF2cXXyZa+rAWrywGoDYY0WKFCA5MZG0zI3IT8TIGlrrGqUUJNhhGOsm2WUtFDvAzlG0VQTXitvnawLHPKycVYX9QKGUirubQFu01iGl1FysYsJCs28/1vkWxnhbbQfrF0WSkZyYSGb20/6cKOvKAX+COat8N1soOo7dkeIuV87XBLA6rHq+QESOuCstA2bZOT2shh4HRh7PUQwrOa1eQoKYSGZ2n6pW9Uem2XY9cXYaNjfjt7o2eR2W8A3YxfOtAFo1iY9yvJK21rfzXh9WH7B6R4OYuW0E4HqsqW9i7U/qxFKIBDGRlOx+Q0Q0o44w1WzbZidgc5MMRBv13QV2UdzepukJ5K7cON8iojTQcKTfHs2/sy0nJyew7SxMB/gY66V1YgqRICa8xhfxbwumMUKdWV/YVnGYWWfXoSyLlhswy+YRffzFWOwbcqI5pjbPzbDT4QxckVPERNVF55trtouWxiG0TvsCoCjK9sVYdZV2IGnVjy/eY5jzKjCNMwrMyx+rcYbJmQeIMqWNadwxP9r7RHJSWmu30yBEvLMf52LlvioTzUXEmOmYRPZlcn9+WgYVO53VsYrU2ji3qHVI5kYbYN/NtibRuqZEz9fc7Kuxclb24MdBrXXA5GiqHekPmf3MdZzfNFqOPTnX8b4QVjHnok4co8SsjyZIlM/S8RmA6VNGBz5L4W0SxIQQnmaGkNpI66Gt7KGmAlgNPXLcbJgj3CFBTAjhWSYHNltrHaspvb1dLW3khkXqkjoxIYTXxTsmpFfHyhTdSIKYEMKzTP1duK1Be826sEdal4oeJsWJcRo6dKgeO3as28kQolfasmULmzZtIisrq8XyXbt2kZOTQ3Z2tkspE22pq6vboLXO685jyLBTcRo7diyLFnV2vFIhhOg9lFKfdfcxpDhRCCFE0pIgJoQQImlJEBNCCJG0JIgJIYRIWhLEhBBCJC0JYkIIIZKWBDEhhBBJS/qJCdFFdu9pZu3mnazctJ2Vm3awfusuRuf2Z9KIbA4cMoC0NOV2EoVIORLEhIjTrj1NrA5bQWrVph2s3LSDlQ3bCG9aT9OmL+izfQ3D2cgItZERagOHs5nVeigP6lF8kT4GNWwC+43KZ9JIHxNHZDN+v0H0yehcYchJJ50EwEsvvdT5ExQiCUkQE6Id764MU1nzDP3WL3YEqY0Upm1kpNpIf3ZaG2Za/zSrDJoHDUcNzENv+oCMHS9bKzbA1vX9+LR+JB80j+JJNZIdg8eTNWIiow4Yx6SRgzl4eDYDsuRnKUS85NciRAw7djdx27/fZ+CbN3FrxlOkZzYDsLvvUMgeSUZuAWmDR8HgUTB4JAweDdkjSRs4jLS09H072rYR1n8E65cwYN1HTFj9ARM3vEfWrpdgK/AJbPm4H0v1SJ7So1jXbxx7hk1i0Ngj+NrokUwckc2wQX1d+QyE8DoJYkJE8cayjdxT/Qj/b/stHJSxkt2Hfpf0ky6H7JH0yUwwoAwYAgOOhbHHooB+9nIT3PS6JaSvep+xa5ZwcMPb9Nv9EqwEVsIXzXks1mNYkeFn55CD6TPycEYeOIGJI32MHTKgS89ZiGQkQUwIhy07G5n71Lvst/hW7sh4gqYBeXBeNX3Gn9r1BzPBTY09lgHAAACt4au18OX77PxiMf0+X8yR6z6gePvbpG1ohg3w1eJ+fKRH81/GcnxhPiu29aHmP2/jP+AAJgzPpn8f+VmL3kOmYolTUVGRllHsU1vth1/yz0ce59e7b+WgtC/Yc9j5ZJxxI/TzuZ002L0d1i2hcfU7bFmxmOa175Ed/ois5u17N9ms+7NcD2d9n9HsHHwgfYaNI3f0RA4Yfyh5Q4aglLSOFD1LKVWntS7q1mNIEIuPBLHUtWHrLq5/fDH+JXdyWcZjNPcfSua5t8P409xOWtuamzn/G8cyuv9OZv/0h3y1+iPYsJQB2z5jyJ51LTZdTw4b+oxm5+CxZOaNY+D+X2PIqHEM2j8f+uWABDjRDXoiiEm5g+i1tNY8+vYq5j/5NNc2/5WDMz6j6dDvkPmNG60bu9elpbFmZxZrdmaRM/UXtEjx7u1sXfspa0Lvs2XlEpo3LGXA1hWMWfcSQ9Y/AR/u23S76semzOHsHDCS5sGjycrzkz08n8H7+1E5ByTHZyF6LQlioldaFd7Bbx5+m0NDd/FA5mNW/dQ5D5F+0BluJ61r9OnPwDGHM27M4S0W72lqZtmq1az7/BO2fhmiceMK0rd8wcAdqxiycQWjGt5i4IqdLd6zPW0A4YFfI/vkXzLw8HMgTQb6Ed4hQUz0Os+9v4aqBY9zg/obB2euoPnQGaSdUQH9c91OWrfLSE8jf8wo8seMAk5psa6xqZlVDdtZvGY14dVL2b5uOYQ/o89XKzkivIiBj/+I9cHxZJ9+NVmHnC1FkMITJIiJXqX+8028OP8WFqTPQ/XPhbP/RdqEM91OlidkpqcxNm8gY/PGw2HjW6z7dM0m/v7InZzy5T3kPfwDGv49gezTryZj4lkSzISrpFxA9BorN22n8t57uSF9HvqA40i/7H8gASwu44bncNFPZ9Nw4avcPvhyNm/eREb199l8yxSalzxldQ0QwgUSxESv8NXORq66+ylubP4Tzb4DyTz/n72i+LCrFR44jJ/+4mpWfOdF/tj/lzRsaiBt/vfYeusU+OhpCWaix0kQEymvqVlT9sBr/Hrz7xjYR9HnggXQd7DbyUpaSilOnjiSX11+DYvP/jfXZ/6M9Rsb4KHvsuP2YyWYiR4lQUykvBue+oBzV1zPuPTVZM78BwzJdztJKSE9TXFe0VjKyq/lpeKn+a26jC/Xb4CHvsuuvx4Hy//jdhJFLyBBTKS0+9/8jMH/u4nT0heRdtofIP+U9t8kEpKVkc6FJ4zjivJrefS4x5jdfClr16+j+R/n0lz3D7eTJ1KcBDGRsl79dAP/feoufp7xKM1fvwCOCridpJQ2qG8mvzx1Ir+64lrumngfrzRNIu3Jn7Fn4Q1SvCi6jQQxkZKWrtvKrQ9U88eMO9kz6ijSzvyTNAXvIXmDsvjdjKP55JR5LNhzIhmvzKXxkUuhqdHtpIkUJEFMpJyGbbu5/N5abuWPZAwcSsZ37oeMLLeT1asopZh18gQyzvsrtzZ9m8z3/sWu+6bDrq/cTppIMRLERErZtaeJy+57g99s+wPD0reR8b2HYOAwt5PVa32rcDSHX1DB1c0B0j9hrU5fAAAgAElEQVR7mV3zTremmhGii0gQEylDa81VD7/Huav+RKH6hLRv3QnDD2//jaJbnTg+jxmlV/HLtCtp2vApuypPgfUfu50skSIkiImUcefLIQa/exczMl6GE8th0nluJ0kYh43y8f8uvYyf972BLV9tpXHeNPjsDbeTJVKABDGREp57fw2v/3sBV2c+gJ5wFpx4pdtJEhHGDh3AH376A2bn3MznuwbQ9I+z4YPH3E6WSHISxETSe3/VZv4y/1n+lnU7DJuAOq9SpgvxqLxBWdxyybncNPJW6veMRVf/CP3GX91OlkhinhrFXilVBoQAPxDUWte3sa0fKAHCAFrrKse6CmAZkA9Uaq1DHTmG8L4NW3fxs3tf4p6Mm+jfN4u08x+CrIFuJ0u0YWBWBn/58VR+veA21i/5Dd94/iqawytJO+0GefgQCfNMEFNKVQNz7KCilKoFpsXY1g9UaK2nm7/rlFKLtNb15n3ljv3UAYWJHkMkh2sff5drdt3MmPQvUTMfh5wD3E6SiEOfjDQqvnMUc5/9M2vfuJ4f//cOmjavJP3bVZDZz+3kiSTipcee4ohcUUgpVRxj20rzsk01AcwPFEXsp8Gxn0SOITzu+Q/Wst+H93Bi2mLUN+bC2OPcTpJIQFqa4sozD6H5tDlc3/h90j96gqZ5xbBhqdtJE0nEE0HMBJJQxOIwUXJJSikfVjAK2su01mHz3wKgIeItIaAgkWMI79u8o5G/PRrkisxqmsedBkU/djtJooN+cryfw2f8mp/sKWPrus9orjwB3q12O1kiSXgiiAG+KMs2YtVbRfIDYaVUsVKqRClV5shNhYFok0TlJ3gM4XE3PvMhl+/+G5mZfUg7688ypFSSO/vwEQR+cgkz025iceMoeOQn8MT/QeMOt5MmPM4rQSyR2QntoNOgta7RWs8FKkxR4iJaByu/2X/CMyAqpUqVUouUUovWr1+f6NtFN3l96QYa6x/guLT3ST/1Ohg80u0kiS4weWwu8y47m6sHV3BH0zlQfx963imw/hO3kyY8zCtBLLIIEGBIjG3DgC+ybgsImGLFuXbOzAS2sFmfyDEAq8Wj1rpIa12Ul5fX3jmIHrBjdxMVD7/CbzMfoHn00VAoxYipZHRufxZcejz1X/sZP9hdzraNq9FVJ8LiB91OmvAorwSxMNGL+yLrsOxl4SjL/ABa63LAp5QqcexzWYLHEB715+AnzNp6JwPTdpN29m3SJDsFDczKoOqCQiad8C2mbvs9S1Q+PHYxPPZT2L3N7eQJj/FEE3utdVApFVnc56dlC0R725Bp3OHkwxGMtNY19v9NbmyB1joc7zGEN73zRZjlr1VzVeabcNLVkDfe7SSJbpKWpig/fQLj9xvItx/O4cq+j/GDxQ+gVi2C6ffCsIPdTqLwCC89xgaVUgWOv/12C0SlVEHEurkRTeOLMMFIKbXJDnImNzbf0Xox5jGEt+3e08x1NW9wQ+Y9NOVNhGN+7naSRA847+ujeKD0WG7jO8zSv2b3lvVQdTK8fb9MtCkAj+TEjFnAbJNzmmz+ts3Eym0FwCoyVErZjTnygVmOUTnKgWI712UafsRzDOFhd768jG9tnEdeZhh1bg1k9HE7SaKHFIzJ4YnLjmXWfVkct2Y4j+53DyMf/yksfwXO/JOM0NLLKS1PM3EpKirSixYtcjsZvdKnX37FNbfN418Z18GUy+C0G9xOkmecdNJJALz00kuupqMnbN+9h8ur3+G591bzt9ELOXX9vahcP5x8FUw8F9K99EwuwBoxSWtd1J3H8FJxohCtNDVrrq5ZxB8yqmgaPMa6YYleqX+fDG4/v4CfFR9E4ItpXJszhz0qAx6+CG4vgrp/wJ5dbidT9DAJYsLT7ntjBcevuYexrCH97FuhzwC3kyRclJam+EXxeP763QLmbxjL8Vtu4OWCW2ju64MnfwZ/OQLeuENaMfYiEsSEZ33RsJ0nnn+eizOeQh/xXcg/2e0kCY8487Dh1Fx8DMN8/fnh68OYsuHXPHvE32jK8cPzs+HPh8DLf4Qdkb1xRKqRICY8SWvN1Y8s5npVieqfgzpV6sFES4eMHMxjlx7D/RcdxYF5A7nkzcFMXvULag7/O40jCuHF31vBLHgtbF3ndnJFN5EgJjzp4fpVjFt+P4eoEOnf+CP0T3jUMNELKKU4btxQHiqdwsOXTOHwUYO5/L/9KFg6i38c9k92HTgVXr0FbjkUnrkCwp+7nWTRxSSICc9Z/9Uu7nnyBa7IrEGPPwMmned2kkQSKDwgl3suPJKn/u84jh8/lGvfSueID8/n9kkPsf2g82DR3XDr1+HRS2DdR24nV3QRCWLCc659/H1+3VxJRp8+qLNulhHqRUIOGTmYO75XSO0vT+CMQ/fnz283c8Tic7hx/EN8dcgP4YNH4Y6j4MHz4fP/up1c0UkSxISnPPf+WvoveYhj0t4nfdp1kD3C7SSJJPW1YYO4ecYRvHT5SUwvGsXd7+3hiEXFXHXAg6w54mfw+Rtw96lw9+nw8XPQ3Ox2kkUHSBATnrF11x7+8vgr/LbPAzSPOQYKL3Q7SSIFjM7tzw3nHcor5Sdz4TFjeXLpbqa8eTQz+t/Fu4dciQ5/AQ/OhL8dY42W39TodpJFAiSICc+4/YWl/HTnPAakNZJ29q0yQr3oUvtl9+XqsybyxlVT+d05k9iwO4OzFx3G0dtu4tlx17GnWVuj5dt9zXZtdTvJIg5ylxCesHzDNj547SnOSv8vaSeUwdBxbidJpKiBWRn8YMpYgr88kft+fCSTRg/l0vfHMWHNb7lz5By+6jfC6mt2yyHwwg2wbYPbSRZtkMHGhCfc8OR7XJ1+H03Zo0k/5v/cTo7oBdLSFCeMz+OE8Xms2LCN+974jL8uyuDGXb9g+n6r+WW/Zxjxn7nw+m1w7M/h+F9BRpbbyRYRJCcmXPfiR+sYtnQBB6nPSD/t95DZ1+0kiV5m7NAB/PabE3nzqqlcf84k6pvHccyKn1CSfgsf+Y6Hl2+EO4+DFa+5nVQRQYKYcNXuPc386cn/UZZZbTXmmHiO20kSvdiArAwumDKW4K9O5J8XHcng0YdwxqofcVna1Wzfvg3u/QY88TPYscntpApDihOFq+55bTnnbH6AwRlfoc64UfqECU9QSnH8uDyOH5fHh6u3cEVNNoWr/dw6/HmK374f9fGzcEaF1RFfvrOukpyYcM26LTt5fOHLXJjxPKrgAhh+uNtJEqKViSOyeeynx/LTUw/j0nXn8l01h3BmHtRcCP+aKUNZuUyCmHBNxXMfczn/JC2zH5zyG7eTI0RMmelpXHbKOJ7+2fFsz51E4dorWTDkUppXvAJ/Pdpqkt/c5HYyeyUJYsIV9Z9vYt3iZzglrZ60k8pg4DC3kyREu8bvN4iHLzmGsjMmcfWXJ/CNxptYm1NoNcm/ayqsecftJPY6EsREj2tu1vzu8Xe5Lut+mn0HwlEXu50kIeKWkZ5G4MR8nv358QzY70CO/jzA3/Kupim8EqpOhn//Ribl7EESxESPq6lbyWFrH8avV5J22u+l741ISvl5A1kQmMJvzprEX9YewgnbKwiNOhdevxXumAKrF7udxF5BgpjoUVt2NnLnc29xRZ9H0AeeABPOdDtJQnRYepriouMO5Lmfn8Co4SM45dNvc0PeTezZ02gNLPxejdtJTHkSxESPujX4KT/Y9RAD2YY6XZrUi9QwdugAHpx1NNefM4kHvhzN1K+uJeybCA9fBLW/lUYf3UiCmOgxS9d9xauvv8oFGUFU4Y9gv0luJ0mILpOWprhgylie/8UJDBo6gqNW/Zwlo6bDa3+Bf82QDtLdRIKY6BFaa6574gOu7nM/KmsAnPxrt5MkRLcYndufBYEpnDxxFGcsPY9HR12BDr0M86bC+o/dTl7KkSAmekTth1+SEQpyHO+QdtKVMGCo20kSotv075PBHd8r4JKT8vnl0q9z/ZAKmndusQLZR8+4nbyUIkFMdLudjU3Meeodftf3X+jcr8HkWW4nSYhul5amKD99An8sOYx/rh7Od9UcdvkOhIfOh5fnykzSXUSCmOh2f391OadseYLRzatQp/8BMvq4nSQhesz0otHcf9FRfLRjMCeuL2OD/1x48Qao/qFMvNkFJIiJbrVm8w4eeKGe/5f1KORPhXGnup0kIXrcUf4hPHbpsfQfOIgpH8/gnYMvh4+egr+fCg3L3U5eUpMgJrrVnGc+4v/UfPrpnXDaH6RJvei1xg4dwKOXHMuRBw7hnLcLmH/Qn9FbVsK8kyH0ktvJS1oSxES3+d/yBj55901mpr2AmvwTGDbB7SQJ4arB/TO598IjOf/IMZQvzuO3w26necB+8M9vwX8r3U5eUpIgJrpFc7Pm+ic/4Pd9H0D1GwwnXel2koTwhMz0NP5w3iFcfebB3P9pOt9pvp6d/mnwbBks/B1o7XYSk4oEMdEtnnx3NfuvfYEi/R7qpKugf67bSRLCM5RS/OR4P3f9oIgPNjRzyucX0TDhu/DKn+CpX8gIHwmQICa63K49TfzpuQ/5Td9q9NDxUHSh20kSwpOmHrwfNZccA2npHPfh2Xw+8WKou9eacHPPLreTlxQkiIkud9/rn1Gw5QXGNH+BOvnXkJ7pdpKE8KyDh1szRx8wZCCnLD6R9yZdAR8+bg1VJU3w2+WpIKaUKlNKlZh/C9rZ1m+2K1VKlTqW+xzLy5RSxY51peblM++v6M7z6Y3C23fztxeWMLv/Y7D/oXDw2W4nSQjPG5bdl/mBoznywFy+Wfd1XpxwHXr5K3Df2bC9we3keZpngphSqhoIaq1rtNZzgZgBRinlByq01nO11lVAwBH0Su3lZj/TlFI+s84HVAKbgFrzf9GF/vriUk5tfJH99qyGk6+GNM98xYTwtOy+mdxz4WTOOmw4Fy4ex3z/HPTa960pXTavcjt5nuWlO0yx1rre8XfImYuKUEnLADTV8d5pEdsuA/zm/2EgB8jRWudrrUOdTbTY54uG7fzr9aWU938cRhbB+NPcTpIQSSUrI51bv/N1Ljx2LFd+MIq/DL8RvWUV3H0abPjU7eR5kieCmAlWkQElTOuAhMlVFWutg/YyrXXYsUluRDHhNGdw1FqHI7YXXeSmf3/M+ekLyWlcB6dcLR2bheiAtDTFb8+ayOwzJnDL0v24anAFzY07rUC2+m23k+c5GW4nwPBFWbYRmBxluR8Im8DnM3/XO4LaLGChWT8fKHe+2dSfNZh9z4/I/YkOendlmOcXL6du0BMw4jjwn+R2koRIWkopAifmkzcoi7Kad1k39PdU8XvS7z0Lzn8QDjzB7SR6hidyYkAinYjsosEGZ/2ZqSfDBKUFWAGuwrE9WHVuVeZ95UC1o75MdJDWmj88s4SL+73AgMYGOOXXkgsTogt8q2AUd/9oMm+EfXx79zXsHjAC7v82LHnS7aR5hleCWLTmN0NibBsGfJH1Z0AAQClVidXoIx+oAmrtRh9R6sDCwIxYiTItGRcppRatX78+vjPphV78eB3vh1ZyccaT1iC/BxzjdpKESBknjM/jodKj+aLRx7TwlWzLnQQLfgD1/3Q7aZ7glSAWJnqRYrSGFyGzfeQyvwlWy+xgpbUOYBUnBkyT+sj5wUNAfqxEmVxbkda6KC8vL85T6V32NDUz55mP+FX2C/RtDFu5MCFElzpslI+HLzkG+udy3Nqfs3G/Y+CJy+DVP/f6Yao8EcRMfVZkkaIfqxl85LYhWgc8HyaQ0TrwVTn+Xx6xzofVelF0UE3dStatW8sFzU/AQWfCyEK3kyRESho7dAA1Fx/DqGF5HPt5gM9HnAHBa+H5q3r1BJueCGJGMKKDs99urKGUKohYNzei+X0RVpP7IDAzYr/FQGVk8DN1YX7Tz0x0wPbde7i59hN+O2QhGXu2wclXuZ0kIVJa3qAsHiw9msn5+3Fi6Hu8PeJ8ePMOeOQnvXaYKq+0TgSrVeFs00BjsvnbNhMrAAUAtNblSim7MUc+MMsuQlRKzTFN7O0cVshRf1allCoz/88nShN+Eb95/1lO01frOHfgk6hDvgX7H+J2koRIeQOzMvj7Dydz5cPvct7bZ3HHAYP5xvt3wrYNMPN+6JvtdhJ7VJcHMaVUNpCrtV6RyPtM3y27uK8mYl1kMWDUZWZ5PRC12bw5xtxE0iWiW//VLir/s4y/7PcC6Vt2wkmz3U6SEL1Gn4w0/jTjcEbl9OPSFxRXjRjErBU3o+49E77/MAwc5nYSe0yHixOVUjcqpZ5XSl1uAhdKqeeBOuBKpdR8pdTYrkmm8Jpbgp+Qs2cDU7c+CYefD0PHuZ0kIXoVpRS/OvUg5n77MCrWFnDNgKtp3vAp/H0abOw9Vf2dqRN7C7hYa32T1nqLUupGrDqmcVrri7XWM4GSrkmm8JKl67by0Ftf8OcRQdL0HjixrP03CSG6xYzJo7n7R5N5eMtEZqlr2LNjc68a3aMzQSxHa73c8XcJrQft3dyJ/QuPqnjuI76WuZHJDU9BwQ8gZ6zbSRKiVztxfB4LLp7C+2oc5+24hp30gXvPgmUvuJ20bteZILY3gCmlDgQOBBZFbLOxE/sXHvS/5Q3Ufvgltwz/N0qlwfGXu50kIQQwacRgHr30WBpz8jll01WEs0bAAzPgvZr235zEOhPEBjv+XwIs11ovjtgm1qgbIglprbnhmSVMHtTAhC+fgskXweCRbidLCGGM8PVjwcVT8PvHccL6K1g58FB4+CJ4469uJ63bdCaIbTaNOq7AKkYsA6t1olLq20qpt4jSWVkkr6ffW8M7X4S5Ke8ZVEZfOO6XbidJCBEhu28md/9oMtMKDmLqup/x7qATrA7R//5NSnaK7nATe631QqVUCKszcb6jfszu07UAKABWdDaRwn279jQx97mPOT1vI2NWPwvH/aJXNeMVIpn0yUjjpumHMSqnH+cuLKVqSDbFr98KW9fB2bdBRh+3k9hlOtVPzASueRGL59OBfmLC2x5483M+b9jOw197ArV7EBzzM7eTJIRog1KKX04bz8icflz8SBq/zR7MD959ADatgJn/TJmH0O7qJ1Yu/cRSx+Ydjdz2wqd8f0wDeStrYcpPoX8is+cIIdwyo2g091x4JHN3nMPV6b+iec1iqDopZZrgd1c/sUukn1jquPPlZWza3kh51sPQLweOvsTtJAkhEnD8uDyqL57C82nH8v3m37G7ScPdp6dEy0XpJybatDq8g7tfXc7/O2gDg754EY79OfQd3P4bhRCecvDwbKoDU/iszzimbb2Or4aYlou110Bzk9vJ6zDpJybadHPtJyjdTGBbJWSPgiNL3U6SEKKDxg4dQPXFU0gfNIxjV/+CNeO+C6/dAv+aCTsip2lMDtJPTMS0ZM0WHq5fyZ/HvUOfDR/Aqb+DPgPcTpYQohNG+PoxPzCFEUOyOfHDs/mw4FoIvQh3TYX1n7idvIRJPzER043PfsSorJ2c9uU8OOBYmPQtt5MkhOgCeYOyeKj0aCaOyOabbx7Eq8fcbeXE7poKnzzvdvIS0uEgprVeCDwMhLH6iT1iVs3EmmHZ7icmktCrn27g5U/Wc+eo50nbGYYzKkApt5MlhOgivv59uP8nRzF5bA4XLMzgsSMfgJwDrKLFV24Grd1OYly6pJ+YyX2dYhbP11pv6XzShFuamzVznl3C8dnrmLiqGop+DPsf6nayhBBdbGBWBvdeeCSX3F/HL55bT8Nplfx4yE2w8Dr48n04+3bo09/tZLapM8WJdtHhAqzcWNC8Npn+Ywd0RQJFz3vindV8sHozN2c/iMrKhpN/7XaShBDdpG9mOpUXFHHmocP53fMruHnwleip18D7j1hTuoS/cDuJbepMZ+fBWDMw12IVJ6ZprdOAccBCoMbuBC2Sx87GJv74/MeUDn2PvA3/hVOulo7NQqS4Phlp3Hr+15lRNIpbX1jK7zefjj7/IWt0j3vOgMadbicxps4UJ84CpmutW/QF01qHgLlKqRpgtnmJJPHPNz5jYzjMr4bcB/sdAoUXup0kIUQPSE9T3Pitw+jfJ4O/v7qcbbtGc8NFQdI3fgKZfd1OXkydCWKbIwOYk9Y6ZAYIFkkivH03t73wKXOGvUDfLath+jxI71S1qRAiiaSlKa755kQGZmVw+4tL2b57BH+acSaZbiesDZ25Q8XTdEWGdkgid7y0jOxdazhn2wKYdB6MPc7tJAkhephSistPO4gBWRlUPPcR23fv4a/fKyArI93tpEXVmSCWo5TKjtUS0dSHDe3E/kUP+qJhO/e+toLqvEdJ254G0653O0lCCBddclI+A7PSeXflZjLTOtUGsFt1JohVYTXe+Buw0A5mJnjNAALA1M4nUfSEm2s/4ei0Dzh8y0tWa0TfaLeTJIRw2QVTxqK1Rnm4j2hnJsXcrJQKAJXAw0opZ/FiPTBD+oslh/dXbeaJtz/nzZx/QdYYOOb/3E6SEMIjvBzAoGs6O59qBgC2R+eojxjdXniY1lbH5ln9XiRvxzI4+37I7Od2soQQIi5d0vTMBC0JXEno5U/W8+HS5dw9sBpGnwgTznI7SUIIEbcuaz+tlPo6UATkAxuwRvFocIypKDymqVlz47Mfce3AR+nTtF3GRxRCJJ0uC2Ja67eBtwGUUlOBaiC7K48hutYj9StJ//I9zs76N+qoi2HYwW4nSQghEtIt7SbNCPdF3bV/0Xk7G5v40/MfM3fAA9awUidd6XaShBAiYd2WSzIjdtR31/5F59z92nKO3PYik/p8AGf8Bfr53E6SEEIkrLuL+mTYKQ8Kb9/NvS99wHP9HoJhh8PXL3A7SUII0SFxFfc55gpLVEMH3ye6UeV/QvxgzyPkNm2AM+ZCmjeHkxFCiPbEW2cV6OD+k2Nq0F5k/Ve7+PdrbxHIfBoOnQFjjnY7SUII0WHxFicWKqXOA2KOWh9DUYLbi252x0tL+T8eJD0tHYqvcTs5QgjRKfEGMT/wcAf2LzkxD1kd3sHiN1/imszXYMqvYPAot5MkhBCdEm8QqwemJ7hvBSxI6A1KlWE1BvEDQa11zNaNSik/UILVqRqtdZVZ7gNKzXIf1jBYwY4cI9XctvBTytLvp6lfLunH/cLt5AghRKfFG8SCHRkPUSm1KIFtq4E5dlBRStUC02Js6wcqtNbTzd91SqlF5r2lWuu5jm0rzLpwIsdINZ9t3Mb6+ieYkvkhnPRH6CtTvQkhkl9cDTu01h3qCau1vjiBzYsjckUhpVRxjG0rzcs21fHeyKC0DCvXlegxUspttUu4MuNf7PH5oehCt5MjhBBdwhMjaphAEtmnLEyUXJIpLix2FhFqrcOOTXKVUhWOv6dpresTOUaq+fTLr+jz3r/4mlpFxqnXQbqXJxsXQoj4eWVcw2jDRWwEJkdZ7gfCJij5zN/Oeq9ZwEKzfj5Q3oFjpJQ7/v0Ov86ooXHkkWQe/E23kyOEEF3GEzkxIDeBbe2iwQatdY2p/6ow9WSY4sIFWEGrwrF9IscAQClVqpRapJRatH79+kTf7gnvr9rMAR//naFqM5mn3yCj1AshUopXgli0kT2GxNg2DPgi67YwHbKVUpVYjT7ygSqgVilVkOAxAKvFo9a6SGtdlJeX1945eNJdz7xOIONpGiecA6OPdDs5QgjRpbxSnGg3h48UbezFkNk+cpnfBKtlWusQgNY6oJRahhXgqhM4Rkqo+6yBIz+rpE9mE+mnXut2coQQost5Iidm6rMii/v8QG2UbUO0DkY+9vX9igxKVYkeI1U89NS/mZnxMs1FF0Guv/03CCFEkvFEEDOCJidl89uNNZRSBRHr5kY0jS/CanIfBGZG7LeYfc3xYx4j1by+dANnrP0bezL6k3myzBUmhEhNXilOBKtV4WzTQGOy+ds2Eyu3FQDQWpebTsx+IB+YZRchKqXmmCb2y8x7Q476s7aOkTK01jz31Hx+l76YxhOutSa9FEKIFOSZIGb6etnN4Wsi1pVH2b7VMrO8HmuYrISOkUpe/GgtMxqq2NZ/OAOmXOJ2coQQott4qThRdIHmZk3dk/M4JG0FWadfC5l93U6SEEJ0GwliKeb5dz7j/G3/YNPgiWQcNsPt5AghRLfyTHGi6LymZs0Xz97MGWoDTWffDWnyjCJ6ry1btrBu3ToaGxvdTkpKyszMZNiwYWRnZ7uaDgliKeSZN9/nO7uqWTf8RIbln+h2coRwzZYtW/jyyy8ZOXIk/fr1Q8lINV1Ka82OHTtYtWoVgKuBTB7VU0RjUzM7X5jDALWTvG9VtP8GIVLYunXrGDlyJP3795cA1g2UUvTv35+RI0eybt06V9MiQSxFPPPya5zb+Bxf+qejhh3sdnKEcFVjYyP9+vVzOxkpr1+/fq4X10oQSwE7G5sY+MoN7EnLZPi517mdHCE8QXJg3c8Ln7EEsRRQ+/yTTNVvsP7QACp7uNvJEUKIHiNBLMntbGxiaN3NhNNyGHNmmdvJEUJ0gWAwSH5+PoFAwBP78TIJYknu+RdfZIp+h82HXQRZA91OjhCiCxQXF1NeHnVQIlf242USxJLYrj1N6DfvZDd9GDPtUreTI4QQPU6CWBJ78o0POL3pJTbmn4ca0Ob8nkIIkZIkiCWpxqZmNrxcSV/VyP6n/sLt5AghulFNTQ05OTkUFhYSDltzAk+fPp38/Hzq6+upr6+npqaGmpoaAoHA3m2iqaqqIhgM7t022cmIHUnq8boVnNv4NBv3P5Yh+010OzlCeN51T37Ah6u3uHLsiSOyueabkzr8/pKSEhoaGqitrcXns+YEDgQCFBUV4fP5yM/Pp7q6moKCAhoaGpgzZw4VFa0HPaiqqsLv91NcbE3HGAol/8T2khNLQk3NmiUL72d/tYncqT93OzlCiB5QWlpKMLhvDt9QKLQ3oNXW1lJQYM33W1RURH191Nmo8Pv9BAIBqqqqCIfDlJaWdn/Cu1s3mloAABHnSURBVJnkxJLQU++s4ps7HmVb9lgGfG2a28kRIil0JifkFTNmzKCqqooZM2bg9/v3Lvf7/dTU1NDQ0EA4HKahoSHq+4uLi6moqKCyspJAIEBpaSmVlZVRt00WkhNLMs3NmoW1T3NEWoh+x18mI9UL0YsEAgEqKysJBoN7iwQBCgsL8fv9lJaWtlgeKRgMUlJSQm1tLZs2bSIUCiV9kaLcAZPMcx+sZdpXj7A7M5u0I853OzlCiB5UUFBAOBxuEXiCwSDhcHhvcaK9LhwOtypWrK2t3bvM5/PtfU8yk+LEJKK15sHgG9yb/j9U0U+lc7MQKaq+vp7q6mpCoRA1NTWUlJTsXVdeXt4it1VcXExBQcHeRhv2q6qqiuLi4hb7yc/Pb5H7ys/Pb1EsmYwkiCWR4JJ1HLPxEdIyQB2V/BWyQojoCgoKqK2tjbouWmOM6urqmH/H2k+qkOLEJKG1pmrhe3wv40X0wd8E3xi3kySEEK6TIJYkXv5kPePXPk02W0mbIkNMCSEESBBLClprbl/4CbP6PE/z8K/D6KPcTpIQQniCBLEk8MayjQxY+R/G6lVWLswDE9EJIYQXSBBLAre+8CkXZz2PHrg/TDzX7eQIIYRnSBDzuLdWNLB++XtM0YtRR/4EMvq4nSQhhPAMCWIed+vCT7mkby06oy8UXuh2coQQwlMkiHnY4i/CvPvpCs5R/0EdOh0GDHU7SUII4SkSxDzstoWfcmG/l8ls3glHX+J2coQQwnMkiHnU+6s28/JHq7koMwgHngj7Jf8I3EKIrhEMBsnPz+/0pJZdtR83SRDzqNtfWMq5WfUM2v0lHC2dm4UQ+xQXF1NeXu6Z/bhJxk70oI/XfsVzH6zl9aFB6OOHcae6nSQhhPAkyYl50O0vLmVKn2WM2Po+HHWJzBkmhBAxyN3RY0Lrt/LUu6v5Td5/IGswHPFdt5MkhOhhNTU15OTkUFhYSDgcBmD69Onk5+e3miMsnu3r6+upqamhpqaGQCCwd5toqqqqCAaDe7f1Ok8VJyqlyoAQ4AeCWuvWV2vftn6gBAgDaK2rzPJKoEJr3Wq6UqWUPYfBAiAXCGitPVUgfNeryxmVvomDN70IR10sc4YJ0QuVlJTQ0NBAbW0tPp8PsGZ1Lioq2vt3Itvn5+dTXV1NQUEBDQ0NzJkzh4qKilb7secks+crS4ZZnz0TxJRS1cAcO3AppWqBaTG29WMFqunm7zql1CLz3hlAqWo5vmBYa50D+IAKoBIrWEbdv1s2bt3Fw3UruXP4G6gNzXCkzBkmRJd59kpY+547x97/UDjjxoTeUlpa2qLRRSgUajEZZiLb19bW7p38sqioqNX8Yza/308gEKC8vJwZM2ZEnbvMa7xUnFgckfMKKaViXbFK87JNdby3Csh3vKYBs8y6MJAD5Git86Pl1tx0/5ufo/bs4IQtT8GEMyHnALeTJIRw0YwZM6iqqiIcDsc1A3Os7f1+PzU1NXuLChsaGqK+v7i4mIqKCqqrq8nJyUmKloueyImZYBUZUMJYASgYsa0PK+DtzUVprcOOdZXO4KSUKraLGp3bes3Oxib++eYKyka8R3pD2GrQIYToOgnmhLwgEAgwa9YscnNzKSkp6fD2hYWFzJs3j5KSEurr65k/f37U9weDQUpKSigpKSEcDjN9+nRCoVBcAdQtnghiWMV8kTYCk6Ms9wNhE/h85u96rXXQBKi9QUopVeoMYPYyoMHse35b9W496fHFq9iwdTfTs2th2EQ44Bi3kySEcFlBQQHhcDjuuqlo2weDQcLhMAUFBcC+eq5o+62trSU3N5eCggJ8Pt/e93iZV4JYbgLb2o8EDVrrIOytE5sekQPz0To4Bh3b1CillimlCmPlzkzAKwUYM2ZMAklMjNaau15Zzjfz1jGo4T04448yZ5gQAoDy8vJWdWH19fVUV1cTCoWoqalpkeuK3L64uJiCgoK9jTbsV1VVFcXFxS32k5+fTygU2hvc8vPzPZ0LA+8EsWgFtENibBsGfJH1Z0AAcBbgzgZa5Jmj1IGFsRqCVBGFycVVARQVFelYie+slz9Zz6frtlI1/lXY0Q8Om9FdhxJCJJlojSsKCgqora2Ne/vIhhzOv2PtJ1l4pWFHmOhFitHy0CEcRYaOZZGPC6XOQKeU8iulNkV5X36Cae1yf391Of5BTYxd8ywc8m3oF+2jEEIIEckTQcwUC0YWKfqBVo8IJjcVeZf34Qh4pgl+tEgQ2dTGByxLNL1dacmaLbzy6QauGfshqnEbFP3YzeQIIURS8UQQM4JKKWctot9R51UQsW5uRPP7Ilo2ufcTkVuLDH6mzswf2fCjp/391eX0y0zjuM1PWH1JRnq/IlUIIbzCK3ViYPXlmm1yUZPZ17cLYCZWAAoAaK3LlVIVZtt8YFaU+q5FUY5RZUYFgX19yFyzbstOHl+8iismfUX6Jx/AmTdLgw4hhEiAZ4KYaSFoF/fVRKxr1eOureGiTA4uGGV5GJjbuZR2nX+8sYI9zZrz0xdC5gA4dLrbSRJCiKTipeLEXmX77j088N/POeegAQz69Ak4bDr0zXY7WUIIkVQkiLnk4bqVhLc38sv93oY9O6DwQreTJIQQSUeCmAuamzV/f3U5h48azJjQfBjxdRhxhNvJEkKIpCNBzAXBJV+yYuN2yiaFUeuXSC5MCCE6SIKYC+56dTkjff2Y0vAEZGVbHZyFEEIkTIJYD3t3ZZj/LW/g4iNzSPvwMWuIKZn4UgjhUF9fT05ODoFAgLlz5zJt2jSUUpSXl1NeXs60adMoLCzs0mMGg0Hy8/OTYvoVJ880se8t7nplOYOyMpie8Qo07ZKiRCFEK6FQiIULF+4dRd7v97No0aIWszEHAoFOHaOqqqrFOIvFxcWUl5ezbJmrgxglTHJiPWhVeAdPv7eG70weRd937oNRR8L+h7idLCGEB7U3DUpncmLhcDjpglUsEsR60D9eXwFA6di1sPFTKJJcmBCitXimPykqKkpon+FweO+/s2bNamfr5CFBrId8tbORB//7Od84dDh5Hz0AfQfDpPPcTpYQwoPimYyyoaGB/Px85s6dS1VVFYWFhYTD4b11W3ZxY319PYWFhXsDlz1JZn19PXPnziUYbDW4ETU1NdTU1BAIBOKekNMtUifWQ+a/9QVf7drDxUXZ8OATMPkiyOzndrKE6FVOOukkV4770ksvdfk+i4uLCQQCzJ8/n7q6OnJzc1sst4sLCwoKmD17NvPnW9MrlpSUEAqF2LhxI2VlZa32GwwG99a9NTQ0UFlZ2aIuzmskJ9YD9jQ1c89rKzhybC6T1j0FzY3SoEMI0SXsoseSkhJ8PmuiDvvfzuwPIDc3d28xpFdJTqwHPPfBWlaFd3DNWRPghUthzDEwbILbyRKi1+mOHJHb4qk/S4Sdo7M1NDR06f67muTEupnWmnmvLGfskP5M7fsxNISkQYcQwjNqamra38jDJIh1s7rPNvHOF2EuOu5A0uvvgX65cPDZbidLCJHC/H5/ixzUW2+91Wq914sJ4yVBrJvd9cpyBvfL5NsHZcJHT8MR34XMvm4nSwiRBEKhEHPnzmXOnDmEw2ECgQBVVdZk9MFgkPnz51NTU7N3ma24uJjc3Ny9rQzz8/Opr6/fm+sqKSmhoaGBqqqqvfVn9fX1VFZWEgwGqampafW3VymttdtpSApFRUV60aJok0XHtnHrLqbMeYFZJxzIFf2fhoW/g8vqYOjXuimVQgiAJUuWcPDBB7udjF6hrc9aKVWntU6sQ1uCpGFHNxoyMIsXLj+R/plpcNe9MPZ4CWBCCNGFpDixm43K6U/umlch/Lk06BBCiC4mQawn1N0D/YfChG+6nRIhhEgpEsS625bV8PGz8PXvQ0Yft1MjhBApRYJYd6v/J+gmKPyh2ykRQoiUI0GsOzU3Qf19kH8K5HZtr3ohRNuk5XX388JnLK0Tu9OuLf+/vftHbtsIwzj8fgULudBQcmIVLjKhupSS3aaSD5AZOT5BxBtY4xN45AtkpBvY8g0kn8Cxb2DOpJCbJDI7Fi6+FFjIEEz9WQoksMvfM8OxBFI7y88g3wWwAKSff5V+4eRmYJF6vZ4mk4nu3bvXdleyNplM1Ov1Wu0DITZPK2vSb3+23Qtg6Tx48EBnZ2d6+PChVlZWZGZtdykr7q7JZKKzszNtbGy02hdCDEB2VldXJUmfP3/W169fW+5Nnnq9njY2Ni5q3RZCDECWVldXW/+CxfwxsQMAkCxCDACQLEIMAJAsQgwAkCxCDACQLEIMAJAsbop5S2b2j6S/Z/zzHyT922B3cke94lCvONQrzl3q9ZO7/9hkZ+oIsQUws7/mfXfTnFCvONQrDvWK0/V6sTsRAJAsQgwAkCxCbDGO2u5AYqhXHOoVh3rF6XS9OCYGAEgWFwCekZk9lzSSNJB06u4fb3j9gaQTdz+9SzupaqJeZrYXfnwjaV3S0N3359RlAAkgxGZgZseSXpZfxGZ2IunJFa/dkbQlaVfSyaztpKypeknqSzqQdKgiELOrVem2oW9mfUlluD9Wpc4x7aSuiXot0yBphnqNVXzeDmsDy/bXL3fnEfmQ9KX2+6GknRv+5qT+mlnaSfHRYL32VARZv+33NOd6HUvaqtbimtceVn4eSPoiaRDbTsqPBuv1XJKHx6dyeW6PyHod1Orl5eevK+sXEzsihS2FUW1xOUpZeDtd1/T7dPexu4/v3LFu2/HLI9pRqOMlZjZQ8WUrSXL3kYpa78a0k4Gm6jWWtCZpzd03w/M5ilkv9srnKvUYzNDO3BBi8fpTlv2nb/+xi26n6xp9n2a2Z2a7ZnZgZlt361r3RIZ+uXu17j6DpLh6lT/kPkiaYb3Y9rD7MAwCpG9h1Yn1i2Ni8dY71k7XNfk+Tyujwbdm9snMtjP70rkq9B/XF7r7RzPbri3ekrQf007imqqXpIvjYufh7197fscQo9aL2tboUNK+u4/DsbJbtzNPhFi88ynL7k9Ztqh2uq6x9zll985Y0u/q+HkskaJC3y9P4thTEfSnlUkKuWukXmHRMgySogeVYQtsV8Xek5eztjMv7E6MN9b00Uzs/vOm2um6Rt6nmQ3M7MuUNjZn7VhHzRT6YWT81N3L3TkMkq4xpV7XDZJyEl0vdx+5+ysVW6wfQu06s34RYpHCqK0+Chno++ngC2mn6xp+n/Xpzn1VDtRnYtbQP5D0tIF2UtNIvZZokBRVr+puwxDyY0kvYtuZJ0JsNqe1SQWDysHPrYgJB1e2k5k71yt8gC4+NOHDNXD3nHYlzhT64Vydg3K3l5ltMUiKq1d4KvtBUky9wuSNerBLxRT7zqxfHBObzR+SXoR9xY/D76VnKlb+oXTxAXkmaUfSupm9DpvmN7WTk6bqdRS+gKRihJzVTLuK0xBE5fGbS6EvfTu2Y2a7kj5KOi+DXdKjsOzKdjJz53q5+1F1qyPXQVJw23qN9H2wDyrLOrF+ce1EoGPCF+gLSe9VmyUXLsfVd/dh/byniidhcseV7eSk4XqVE2I2VWyt5bb79db1Cr+XV9AZS9pWcULz25vaWSRCDACQLI6JAQCSRYgBAJJFiAEAkkWIAQCSRYgBAJJFiAEAkkWIAQCSxRU7gAyEK1GsS3qT2VXXgWuxJQYkzMz6Znas4lJKbyS9a7lLwEIRYkCiwmV/3kl6GW6XUV7QdrfdngGLQ4gB6TpQcSPH6vXqRsr3wsjAdzgmBiQoXG18T9Ja7am+pt/nCcgSW2JAmoaSjqZM4nik4orjwFJgSwxI055quw3DrUauunU8kCVCDEiMmZX3vBqa2bDy1CD8+2HBXQJaw/3EgMSY2YmkdXffri0/lrQraTPHmzkC03BMDEjPI0nTbgO/I2lEgGGZEGJAevqSPlUXVI6HHbTSI6AlhBiQkHCCs1ScD1Y1lCR3P1psj4B2EWJAmuozEPckvWqjI0CbCDEgIeG8sEvngZnZc0nn7r7fTq+A9hBiQHqOVEzuKI+FDcWlprCkmGIPJChMp38vaVPSPrdfwbIixAAAyWJ3IgAgWYQYACBZhBgAIFmEGAAgWYQYACBZhBgAIFmEGAAgWYQYACBZhBgAIFmEGAAgWYQYACBZ/wPi+CiDKDfCpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"StoUD vs. Loss (Full Phase Space)\\n $F_{dropout} = \\phi_{dropout} = 0.2$\\nDCTR Change\")\n",
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.vlines(0.200, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"probStoUD(200) Vs Loss-FDropoutPhiDropout02-Copy2.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T13:25:08.791027Z",
     "start_time": "2020-07-25T13:25:08.781056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18333333333333335"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[np.argmax(vlvals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:00.162513Z",
     "start_time": "2020-07-23T21:39:00.081088Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:00.166222Z",
     "start_time": "2020-07-23T21:39:00.060Z"
    }
   },
   "outputs": [],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = int(len(X_train_theta) / 10)\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1, MSE_loss=False):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        #  when not training theta, fetch as np array\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "\n",
    "    weights = reweight(events = x, param = theta0) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:00.168447Z",
     "start_time": "2020-07-23T21:39:00.063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration + 1)\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=1000,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train_theta, Y_train_theta, epochs=1, batch_size=batch_size,verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "\n",
    "        mean_fit = np.array([\n",
    "            np.mean(fit_vals_recent[len(fit_vals_recent) -\n",
    "                                    4:len(fit_vals_recent)])\n",
    "        ])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T21:39:00.170230Z",
     "start_time": "2020-07-23T21:39:00.064Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.200, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
