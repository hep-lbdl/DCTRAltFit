{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTR Alternative Fitting Algorithm for probStoUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:22.137334Z",
     "start_time": "2020-07-25T20:56:22.132406Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:24.354512Z",
     "start_time": "2020-07-25T20:56:22.140809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:24.361242Z",
     "start_time": "2020-07-25T20:56:24.356996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22)\n",
    "rc('xtick', labelsize=15)\n",
    "rc('ytick', labelsize=15)\n",
    "rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:24.373913Z",
     "start_time": "2020-07-25T20:56:24.363500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:24.382262Z",
     "start_time": "2020-07-25T20:56:24.376064Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:24.391934Z",
     "start_time": "2020-07-25T20:56:24.384404Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:24.398575Z",
     "start_time": "2020-07-25T20:56:24.395036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/data0/users/aandreassen/zenodo/'\n",
    "data_dir1 = '/data1/users/asuresh/DCTRFitting/StoUDFitting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T20:56:31.883170Z",
     "start_time": "2020-07-25T20:56:24.401779Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "#unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')\n",
    "unknown_dataset =  np.load(data_dir1 + 'test1D_strange200.npz', allow_pickle=True)['dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:00:47.667557Z",
     "start_time": "2020-07-25T20:56:31.884835Z"
    }
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:00:49.385291Z",
     "start_time": "2020-07-25T21:00:47.671763Z"
    }
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:00:52.228202Z",
     "start_time": "2020-07-25T21:00:49.389053Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.5, shuffle=True)\n",
    "X_train_theta, X_test_theta, Y_train_theta, Y_test_theta = data_split(X_fit, Y_fit, test=0., shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:00:53.088673Z",
     "start_time": "2020-07-25T21:00:52.230670Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100, 100, 128)\n",
    "F_sizes = (100, 100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights(\n",
    "    './saved_models/DCTR_ee_dijets_1D_probStoUD_Copy3.h5')  #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:00:53.103950Z",
     "start_time": "2020-07-25T21:00:53.091223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "def reweight(events, param):  #from NN (DCTR)\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, param]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(events[...,0])>0,\n",
    "                                   K.ones_like(events[...,0]),\n",
    "                                   K.zeros_like(events[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "\n",
    "    model_inputs = K.concatenate([events, concat_input_and_params], -1)\n",
    "    # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    \n",
    "    f = dctr.model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T21:00:53.877280Z",
     "start_time": "2020-07-25T21:00:53.105960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = PFN(input_dim=4,\n",
    "            Phi_sizes=Phi_sizes,\n",
    "            F_sizes=F_sizes,\n",
    "            latent_dropout= 0.2,\n",
    "            F_dropouts= 0.2,\n",
    "            output_dim=1,\n",
    "            output_act='sigmoid',\n",
    "            summary=False)\n",
    "reinitialize_weights = model.model.get_weights()\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0., MSE_loss = False):\n",
    "    x = inputs  #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(\n",
    "        x, np.arange(51),\n",
    "        axis=1)  # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    weights = reweight(events = x, param = val)  # NN reweight\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-25T20:56:22.131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-13-2bac8f3c1b35>:9: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 375s 417us/step - loss: 0.7007 - acc: 0.4978 - val_loss: 0.6834 - val_acc: 0.4967\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 436s 484us/step - loss: 0.6839 - acc: 0.4966 - val_loss: 0.6814 - val_acc: 0.4951\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 472s 524us/step - loss: 0.6805 - acc: 0.4941 - val_loss: 0.6759 - val_acc: 0.4920\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 519s 576us/step - loss: 0.6761 - acc: 0.4913 - val_loss: 0.6756 - val_acc: 0.4909\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 544s 604us/step - loss: 0.6747 - acc: 0.4906 - val_loss: 0.6740 - val_acc: 0.4911\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 655s 728us/step - loss: 0.6742 - acc: 0.4901 - val_loss: 0.6735 - val_acc: 0.4905\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 614s 683us/step - loss: 0.6741 - acc: 0.4903 - val_loss: 0.6732 - val_acc: 0.4903\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 614s 683us/step - loss: 0.6739 - acc: 0.4906 - val_loss: 0.6729 - val_acc: 0.4900\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 614s 682us/step - loss: 0.6738 - acc: 0.4902 - val_loss: 0.6736 - val_acc: 0.4901\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 610s 678us/step - loss: 0.6737 - acc: 0.4901 - val_loss: 0.6737 - val_acc: 0.4897\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 612s 680us/step - loss: 0.6736 - acc: 0.4902 - val_loss: 0.6732 - val_acc: 0.4901\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 608s 676us/step - loss: 0.6735 - acc: 0.4904 - val_loss: 0.6734 - val_acc: 0.4902\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 609s 677us/step - loss: 0.6735 - acc: 0.4903 - val_loss: 0.6733 - val_acc: 0.4900\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 608s 675us/step - loss: 0.6734 - acc: 0.4905 - val_loss: 0.6737 - val_acc: 0.4898\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 608s 676us/step - loss: 0.6733 - acc: 0.4904 - val_loss: 0.6731 - val_acc: 0.4900\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 608s 676us/step - loss: 0.6733 - acc: 0.4905 - val_loss: 0.6742 - val_acc: 0.4899\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 607s 674us/step - loss: 0.6733 - acc: 0.4903 - val_loss: 0.6750 - val_acc: 0.4899\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 608s 676us/step - loss: 0.6732 - acc: 0.4907 - val_loss: 0.6748 - val_acc: 0.4898\n",
      "trainnig theta = : 0.10833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 605s 672us/step - loss: 0.6756 - acc: 0.4903 - val_loss: 0.6755 - val_acc: 0.4900\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 603s 670us/step - loss: 0.6755 - acc: 0.4902 - val_loss: 0.6760 - val_acc: 0.4901\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6753 - acc: 0.4900 - val_loss: 0.6753 - val_acc: 0.4902\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6753 - acc: 0.4905 - val_loss: 0.6747 - val_acc: 0.4899\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 601s 667us/step - loss: 0.6752 - acc: 0.4903 - val_loss: 0.6757 - val_acc: 0.4899\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6752 - acc: 0.4904 - val_loss: 0.6758 - val_acc: 0.4898\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 597s 664us/step - loss: 0.6752 - acc: 0.4902 - val_loss: 0.6756 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6751 - acc: 0.4903 - val_loss: 0.6754 - val_acc: 0.4897\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6751 - acc: 0.4904 - val_loss: 0.6760 - val_acc: 0.4897\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6751 - acc: 0.4902 - val_loss: 0.6756 - val_acc: 0.4897\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 588s 653us/step - loss: 0.6751 - acc: 0.4903 - val_loss: 0.6757 - val_acc: 0.4901\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 594s 661us/step - loss: 0.6750 - acc: 0.4897 - val_loss: 0.6755 - val_acc: 0.4897\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 590s 656us/step - loss: 0.6750 - acc: 0.4901 - val_loss: 0.6758 - val_acc: 0.4898\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 594s 659us/step - loss: 0.6750 - acc: 0.4905 - val_loss: 0.6753 - val_acc: 0.4897\n",
      "trainnig theta = : 0.11666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6771 - acc: 0.4902 - val_loss: 0.6770 - val_acc: 0.4896\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 601s 667us/step - loss: 0.6770 - acc: 0.4901 - val_loss: 0.6769 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 605s 672us/step - loss: 0.6771 - acc: 0.4901 - val_loss: 0.6767 - val_acc: 0.4901\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 592s 657us/step - loss: 0.6770 - acc: 0.4903 - val_loss: 0.6779 - val_acc: 0.4897\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6769 - acc: 0.4904 - val_loss: 0.6773 - val_acc: 0.4897\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6769 - acc: 0.4901 - val_loss: 0.6767 - val_acc: 0.4897\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6769 - acc: 0.4905 - val_loss: 0.6769 - val_acc: 0.4897\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 593s 658us/step - loss: 0.6769 - acc: 0.4901 - val_loss: 0.6773 - val_acc: 0.4899\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 593s 659us/step - loss: 0.6769 - acc: 0.4903 - val_loss: 0.6769 - val_acc: 0.4898\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 591s 657us/step - loss: 0.6769 - acc: 0.4902 - val_loss: 0.6771 - val_acc: 0.4896\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 589s 655us/step - loss: 0.6769 - acc: 0.4904 - val_loss: 0.6769 - val_acc: 0.4897\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 588s 653us/step - loss: 0.6769 - acc: 0.4901 - val_loss: 0.6774 - val_acc: 0.4900\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 585s 650us/step - loss: 0.6769 - acc: 0.4904 - val_loss: 0.6772 - val_acc: 0.4898\n",
      "trainnig theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 586s 651us/step - loss: 0.6789 - acc: 0.4901 - val_loss: 0.6789 - val_acc: 0.4900\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 580s 644us/step - loss: 0.6789 - acc: 0.4904 - val_loss: 0.6792 - val_acc: 0.4900\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 582s 646us/step - loss: 0.6789 - acc: 0.4900 - val_loss: 0.6792 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 579s 643us/step - loss: 0.6788 - acc: 0.4904 - val_loss: 0.6791 - val_acc: 0.4899\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 580s 645us/step - loss: 0.6788 - acc: 0.4903 - val_loss: 0.6789 - val_acc: 0.4898\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 581s 645us/step - loss: 0.6788 - acc: 0.4906 - val_loss: 0.6791 - val_acc: 0.4898\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 581s 645us/step - loss: 0.6788 - acc: 0.4901 - val_loss: 0.6785 - val_acc: 0.4898\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 582s 647us/step - loss: 0.6788 - acc: 0.4903 - val_loss: 0.6790 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6788 - acc: 0.4904 - val_loss: 0.6787 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 577s 642us/step - loss: 0.6787 - acc: 0.4903 - val_loss: 0.6787 - val_acc: 0.4897\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6788 - acc: 0.4903 - val_loss: 0.6793 - val_acc: 0.4897\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 571s 635us/step - loss: 0.6787 - acc: 0.4903 - val_loss: 0.6793 - val_acc: 0.4899\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 575s 639us/step - loss: 0.6787 - acc: 0.4902 - val_loss: 0.6791 - val_acc: 0.4901\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 573s 636us/step - loss: 0.6787 - acc: 0.4904 - val_loss: 0.6791 - val_acc: 0.4896\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 574s 638us/step - loss: 0.6788 - acc: 0.4905 - val_loss: 0.6791 - val_acc: 0.4898\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 570s 634us/step - loss: 0.6786 - acc: 0.4907 - val_loss: 0.6786 - val_acc: 0.4895\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 574s 638us/step - loss: 0.6787 - acc: 0.4904 - val_loss: 0.6792 - val_acc: 0.4898\n",
      "trainnig theta = : 0.13333333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 575s 639us/step - loss: 0.6808 - acc: 0.4904 - val_loss: 0.6809 - val_acc: 0.4902\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 641s 712us/step - loss: 0.6808 - acc: 0.4903 - val_loss: 0.6808 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 658s 731us/step - loss: 0.6807 - acc: 0.4902 - val_loss: 0.6805 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 663s 737us/step - loss: 0.6807 - acc: 0.4902 - val_loss: 0.6810 - val_acc: 0.4896\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6807 - acc: 0.4906 - val_loss: 0.6807 - val_acc: 0.4896\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 695s 773us/step - loss: 0.6808 - acc: 0.4904 - val_loss: 0.6808 - val_acc: 0.4898\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6807 - acc: 0.4900 - val_loss: 0.6807 - val_acc: 0.4899\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 696s 773us/step - loss: 0.6807 - acc: 0.4904 - val_loss: 0.6809 - val_acc: 0.4899\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 698s 775us/step - loss: 0.6807 - acc: 0.4906 - val_loss: 0.6807 - val_acc: 0.4898\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 660s 734us/step - loss: 0.6807 - acc: 0.4903 - val_loss: 0.6809 - val_acc: 0.4898\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 669s 744us/step - loss: 0.6806 - acc: 0.4905 - val_loss: 0.6812 - val_acc: 0.4898\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 671s 746us/step - loss: 0.6807 - acc: 0.4905 - val_loss: 0.6806 - val_acc: 0.4897\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6807 - acc: 0.4904 - val_loss: 0.6809 - val_acc: 0.4900\n",
      "trainnig theta = : 0.14166666666666666\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 705s 784us/step - loss: 0.6829 - acc: 0.4903 - val_loss: 0.6830 - val_acc: 0.4899\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6829 - acc: 0.4906 - val_loss: 0.6829 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 701s 779us/step - loss: 0.6828 - acc: 0.4907 - val_loss: 0.6830 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 698s 776us/step - loss: 0.6829 - acc: 0.4903 - val_loss: 0.6829 - val_acc: 0.4899\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 665s 739us/step - loss: 0.6828 - acc: 0.4907 - val_loss: 0.6828 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 669s 743us/step - loss: 0.6828 - acc: 0.4903 - val_loss: 0.6826 - val_acc: 0.4897\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 669s 743us/step - loss: 0.6828 - acc: 0.4905 - val_loss: 0.6828 - val_acc: 0.4900\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6828 - acc: 0.4907 - val_loss: 0.6828 - val_acc: 0.4900\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6828 - acc: 0.4906 - val_loss: 0.6828 - val_acc: 0.4897\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6828 - acc: 0.4906 - val_loss: 0.6830 - val_acc: 0.4898\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6828 - acc: 0.4905 - val_loss: 0.6830 - val_acc: 0.4898\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6828 - acc: 0.4904 - val_loss: 0.6826 - val_acc: 0.4898\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6828 - acc: 0.4905 - val_loss: 0.6830 - val_acc: 0.4901\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 662s 735us/step - loss: 0.6828 - acc: 0.4905 - val_loss: 0.6828 - val_acc: 0.4897\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 665s 739us/step - loss: 0.6828 - acc: 0.4904 - val_loss: 0.6827 - val_acc: 0.4900\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 671s 746us/step - loss: 0.6828 - acc: 0.4905 - val_loss: 0.6832 - val_acc: 0.4899\n",
      "trainnig theta = : 0.15000000000000002\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6851 - acc: 0.4909 - val_loss: 0.6853 - val_acc: 0.4899\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6851 - acc: 0.4904 - val_loss: 0.6851 - val_acc: 0.4897\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6851 - acc: 0.4902 - val_loss: 0.6849 - val_acc: 0.4897\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 687s 764us/step - loss: 0.6851 - acc: 0.4907 - val_loss: 0.6853 - val_acc: 0.4897\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 673s 748us/step - loss: 0.6851 - acc: 0.4909 - val_loss: 0.6851 - val_acc: 0.4901\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 660s 734us/step - loss: 0.6851 - acc: 0.4907 - val_loss: 0.6851 - val_acc: 0.4899\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 659s 732us/step - loss: 0.6851 - acc: 0.4905 - val_loss: 0.6850 - val_acc: 0.4899\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6851 - acc: 0.4904 - val_loss: 0.6851 - val_acc: 0.4899\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6851 - acc: 0.4909 - val_loss: 0.6850 - val_acc: 0.4902\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6850 - acc: 0.4908 - val_loss: 0.6853 - val_acc: 0.4899\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6851 - acc: 0.4907 - val_loss: 0.6850 - val_acc: 0.4900\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6851 - acc: 0.4908 - val_loss: 0.6850 - val_acc: 0.4898\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 681s 757us/step - loss: 0.6850 - acc: 0.4908 - val_loss: 0.6852 - val_acc: 0.4897\n",
      "trainnig theta = : 0.15833333333333333\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 656s 729us/step - loss: 0.6873 - acc: 0.4905 - val_loss: 0.6872 - val_acc: 0.4898\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 656s 729us/step - loss: 0.6873 - acc: 0.4907 - val_loss: 0.6874 - val_acc: 0.4900\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 656s 729us/step - loss: 0.6874 - acc: 0.4908 - val_loss: 0.6873 - val_acc: 0.4901\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6873 - acc: 0.4906 - val_loss: 0.6872 - val_acc: 0.4902\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6873 - acc: 0.4906 - val_loss: 0.6874 - val_acc: 0.4902\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6873 - acc: 0.4909 - val_loss: 0.6873 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6873 - acc: 0.4909 - val_loss: 0.6874 - val_acc: 0.4901\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6873 - acc: 0.4908 - val_loss: 0.6872 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 646s 718us/step - loss: 0.6873 - acc: 0.4906 - val_loss: 0.6872 - val_acc: 0.4901\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 661s 734us/step - loss: 0.6873 - acc: 0.4910 - val_loss: 0.6874 - val_acc: 0.4902\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 661s 734us/step - loss: 0.6873 - acc: 0.4910 - val_loss: 0.6875 - val_acc: 0.4902\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6873 - acc: 0.4905 - val_loss: 0.6872 - val_acc: 0.4900\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6873 - acc: 0.4908 - val_loss: 0.6875 - val_acc: 0.4903\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6873 - acc: 0.4910 - val_loss: 0.6873 - val_acc: 0.4896\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6873 - acc: 0.4907 - val_loss: 0.6873 - val_acc: 0.4901\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6873 - acc: 0.4908 - val_loss: 0.6872 - val_acc: 0.4896\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 656s 728us/step - loss: 0.6873 - acc: 0.4912 - val_loss: 0.6872 - val_acc: 0.4898\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 660s 733us/step - loss: 0.6872 - acc: 0.4911 - val_loss: 0.6873 - val_acc: 0.4898\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 661s 735us/step - loss: 0.6873 - acc: 0.4907 - val_loss: 0.6873 - val_acc: 0.4901\n",
      "trainnig theta = : 0.16666666666666669\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 676s 751us/step - loss: 0.6891 - acc: 0.4912 - val_loss: 0.6890 - val_acc: 0.4901\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6891 - acc: 0.4915 - val_loss: 0.6890 - val_acc: 0.4898\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6891 - acc: 0.4910 - val_loss: 0.6891 - val_acc: 0.4899\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6891 - acc: 0.4906 - val_loss: 0.6890 - val_acc: 0.4903\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6891 - acc: 0.4908 - val_loss: 0.6893 - val_acc: 0.4905\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 662s 736us/step - loss: 0.6891 - acc: 0.4910 - val_loss: 0.6890 - val_acc: 0.4899\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 653s 726us/step - loss: 0.6891 - acc: 0.4909 - val_loss: 0.6890 - val_acc: 0.4903\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 654s 727us/step - loss: 0.6891 - acc: 0.4911 - val_loss: 0.6890 - val_acc: 0.4900\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 663s 737us/step - loss: 0.6891 - acc: 0.4913 - val_loss: 0.6891 - val_acc: 0.4902\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 716s 796us/step - loss: 0.6890 - acc: 0.4916 - val_loss: 0.6891 - val_acc: 0.4902\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6891 - acc: 0.4913 - val_loss: 0.6891 - val_acc: 0.4903\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6891 - acc: 0.4913 - val_loss: 0.6891 - val_acc: 0.4901\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 687s 763us/step - loss: 0.6891 - acc: 0.4911 - val_loss: 0.6893 - val_acc: 0.4902\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 682s 758us/step - loss: 0.6891 - acc: 0.4915 - val_loss: 0.6891 - val_acc: 0.4901\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 649s 721us/step - loss: 0.6890 - acc: 0.4918 - val_loss: 0.6891 - val_acc: 0.4900\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 753s 837us/step - loss: 0.6890 - acc: 0.4911 - val_loss: 0.6892 - val_acc: 0.4904\n",
      "trainnig theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6901 - acc: 0.4913 - val_loss: 0.6901 - val_acc: 0.4899\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 685s 762us/step - loss: 0.6901 - acc: 0.4915 - val_loss: 0.6903 - val_acc: 0.4902\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6901 - acc: 0.4919 - val_loss: 0.6901 - val_acc: 0.4913\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6901 - acc: 0.4928 - val_loss: 0.6901 - val_acc: 0.4904\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 685s 761us/step - loss: 0.6901 - acc: 0.4916 - val_loss: 0.6900 - val_acc: 0.4901\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 672s 746us/step - loss: 0.6901 - acc: 0.4923 - val_loss: 0.6901 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 663s 737us/step - loss: 0.6901 - acc: 0.4918 - val_loss: 0.6901 - val_acc: 0.4906\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 694s 772us/step - loss: 0.6901 - acc: 0.4919 - val_loss: 0.6901 - val_acc: 0.4903\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6901 - acc: 0.4919 - val_loss: 0.6901 - val_acc: 0.4903\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 692s 768us/step - loss: 0.6901 - acc: 0.4923 - val_loss: 0.6901 - val_acc: 0.4906\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6901 - acc: 0.4917 - val_loss: 0.6901 - val_acc: 0.4904\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6901 - acc: 0.4923 - val_loss: 0.6901 - val_acc: 0.4903\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 694s 772us/step - loss: 0.6901 - acc: 0.4916 - val_loss: 0.6901 - val_acc: 0.4906\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6901 - acc: 0.4921 - val_loss: 0.6901 - val_acc: 0.4903\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6901 - acc: 0.4925 - val_loss: 0.6901 - val_acc: 0.4906\n",
      "trainnig theta = : 0.18333333333333335\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 703s 781us/step - loss: 0.6901 - acc: 0.4935 - val_loss: 0.6901 - val_acc: 0.4907\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 695s 772us/step - loss: 0.6901 - acc: 0.4932 - val_loss: 0.6901 - val_acc: 0.4918\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 693s 769us/step - loss: 0.6901 - acc: 0.4926 - val_loss: 0.6901 - val_acc: 0.4914\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6901 - acc: 0.4934 - val_loss: 0.6901 - val_acc: 0.4914\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6901 - acc: 0.4931 - val_loss: 0.6901 - val_acc: 0.4914\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6901 - acc: 0.4927 - val_loss: 0.6901 - val_acc: 0.4908\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6901 - acc: 0.4931 - val_loss: 0.6901 - val_acc: 0.4912\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6901 - acc: 0.4945 - val_loss: 0.6901 - val_acc: 0.4910\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 693s 769us/step - loss: 0.6901 - acc: 0.4936 - val_loss: 0.6901 - val_acc: 0.4905\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6901 - acc: 0.4941 - val_loss: 0.6901 - val_acc: 0.4902\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 693s 771us/step - loss: 0.6901 - acc: 0.4929 - val_loss: 0.6901 - val_acc: 0.4905\n",
      "trainnig theta = : 0.19166666666666665\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 710s 789us/step - loss: 0.6893 - acc: 0.4950 - val_loss: 0.6893 - val_acc: 0.4927\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 696s 773us/step - loss: 0.6893 - acc: 0.4960 - val_loss: 0.6893 - val_acc: 0.4932\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6894 - acc: 0.4950 - val_loss: 0.6893 - val_acc: 0.4950\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 694s 771us/step - loss: 0.6893 - acc: 0.4949 - val_loss: 0.6893 - val_acc: 0.4929\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6893 - acc: 0.4948 - val_loss: 0.6894 - val_acc: 0.4933\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 690s 767us/step - loss: 0.6893 - acc: 0.4954 - val_loss: 0.6893 - val_acc: 0.4933\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 693s 770us/step - loss: 0.6893 - acc: 0.4954 - val_loss: 0.6893 - val_acc: 0.4916\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 692s 769us/step - loss: 0.6893 - acc: 0.4956 - val_loss: 0.6893 - val_acc: 0.4945\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 691s 768us/step - loss: 0.6893 - acc: 0.4961 - val_loss: 0.6893 - val_acc: 0.4955\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6893 - acc: 0.4972 - val_loss: 0.6894 - val_acc: 0.4929\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 691s 767us/step - loss: 0.6894 - acc: 0.4959 - val_loss: 0.6894 - val_acc: 0.4931\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6894 - acc: 0.4961 - val_loss: 0.6893 - val_acc: 0.4945\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 687s 764us/step - loss: 0.6893 - acc: 0.4957 - val_loss: 0.6893 - val_acc: 0.4931\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6893 - acc: 0.4970 - val_loss: 0.6893 - val_acc: 0.4942\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 688s 764us/step - loss: 0.6893 - acc: 0.4971 - val_loss: 0.6893 - val_acc: 0.4943\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6893 - acc: 0.4957 - val_loss: 0.6894 - val_acc: 0.4931\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6893 - acc: 0.4966 - val_loss: 0.6894 - val_acc: 0.4958\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 688s 765us/step - loss: 0.6893 - acc: 0.4973 - val_loss: 0.6894 - val_acc: 0.4941\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6893 - acc: 0.4972 - val_loss: 0.6893 - val_acc: 0.4950\n",
      "trainnig theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 699s 776us/step - loss: 0.6884 - acc: 0.4990 - val_loss: 0.6884 - val_acc: 0.4975\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 689s 766us/step - loss: 0.6884 - acc: 0.4991 - val_loss: 0.6884 - val_acc: 0.4996\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6884 - acc: 0.4992 - val_loss: 0.6884 - val_acc: 0.4988\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6884 - acc: 0.4991 - val_loss: 0.6884 - val_acc: 0.4958\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6884 - acc: 0.4994 - val_loss: 0.6884 - val_acc: 0.4990\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 686s 762us/step - loss: 0.6884 - acc: 0.4996 - val_loss: 0.6884 - val_acc: 0.4999\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 686s 763us/step - loss: 0.6884 - acc: 0.4998 - val_loss: 0.6884 - val_acc: 0.4978\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 684s 760us/step - loss: 0.6884 - acc: 0.5001 - val_loss: 0.6884 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6884 - acc: 0.4988 - val_loss: 0.6884 - val_acc: 0.4998\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6884 - acc: 0.4990 - val_loss: 0.6884 - val_acc: 0.5004\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6884 - acc: 0.4995 - val_loss: 0.6884 - val_acc: 0.4989\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6884 - acc: 0.4996 - val_loss: 0.6884 - val_acc: 0.4972\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 683s 759us/step - loss: 0.6884 - acc: 0.4998 - val_loss: 0.6884 - val_acc: 0.4995\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 681s 757us/step - loss: 0.6884 - acc: 0.4991 - val_loss: 0.6884 - val_acc: 0.4979\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 680s 755us/step - loss: 0.6884 - acc: 0.4996 - val_loss: 0.6884 - val_acc: 0.4991\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 677s 753us/step - loss: 0.6884 - acc: 0.5005 - val_loss: 0.6884 - val_acc: 0.4993\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6883 - acc: 0.5002 - val_loss: 0.6884 - val_acc: 0.4992\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 681s 756us/step - loss: 0.6884 - acc: 0.4995 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6884 - acc: 0.5004 - val_loss: 0.6884 - val_acc: 0.4992\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6884 - acc: 0.5000 - val_loss: 0.6884 - val_acc: 0.5002\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 676s 751us/step - loss: 0.6884 - acc: 0.5002 - val_loss: 0.6884 - val_acc: 0.4996\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6884 - acc: 0.5002 - val_loss: 0.6884 - val_acc: 0.4995\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 676s 751us/step - loss: 0.6884 - acc: 0.5012 - val_loss: 0.6884 - val_acc: 0.4994\n",
      "trainnig theta = : 0.20833333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 686s 763us/step - loss: 0.6878 - acc: 0.5007 - val_loss: 0.6878 - val_acc: 0.5042\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6878 - acc: 0.5016 - val_loss: 0.6878 - val_acc: 0.5080\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 680s 755us/step - loss: 0.6877 - acc: 0.5041 - val_loss: 0.6877 - val_acc: 0.5061\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6877 - acc: 0.5046 - val_loss: 0.6877 - val_acc: 0.5064\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6877 - acc: 0.5057 - val_loss: 0.6877 - val_acc: 0.5042\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 679s 754us/step - loss: 0.6877 - acc: 0.5053 - val_loss: 0.6877 - val_acc: 0.5051\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 676s 751us/step - loss: 0.6877 - acc: 0.5054 - val_loss: 0.6877 - val_acc: 0.5036\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 677s 753us/step - loss: 0.6877 - acc: 0.5056 - val_loss: 0.6877 - val_acc: 0.5062\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 677s 753us/step - loss: 0.6878 - acc: 0.5054 - val_loss: 0.6877 - val_acc: 0.5025\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 677s 752us/step - loss: 0.6877 - acc: 0.5037 - val_loss: 0.6877 - val_acc: 0.5035\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 678s 753us/step - loss: 0.6877 - acc: 0.5041 - val_loss: 0.6877 - val_acc: 0.5043\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 675s 749us/step - loss: 0.6877 - acc: 0.5058 - val_loss: 0.6877 - val_acc: 0.5059\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 678s 754us/step - loss: 0.6877 - acc: 0.5054 - val_loss: 0.6877 - val_acc: 0.5052\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 678s 753us/step - loss: 0.6877 - acc: 0.5051 - val_loss: 0.6877 - val_acc: 0.5068\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 674s 749us/step - loss: 0.6877 - acc: 0.5065 - val_loss: 0.6877 - val_acc: 0.5056\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 676s 751us/step - loss: 0.6877 - acc: 0.5050 - val_loss: 0.6877 - val_acc: 0.5028\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 675s 750us/step - loss: 0.6877 - acc: 0.5062 - val_loss: 0.6877 - val_acc: 0.5042\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 674s 749us/step - loss: 0.6877 - acc: 0.5052 - val_loss: 0.6877 - val_acc: 0.5049\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 672s 746us/step - loss: 0.6877 - acc: 0.5064 - val_loss: 0.6877 - val_acc: 0.5054\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 671s 746us/step - loss: 0.6877 - acc: 0.5058 - val_loss: 0.6877 - val_acc: 0.5077\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 673s 748us/step - loss: 0.6877 - acc: 0.5067 - val_loss: 0.6877 - val_acc: 0.5051\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 671s 745us/step - loss: 0.6877 - acc: 0.5064 - val_loss: 0.6877 - val_acc: 0.5045\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 668s 743us/step - loss: 0.6877 - acc: 0.5055 - val_loss: 0.6877 - val_acc: 0.5049\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 670s 745us/step - loss: 0.6877 - acc: 0.5055 - val_loss: 0.6877 - val_acc: 0.5062\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 667s 741us/step - loss: 0.6877 - acc: 0.5059 - val_loss: 0.6878 - val_acc: 0.5072\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 690s 766us/step - loss: 0.6877 - acc: 0.5051 - val_loss: 0.6877 - val_acc: 0.5036\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 697s 774us/step - loss: 0.6877 - acc: 0.5058 - val_loss: 0.6877 - val_acc: 0.5053\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 699s 777us/step - loss: 0.6877 - acc: 0.5069 - val_loss: 0.6877 - val_acc: 0.5053\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 704s 782us/step - loss: 0.6877 - acc: 0.5057 - val_loss: 0.6877 - val_acc: 0.5058\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 713s 792us/step - loss: 0.6877 - acc: 0.5066 - val_loss: 0.6877 - val_acc: 0.5072\n",
      "Epoch 31/100\n",
      "900000/900000 [==============================] - 720s 800us/step - loss: 0.6877 - acc: 0.5064 - val_loss: 0.6877 - val_acc: 0.5030\n",
      "Epoch 32/100\n",
      "900000/900000 [==============================] - 726s 806us/step - loss: 0.6877 - acc: 0.5066 - val_loss: 0.6877 - val_acc: 0.5072\n",
      "Epoch 33/100\n",
      "900000/900000 [==============================] - 720s 800us/step - loss: 0.6877 - acc: 0.5068 - val_loss: 0.6877 - val_acc: 0.5075\n",
      "Epoch 34/100\n",
      "900000/900000 [==============================] - 590s 655us/step - loss: 0.6877 - acc: 0.5062 - val_loss: 0.6877 - val_acc: 0.5063\n",
      "Epoch 35/100\n",
      "900000/900000 [==============================] - 550s 611us/step - loss: 0.6876 - acc: 0.5065 - val_loss: 0.6877 - val_acc: 0.5054\n",
      "Epoch 36/100\n",
      "900000/900000 [==============================] - 568s 631us/step - loss: 0.6877 - acc: 0.5066 - val_loss: 0.6877 - val_acc: 0.5051\n",
      "Epoch 37/100\n",
      "900000/900000 [==============================] - 588s 654us/step - loss: 0.6877 - acc: 0.5068 - val_loss: 0.6877 - val_acc: 0.5055\n",
      "Epoch 38/100\n",
      "900000/900000 [==============================] - 565s 627us/step - loss: 0.6877 - acc: 0.5068 - val_loss: 0.6877 - val_acc: 0.5049\n",
      "Epoch 39/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6876 - acc: 0.5072 - val_loss: 0.6877 - val_acc: 0.5064\n",
      "Epoch 40/100\n",
      "900000/900000 [==============================] - 611s 679us/step - loss: 0.6876 - acc: 0.5071 - val_loss: 0.6877 - val_acc: 0.5056\n",
      "Epoch 41/100\n",
      "900000/900000 [==============================] - 614s 682us/step - loss: 0.6877 - acc: 0.5066 - val_loss: 0.6877 - val_acc: 0.5061\n",
      "Epoch 42/100\n",
      "900000/900000 [==============================] - 626s 696us/step - loss: 0.6877 - acc: 0.5065 - val_loss: 0.6877 - val_acc: 0.5052\n",
      "Epoch 43/100\n",
      "900000/900000 [==============================] - 637s 707us/step - loss: 0.6876 - acc: 0.5066 - val_loss: 0.6877 - val_acc: 0.5059\n",
      "Epoch 44/100\n",
      "900000/900000 [==============================] - 624s 693us/step - loss: 0.6876 - acc: 0.5077 - val_loss: 0.6877 - val_acc: 0.5058\n",
      "Epoch 45/100\n",
      "900000/900000 [==============================] - 623s 692us/step - loss: 0.6876 - acc: 0.5073 - val_loss: 0.6877 - val_acc: 0.5044\n",
      "trainnig theta = : 0.21666666666666667\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 644s 715us/step - loss: 0.6875 - acc: 0.5084 - val_loss: 0.6875 - val_acc: 0.5079\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 639s 710us/step - loss: 0.6875 - acc: 0.5089 - val_loss: 0.6875 - val_acc: 0.5085\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 634s 704us/step - loss: 0.6875 - acc: 0.5092 - val_loss: 0.6875 - val_acc: 0.5088\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 633s 704us/step - loss: 0.6875 - acc: 0.5086 - val_loss: 0.6875 - val_acc: 0.5088\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6874 - acc: 0.5088 - val_loss: 0.6875 - val_acc: 0.5089\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 608s 675us/step - loss: 0.6875 - acc: 0.5083 - val_loss: 0.6875 - val_acc: 0.5071\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 617s 685us/step - loss: 0.6874 - acc: 0.5091 - val_loss: 0.6875 - val_acc: 0.5086\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 621s 691us/step - loss: 0.6874 - acc: 0.5093 - val_loss: 0.6875 - val_acc: 0.5077\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 608s 676us/step - loss: 0.6874 - acc: 0.5093 - val_loss: 0.6875 - val_acc: 0.5092\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 606s 674us/step - loss: 0.6874 - acc: 0.5093 - val_loss: 0.6875 - val_acc: 0.5085\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 621s 690us/step - loss: 0.6874 - acc: 0.5093 - val_loss: 0.6875 - val_acc: 0.5084\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 626s 695us/step - loss: 0.6874 - acc: 0.5093 - val_loss: 0.6875 - val_acc: 0.5084\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 616s 684us/step - loss: 0.6874 - acc: 0.5092 - val_loss: 0.6875 - val_acc: 0.5081\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 600s 667us/step - loss: 0.6874 - acc: 0.5095 - val_loss: 0.6875 - val_acc: 0.5090\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 595s 661us/step - loss: 0.6875 - acc: 0.5086 - val_loss: 0.6875 - val_acc: 0.5090\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6874 - acc: 0.5097 - val_loss: 0.6875 - val_acc: 0.5085\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6874 - acc: 0.5098 - val_loss: 0.6875 - val_acc: 0.5092\n",
      "trainnig theta = : 0.225\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 583s 648us/step - loss: 0.6865 - acc: 0.5096 - val_loss: 0.6865 - val_acc: 0.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 591s 656us/step - loss: 0.6865 - acc: 0.5095 - val_loss: 0.6865 - val_acc: 0.5087\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6864 - acc: 0.5097 - val_loss: 0.6865 - val_acc: 0.5092\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 592s 658us/step - loss: 0.6864 - acc: 0.5096 - val_loss: 0.6865 - val_acc: 0.5086\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 591s 656us/step - loss: 0.6864 - acc: 0.5097 - val_loss: 0.6865 - val_acc: 0.5091\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 590s 656us/step - loss: 0.6864 - acc: 0.5097 - val_loss: 0.6865 - val_acc: 0.5084\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 591s 656us/step - loss: 0.6864 - acc: 0.5099 - val_loss: 0.6865 - val_acc: 0.5092\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 588s 654us/step - loss: 0.6864 - acc: 0.5098 - val_loss: 0.6865 - val_acc: 0.5087\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 591s 657us/step - loss: 0.6864 - acc: 0.5095 - val_loss: 0.6865 - val_acc: 0.5087\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 572s 635us/step - loss: 0.6864 - acc: 0.5100 - val_loss: 0.6865 - val_acc: 0.5089\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 586s 651us/step - loss: 0.6864 - acc: 0.5098 - val_loss: 0.6865 - val_acc: 0.5091\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6864 - acc: 0.5106 - val_loss: 0.6865 - val_acc: 0.5092\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 587s 652us/step - loss: 0.6864 - acc: 0.5101 - val_loss: 0.6864 - val_acc: 0.5095\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 590s 655us/step - loss: 0.6864 - acc: 0.5099 - val_loss: 0.6865 - val_acc: 0.5090\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 596s 662us/step - loss: 0.6864 - acc: 0.5102 - val_loss: 0.6865 - val_acc: 0.5093\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 594s 660us/step - loss: 0.6864 - acc: 0.5104 - val_loss: 0.6865 - val_acc: 0.5094\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 589s 654us/step - loss: 0.6864 - acc: 0.5099 - val_loss: 0.6865 - val_acc: 0.5092\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 591s 657us/step - loss: 0.6864 - acc: 0.5102 - val_loss: 0.6865 - val_acc: 0.5094\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 586s 652us/step - loss: 0.6864 - acc: 0.5107 - val_loss: 0.6865 - val_acc: 0.5093\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 590s 655us/step - loss: 0.6864 - acc: 0.5098 - val_loss: 0.6865 - val_acc: 0.5092\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 587s 652us/step - loss: 0.6864 - acc: 0.5098 - val_loss: 0.6865 - val_acc: 0.5090\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 573s 636us/step - loss: 0.6863 - acc: 0.5099 - val_loss: 0.6864 - val_acc: 0.5092\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 559s 622us/step - loss: 0.6863 - acc: 0.5105 - val_loss: 0.6865 - val_acc: 0.5090\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 561s 624us/step - loss: 0.6864 - acc: 0.5104 - val_loss: 0.6865 - val_acc: 0.5093\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 561s 623us/step - loss: 0.6863 - acc: 0.5104 - val_loss: 0.6865 - val_acc: 0.5091\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 557s 619us/step - loss: 0.6863 - acc: 0.5103 - val_loss: 0.6865 - val_acc: 0.5096\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 561s 624us/step - loss: 0.6863 - acc: 0.5109 - val_loss: 0.6865 - val_acc: 0.5087\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 556s 618us/step - loss: 0.6863 - acc: 0.5107 - val_loss: 0.6865 - val_acc: 0.5093\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 568s 631us/step - loss: 0.6863 - acc: 0.5104 - val_loss: 0.6865 - val_acc: 0.5088\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 565s 628us/step - loss: 0.6863 - acc: 0.5105 - val_loss: 0.6865 - val_acc: 0.5091\n",
      "Epoch 31/100\n",
      "900000/900000 [==============================] - 577s 641us/step - loss: 0.6863 - acc: 0.5103 - val_loss: 0.6865 - val_acc: 0.5094\n",
      "Epoch 32/100\n",
      "900000/900000 [==============================] - 552s 613us/step - loss: 0.6863 - acc: 0.5110 - val_loss: 0.6865 - val_acc: 0.5091\n",
      "trainnig theta = : 0.23333333333333334\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 564s 626us/step - loss: 0.6849 - acc: 0.5103 - val_loss: 0.6850 - val_acc: 0.5092\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 560s 622us/step - loss: 0.6849 - acc: 0.5106 - val_loss: 0.6850 - val_acc: 0.5091\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 558s 619us/step - loss: 0.6850 - acc: 0.5099 - val_loss: 0.6851 - val_acc: 0.5088\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 573s 637us/step - loss: 0.6849 - acc: 0.5102 - val_loss: 0.6851 - val_acc: 0.5089\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 568s 631us/step - loss: 0.6849 - acc: 0.5108 - val_loss: 0.6850 - val_acc: 0.5094\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 581s 646us/step - loss: 0.6849 - acc: 0.5108 - val_loss: 0.6850 - val_acc: 0.5094\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 558s 620us/step - loss: 0.6849 - acc: 0.5099 - val_loss: 0.6850 - val_acc: 0.5093\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 575s 638us/step - loss: 0.6849 - acc: 0.5104 - val_loss: 0.6850 - val_acc: 0.5092\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 578s 642us/step - loss: 0.6849 - acc: 0.5106 - val_loss: 0.6850 - val_acc: 0.5094\n",
      "Epoch 10/100\n",
      "188000/900000 [=====>........................] - ETA: 4:49 - loss: 0.6849 - acc: 0.5092"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 25)   #iterating across possible StoUD values\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam',\n",
    "                        loss=my_loss_wrapper(myinputs, theta),\n",
    "                        metrics=['accuracy'])\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystopping])\n",
    "    lvals += [history.history['loss'][np.argmin(history.history['val_loss'])]]\n",
    "    vlvals += [np.min(history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T01:55:26.438495Z",
     "start_time": "2020-07-29T01:55:25.861209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"StoUD vs. Loss (Full Phase Space)\\n $F_{dropout} = \\phi_{dropout} = 0.2$\\nDCTR Change\")\n",
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.vlines(0.200, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"probStoUD(200) Vs Loss-FDropoutPhiDropout02-Copy3.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T01:55:26.448032Z",
     "start_time": "2020-07-29T01:55:26.441594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18333333333333335"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[np.argmax(vlvals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.342771Z",
     "start_time": "2020-07-19T00:03:27.336321Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:03:27.590548Z",
     "start_time": "2020-07-19T00:03:27.345029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_21[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = int(len(X_train_theta) / 10)\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1, MSE_loss=False):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        #  when not training theta, fetch as np array\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "\n",
    "    weights = reweight(events = x, param = theta0) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.821775Z",
     "start_time": "2020-07-19T00:03:27.593159Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 151s 167us/step - loss: 0.5987 - acc: 0.4972 - val_loss: 0.5872 - val_acc: 0.4956\n",
      ". theta fit =  0.0\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "1800000/1800000 [==============================] - 225s 125us/step - loss: nan - acc: 0.4957         \n",
      ". theta fit =  nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'argrelmin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f6db20d6f348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     extrema = np.concatenate(\n\u001b[0;32m---> 36\u001b[0;31m         (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mextrema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextrema\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mextrema\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mindex_refine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argrelmin' is not defined"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration + 1)\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=1000,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train_theta, Y_train_theta, epochs=1, batch_size=batch_size,verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "\n",
    "        mean_fit = np.array([\n",
    "            np.mean(fit_vals_recent[len(fit_vals_recent) -\n",
    "                                    4:len(fit_vals_recent)])\n",
    "        ])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T00:09:49.823619Z",
     "start_time": "2020-07-17T18:54:03.260Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.200, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
