{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses the original DCTR model (i.e. model expects “empty particles” to have 0 in every entry). As opposerd to the, modified DCTR that was trained to allow non-zero inputs for theta on empty particles, the fit appears less noisy and more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_alphaS.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small = X_train[0:int(0.8*10**5)]\n",
    "Y_train_small = Y_train[0:int(0.8*10**5)]\n",
    "X_test_small = X_test[0:int(0.2*10**5)]\n",
    "Y_test_small = Y_test[0:int(0.2*10**5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_alphaS.h5') #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    theta_prime = [val, 0.68, 0.217]\n",
    "    \n",
    "    # zip theta_prime to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0, #checks if pT != 0, which means we have a particle\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-16-bfbe7827f025>:28: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2155 - acc: 0.5774 - val_loss: 0.2115 - val_acc: 0.5799\n",
      "trainnig theta = : 0.10250000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2132 - acc: 0.5815 - val_loss: 0.2115 - val_acc: 0.5835\n",
      "trainnig theta = : 0.10500000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2143 - acc: 0.5826 - val_loss: 0.2135 - val_acc: 0.5836\n",
      "trainnig theta = : 0.1075\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2160 - acc: 0.5833 - val_loss: 0.2154 - val_acc: 0.5838\n",
      "trainnig theta = : 0.11\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2181 - acc: 0.5839 - val_loss: 0.2178 - val_acc: 0.5845\n",
      "trainnig theta = : 0.1125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2204 - acc: 0.5842 - val_loss: 0.2212 - val_acc: 0.5813\n",
      "trainnig theta = : 0.115\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2228 - acc: 0.5848 - val_loss: 0.2228 - val_acc: 0.5856\n",
      "trainnig theta = : 0.11750000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2252 - acc: 0.5851 - val_loss: 0.2255 - val_acc: 0.5836\n",
      "trainnig theta = : 0.12\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2276 - acc: 0.5855 - val_loss: 0.2277 - val_acc: 0.5860\n",
      "trainnig theta = : 0.1225\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2299 - acc: 0.5854 - val_loss: 0.2298 - val_acc: 0.5854\n",
      "trainnig theta = : 0.125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2320 - acc: 0.5858 - val_loss: 0.2322 - val_acc: 0.5848\n",
      "trainnig theta = : 0.1275\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2339 - acc: 0.5861 - val_loss: 0.2341 - val_acc: 0.5850\n",
      "trainnig theta = : 0.13\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2357 - acc: 0.5862 - val_loss: 0.2364 - val_acc: 0.5836\n",
      "trainnig theta = : 0.1325\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 107us/step - loss: 0.2373 - acc: 0.5865 - val_loss: 0.2376 - val_acc: 0.5858\n",
      "trainnig theta = : 0.135\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2391 - acc: 0.5866 - val_loss: 0.2401 - val_acc: 0.5851\n",
      "trainnig theta = : 0.1375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 107us/step - loss: 0.2411 - acc: 0.5862 - val_loss: 0.2415 - val_acc: 0.5855\n",
      "trainnig theta = : 0.14\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2431 - acc: 0.5856 - val_loss: 0.2435 - val_acc: 0.5847\n",
      "trainnig theta = : 0.14250000000000002\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2446 - acc: 0.5855 - val_loss: 0.2450 - val_acc: 0.5823\n",
      "trainnig theta = : 0.145\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2459 - acc: 0.5850 - val_loss: 0.2466 - val_acc: 0.5758\n",
      "trainnig theta = : 0.1475\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2472 - acc: 0.5840 - val_loss: 0.2476 - val_acc: 0.5821\n",
      "trainnig theta = : 0.15\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2483 - acc: 0.5806 - val_loss: 0.2487 - val_acc: 0.5824\n",
      "trainnig theta = : 0.1525\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2491 - acc: 0.5776 - val_loss: 0.2497 - val_acc: 0.5599\n",
      "trainnig theta = : 0.155\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2497 - acc: 0.5704 - val_loss: 0.2500 - val_acc: 0.5625\n",
      "trainnig theta = : 0.1575\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2499 - acc: 0.5538 - val_loss: 0.2504 - val_acc: 0.5464\n",
      "trainnig theta = : 0.16\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2500 - acc: 0.5256 - val_loss: 0.2504 - val_acc: 0.5125\n",
      "trainnig theta = : 0.1625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2499 - acc: 0.4940 - val_loss: 0.2503 - val_acc: 0.4650\n",
      "trainnig theta = : 0.16499999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2495 - acc: 0.4682 - val_loss: 0.2500 - val_acc: 0.4684\n",
      "trainnig theta = : 0.16749999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2490 - acc: 0.4549 - val_loss: 0.2494 - val_acc: 0.4392\n",
      "trainnig theta = : 0.16999999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2482 - acc: 0.4473 - val_loss: 0.2489 - val_acc: 0.4528\n",
      "trainnig theta = : 0.1725\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 109us/step - loss: 0.2474 - acc: 0.4441 - val_loss: 0.2482 - val_acc: 0.4319\n",
      "trainnig theta = : 0.175\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2464 - acc: 0.4428 - val_loss: 0.2472 - val_acc: 0.4435\n",
      "trainnig theta = : 0.1775\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 109us/step - loss: 0.2452 - acc: 0.4421 - val_loss: 0.2461 - val_acc: 0.4419\n",
      "trainnig theta = : 0.18\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2439 - acc: 0.4427 - val_loss: 0.2450 - val_acc: 0.4432\n",
      "[[0.21545321300832762], [0.21320670744818118], [0.21427190159964893], [0.21600884579949908], [0.2180913633149531], [0.2203975125422908], [0.22277363829521668], [0.2252077442386912], [0.22762178098782898], [0.22985038127129276], [0.2319808946715461], [0.23389454455011421], [0.23567747064969605], [0.23733307556766603], [0.23912259595882562], [0.2411479534374343], [0.24305367945796913], [0.24457945177952448], [0.24594977470114826], [0.24723519961246185], [0.24829403785988688], [0.24910752758797672], [0.24965062933042645], [0.24994903881516722], [0.25001448064835535], [0.2498855436945127], [0.24953465532097552], [0.24896939413415062], [0.2482402424948911], [0.24736307265443935], [0.2463511453424063], [0.24517832743003964], [0.24393402135206593]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.18, 33) #iterating across possible alphaS values\n",
    "vlvals = []\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas,lvals, label = lvals)\n",
    "plt.plot(thetas,vlvals, label = vlvals)\n",
    "plt.vlines(0.160, ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"MSE for alphaS altFit SUCCESS, proper method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate plot from previous loss values\n",
    "'''\n",
    "lvals_mod= [[0.2205248251867791], \n",
    "            [0.21686763239817486], \n",
    "            [0.21641605897910066], \n",
    "            [0.2165923680799703], \n",
    "            [0.21757270126707023], \n",
    "            [0.21946977500079407], \n",
    "            [0.22129104937323266], \n",
    "            [0.2230205901588003], \n",
    "            [0.22444402735887303], \n",
    "            [0.22604122912097308],\n",
    "            [0.22767174139411914], \n",
    "            [0.2295547942423986], \n",
    "            [0.2314727822939555], \n",
    "            [0.2333153494116333], \n",
    "            [0.23476429952101574], \n",
    "            [0.23622041649909484], \n",
    "            [0.23832544412256942], \n",
    "            [0.24102286402550008],\n",
    "            [0.24376624565985466], \n",
    "            [0.24562504440546035], \n",
    "            [0.24651510941071642], \n",
    "            [0.24727022836191787], \n",
    "            [0.24861559607088565], \n",
    "            [0.2502629839401278], \n",
    "            [0.2513116162063347], \n",
    "            [0.2514477237645123], \n",
    "            [0.251843279817452], \n",
    "            [0.2516317962668836], \n",
    "            [0.2509480234235525], \n",
    "            [0.2501125036428372], \n",
    "            [0.24921633639476365], \n",
    "            [0.24822285794135596],\n",
    "            [0.2471035583048231]]\n",
    "\n",
    "lvals_orig= [[0.21545321300832762],\n",
    "             [0.21320670744818118],\n",
    "             [0.21427190159964893],\n",
    "             [0.21600884579949908],\n",
    "             [0.2180913633149531],\n",
    "             [0.2203975125422908],\n",
    "             [0.22277363829521668],\n",
    "             [0.2252077442386912],\n",
    "             [0.22762178098782898],\n",
    "             [0.22985038127129276],\n",
    "             [0.2319808946715461],\n",
    "             [0.23389454455011421],\n",
    "             [0.23567747064969605],\n",
    "             [0.23733307556766603],\n",
    "             [0.23912259595882562],\n",
    "             [0.2411479534374343],\n",
    "             [0.24305367945796913],\n",
    "             [0.24457945177952448],\n",
    "             [0.24594977470114826],\n",
    "             [0.24723519961246185],\n",
    "             [0.24829403785988688],\n",
    "             [0.24910752758797672],\n",
    "             [0.24965062933042645],\n",
    "             [0.24994903881516722],\n",
    "             [0.25001448064835535],\n",
    "             [0.2498855436945127],\n",
    "             [0.24953465532097552],\n",
    "             [0.24896939413415062],\n",
    "             [0.2482402424948911],\n",
    "             [0.24736307265443935],\n",
    "             [0.2463511453424063],\n",
    "             [0.24517832743003964],\n",
    "             [0.24393402135206593]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "thetas = np.linspace(0.10, 0.18, 33) #iterating across possible alphaS values\n",
    "\n",
    "plt.plot(thetas,lvals_orig, label = \"Loss using original DCTR\") #NEW (BEST) METHOD\n",
    "plt.plot(thetas,lvals_mod, label = \"Loss using modified DCTR\") #OLD METHOD\n",
    "plt.vlines(0.160, ymin = np.min(lvals_mod), ymax = np.max(lvals_mod), label = 'Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"MSE for alphaS altFit SUCCESS-orig vs modified DCTR.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.12\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_14[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch:  0\n",
      "WARNING:tensorflow:From <ipython-input-15-24940b61cb9f>:41: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training g\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2327 - acc: 0.5773 - val_loss: 0.2298 - val_acc: 0.5810\n",
      ". theta fit =  0.12\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2301 - acc: 0.5822 - val_loss: -0.2304 - val_acc: 0.5810\n",
      ". theta fit =  0.12071699\n",
      "Epoch:  1\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2305 - acc: 0.5816 - val_loss: 0.2299 - val_acc: 0.5829\n",
      ". theta fit =  0.12071699\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2305 - acc: 0.5837 - val_loss: -0.2309 - val_acc: 0.5829\n",
      ". theta fit =  0.12187422\n",
      "Epoch:  2\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 109us/step - loss: 0.2306 - acc: 0.5834 - val_loss: 0.2304 - val_acc: 0.5822\n",
      ". theta fit =  0.12187422\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2308 - acc: 0.5833 - val_loss: -0.2315 - val_acc: 0.5822\n",
      ". theta fit =  0.12310385\n",
      "Epoch:  3\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2312 - acc: 0.5843 - val_loss: 0.2308 - val_acc: 0.5837\n",
      ". theta fit =  0.12310385\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2312 - acc: 0.5854 - val_loss: -0.2319 - val_acc: 0.5837\n",
      ". theta fit =  0.12435214\n",
      "Epoch:  4\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2320 - acc: 0.5847 - val_loss: 0.2325 - val_acc: 0.5829\n",
      ". theta fit =  0.12435214\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2327 - acc: 0.5843 - val_loss: -0.2334 - val_acc: 0.5829\n",
      ". theta fit =  0.12559955\n",
      "Epoch:  5\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2329 - acc: 0.5852 - val_loss: 0.2330 - val_acc: 0.5837\n",
      ". theta fit =  0.12559955\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2332 - acc: 0.5856 - val_loss: -0.2340 - val_acc: 0.5837\n",
      ". theta fit =  0.12685455\n",
      "Epoch:  6\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 160s 111us/step - loss: 0.2337 - acc: 0.5857 - val_loss: 0.2350 - val_acc: 0.5799\n",
      ". theta fit =  0.12685455\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 136us/step - loss: -0.2351 - acc: 0.5822 - val_loss: -0.2360 - val_acc: 0.5799\n",
      ". theta fit =  0.12809907\n",
      "Epoch:  7\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2345 - acc: 0.5858 - val_loss: 0.2347 - val_acc: 0.5844\n",
      ". theta fit =  0.12809907\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2348 - acc: 0.5866 - val_loss: -0.2357 - val_acc: 0.5844\n",
      ". theta fit =  0.12934646\n",
      "Epoch:  8\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2354 - acc: 0.5858 - val_loss: 0.2355 - val_acc: 0.5848\n",
      ". theta fit =  0.12934646\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 195s 135us/step - loss: -0.2356 - acc: 0.5872 - val_loss: -0.2364 - val_acc: 0.5848\n",
      ". theta fit =  0.1305986\n",
      "Epoch:  9\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2362 - acc: 0.5861 - val_loss: 0.2365 - val_acc: 0.5846\n",
      ". theta fit =  0.1305986\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 194s 135us/step - loss: -0.2366 - acc: 0.5863 - val_loss: -0.2374 - val_acc: 0.5846\n",
      ". theta fit =  0.13185368\n",
      "Epoch:  10\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2371 - acc: 0.5858 - val_loss: 0.2375 - val_acc: 0.5836\n",
      ". theta fit =  0.13185368\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 193s 134us/step - loss: -0.2375 - acc: 0.5863 - val_loss: -0.2383 - val_acc: 0.5836\n",
      ". theta fit =  0.13311931\n",
      "Epoch:  11\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2379 - acc: 0.5865 - val_loss: 0.2383 - val_acc: 0.5847\n",
      ". theta fit =  0.13311931\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 192s 133us/step - loss: -0.2383 - acc: 0.5871 - val_loss: -0.2392 - val_acc: 0.5847\n",
      ". theta fit =  0.13439102\n",
      "Epoch:  12\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2387 - acc: 0.5864 - val_loss: 0.2391 - val_acc: 0.5847\n",
      ". theta fit =  0.13439102\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 192s 134us/step - loss: -0.2392 - acc: 0.5866 - val_loss: -0.2402 - val_acc: 0.5847\n",
      ". theta fit =  0.13566406\n",
      "Epoch:  13\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2397 - acc: 0.5866 - val_loss: 0.2401 - val_acc: 0.5838\n",
      ". theta fit =  0.13566406\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 191s 132us/step - loss: -0.2401 - acc: 0.5866 - val_loss: -0.2412 - val_acc: 0.5838\n",
      ". theta fit =  0.13693102\n",
      "Epoch:  14\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2406 - acc: 0.5865 - val_loss: 0.2411 - val_acc: 0.5854\n",
      ". theta fit =  0.13693102\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 191s 133us/step - loss: -0.2411 - acc: 0.5880 - val_loss: -0.2422 - val_acc: 0.5854\n",
      ". theta fit =  0.1381829\n",
      "Epoch:  15\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2416 - acc: 0.5867 - val_loss: 0.2424 - val_acc: 0.5851\n",
      ". theta fit =  0.1381829\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 191s 133us/step - loss: -0.2422 - acc: 0.5877 - val_loss: -0.2435 - val_acc: 0.5851\n",
      ". theta fit =  0.13942409\n",
      "Epoch:  16\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2425 - acc: 0.5866 - val_loss: 0.2432 - val_acc: 0.5840\n",
      ". theta fit =  0.13942409\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2430 - acc: 0.5875 - val_loss: -0.2441 - val_acc: 0.5840\n",
      ". theta fit =  0.14064932\n",
      "Epoch:  17\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 104us/step - loss: 0.2434 - acc: 0.5864 - val_loss: 0.2440 - val_acc: 0.5839\n",
      ". theta fit =  0.14064932\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2438 - acc: 0.5868 - val_loss: -0.2449 - val_acc: 0.5839\n",
      ". theta fit =  0.14189446\n",
      "Epoch:  18\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2441 - acc: 0.5864 - val_loss: 0.2448 - val_acc: 0.5832\n",
      ". theta fit =  0.14189446\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2446 - acc: 0.5864 - val_loss: -0.2456 - val_acc: 0.5832\n",
      ". theta fit =  0.14313978\n",
      "Epoch:  19\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 104us/step - loss: 0.2448 - acc: 0.5864 - val_loss: 0.2455 - val_acc: 0.5832\n",
      ". theta fit =  0.14313978\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2451 - acc: 0.5878 - val_loss: -0.2462 - val_acc: 0.5832\n",
      ". theta fit =  0.14437863\n",
      "Epoch:  20\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2455 - acc: 0.5858 - val_loss: 0.2461 - val_acc: 0.5829\n",
      ". theta fit =  0.14437863\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2458 - acc: 0.5864 - val_loss: -0.2469 - val_acc: 0.5829\n",
      ". theta fit =  0.14561895\n",
      "Epoch:  21\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2461 - acc: 0.5850 - val_loss: 0.2468 - val_acc: 0.5823\n",
      ". theta fit =  0.14561895\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2464 - acc: 0.5865 - val_loss: -0.2475 - val_acc: 0.5823\n",
      ". theta fit =  0.14685352\n",
      "Epoch:  22\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2468 - acc: 0.5847 - val_loss: 0.2473 - val_acc: 0.5823\n",
      ". theta fit =  0.14685352\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2469 - acc: 0.5868 - val_loss: -0.2480 - val_acc: 0.5823\n",
      ". theta fit =  0.14808282\n",
      "Epoch:  23\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2473 - acc: 0.5839 - val_loss: 0.2480 - val_acc: 0.5795\n",
      ". theta fit =  0.14808282\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2474 - acc: 0.5854 - val_loss: -0.2485 - val_acc: 0.5795\n",
      ". theta fit =  0.14928076\n",
      "Epoch:  24\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2478 - acc: 0.5834 - val_loss: 0.2484 - val_acc: 0.5805\n",
      ". theta fit =  0.14928076\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2479 - acc: 0.5857 - val_loss: -0.2489 - val_acc: 0.5805\n",
      ". theta fit =  0.15047863\n",
      "Epoch:  25\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2482 - acc: 0.5815 - val_loss: 0.2490 - val_acc: 0.5801\n",
      ". theta fit =  0.15047863\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2484 - acc: 0.5857 - val_loss: -0.2495 - val_acc: 0.5801\n",
      ". theta fit =  0.15168029\n",
      "Epoch:  26\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2486 - acc: 0.5794 - val_loss: 0.2494 - val_acc: 0.5754\n",
      ". theta fit =  0.15168029\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2488 - acc: 0.5824 - val_loss: -0.2498 - val_acc: 0.5754\n",
      ". theta fit =  0.15285657\n",
      "Epoch:  27\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2489 - acc: 0.5768 - val_loss: 0.2496 - val_acc: 0.5738\n",
      ". theta fit =  0.15285657\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2489 - acc: 0.5813 - val_loss: -0.2499 - val_acc: 0.5738\n",
      ". theta fit =  0.15401563\n",
      "Epoch:  28\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2492 - acc: 0.5728 - val_loss: 0.2500 - val_acc: 0.5639\n",
      ". theta fit =  0.15401563\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2492 - acc: 0.5721 - val_loss: -0.2502 - val_acc: 0.5639\n",
      ". theta fit =  0.1551214\n",
      "Epoch:  29\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2494 - acc: 0.5682 - val_loss: 0.2503 - val_acc: 0.5603\n",
      ". theta fit =  0.1551214\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2494 - acc: 0.5678 - val_loss: -0.2505 - val_acc: 0.5603\n",
      ". theta fit =  0.15621948\n",
      "Epoch:  30\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2495 - acc: 0.5618 - val_loss: 0.2504 - val_acc: 0.5543\n",
      ". theta fit =  0.15621948\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2495 - acc: 0.5645 - val_loss: -0.2505 - val_acc: 0.5543\n",
      ". theta fit =  0.15722224\n",
      "Epoch:  31\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2496 - acc: 0.5567 - val_loss: 0.2504 - val_acc: 0.5507\n",
      ". theta fit =  0.15722224\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2494 - acc: 0.5622 - val_loss: -0.2506 - val_acc: 0.5507\n",
      ". theta fit =  0.15813297\n",
      "Epoch:  32\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2496 - acc: 0.5468 - val_loss: 0.2505 - val_acc: 0.5436\n",
      ". theta fit =  0.15813297\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 190s 132us/step - loss: -0.2494 - acc: 0.5559 - val_loss: -0.2506 - val_acc: 0.5436\n",
      ". theta fit =  0.15890452\n",
      "Epoch:  33\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2496 - acc: 0.5406 - val_loss: 0.2506 - val_acc: 0.5382\n",
      ". theta fit =  0.15890452\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2494 - acc: 0.5515 - val_loss: -0.2507 - val_acc: 0.5382\n",
      ". theta fit =  0.1596066\n",
      "Epoch:  34\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2495 - acc: 0.5339 - val_loss: 0.2507 - val_acc: 0.5109\n",
      ". theta fit =  0.1596066\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 190s 132us/step - loss: -0.2495 - acc: 0.5224 - val_loss: -0.2507 - val_acc: 0.5109\n",
      ". theta fit =  0.15936692\n",
      "Epoch:  35\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2495 - acc: 0.5348 - val_loss: 0.2508 - val_acc: 0.5190\n",
      ". theta fit =  0.15936692\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2493 - acc: 0.5346 - val_loss: -0.2508 - val_acc: 0.5190\n",
      ". theta fit =  0.1596653\n",
      "Epoch:  36\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2494 - acc: 0.5314 - val_loss: 0.2509 - val_acc: 0.5205\n",
      ". theta fit =  0.1596653\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2490 - acc: 0.5369 - val_loss: -0.2509 - val_acc: 0.5205\n",
      ". theta fit =  0.15987948\n",
      "Epoch:  37\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2493 - acc: 0.5298 - val_loss: 0.2508 - val_acc: 0.5186\n",
      ". theta fit =  0.15987948\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2492 - acc: 0.5341 - val_loss: -0.2508 - val_acc: 0.5186\n",
      ". theta fit =  0.16013739\n",
      "Epoch:  38\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2493 - acc: 0.5274 - val_loss: 0.2509 - val_acc: 0.5219\n",
      ". theta fit =  0.16013739\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2488 - acc: 0.5383 - val_loss: -0.2509 - val_acc: 0.5219\n",
      ". theta fit =  0.16033578\n",
      "Epoch:  39\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2492 - acc: 0.5270 - val_loss: 0.2511 - val_acc: 0.5275\n",
      ". theta fit =  0.16033578\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2489 - acc: 0.5440 - val_loss: -0.2511 - val_acc: 0.5275\n",
      ". theta fit =  0.16058363\n",
      "Epoch:  40\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2491 - acc: 0.5256 - val_loss: 0.2507 - val_acc: 0.5160\n",
      ". theta fit =  0.16058363\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2489 - acc: 0.5323 - val_loss: -0.2507 - val_acc: 0.5160\n",
      ". theta fit =  0.16059904\n",
      "Epoch:  41\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2490 - acc: 0.5243 - val_loss: 0.2509 - val_acc: 0.5201\n",
      ". theta fit =  0.16059904\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2487 - acc: 0.5378 - val_loss: -0.2509 - val_acc: 0.5201\n",
      ". theta fit =  0.160622\n",
      "Epoch:  42\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2489 - acc: 0.5264 - val_loss: 0.2511 - val_acc: 0.5176\n",
      ". theta fit =  0.160622\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2488 - acc: 0.5362 - val_loss: -0.2511 - val_acc: 0.5176\n",
      ". theta fit =  0.1604888\n",
      "Epoch:  43\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2489 - acc: 0.5262 - val_loss: 0.2511 - val_acc: 0.5028\n",
      ". theta fit =  0.1604888\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2484 - acc: 0.5223 - val_loss: -0.2511 - val_acc: 0.5028\n",
      ". theta fit =  0.159978\n",
      "Epoch:  44\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2488 - acc: 0.5293 - val_loss: 0.2512 - val_acc: 0.5117\n",
      ". theta fit =  0.159978\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2486 - acc: 0.5292 - val_loss: -0.2512 - val_acc: 0.5117\n",
      ". theta fit =  0.1597957\n",
      "Epoch:  45\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2487 - acc: 0.5325 - val_loss: 0.2513 - val_acc: 0.5152\n",
      ". theta fit =  0.1597957\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2484 - acc: 0.5359 - val_loss: -0.2513 - val_acc: 0.5152\n",
      ". theta fit =  0.15998474\n",
      "Epoch:  46\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2486 - acc: 0.5317 - val_loss: 0.2513 - val_acc: 0.5019\n",
      ". theta fit =  0.15998474\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2483 - acc: 0.5235 - val_loss: -0.2513 - val_acc: 0.5019\n",
      ". theta fit =  0.15967296\n",
      "Epoch:  47\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2485 - acc: 0.5346 - val_loss: 0.2514 - val_acc: 0.5151\n",
      ". theta fit =  0.15967296\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2481 - acc: 0.5380 - val_loss: -0.2514 - val_acc: 0.5151\n",
      ". theta fit =  0.15960507\n",
      "Epoch:  48\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2484 - acc: 0.5363 - val_loss: 0.2512 - val_acc: 0.5200\n",
      ". theta fit =  0.15960507\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2479 - acc: 0.5429 - val_loss: -0.2512 - val_acc: 0.5200\n",
      ". theta fit =  0.15965728\n",
      "Epoch:  49\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2483 - acc: 0.5348 - val_loss: 0.2518 - val_acc: 0.5268\n",
      ". theta fit =  0.15965728\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2481 - acc: 0.5489 - val_loss: -0.2518 - val_acc: 0.5268\n",
      ". theta fit =  0.16015965\n",
      "Epoch:  50\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2482 - acc: 0.5322 - val_loss: 0.2515 - val_acc: 0.5084\n",
      ". theta fit =  0.16015965\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2480 - acc: 0.5307 - val_loss: -0.2515 - val_acc: 0.5084\n",
      ". theta fit =  0.15959324\n",
      "Epoch:  51\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2482 - acc: 0.5361 - val_loss: 0.2521 - val_acc: 0.5154\n",
      ". theta fit =  0.15959324\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 179s 125us/step - loss: -0.2477 - acc: 0.5401 - val_loss: -0.2521 - val_acc: 0.5154\n",
      ". theta fit =  0.159684\n",
      "Epoch:  52\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2480 - acc: 0.5369 - val_loss: 0.2515 - val_acc: 0.5098\n",
      ". theta fit =  0.159684\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2480 - acc: 0.5321 - val_loss: -0.2515 - val_acc: 0.5098\n",
      ". theta fit =  0.159615\n",
      "Epoch:  53\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2480 - acc: 0.5372 - val_loss: 0.2516 - val_acc: 0.5234\n",
      ". theta fit =  0.159615\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 180s 125us/step - loss: -0.2477 - acc: 0.5489 - val_loss: -0.2516 - val_acc: 0.5234\n",
      ". theta fit =  0.15991238\n",
      "Epoch:  54\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2479 - acc: 0.5361 - val_loss: 0.2518 - val_acc: 0.5053\n",
      ". theta fit =  0.15991238\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2476 - acc: 0.5317 - val_loss: -0.2518 - val_acc: 0.5053\n",
      ". theta fit =  0.15974288\n",
      "Epoch:  55\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2478 - acc: 0.5373 - val_loss: 0.2519 - val_acc: 0.5169\n",
      ". theta fit =  0.15974288\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 128us/step - loss: -0.2473 - acc: 0.5433 - val_loss: -0.2519 - val_acc: 0.5169\n",
      ". theta fit =  0.15986218\n",
      "Epoch:  56\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2477 - acc: 0.5376 - val_loss: 0.2519 - val_acc: 0.5071\n",
      ". theta fit =  0.15986218\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2474 - acc: 0.5342 - val_loss: -0.2519 - val_acc: 0.5071\n",
      ". theta fit =  0.15937963\n",
      "Epoch:  57\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2476 - acc: 0.5413 - val_loss: 0.2520 - val_acc: 0.5172\n",
      ". theta fit =  0.15937963\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 127us/step - loss: -0.2474 - acc: 0.5432 - val_loss: -0.2520 - val_acc: 0.5172\n",
      ". theta fit =  0.15962008\n",
      "Epoch:  58\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2475 - acc: 0.5397 - val_loss: 0.2521 - val_acc: 0.5136\n",
      ". theta fit =  0.15962008\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 182s 127us/step - loss: -0.2470 - acc: 0.5425 - val_loss: -0.2521 - val_acc: 0.5136\n",
      ". theta fit =  0.15948965\n",
      "Epoch:  59\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2474 - acc: 0.5419 - val_loss: 0.2524 - val_acc: 0.5129\n",
      ". theta fit =  0.15948965\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2467 - acc: 0.5421 - val_loss: -0.2524 - val_acc: 0.5129\n",
      ". theta fit =  0.15926488\n",
      "Epoch:  60\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2473 - acc: 0.5446 - val_loss: 0.2527 - val_acc: 0.5242\n",
      ". theta fit =  0.15926488\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 127us/step - loss: -0.2465 - acc: 0.5549 - val_loss: -0.2527 - val_acc: 0.5242\n",
      ". theta fit =  0.15964785\n",
      "Epoch:  61\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 107us/step - loss: 0.2471 - acc: 0.5430 - val_loss: 0.2523 - val_acc: 0.5128\n",
      ". theta fit =  0.15964785\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2466 - acc: 0.5432 - val_loss: -0.2523 - val_acc: 0.5128\n",
      ". theta fit =  0.1595467\n",
      "Epoch:  62\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 107us/step - loss: 0.2471 - acc: 0.5434 - val_loss: 0.2525 - val_acc: 0.5175\n",
      ". theta fit =  0.1595467\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 128us/step - loss: -0.2465 - acc: 0.5482 - val_loss: -0.2525 - val_acc: 0.5175\n",
      ". theta fit =  0.15955894\n",
      "Epoch:  63\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2470 - acc: 0.5433 - val_loss: 0.2528 - val_acc: 0.5183\n",
      ". theta fit =  0.15955894\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2467 - acc: 0.5489 - val_loss: -0.2528 - val_acc: 0.5183\n",
      ". theta fit =  0.15980032\n",
      "Epoch:  64\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2469 - acc: 0.5432 - val_loss: 0.2529 - val_acc: 0.5118\n",
      ". theta fit =  0.15980032\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2464 - acc: 0.5433 - val_loss: -0.2529 - val_acc: 0.5118\n",
      ". theta fit =  0.15968671\n",
      "Epoch:  65\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2468 - acc: 0.5443 - val_loss: 0.2528 - val_acc: 0.5132\n",
      ". theta fit =  0.15968671\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 177s 123us/step - loss: -0.2460 - acc: 0.5457 - val_loss: -0.2528 - val_acc: 0.5132\n",
      ". theta fit =  0.15956043\n",
      "Epoch:  66\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2467 - acc: 0.5458 - val_loss: 0.2525 - val_acc: 0.5151\n",
      ". theta fit =  0.15956043\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 177s 123us/step - loss: -0.2461 - acc: 0.5473 - val_loss: -0.2524 - val_acc: 0.5151\n",
      ". theta fit =  0.15939218\n",
      "Epoch:  67\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2465 - acc: 0.5464 - val_loss: 0.2536 - val_acc: 0.5133\n",
      ". theta fit =  0.15939218\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2461 - acc: 0.5446 - val_loss: -0.2536 - val_acc: 0.5133\n",
      ". theta fit =  0.15928876\n",
      "Epoch:  68\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2465 - acc: 0.5479 - val_loss: 0.2534 - val_acc: 0.5096\n",
      ". theta fit =  0.15928876\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 179s 124us/step - loss: -0.2459 - acc: 0.5440 - val_loss: -0.2534 - val_acc: 0.5096\n",
      ". theta fit =  0.15904684\n",
      "Epoch:  69\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2464 - acc: 0.5486 - val_loss: 0.2534 - val_acc: 0.5142\n",
      ". theta fit =  0.15904684\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 179s 124us/step - loss: -0.2455 - acc: 0.5493 - val_loss: -0.2534 - val_acc: 0.5142\n",
      ". theta fit =  0.15903133\n",
      "Epoch:  70\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2463 - acc: 0.5496 - val_loss: 0.2532 - val_acc: 0.5097\n",
      ". theta fit =  0.15903133\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 182s 126us/step - loss: -0.2458 - acc: 0.5450 - val_loss: -0.2532 - val_acc: 0.5097\n",
      ". theta fit =  0.15880732\n",
      "Epoch:  71\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2462 - acc: 0.5509 - val_loss: 0.2534 - val_acc: 0.5215\n",
      ". theta fit =  0.15880732\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 179s 125us/step - loss: -0.2456 - acc: 0.5570 - val_loss: -0.2534 - val_acc: 0.5215 -0.2456\n",
      ". theta fit =  0.15918052\n",
      "Epoch:  72\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2461 - acc: 0.5491 - val_loss: 0.2537 - val_acc: 0.5163\n",
      ". theta fit =  0.15918052\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 128us/step - loss: -0.2454 - acc: 0.5524 - val_loss: -0.2537 - val_acc: 0.5163\n",
      ". theta fit =  0.15923464\n",
      "Epoch:  73\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 107us/step - loss: 0.2460 - acc: 0.5497 - val_loss: 0.2539 - val_acc: 0.5129\n",
      ". theta fit =  0.15923464\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 180s 125us/step - loss: -0.2455 - acc: 0.5484 - val_loss: -0.2538 - val_acc: 0.5129\n",
      ". theta fit =  0.1590381\n",
      "Epoch:  74\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2459 - acc: 0.5513 - val_loss: 0.2537 - val_acc: 0.5215\n",
      ". theta fit =  0.1590381\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 125us/step - loss: -0.2451 - acc: 0.5596 - val_loss: -0.2537 - val_acc: 0.5215\n",
      ". theta fit =  0.15939511\n",
      "Epoch:  75\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2458 - acc: 0.5508 - val_loss: 0.2536 - val_acc: 0.5151\n",
      ". theta fit =  0.15939511\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2454 - acc: 0.5509 - val_loss: -0.2536 - val_acc: 0.5151\n",
      ". theta fit =  0.15934335\n",
      "Epoch:  76\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 107us/step - loss: 0.2457 - acc: 0.5499 - val_loss: 0.2541 - val_acc: 0.5189\n",
      ". theta fit =  0.15934335\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 182s 126us/step - loss: -0.2451 - acc: 0.5543 - val_loss: -0.2541 - val_acc: 0.5189\n",
      ". theta fit =  0.15923668\n",
      "Epoch:  77\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2456 - acc: 0.5523 - val_loss: 0.2543 - val_acc: 0.5128\n",
      ". theta fit =  0.15923668\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2447 - acc: 0.5512 - val_loss: -0.2543 - val_acc: 0.5128\n",
      ". theta fit =  0.15912984\n",
      "Epoch:  78\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2454 - acc: 0.5531 - val_loss: 0.2534 - val_acc: 0.5161\n",
      ". theta fit =  0.15912984\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2450 - acc: 0.5534 - val_loss: -0.2534 - val_acc: 0.5161\n",
      ". theta fit =  0.15914768\n",
      "Epoch:  79\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2454 - acc: 0.5526 - val_loss: 0.2540 - val_acc: 0.5196\n",
      ". theta fit =  0.15914768\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2445 - acc: 0.5581 - val_loss: -0.2540 - val_acc: 0.5196\n",
      ". theta fit =  0.15926658\n",
      "Epoch:  80\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2452 - acc: 0.5520 - val_loss: 0.2543 - val_acc: 0.5175\n",
      ". theta fit =  0.15926658\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 183s 127us/step - loss: -0.2448 - acc: 0.5564 - val_loss: -0.2543 - val_acc: 0.5175\n",
      ". theta fit =  0.15919794\n",
      "Epoch:  81\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2451 - acc: 0.5542 - val_loss: 0.2540 - val_acc: 0.5131\n",
      ". theta fit =  0.15919794\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 183s 127us/step - loss: -0.2446 - acc: 0.5521 - val_loss: -0.2540 - val_acc: 0.5131\n",
      ". theta fit =  0.15908606\n",
      "Epoch:  82\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2451 - acc: 0.5540 - val_loss: 0.2549 - val_acc: 0.5146\n",
      ". theta fit =  0.15908606\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2443 - acc: 0.5557 - val_loss: -0.2549 - val_acc: 0.5146\n",
      ". theta fit =  0.1590661\n",
      "Epoch:  83\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2450 - acc: 0.5549 - val_loss: 0.2553 - val_acc: 0.5177\n",
      ". theta fit =  0.1590661\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2440 - acc: 0.5584 - val_loss: -0.2553 - val_acc: 0.5177\n",
      ". theta fit =  0.15921664\n",
      "Epoch:  84\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2449 - acc: 0.5542 - val_loss: 0.2547 - val_acc: 0.5223\n",
      ". theta fit =  0.15921664\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2441 - acc: 0.5617 - val_loss: -0.2547 - val_acc: 0.5223\n",
      ". theta fit =  0.15943253\n",
      "Epoch:  85\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 109us/step - loss: 0.2447 - acc: 0.5538 - val_loss: 0.2551 - val_acc: 0.5146\n",
      ". theta fit =  0.15943253\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2437 - acc: 0.5565 - val_loss: -0.2551 - val_acc: 0.5146\n",
      ". theta fit =  0.15937848\n",
      "Epoch:  86\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-24940b61cb9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_loss_wrapper_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyinputs_fit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m#Now, fix g and train \\theta.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 5e-7 #smaller learning rate yields better precision\n",
    "epochs = 60 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "        \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "        \n",
    "    theta_prime = [theta0, 0.68, 0.217]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0,\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.16, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"alphaS Fit (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"alphaS Fit (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
