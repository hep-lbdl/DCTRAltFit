{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_alphaS.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small = X_train[0:int(0.8*10**5)]\n",
    "Y_train_small = Y_train[0:int(0.8*10**5)]\n",
    "X_test_small = X_test[0:int(0.2*10**5)]\n",
    "Y_test_small = Y_test[0:int(0.2*10**5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_alphaS_modified.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    #Creating theta_prime\n",
    "    alphaS = K.ones(shape =x.shape[0:2])*val # Fitting parameter\n",
    "    aLund = K.ones(shape =x.shape[0:2])*0.68 # Fixed at default\n",
    "    probStoUD = K.ones(shape =x.shape[0:2])*0.217 # Fixed at default\n",
    "    theta_prime = K.stack((alphaS, aLund, probStoUD), axis = 2)\n",
    "\n",
    "\n",
    "    data = K.concatenate((x, theta_prime), axis =2)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 159s 110us/step - loss: 0.2214 - acc: 0.5779 - val_loss: 0.2174 - val_acc: 0.5806\n",
      "trainnig theta = : 0.10250000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2170 - acc: 0.5824 - val_loss: 0.2170 - val_acc: 0.5825\n",
      "trainnig theta = : 0.10500000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2165 - acc: 0.5833 - val_loss: 0.2165 - val_acc: 0.5807\n",
      "trainnig theta = : 0.1075\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2166 - acc: 0.5837 - val_loss: 0.2167 - val_acc: 0.5815\n",
      "trainnig theta = : 0.11\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2176 - acc: 0.5840 - val_loss: 0.2179 - val_acc: 0.5814\n",
      "trainnig theta = : 0.1125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2194 - acc: 0.5848 - val_loss: 0.2198 - val_acc: 0.5824\n",
      "trainnig theta = : 0.115\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2213 - acc: 0.5849 - val_loss: 0.2223 - val_acc: 0.5797\n",
      "trainnig theta = : 0.11750000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 102us/step - loss: 0.2229 - acc: 0.5854 - val_loss: 0.2236 - val_acc: 0.5842\n",
      "trainnig theta = : 0.12\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2244 - acc: 0.5851 - val_loss: 0.2248 - val_acc: 0.5840\n",
      "trainnig theta = : 0.1225\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2260 - acc: 0.5859 - val_loss: 0.2271 - val_acc: 0.5811\n",
      "trainnig theta = : 0.125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2277 - acc: 0.5855 - val_loss: 0.2283 - val_acc: 0.5848\n",
      "trainnig theta = : 0.1275\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2295 - acc: 0.5861 - val_loss: 0.2302 - val_acc: 0.5844\n",
      "trainnig theta = : 0.13\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2314 - acc: 0.5865 - val_loss: 0.2321 - val_acc: 0.5841\n",
      "trainnig theta = : 0.1325\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2333 - acc: 0.5865 - val_loss: 0.2342 - val_acc: 0.5831\n",
      "trainnig theta = : 0.135\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2348 - acc: 0.5867 - val_loss: 0.2358 - val_acc: 0.5815\n",
      "trainnig theta = : 0.1375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2362 - acc: 0.5865 - val_loss: 0.2369 - val_acc: 0.5847\n",
      "trainnig theta = : 0.14\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2383 - acc: 0.5870 - val_loss: 0.2389 - val_acc: 0.5847\n",
      "trainnig theta = : 0.14250000000000002\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2410 - acc: 0.5864 - val_loss: 0.2416 - val_acc: 0.5844\n",
      "trainnig theta = : 0.145\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2438 - acc: 0.5860 - val_loss: 0.2443 - val_acc: 0.5821\n",
      "trainnig theta = : 0.1475\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2456 - acc: 0.5849 - val_loss: 0.2463 - val_acc: 0.5776\n",
      "trainnig theta = : 0.15\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2465 - acc: 0.5839 - val_loss: 0.2470 - val_acc: 0.5802\n",
      "trainnig theta = : 0.1525\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2473 - acc: 0.5827 - val_loss: 0.2478 - val_acc: 0.5791\n",
      "trainnig theta = : 0.155\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2486 - acc: 0.5786 - val_loss: 0.2491 - val_acc: 0.5772\n",
      "trainnig theta = : 0.1575\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2503 - acc: 0.5647 - val_loss: 0.2507 - val_acc: 0.5643\n",
      "trainnig theta = : 0.16\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2514 - acc: 0.5319 - val_loss: 0.2517 - val_acc: 0.5169\n",
      "trainnig theta = : 0.1625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2515 - acc: 0.5030 - val_loss: 0.2519 - val_acc: 0.4980\n",
      "trainnig theta = : 0.16499999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2519 - acc: 0.4759 - val_loss: 0.2524 - val_acc: 0.4799\n",
      "trainnig theta = : 0.16749999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2517 - acc: 0.4575 - val_loss: 0.2522 - val_acc: 0.4548\n",
      "trainnig theta = : 0.16999999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2510 - acc: 0.4518 - val_loss: 0.2514 - val_acc: 0.4385\n",
      "trainnig theta = : 0.1725\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2502 - acc: 0.4484 - val_loss: 0.2507 - val_acc: 0.4424\n",
      "trainnig theta = : 0.175\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 104us/step - loss: 0.2492 - acc: 0.4448 - val_loss: 0.2499 - val_acc: 0.4343\n",
      "trainnig theta = : 0.1775\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2482 - acc: 0.4444 - val_loss: 0.2491 - val_acc: 0.4464\n",
      "trainnig theta = : 0.18\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2471 - acc: 0.4435 - val_loss: 0.2479 - val_acc: 0.4380\n",
      "[[0.22137477812874648], [0.21703989108403524], [0.21648895731195808], [0.21661257067074377], [0.21762132816430596], [0.21944715375494625], [0.22126773997313445], [0.2229303162234525], [0.22444050016088618], [0.22599984856529368], [0.22770258308284813], [0.22953290323623352], [0.23142603193927141], [0.2333012963231239], [0.23475367535526553], [0.23622168957566222], [0.23828660022053455], [0.24102714501528277], [0.24377428261149262], [0.24562977058812976], [0.2465265239795877], [0.24727935270509785], [0.2486292091715667], [0.2502777778957453], [0.2513585097880827], [0.25148530460687146], [0.2518951222817931], [0.2516558851115406], [0.2509761217671136], [0.25015956332079237], [0.24922220586902566], [0.24818791372494564], [0.2470780098810792]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.18, 33) #iterating across possible alphaS values\n",
    "vlvals = []\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEMCAYAAADu7jDJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xdc1WX7wPHPBYKouMWJe+NEEXOVlpamqZl75CrLHls+DcvqVzaeJ6vHtmmWplZqmmXO0tTKMkHFvXAhblFwoCBw/f44RzuZGw7nANf79Tovz/f+Dq4vAte5x/e+RVUxxhhjbpaPpwMwxhiTtVkiMcYYky6WSIwxxqSLJRJjjDHpYonEGGNMulgiMcYYky6WSIwxxqSLJRJjjDHpYonEGGNMuuTydACZoVixYlqhQgVPh2GMMVnK6tWrj6lq0LWOyxGJpEKFCkRGRno6DGOMyVJEZO/1HGdNW8YYY9LFEokxxph0sURijDEmXXJEH8nlnD9/ntjYWM6dO+fpUDwiICCA4OBg/Pz8PB2KMSaLy7GJJDY2lvz581OhQgVExNPhZCpVJS4ujtjYWCpWrOjpcIwxWVyObdo6d+4cRYsWzXFJBEBEKFq0aI6tjRljMlaOTSRAjkwiF+TkezfGZKwc27RljMmhUlNgz69wMAqKVILiIVC4Ivjan8ObZd85DwoMDOT06dM3fN6ePXvo0KEDGzdudENUxmRDaWmw70/YOAs2fwdnjv59v68/FKsOxWtAUA1HcileAwpVAJ8c3XBzXSyRGGOyJ1U4sAY2fgubZsPJ/ZArAKq1hdpdoEILiN8LR7b89YpZCRu++esafnmhUisIfxAqtQRrEr4sSyReoGfPnvTr14/27dsDMGDAADp06EBYWBj9+vXjzJkzAHz44Yc0bdr0b+du2rSJgQMHkpycTFpaGrNmzaJq1aqZfg/GeJQqJJ2EM8fg9GGIXuyofZzYAz5+UKU1tH4FqrflREpuIvYcZ29kPKULlaR8UCXK1ehGgQDnUPhzJ+HoNji6BQ6uh03fwrZ5jhpL+INQrxfkDvTo7XobSyTAKz9sYvOBkxl6zZDSBfi/e2pd17E9evRgxowZtG/fnuTkZJYsWcLYsWNRVX766ScCAgLYsWMHvXr1+secYZ988gmPP/44ffr0ITk5mdTU1Ay9D2O8RZ92t9AyKJ4He3VyJIzEY3AmzvnvMUg7/9fB4guVbkNbPMX+knew6lAaEdEniFi8hugjl29OLpLPn3JF8lKhaF7KFS1IhaK3U752B2rd/goB276HP8fB/KdgySio3xsaPQjFqmTS3Xs3SyReoF27djz++OMkJSWxcOFCbr31VvLkyUNCQgLDhg0jKioKX19ftm/f/o9zmzRpwuuvv05sbCxdunSx2ojJfs7EwfI3+aLRFnL5AFFfQd6ikK8YFAyG0vXQPMU451+Y07kKkSAFWHO+PL8e9CFi4XEOnVwLQP7cuWhYoTD3hpahUYUiVC0eyIGEs8TEJbL3eCJ7486wNy6RiD0n+H7dAVQdXz5/QC7uDa1Ljw7fUyttB6waBxGfwZ+fOGo64UOgSpsc3ZdiiQSuu+bgLgEBAbRs2ZJFixYxffp0evbsCcCYMWMoUaIE69atIy0tjYCAgH+c27t3bxo3bsy8efO4++67GTduHLfffntm34IxGe/8Occf7V/egeRT/HAsmPfP3k1ouz7EJ57nRGIyCfHOf8+eJ01dTz5EiQK5aVShyMVX9ZL58fX5ex9H4Xz+1Cpd8B9fOiklldgTZ9l19Azz1h9gWsQ+Jv+xl7rBBenR6EU63fYygRu/hMjP4avujlFfjQZD/T6Qt4h7vy9eyBKJl+jRowcTJkwgMjKSSZMmAZCQkEBwcDA+Pj588cUXl2222rVrF5UqVeKxxx4jJiaG9evXWyIxWVtamqNfYvErkBBDauU2fJZnIG8kpkFgGsm7jlMorx+F8vpRulAeCuf1o3BefwrmcfxbOJ8fVYLyU7ZInpt+Xip3Ll8qBwVSOSiQNiEleDkxme/W7mdaxD5Gzt7Ia36+dKjbll5d+hN6ZgUS8Sn8+AL8/BrU7grhD0Dp0Az+xngvSyRe4s4776Rfv3506tQJf39/AB555BHuu+8+Jk+eTNu2bcmXL98/zpsxYwZTpkzBz8+PkiVL8vzzz2d26MZknL2/w6KRjtFWJeqwNnQiw1YWYn/8WQKPbKBwzC/8umRhpodVKK8/A5pVpH/TCqyLTWDaqhjmrDvAN6tjqVq8GD3Dx9KrdQJ5102C9dMhaiqUCXN0zod0Br9/tiZkJ6Kq1z4qiwsLC9NLO6m3bNlCzZo1PRSRd7DvgfEacTvhp5dg61zIX5qEpiN4fmdN5m08StXigbzRpQ5PD+gCwLJlyzwbq9PppBTmrnM0e0Xti6dgHj/6NynPgIZFKLJjFkRMgLgdjv6c0H4QNggKl/d02DdERFarati1jrMaiTHGs3Ytg697gfiQ1uoFvvbtwH8WxnA+NY6n76rOgy0q4Z/L+zqyA3Pnomd4OXqGlyNqXzxjl0Xz/s/RjP/Vh56NWvBg3/spc2IVrPoUfn8fVrwHNdpD00ehbONs9UyKWxOJiLQF3gN8gQmq+t9L9g8HHgBSgKPAIFXd69yXCmxwHhqjqh2d5RWBaUBRYDXQT1WT3Xkfxhg32bYQZtwPRSuzrc0knll0lHWxu2hRtRivdqpNhWL/bM71RvXLFmJcvzCij5xi7LJdTF25l6kr99KpfhmG3v4JVdolQOREiPzMUesKbuRIKDU6gI+vp8NPN7eleRHxBT4C2gEhQC8RCbnksLVAmKrWBWYCo132nVXV+s5XR5fyN4ExqloFOAEMdtc9GGPcaNNsmN4HLV6T98q+y90Td7I//izv9azP5EHhWSaJuKpSPD/vdK/H8mda0feW8szbcIA2Y37hoTmHiKr2KDy5Ce5+2/Hcy4z74YMGjhpL8hlPh54u7qwvhgPRqrrLWWOYBnRyPUBVl6pqonNzJRB8tQuKYwjG7TiSDsAXQOcMjdoY435RX8PMQWiZhjwX+BpjVhzjvgZlWDK8JZ3ql8nys1OXKZSHlzvWYsWzt/Noqyr8sTOOzh+toP/UTWwO7gGProbukyFvMcdDjmNqOUZ8nT7i6dBvijsTSRlgn8t2rLPsSgYDC1y2A0QkUkRWisiFZFEUiFfVlOu8pjHG20RMgO8eJq18Cx7L9SLTNpzk6buqM7prPQrmzV4rdhYNzM3wO6vz+3N3MKJdDaL2xdP+g18Z/s0GYku1gQcWw6BFUL4Z/PI2jKkNcx6FY9GeDv2GeEVnu4j0BcKA21yKy6vqfhGpBPwsIhuAhBu45hBgCEC5cuUyMlxjzM36/QP48QVSq9zJkKTHWbIjgZfvCWFAs+y9Umdg7lw8fFtlejUqx9jlO5m4Yjdz1x/k/ibl+VerBhTu+aUjeaz8yPHk/popENIRmj0BZRp4OvxrcmeNZD9Q1mU72Fn2NyLSGhgJdFTVpAvlqrrf+e8uYBkQCsQBhUTkQgK87DWd541X1TBVDQsKCkr/3WSCPXv2ULt27Zs6d9myZXTo0CGDIzImg6jCsjfhxxc4X6MTfU8/ytLoBEbfVzfbJxFXBfP6MaJdDZY+1ZLOoaX5fMVubn1rKR8vi+ZcwYrQYQw8sQFaDIedy+DTVvBFR9i5FLz4UQ13JpIIoKqIVBQRf6AnMMf1ABEJBcbhSCJHXMoLi0hu5/tiQDNgszoeelkKdHUe2h/43o33YIxJL1VY/H+w7A2Sa/eg+9HBRMSc5r2eoXRvVPba52dDpQvlYXTXeix4/FYaVyzC6IXbaPnWMmZE7CM1bxDc8RI8uRHajIKjW2FKZxjfEjZ9B2neNzGr2xKJsx9jGLAI2ALMUNVNIjJKRC6MwnoLCAS+EZEoEbmQaGoCkSKyDkfi+K+qbnbuexYYLiLROPpMPnPXPbjTiBEj+Oijjy5uv/zyy8ycOfPi9i233MKmTZsubrds2ZLIyEhWrVpFkyZNCA0NpWnTpmzbtu0f116+fDn169enfv36hIaGcurUKffejDFXkpYG85+GFe9xtt4AOu3rzabDiXzStyH31Cvt6eg8rnrJ/Ezo34jpQ26hVKEAnpm1nnbv/cKK6GMQUACaPQ6Pr4d73nNMk/9Nf/iwEaz+AlKSrv0FMok92Q6wYAQc2nCZM9OhZB1o998r7l67di1PPPEEy5cvByAkJIRx48YxdOhQNm7cyJgxY4iPj+eVV17h4MGDtGzZkm3btnHy5Eny5s1Lrly5WLx4MWPHjmXWrFksW7aMt99+m7lz53LPPfcwYsQImjVrxunTpwkICCBXrn92h9mT7catEo/D9/+CbfM53fBhOm5ty8GTSUzoH0azKsVu+HItW7YEvOfJ9oymqizadIg35m8l5ngi7euUYmT7mpQulMdxQFoqbJkDv42Bg+ugWDXoMt6tc3pd75Pt3ve4aA4RGhrKkSNHOHDgAOvWraNw4cKULftXNb979+4XaygzZsyga1dHa15CQgLdunWjdu3aPPnkk3+rtVzQrFkzhg8fzvvvv098fPxlk4gxbrX7VxjbDKIXc/zWUbTddCdHTyczZXD4TSWRnEBEaFu7FD8+eSvD21Rj8ZbD3PHOcj5eFk1SSqrjwcVa98KQ5dBrOiSdhgmtYfloxzr0HmR/YeCqNQd36tatGzNnzuTQoUP06NHjb/vKlClD0aJFWb9+PdOnT+eTTz4B4MUXX6RVq1bMnj2bPXv2XPyU5mrEiBG0b9+e+fPn06xZMxYtWkSNGjUy45ZMTpeaAsvfhF/egiKV2N35O3r+cJbklFS+HnILtcv8c8p283cBfr48dkdV7g0tw6tzNzN64TZmRsbyfx1rcVu1IMfUKtXbQrnGMO8pWPo6bF/kqJ0UreyRmK1G4kE9evRg2rRpzJw5k27dul12/+jRo0lISKBu3bqAo0ZSpozj0ZkL081faufOndSpU4dnn32WRo0asXXrVrfdgzEXxe+DSe3hl9FQrxdzm07j7hmnUIXpDzWxJHKDyhbJy/j7w5g0sBEK9P98FQ9NiST2hPMZ7jyFoetncN9njskhP2nuWHDLA90Vlkg8qFatWpw6dYoyZcpQqlSpf+zv2rUr06ZNo3v37hfLnnnmGZ577jlCQ0NJSbl8dfbdd9+ldu3a1K1bFz8/P9q1a+e2ezAGgM1z4JNmcHgTKZ3G8bLvMIbN3EGdMgWZ+2hzqpXI7+kIs6yW1Yuz8IkWPH1XdX7ZfozW/1vOB0t2cO68c/RWna7wyErHRJDzhsOX3eDUoUyN0TrbczD7Hph0O38WFj3vWCmwdChH7/qEh+bFsSYmngeaV+TZdjXw882Yz6vZvbP9euyPP8vr8zYzf8MhKhXLxxtd6nBLpaKOnWlpjlkDfnrJsf5Jh3ehVvpmkLLOdmOMex3ZAp/e7kgiTR9jZcuvaTdlH1sPneLD3qG80CEkw5KIcShTKA8f92nI5EHhnE9Lo+f4lTz37QZOnjvvWDO+8RB4+FfH0r/f9Idvh8DZeLfHZf/LxpgbcyAKZg52jMo6cxTtM4txuQfQZ9JaCubxY86wZnSoa8+IuNOt1YJY9MStPNiiItMjYmj9znIWbXI2ZxWrCoN/hJbPwdZ5mTIRpI3aMsZcW1oaRC92LNC051fwzw9NHuF0w6E8Nf8QCzdt5e46JRndtR6Bue3PSmbI65+Lke1DuKdeaZ6dtYGHpqymXe2SvNKxFsULBEDLERA+BPIWcXss9j9ujLmylCRYPwP++NAxVUeBMnDna9DgfnYk+PDQpNXsjUvkhfY1Gdy8Ypaf/j0rqhtciDnDmvHpr7t4d/EOfos+xsi7a9KjUVkkE5IIWCIxxlxO4nFYPRH+HAenD0OJOnDveKjdhagDZ/h89i7mbzhIobz+fPVAYxpf6PA1HuHn68MjLavQrnYpRsxaz4hvN/Bd1H7+06UuFTNhgTBLJMaYvxzdBqvGOxaeOn8GKt8B947jfPlbWbDpMBPHrWJtTDz5c+fi/iYVePi2So5mFOMVKhbLx9cP3sKMyH28Pn8Lbd/9hRkPNaFe2UJu/bqWSDwkLi6OO+64A4BDhw7h6+vLhenuV61ahb+//zWv8e233xISEnLxqfXmzZvz4YcfUr9+ffcFbrKftFTYvtBR+9i9HHz9oXZXaPIvTuSvxlerYpgyfRmHTp6jQtG8vHxPCF3DylpfiJfy8RF6hpfj9hrFmfT7nkx5ENR+EjykaNGiREVFAY6ZfwMDA3nqqaf+doyqoqr4+Fx+cN23336Lj4+PTX9ibk7icVgz2fE0dEKMo//j9heh4QC2ncrNxBW7mb12CUkpaTSvUow3utSmZbXi+PhYP0hWULxAAM+0zZy/DZZIvEx0dDQdO3YkNDSUtWvXsmDBAurVq0d8vGMs+LRp01i8eDH9+/dn/vz5rFixgpdffpnvvvvu4v4hQ4aQkJDAxIkTadq0qSdvx3ijg+sdzVcbvoGUc1C+Odz1GlRvz6bDZxgzczuLtxwhdy4fujQIZmCzCvZkurkqSyROl5v8MD3S8/Tt1q1bmTx5MmFhYVecBqVFixbcfffddO3alc6d/3p6VVVZtWoVc+bMYdSoUSxcuPCm4zDZTNJpmHE/7FwCufJA3R6O4aElaxN95DRjpq9n3vqDFAjIxb/bVKPvLeUpnO/aTazGWCLxQpUrVyYs7JqzElxWly5dAGjYsCF79uzJwKhMlpaaAt8MgF1L4Y7/g7CBkKcwMXGJvDdjHbPXxpLHz5dHb6/CAy0qUTCPn6cjNlmIJRInb5q/J1++v4br+fj44Dof2rlz5656bu7cuQHw9fW9Ym3G5DCqjsn8on9yzL8UNpBDCef4YOEGpkfsw9dHGNy8Ig/fVpmigbk9Ha3JgiyReDkfHx8KFy7Mjh07qFy5MrNnz744uit//vy2jK65tl/fgTVfQPPhHKvRm7FzNzNl5V5UlV7h5Rh2exVK2BBekw5unWtLRNqKyDYRiRaREZfZP1xENovIehFZIiLlL9lfQERiReRDl7JlzmtGOV/F3XkP3uDNN9/krrvuomnTpgQHB18s79WrF2+88Qb169e3Zixzeeumw8+vQp1ubK31BK3/t5yJK3bTqV5pfv53S17tXNuSiEk3t00jLyK+wHagDRALRAC9VHWzyzGtgD9VNVFEhgItVbWHy/73gCDguKoOc5YtA55S1b/PC38VNo385dn3IJvb/QtM6QLlbmFPu8l0m7AGXxGmDA6nahYchWXTyGc+b5hGPhyIVtVdqpoMTAM6uR6gqktV1bncFyuBix+3RaQhUAL40Y0xGpM9HdkC0/pC0cocbjeBvpOiSElNY+oDWTOJGO/mzkRSBtjnsh3rLLuSwcACABHxAd4BnrrCsROdzVovis0SZ8zfnTwIU7uCXwAJ935F3y+3EZ94ni8GhVOluCURk/G8Yj0SEekLhAFvOYseAearauxlDu+jqnWAFs5Xvytcc4iIRIpI5NGjRy/7dXPC6pBXkpPvPVtLOgVfdYezJ0jsNo37vz3I3uOJfHp/GHWD3Tvfksm53JlI9gNlXbaDnWV/IyKtgZFAR1VNchY3AYaJyB7gbeB+EfkvgKrud/57CvgKRxPaP6jqeFUNU9WwC6OcXAUEBBAXF5cj/6CqKnFxcQQEWCdrtnLhWZHDm0ju8jmDFyWz8cBJPu7dgCaVbXZe4z7uHP4bAVQVkYo4EkhPoLfrASISCowD2qrqxWW8VLWPyzEDgDBVHSEiuYBCqnpMRPyADsDimwkuODiY2NhYrlRbye4CAgL+NgLMZHGqMO9JiF5MSof3eGRVMVbuPsyY7vVpHVLC09GZbM5tiURVU0RkGLAI8AU+V9VNIjIKiFTVOTiasgKBb5xdHTGq2vEql80NLHImEV8cSeTTm4nPz8+PihUr3sypxnifPz+BNZPRFk/x9M76LN6yn1c71aJz6NW6JY3JGG59IFFV5wPzLyl7yeV96+u4xiRgkvP9GaBhhgZpTFZ3fBcsfgWtdhcvn+rM7LUxPHVnNfo1qeDpyEwO4RWd7caYm6QKPzwOPrn4NP8wvlgZw4MtKvKvVlU8HZnJQSyRGJOVrZ0Ku38hstoTvLHiFD3CyvL83TVt7XSTqWyuLWOyqlOH4MeRJJdpwqANtWhauTBvdKljScRkOquRGJNVzX8azp/jdd+HSUqF/3Spg6+tXmg8wBKJMVnRlh9gyxx2hPyLL7b78dgdVSlfNN+1zzPGDaxpy5is5mw8zHuK1BJ1GLjtFqqXyMOQWyt5OiqTg1mNxJis5qcX4cwRJhR+kv2nUvjPfXXw87VfZeM59tNnTFay+xdYM5kjtR/kv+sC6Nu4PA3KFfZ0VCaHs6YtY7KK5ESY8xhauCJD9rWheH7h6bbVPR2VMVYjMSbLWPYfOLGbueWfJepQMq90rEWBAD9PR2WM1UiMyRIOrIU/PuR0SG+eXl2INiFB3FWrpKejMgawGokx3i/1PHz/KJoviKdOdsVXhFc61rIHD43XsERijLf7/X04vIGIkOdZGH2Op+6qTulCeTwdlTEXWdOWMd4sbicse5Pkau15ZE0Z6gXn4X6b1dd4GauRGOOtVGHuE5ArN6PlAU4knuc/XeraNCjG61giMcZbrZsGu39hd/2nmLDuLA80r0hI6QKejsqYf7CmLWO80Zk4WPQ8aWXCeGBTHcoWgcdbV/V0VMZcltVIjPFGP70ISSeZGvRvdh47y6udapPX3z73Ge9kicQYb7P7F4j6khP1HuLVCOhUvzQtqxf3dFTGXJFbE4mItBWRbSISLSIjLrN/uIhsFpH1IrJERMpfsr+AiMSKyIcuZQ1FZIPzmu+LDaY32cn5czD3SbRQeYYdaE1e/1y82CHE01EZc1VuSyQi4gt8BLQDQoBeInLpb8RaIExV6wIzgdGX7H8V+OWSsrHAg0BV56ttBodujOf89j+Ii2ZpledZsfcsI9vXpFhgbk9HZcxVubNGEg5Eq+ouVU0GpgGdXA9Q1aWqmujcXAkEX9gnIg2BEsCPLmWlgAKqulJVFZgMdHbjPRiTeY5ug1//x7kaXXg8sjC3VCpCt4bB1z7PGA9zZyIpA+xz2Y51ll3JYGABgIj4AO8AT13mmrE3cE1jsoa0NPjhCfDPx/8l9yUpJY037rX1103W4BWd7SLSFwgD3nIWPQLMV9XYK591zWsOEZFIEYk8evRoRoRpjPtETYWY39lS+ymmbz7Ho62qUCko0NNRGXNd3DmecD9Q1mU72Fn2NyLSGhgJ3KaqSc7iJkALEXkECAT8ReQ08B4uzV9XuiaAqo4HxgOEhYVp+m7FGDc6fRR+fJHUsk14cENNqhb346HbKns6KmOumzsTSQRQVUQq4vhj3xPo7XqAiIQC44C2qnrkQrmq9nE5ZgCODvkRzu2TInIL8CdwP/CBG+/BGPdb9Dwkn2F8gceI3ZHErKEN8M/lFY0FxlwXt/20qmoKMAxYBGwBZqjqJhEZJSIdnYe9haPG8Y2IRInInOu49CPABCAa2ImzX8WYLGnnz7BhBofrDeWtNUqfxuVoWL6Ip6My5oa49VFZVZ0PzL+k7CWX962v4xqTgEku25FA7QwL0hhPSU50PDNSpApDdrekWKDyTNsano7KmBtm9WdjPEHV0aR1Yg9zyz/NukPneKVjLQrmsaVzTdZjicQYT/htDKyeyMmG/+LpyIK0rlmCtrVt6VyTNVkiMSazrZsGS15Ba3flsSMd8RVhVCdbOtdkXZZIjMlMO5fC9/+CCi34rtxIlu2Is6VzTZZnicSYzHJoA0zvB8Wqs77Fx4yYs43wCkVs6VyT5VkiMSYzxO+DL7tBQAEO3TOFQV9vp3iB3Izt28CWzjVZnq2UY4y7nT0BU++D5EQS+81j4KwDJJ1P5asHG1PUZvY12YAlEmPc6fw5+Lo3nNhNWp9ZPPFzEtsOneSzAY2oViK/p6MzJkNY05Yx7pKWBrMfgpjfofNYRm8rzo+bD/NC+xBa2YqHJhuxRGKMu/z4Amz+Du58jZnJt/DJ8p30blyOgc0qeDoyYzKUJRJj3OH3D2HlR9B4KBGlevPct+tpVqUor3S050VM9mN9JMZkpFOHYeGzsGk21OxITNhIHvpkJWUL5+Xj3g3x87XPbib7sURiTEZQhbVT4ceRcP4stHqBU2GPMHhcJKlpyoT+YRTMa/NomezJEokx6RW3E354HPb8CuWbwT3vkVK4Mo9OjmT3sTNMHhRuqx2abM0SiTE3K/U8/P4BLH8TfHNDh3ehQX9UhNd+2MyybUd54946NK1SzNORGuNWlkiMuRn7V8Ocx+HwBqh5D7R7CwqUIjkljRe+W8+MyFgGNatI78blPB2pMW53XYlERCoDsaqaJCItgbrAZFWNd2dwxnid5DPw8+vw51jIVxx6THUkEiA+MZmHp65m5a7jPHZ7FZ5oXc3DwRqTOa63RjILCBORKsB44HvgK+BudwVmjNc5Ewdf3gcH1kLYIGj9MgQUBGD3sTMMmhTB/hNnGdOjHveGBns0VGMy0/WORUxzrsF+L/CBqj4NlLrWSSLSVkS2iUi0iIy4zP7hIrJZRNaLyBIRKe8sLy8ia5zruG8SkYddzlnmvGaU82WPCBv3O3kAJraDI1ug1zToMOZiEvljZxydP1pBwtnzfPlgY0siJse53hrJeRHpBfQH7nGWXXUso4j4Ah8BbYBYIEJE5qjqZpfD1gJhqpooIkOB0UAP4CDQxNmUFghsdJ57wHleH+fa7ca4X9xOmNzZMfli31lQofnFXTMi9zFy9gbKFcnLxAHhlCua14OBGuMZ11sjGQg0AV5X1d0iUhGYco1zwoFoVd2lqsnANKCT6wGqulRVE52bK4FgZ3myqiY5y3PfQJzGZKxDG+HztpB8Ggb8cDGJpKUp/12wlWdmrqdxxaJ8+0gzSyImx7quGomzFvEYgIgUBvKr6pvXOK0MsM9lOxZofJXjBwMLLmyISFlgHlAFeNqlNgIwUURScfTdvKaqej33YcwN2bcKvuwKfvlgwFwIqg7A2eRUnpwexcJNh+jduByvdKxlT6ybHO16R20tAzo6j18NHBGRFaoV76unAAAeDElEQVQ6PCOCEJG+QBhw24UyVd0H1BWR0sB3IjJTVQ/jaNbaLyL5cSSSfsDky1xzCDAEoFw5G4JpbtDOn2FaH8hfEu7/Hgo5foYOnzzHA19EsvFAAi+0r8ng5hVt7iyT413vx6iCqnoS6IJj2G9joPU1ztkPlHXZDnaW/Y2ItAZGAh1dmrMuctZENgItnNv7nf+ewjFyLPxyX1xVx6tqmKqGBQUFXSNUY1xs/h6+7A5FKsOgRReTyMb9CXT6cAU7j57m035hPNCikiURY7j+RJJLREoB3YG513lOBFBVRCqKiD/QE5jjeoCIhALjcCSRIy7lwSKSx/m+MNAc2CYiuUSkmLPcD+iAI8kYkzHWTIFvBkCZBo7mrEDHoMCFGw/S7ZM/8BH45uEmtA4p4dk4jfEi1ztqaxSwCFihqhEiUgnYcbUTVDVFRIY5z/MFPlfVTSIyCohU1TnAW0Ag8I3zk12MqnYEagLviIgCArytqhtEJB+wyJlEfIHFwKc3eM/GXN4fH8Gi56HyHdBjCvjnQ1X5eNlO3lq0jfplCzH+/oYUzx/g6UiN8SqSE/qpw8LCNDLSRgubq1g/A759EEI6QZcJkMufpJRUnvt2A9+u2U/HeqUZ3bUuAX6+no40x2rZsiUAy5Yt82gcOYmIrFbVsGsdd11NW86mptkicsT5miUi9tSVyR4ORMGcRx0z9973GeTyJ+50En0+/ZNv1+xneJtqvNezviURY67gevtIJuLo3yjtfP3gLDMmazt91DE6K28x6PYF+Pqx7dApOn20gg37E/iwdyiP3VHVOtWNuYrrTSRBqjpRVVOcr0mADYUyWVvqeUfHeuIx6DkVAoNYuvUI9439neSUNGY81IQOdUt7OkpjvN71JpI4EekrIr7OV18gzp2BGeN2i0bC3t/gnvehdCif/7abwV9EUL5oXr4f1ox6ZQt5OkJjsoTrTSSDcAz9PYRjHqyuwAA3xWSM+62dCqvGQZNhUK8HE37dxai5m2kTUoJvHm5CqYJ5PB2hMVnG9U6RshfHk+0XicgTwLvuCMoYt4qNhLlPQqWW0PoVZq6O5bV5W7i7Tkk+6NUAXx/rDzHmRqRngqAMmR7FmEx16hBM7wv5S0HXify49RjPzlpP8yrFGNOjviURY25Cepbatd84k7WkJMH0fnAuAQb/xB8HlWFfr6V2mYKM69eQ3LlseK8xNyM9iST7P8lospcFz0DsKug2iY2pZXlw8krKFcnLpAGNyJc7Pb8KxuRsV/3tEZFTXD5hCGC9kSbriPgMVk+C5sPZVbwN/T/5g4J5/JgyOJzC+fw9HZ0xWdpVE4mq5s+sQIxxm72/O2ojVdpwsOG/6TduFQBTBofb6CxjMoDV5032dngTfN0TClcgvt1Y+k1cTcLZ80wbcguVggI9HZ0x2YIlEpN9Hd8NU7qAX14Se8yk/7TtxBxPZPKgcGqXKejp6IzJNiyRmOzp1GGYci+knCO5/zyGzDnCxv0JfNK3IbdUKurp6IzJViyRmOznbDxMvQ9OHya13/c8+XMyv0Uf451u9WhjC1IZk+HS80CiMd4nOdHRJ3J0K9p9Ci+tzsO8DQd5oX1N7mtoKx8Y4w6WSEz2kXoeZg6EmJXQZRxj9pTjyz9jGNqyMg+0qOTp6IzJtiyRmOwhLQ2+HwbbF0L7t/niZEPeX7KD7mHBPHNXdU9HZ0y2ZonEZH2q8ONIWD8NWr3A937tePmHTbQJKcEb99axRamMcTO3JhIRaSsi20QkWkRGXGb/cBHZLCLrRWSJiJR3lpcXkTUiEiUim0TkYZdzGorIBuc13xf7K2F+fQdWfgyNh7K8ZH/+PWMdjSoU4YNeoeTytc9Kxrib237LRMQX+AhoB4QAvUQk5JLD1gJhqloXmAmMdpYfBJqoan2gMTBCRC4sVTcWeBCo6ny1ddc9mCwg8nP4+VWo24O1IU8z9Ms1VC2Rnwn9w2yNdWMyiTs/roUD0aq6S1WTgWlAJ9cDVHWpqiY6N1cCwc7yZFVNcpbnvhCniJQCCqjqSlVVYDLQ2Y33YLzZpu9g7nCoehfRTf7LoC9WUywwN18MakSBAD9PR2dMjuHORFIG2OeyHessu5LBwIILGyJSVkTWO6/xpqoecJ4fewPXNNnVrmXw7YNQNpyDd46l36S1+Pr4MGVwOMXzB3g6OmNyFK9oQHauAR8GvHWhTFX3OZu8qgD9ReSGniQTkSEiEikikUePHs3YgI1nHVgL0/pA0SrEd55K38kbOH0uhUkDG1G+aD5PR2dMjuPORLIfKOuyHews+xsRaQ2MBDq6NGdd5KyJbARaOM93farsstd0njdeVcNUNSwoKOimb8J4mbidMLUr5ClCYvfpDJi2g30nzjL+/jCbP8sYD3FnIokAqopIRRHxB3oCc1wPEJFQYByOJHLEpTxYRPI43xcGmgPbVPUgcFJEbnGO1rof+N6N92C8ycmDMKUzoCT1msngbw+wYX8C7/cMpUllmz/LGE9x21xbqpoiIsOARYAv8LmqbhKRUUCkqs7B0ZQVCHzjHMUbo6odgZrAOyKiOBbReltVNzgv/QgwCcfCWgtw6Vcx2djZEzC1CyQeJ6XfHB5ZeJI/dsXxv+71aFu7pKejMyZHc+ukjao6H5h/SdlLLu9bX+G8n4C6V9gXCdTOwDCNt0tOhK96wrEdpPb+hid/82HJ1kO82qkWXRrY/FnGeJpXdLYbc0WpKY75s/b9iXYZz8h1Rflh3QFGtKtBvyYVPB2dMQZLJMabqcIPj8H2hejdb/Pq7hpMi9jHsFZVePi2yp6OzhjjZInEeK/F/wdRX0LL53g34VY+X7GbAU0r8O87q3k6MmOMC1vYyniflGRY+hqseA8aPcCn0o33lmylW8NgXuoQYpMwGuNlLJEY73JkC8x+CA6ug4YD+LroMF7/bjPt65Tiv/fVxcfHkogx3sYSifEOaWmOGXyXjILc+aHHl3yfFMrz06NoVT2IMT3q42tJxBivZInEeN6JvfDdI7D3N6jeHu55j0V7Uxk+Yw3hFYowtm9D/HNZd54x3soSifEcVUdn+gLnUjWdPob6vVm46TDDvlpDnTIF+WxAI5sO3hgvZ4nEeMbpI/DD47BtPlRoAZ0/hkLlmLf+II9NW0u94IJMGhROYG77ETXG29lvqcl8W+Y6kkjSKbjrDWg8FHx8+D5qP8NnrKNBuUJMHGhJxJiswn5TTebatgCm94FS9eDe8VC8BgCz18ZeXCL38wGNyGdJxJgsw35bTeaJ3wezH4aSdWHQj+DnWIDqm8h9PDNrPU0qFWVC/zDy+tuPpTFZif3GmsyRkuyYM0vToPsXF5PItFUxPDd7A82rFGN8vzDy+FvHujFZjSUSkzmWvAKxEdBtEhSpBMCXf+5l5OyN3FYtiHH9GtroLGOyKEskxv22zoc/PoRGD0KtewGY/MceXvp+E7fXKM7Yvg3IncuSiDFZlSUS417xMfDdUEfn+l2vA/D5b7sZNXczbUJK8GHvUEsixmRxlkiM+6QkwzfOfpFuk1Bff979aTvvLdlB21oleb9XqD2xbkw2YInEuM+SV2B/JHT7gpSCFXhx9ka+XhVDt4bBvNGlDn6+lkSMyQ7c+pssIm1FZJuIRIvIiMvsHy4im0VkvYgsEZHyzvL6IvKHiGxy7uvhcs4kEdktIlHOV3133oO5SVvnOfpFwodwrto9DP1yDV+viuFfrSozumtdSyLGZCNuq5GIiC/wEdAGiAUiRGSOqm52OWwtEKaqiSIyFBgN9AASgftVdYeIlAZWi8giVY13nve0qs50V+wmnU7sdfaL1Ce++UsMnvAna2JO8ErHWvRvWsHT0RljMpg7PxaGA9GquktVk4FpQCfXA1R1qaomOjdXAsHO8u2qusP5/gBwBAhyY6wmo1x8XkQ53HYc3SasYUNsAh/2amBJxJhsyp2JpAywz2U71ll2JYOBBZcWikg44A/sdCl+3dnkNUZEcmdEsCaDLP4/2L+aA7e9Racv93Mo4RyTBjWifd1Sno7MGOMmXtFQLSJ9gTDgrUvKSwFTgIGqmuYsfg6oATQCigDPXuGaQ0QkUkQijx496rbYjYuor2Hlxxyu2Z+2PxYmTZXpDzWhaeVino7MGONG7kwk+4GyLtvBzrK/EZHWwEigo6omuZQXAOYBI1V15YVyVT2oDknARBxNaP+gquNVNUxVw4KCrFXMrRKPw6wH4buHOVGsIXdsaE2xwNzMGtqUkNIFPB2dMcbN3Dn8NwKoKiIVcSSQnkBv1wNEJBQYB7RV1SMu5f7AbGDypZ3qIlJKVQ+KiACdgY1uvAdzLdsXwZzHIPEY66oMpdumJtQMLsrEAY0oks/f09EZYzKB2xKJqqaIyDBgEeALfK6qm0RkFBCpqnNwNGUFAt848gIxqtoR6A7cChQVkQHOSw5Q1SjgSxEJAgSIAh521z2YqziXAAufh6ipaPEQJlV4k1ci/WhVPYiP+jSwGXyNyUHc+tuuqvOB+ZeUveTyvvUVzpsKTL3CvtszMkZzE6KXwJxH4dRBUpoO5+mjbZkdeYzejcsxqmMtctkzIsbkKPax0Vy/pFPw44uweiIUq8apvgsY9FMaEXuOMaJdDR66tRLOmqUxJgexRGKuz+5f4ftHHItTNX2UmHrDGTB1PbHHz/JBr1DuqVfa0xEaYzzEEom5utTzsPR1+O1dKFIRBi0kSmoweHwEKWnK1AcaE16xiKejNMZ4kCUSc2XxMTBzMMSuggb9oe1/+HHHKR6b9gdB+XMzaWA4lYMCPR2lMcbDLJGYy9s8B+YMA1Xo+jnUvo9JK3bzytzN1A0uxGf9wygWaJMKGGMskZhLnT8HP74AEZ9C6VDoOpG0QhV4Y+5mJvy2mzYhJXi/Z6itrW6MucgSifnLsR2OhagOb4Amw+CO/yPVx49nZ61n5upYBjStwIsdQvD1sZFZxpi/WCIxDlFfwbynIFdu6D0Dqt3F+dQ0npy2lrnrD/Jk62o8dkcVG95rjPkHSyQ5XdJpmPdvWD8NyjeH+z6FAqVJSkll2Fdr+WnzYZ6/uwZDbq3s6UiNMV7KEklOlpYG0/vA7l+g5XNw69Pg48vZ5FSGTInk1x3HeLVTLfo1qeDpSI0xXswSSU4WMQF2LYMOYyBsEACnk1IYNCmCiD3HGd21Lt3Dyl79GsaYHM8SSU51LBp+egmqtIaGAwFISDxP/4mr2LA/gXd71KdT/autQ2aMMQ6WSHKitFT47mHI5Q8dPwAR4k4n0e+zVUQfOc3HfRpwV62Sno7SGJNFWCK5hpTUtOw3m+2K9yA2ArpMgAKlOXLyHH0m/EnM8UTG39+QltWLezpCY0wWks3+Qmasp75Zx9Av13g6jIx1aCMsfQNCOkGdruyPP0v3cX+wP/4skwaGWxIxxtwwSyRXUbJAAEu2HOZA/FlPh5IxUpJh9sOQpxC0/x+HTyXRa/xK4k4nM2VwY5pULurpCI0xWZAlkqvo0agsCsyI3OfpUDLG8jcdT63f8z5xmp++E/4k7nQSXwwOp2H5wp6OzhiTRVkiuYqyRfLSomoQ0yP2kZqmng4nfWIj4bf/Qf0+JJRvw/2fryLmeCIT+jeiQTlLIsaYm2eJ5Bp6h5flYMI5lm074ulQbl5yIsx+CPKX5kyr1xg0KYLth0/xSd+G1pxljEk3tyYSEWkrIttEJFpERlxm/3AR2Swi60VkiYiUd5bXF5E/RGSTc18Pl3MqisifzmtOFxF/d97DHTVLEJQ/N1+vinHnl3GvJaMgLprkDh/w4IztrI05wfs9Q2lVwzrWjTHp57ZEIiK+wEdAOyAE6CUiIZccthYIU9W6wExgtLM8EbhfVWsBbYF3RaSQc9+bwBhVrQKcAAa76x4A/Hx96B4WzM9bj3AwIQt2uu/+Bf4cS2qjBxn6e35+3xnHW13r0a5OKU9HZozJJtxZIwkHolV1l6omA9OATq4HqOpSVU10bq4Egp3l21V1h/P9AeAIECSOqWdvx5F0AL4AOrvxHgDo2agcCkyPyGKd7udOwnf/QotU5qkTXViy9QivdqrFfQ2DPR2ZMSYbcWciKQO4/uWNdZZdyWBgwaWFIhIO+AM7gaJAvKqmXOuaIjJERCJFJPLo0aM3Ef5fsmyn+6Ln0ZOxfFTw38zeeILn2tWwCRiNMRnOKzrbRaQvEAa8dUl5KWAKMFBV027kmqo6XlXDVDUsKCgo3TFe6HRfvj2LdLqv/gLWTuG34n14e0shHru9Cg/dZlPBG2MynjsTyX7AderYYGfZ34hIa2Ak0FFVk1zKCwDzgJGqutJZHAcUEpELU7tc9prucEfNEhQLzM1Xf2aB5q2t82DuE+wueAsD97ZhcPOKPNmmmqejMsZkU+5MJBFAVecoK3+gJzDH9QARCQXG4UgiR1zK/YHZwGRVvdAfgqoqsBTo6izqD3zvxnu46K9O98Pe3ekesxJmDuJwYE3aHx5Ct/CKvNC+pq1saIxxG7clEmc/xjBgEbAFmKGqm0RklIh0dB72FhAIfCMiUSJyIdF0B24FBjjLo0SkvnPfs8BwEYnG0Wfymbvu4VK9wsuRpjAjIjazvuSNObIFvupOgn8J2h19lDb1K/Fa5zqWRIwxbuXW2X9VdT4w/5Kyl1zet77CeVOBqVfYtwvHiLBM5+h0L8b0iBiG3V4FXx8v+gMdvw+mdOGs+tP+xHAahlTl7W71vCtGY0y25BWd7VlJ7/ByHPC2TvfE4zD1Ps6fO8V9p/5Nhco1+aBXKH7Zbfp7Y4xXsr80N6h1iJd1uicnwlfdST2+m/6JT5C3bD3G39+QAD9fT0dmjMkhLJHcINdO90MJ5zwbTGoKzByI7l/N4+eHcbJkYz4f2Ii8/rZemTEm81giuQk9Gzk63T36pLsq/PA4bF/IqNRBbCvcksmDGlMgwM9zMRljciRLJDehXNG/Ot099qT7klEQNZWxdOXn/B2Y+kBjiuRz6/yVxhhzWZZIblIvZ6f7L9vTN/3KDUtNgaX/gd/+xyxpw2T/Xkwd3JgSBQIyNw5jjHGyRHKT2jg73b/8MxOnlz+8CT5rDcv/ywKf23jT5wGmPngLZYvkzbwYjDHmEpZIrubEXscMupfh5+tDt8zqdE89D8veRMfdxtmjexiuT/Js2r+YNLgplYMC3fu1jTHmGiyRXM38p+HN8jDuNlg00jGHVeLxi7t7NirreNLdnWu6H4iC8a1g2Rv87NOEpqf+Q3zF9swZ1pyQ0gXc93WNMeY62TjRq2n+BJSqB3t/h1Wfwh8fOsqL14LyTSlfvikdKuVh2qoY/tUqg590T0mC5W+iv71LvE9Bnkkezs7A2/hftxBaVbeVDY0x3sMSydWUb+p4geMP+/41sPc3R2KJ+goiPuVDYGdaKea8GU5a9Q7UaXInVUsWTN/8VrGRpM4eim/cdmal3cb/0vozsG0oHzWtgH8uq0QaY7yLJZLrlSs3lG/ieIFj9NShdaTtWUHuNQvpEDcPv/XfE7cuPwv8GpNYqS1VbulA3Qol8blWTUUVTh6Ao1vQ7Ytg1QSOUoRnk5+lRIP2fH9XDYLy53b/PRpjzE2wRHKzfHNBmYb4lGlIcLPHIOkU8RsWcHL1bG47tJx82xdzZttIlvmEcrzcnZQN70zV8sGcOn6Q5AOb0COb8YvbRt747RQ8vZOA1NMACPBlyh3MLzmUZzs3om5woavHYYwxHmaJJKPkzk+hsO4UCusOKcmc2b6cw6u+IXTfYgrvXcn5Pa9zijyUl9MXTzmhgWzXYLalNWFfrnIcCqhEQmBl7m1Wh6n1y9j078aYLEEca0Vlb2FhYRoZGemZL56WxrmYSPb/MZOU00dJKlyNtGI18SlZk/xFy1Aorz8F8vjZdO/GGK8jIqtVNexax1mNxN18fAioEE7lCh5ZQsUYY9zOhgAZY4xJF0skxhhj0sWtiURE2orINhGJFpERl9k/XEQ2i8h6EVkiIuVd9i0UkXgRmXvJOZNEZPdl1nI3xhjjAW5LJCLiC3wEtANCgF4iEnLJYWuBMFWtC8wERrvsewvod4XLP62q9Z2vqAwO3RhjzA1wZ40kHIhW1V2qmgxMAzq5HqCqS1U10bm5Egh22bcEOOXG+IwxxmQAdyaSMoDrbIaxzrIrGQwsuM5rv+5sDhsjIvbItzHGeJBXdLaLSF8gDEdz1rU8B9QAGgFFgGevcM0hIhIpIpFHj2by4lPGGJODuDOR7AfKumwHO8v+RkRaAyOBjqqadK2LqupBdUgCJuJoQrvcceNVNUxVw4KCgm7qBowxxlybOx9IjACqikhFHAmkJ9Db9QARCQXGAW1V9cj1XFRESqnqQXHMH9IZ2Hitc1avXn1MRPbe6A04FQOO3eS57mRx3RiL68ZYXDcmu8ZV/tqHuHmKFBG5G3gX8AU+V9XXRWQUEKmqc0RkMVAHOOg8JUZVOzrP/RVHE1YgEAcMVtVFIvIzEIRjfsMo4GFVPY2biEjk9UwRkNksrhtjcd0Yi+vG5PS43DpFiqrOB+ZfUvaSy/vWVzm3xRXKb8+wAI0xxqSbV3S2G2OMyboskVzbeE8HcAUW142xuG6MxXVjcnRcOWIaeWOMMe5jNRJjjDHpkqMTyXVMKnmriKwRkRQR6XrJvv4issP56u9FcV12sktPxiUi9UXkDxHZ5JyRoIeXxFXeWR7ljO1hb4jLZX8BEYkVkQ+9JS4RSXWZMHWOF8VVTkR+FJEtzolgK3g6LhFp5fK9ihKRcyLS2dNxOfeNdv7MbxGR90XSuRyrqubIF44hyTuBSoA/sA4IueSYCkBdYDLQ1aW8CLDL+W9h5/vCno7Lue8O4B5grhd9v6oBVZ3vS+MY7l3IC+LyB3I73wcCe4DSno7LZf97wFfAh97w/+jcdzojf64yMK5lQBuX/8u83hCXyzFFgOPeEBfQFFjhvIYv8AfQMj3x5OQayfVMKrlHVdcDaZecexfwk6oeV9UTwE9AWy+IC3XfZJc3HZeqblfVHc73B4AjOJ4F8nRcyfrXbAq5ydgaerr+H0WkIVAC+DEDY0p3XG5003GJY1bxXKr6k/O40/rXZLAei+sSXYEFXhKXAgE4P0gBfsDh9ASTkxPJjU4qmVHnevLa6ZEhcYlIOI4f4J3eEJeIlBWR9c5rvOlMdB6NS0R8gHeApzIolgyJyylAHHPYrczIZpp0xlUNiBeRb0VkrYi8JY5lLDwdl6uewNcZEpHDTcelqn8AS3G0DBwEFqnqlvQEk5MTiclkIlIKmAIMVNXM/LR7Raq6Tx3r4VQB+otICU/HBDwCzFfVWE8Hchnl1fGkdG/gXRGp7OmAcDxY3QJH4m2Eo7lngCcDcuX8ua8DLPJ0LAAiUgWoiWP+wzLA7SJy2QfAr1dOTiTXNamkG8715LXTI11xiUgBYB4wUlVXektcFzhrIhtx/EHydFxNgGEisgd4G7hfRP7rBXGhqvud/+7C0S8R6gVxxQJRzmaeFOA7oIEXxHVBd2C2qp7PoJggfXHdC6x0NgGexrF8R5P0BJOTE8nFSSVFxB9H1fN6R6EsAu4UkcIiUhi4k4z7tJGeuNzppuNyHj8bmKyqM70ormARyeN8XxhoDmzzdFyq2kdVy6lqBRyfsier6j9G5WR2XM6f99zO98WAZsBmT8flPLeQiFzod7vdS+K6oBcZ26yV3rhigNtEJJeI+AG3Aelq2srw0RdZ6QXcDWzH0V4/0lk2CseU9uCoJscCZ3BMHLnJ5dxBQLTzNdCL4voVOAqcdR5zl6fjAvoC53FMsnnhVd8L4moD/9/eHaNIEYRRAH6/u5gYKaaCmJoMmCzGZkbewMjc0MQjCHoC7yBiYr7pshMIBu4tNCyD7kBMhP2HrpH9vqinYZpHVfCYoqcql1neeLlM8upY5vGPZ7zMAd/aao7X0yT7dbz2WTZSnZ7rr7ncJ/mY5PaR5HqY5ZfCrUOOVXMeT7Lsuv4tS+G+62bxz3YAWm7y0hYAB6BIAGhRJAC0KBIAWhQJAC2KBIAWRQJAiyKBCarqpKrer2dC7Kvq0exMcF2KBOZ4k+THGONxkg9ZNmqE/9Lp7ABw01TVnSQvxhhP1ltXSZ5PjAQtigS29yzJg6q6WD/fS/J1Yh5osbQF29sleTvG2I0xdllOQbz4x3fgaCkS2N7dJD+TpKpOsxxD8GlqImhQJLC970nO1uvXST6PMa4m5oEW28jDxtZDtL4kuZ/kPMs5KL/mpoLrUyQAtFjaAqBFkQDQokgAaFEkALQoEgBaFAkALYoEgBZFAkDLb0Gvo3vuQT0hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals, label = \"lvals\")\n",
    "plt.plot(thetas,vlvals, label = \"vlvals\")\n",
    "plt.vlines(0.160, ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"MSE for alphaS altFit SUCCESS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx  = X_test\\nprint(x.shape)\\n#x = K.squeeze(x, axis = 1)\\nx = tf.gather(x, np.arange(2))\\nx = tf.gather(x, np.arange(51), axis = 1)\\nprint(x.shape)\\n\\npar1 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.16\\npar2 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.68\\npar3 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.217\\npar = K.stack((par1, par2, par3), axis = 2)\\n\\n#combining and reshaping into correct format:\\n#data = K.stack((x, theta0_stack), axis=-1) \\ndata = K.concatenate((x, par), axis =2)\\nprint(data.shape)\\nw = reweight(data)\\nprint(w.shape)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check to ensure that \"data\" in loss wrapper is being constructed properly\n",
    "\n",
    "'''\n",
    "x  = X_test\n",
    "print(x.shape)\n",
    "#x = K.squeeze(x, axis = 1)\n",
    "x = tf.gather(x, np.arange(2))\n",
    "x = tf.gather(x, np.arange(51), axis = 1)\n",
    "print(x.shape)\n",
    "\n",
    "par1 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.16\n",
    "par2 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.68\n",
    "par3 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.217\n",
    "par = K.stack((par1, par2, par3), axis = 2)\n",
    "\n",
    "#combining and reshaping into correct format:\n",
    "#data = K.stack((x, theta0_stack), axis=-1) \n",
    "data = K.concatenate((x, par), axis =2)\n",
    "print(data.shape)\n",
    "w = reweight(data)\n",
    "print(w.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.Session() as sess:\\n    print(K.eval(data))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with tf.Session() as sess:\n",
    "    print(K.eval(data))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.12\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_14[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch:  0\n",
      "Training g\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 113us/step - loss: 0.2554 - acc: 0.5443 - val_loss: 0.2319 - val_acc: 0.5728\n",
      ". theta fit =  0.12\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2438 - acc: 0.5732 - val_loss: -0.2623 - val_acc: 0.5728\n",
      ". theta fit =  0.15425192\n",
      "Epoch:  1\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 161s 112us/step - loss: 0.2693 - acc: 0.5249 - val_loss: 0.2518 - val_acc: 0.5471\n",
      ". theta fit =  0.15425192\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2578 - acc: 0.5490 - val_loss: -0.2591 - val_acc: 0.5471\n",
      ". theta fit =  0.1736672\n",
      "Epoch:  2\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 162s 112us/step - loss: 0.2585 - acc: 0.4674 - val_loss: 0.2513 - val_acc: 0.4399\n",
      ". theta fit =  0.1736672\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2529 - acc: 0.4404 - val_loss: -0.2530 - val_acc: 0.4399\n",
      ". theta fit =  0.16652806\n",
      "Epoch:  3\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 161s 112us/step - loss: 0.2550 - acc: 0.4712 - val_loss: 0.2525 - val_acc: 0.4353\n",
      ". theta fit =  0.16652806\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2527 - acc: 0.4344 - val_loss: -0.2525 - val_acc: 0.4353\n",
      ". theta fit =  0.16671108\n",
      "Epoch:  4\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 162s 112us/step - loss: 0.2540 - acc: 0.4563 - val_loss: 0.2525 - val_acc: 0.4355\n",
      ". theta fit =  0.16671108\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2527 - acc: 0.4351 - val_loss: -0.2525 - val_acc: 0.4355\n",
      ". theta fit =  0.16644704\n",
      "Epoch:  5\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 162s 112us/step - loss: 0.2530 - acc: 0.4399 - val_loss: 0.2525 - val_acc: 0.4400\n",
      ". theta fit =  0.16644704\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2526 - acc: 0.4402 - val_loss: -0.2525 - val_acc: 0.4400\n",
      ". theta fit =  0.16610709\n",
      "Epoch:  6\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 162s 113us/step - loss: 0.2530 - acc: 0.4399 - val_loss: 0.2524 - val_acc: 0.4343\n",
      ". theta fit =  0.16610709\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2525 - acc: 0.4336 - val_loss: -0.2524 - val_acc: 0.4343\n",
      ". theta fit =  0.16593656\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 113us/step - loss: 0.2532 - acc: 0.4462 - val_loss: 0.2524 - val_acc: 0.4338\n",
      ". theta fit =  0.16593656\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 190s 132us/step - loss: -0.2525 - acc: 0.4339 - val_loss: -0.2524 - val_acc: 0.4338\n",
      ". theta fit =  0.16615292\n",
      "Epoch:  8\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2527 - acc: 0.4379 - val_loss: 0.2523 - val_acc: 0.4359\n",
      ". theta fit =  0.16615292\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 190s 132us/step - loss: -0.2525 - acc: 0.4356 - val_loss: -0.2523 - val_acc: 0.4359\n",
      ". theta fit =  0.16633661\n",
      "Epoch:  9\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2526 - acc: 0.4351 - val_loss: 0.2523 - val_acc: 0.4320\n",
      ". theta fit =  0.16633661\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 191s 133us/step - loss: -0.2524 - acc: 0.4321 - val_loss: -0.2523 - val_acc: 0.4320\n",
      ". theta fit =  0.16627647\n",
      "Epoch:  10\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2526 - acc: 0.4381 - val_loss: 0.2522 - val_acc: 0.4307\n",
      ". theta fit =  0.16627647\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 192s 133us/step - loss: -0.2524 - acc: 0.4304 - val_loss: -0.2522 - val_acc: 0.4307\n",
      ". theta fit =  0.1653906\n",
      "Epoch:  11\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 165s 114us/step - loss: 0.2526 - acc: 0.4399 - val_loss: 0.2523 - val_acc: 0.4322\n",
      ". theta fit =  0.1653906\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 192s 134us/step - loss: -0.2525 - acc: 0.4325 - val_loss: -0.2523 - val_acc: 0.4322\n",
      ". theta fit =  0.16609815\n",
      "Epoch:  12\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 165s 115us/step - loss: 0.2525 - acc: 0.4345 - val_loss: 0.2522 - val_acc: 0.4313\n",
      ". theta fit =  0.16609815\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 193s 134us/step - loss: -0.2523 - acc: 0.4316 - val_loss: -0.2522 - val_acc: 0.4313\n",
      ". theta fit =  0.16579294\n",
      "Epoch:  13\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 166s 115us/step - loss: 0.2524 - acc: 0.4336 - val_loss: 0.2522 - val_acc: 0.4336\n",
      ". theta fit =  0.16579294\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 194s 135us/step - loss: -0.2523 - acc: 0.4337 - val_loss: -0.2522 - val_acc: 0.4336\n",
      ". theta fit =  0.16623442\n",
      "Epoch:  14\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 166s 116us/step - loss: 0.2524 - acc: 0.4338 - val_loss: 0.2522 - val_acc: 0.4330\n",
      ". theta fit =  0.16623442\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 194s 135us/step - loss: -0.2523 - acc: 0.4324 - val_loss: -0.2522 - val_acc: 0.4330\n",
      ". theta fit =  0.16651332\n"
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = int(len(X_default)/20) #larger batch_size leads to better precision (at least for Guassian case)\n",
    "epochs = 15 #but requires more epochs to train\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "        \n",
    "    #Creating theta_prime\n",
    "    alphaS = K.ones(shape =x.shape[0:2])*theta0 # Fitting parameter\n",
    "    aLund = K.ones(shape =x.shape[0:2])*0.68 # Fixed at default\n",
    "    probStoUD = K.ones(shape =x.shape[0:2])*0.217 # Fixed at default\n",
    "    \n",
    "    theta_prime = K.stack((alphaS, aLund, probStoUD), axis = 2)\n",
    "    \n",
    "    data = K.concatenate((x, theta_prime), axis =2)\n",
    "    # print(data.shape) # = (batch_size, 51, 7); correct format to pass to DCTR\n",
    "   \n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.16, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"(Starting at fit = 0.12) \\nN = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}\".format(len(X_default), batch_size, epochs*2))\n",
    "plt.savefig(\"(Starting at fit = 0.12) N = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}.png\".format(len(X_default), batch_size, epochs))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
