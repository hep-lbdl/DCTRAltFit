{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses the original DCTR model (i.e. model expects “empty particles” to have 0 in every entry). As opposerd to the, modified DCTR that was trained to allow non-zero inputs for theta on empty particles, the fit appears less noisy and more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test_3D_known.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small = X_train[0:int(0.8*10**5)]\n",
    "Y_train_small = Y_train[0:int(0.8*10**5)]\n",
    "X_test_small = X_test[0:int(0.2*10**5)]\n",
    "Y_test_small = Y_test[0:int(0.2*10**5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_3D.h5') #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model = PFN(input_dim=4, \\n            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \\n            output_dim = 1, output_act = 'sigmoid',\\n            summary=False)\\nmyinputs = model.inputs[0]\\nbatch_size = 1000\\n\\ndef my_loss_wrapper(inputs, val=0):\\n    x  = inputs #x.shape = (?,?,4)\\n    #Reshaping to correct format\\n    x = tf.gather(x, np.arange(batch_size))\\n    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\\n\\n    theta_prime = [0.1365, 0.68, val]\\n    \\n    # zip theta_prime to each input particle (but not to the padded rows)\\n    concat_input_and_params = tf.where(K.abs(x[...,0])>0, #checks if pT != 0, which means we have a particle\\n                                   K.ones_like(x[...,0]),\\n                                   K.zeros_like(x[...,0]))\\n    \\n    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\\n    \\n    data = K.concatenate([x, concat_input_and_params], -1)\\n    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\\n    w = reweight(data) # NN reweight\\n    \\n    def my_loss(y_true,y_pred):\\n        # Mean-Squared Loss:\\n        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\\n        \\n        # Categorical Cross-Entropy Loss\\n        '''\\n        #Clip the prediction value to prevent NaN's and Inf's\\n        \\n        epsilon = K.epsilon()\\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\\n        \\n        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\\n        '''\\n        return K.mean(t_loss)\\n    return my_loss\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, val]\n",
    "    \n",
    "    # zip theta_prime to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0, #checks if pT != 0, which means we have a particle\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thetas = np.linspace(0.10, 0.30, 33) #iterating across possible alphaS values\\nvlvals = []\\nlvals = []\\n\\n\\nfor theta in thetas:\\n    print(\"trainnig theta = :\", theta)\\n    model.model.compile(optimizer=\\'adam\\', loss=my_loss_wrapper(myinputs,theta),metrics=[\\'accuracy\\'])\\n    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\\n    vlvals+=[history.history[\\'val_loss\\']]\\n    lvals+=[history.history[\\'loss\\']]\\n    print\\n    pass\\nprint(lvals)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"thetas = np.linspace(0.10, 0.30, 33) #iterating across possible alphaS values\n",
    "vlvals = []\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.plot(thetas,lvals, label = lvals)\\nplt.plot(thetas,vlvals, label = vlvals)\\nplt.vlines(, ymin = np.min(lvals), ymax = np.max(lvals), label = \\'Truth\\')\\nplt.xlabel(r\\'$\\theta$\\')\\nplt.ylabel(\\'Loss\\')\\nplt.legend()\\n#plt.savefig(\"MSE for alphaS altFit SUCCESS, proper method\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.plot(thetas,lvals, label = lvals)\n",
    "plt.plot(thetas,vlvals, label = vlvals)\n",
    "plt.vlines(, ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"MSE for alphaS altFit SUCCESS, proper method\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = [0.1365,0.68, 0.217]\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "print(np.shape(theta_fit_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            3           activation_14[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,732\n",
      "Trainable params: 56,732\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch:  0\n",
      "[0.1365 0.68   0.217 ]\n",
      "WARNING:tensorflow:From <ipython-input-18-ac9808acf7d3>:42: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training g\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2434 - acc: 0.5668 - val_loss: 0.2398 - val_acc: 0.5782\n",
      ". theta fit =  [0.1365 0.68   0.217 ]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 273s 189us/step - loss: -0.2400 - acc: 0.5797 - val_loss: -0.2407 - val_acc: 0.5782\n",
      ". theta fit =  [0.13508321 0.6785744  0.21558821]\n",
      "Epoch:  1\n",
      "[0.13508321 0.6785744  0.21558821]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 232s 161us/step - loss: 0.2403 - acc: 0.5816 - val_loss: 0.2397 - val_acc: 0.5841\n",
      ". theta fit =  [0.13508321 0.6785744  0.21558821]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 285s 198us/step - loss: -0.2406 - acc: 0.5845 - val_loss: -0.2417 - val_acc: 0.5841\n",
      ". theta fit =  [0.13272564 0.6762336  0.21327312]\n",
      "Epoch:  2\n",
      "[0.13272564 0.6762336  0.21327312]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 232s 161us/step - loss: 0.2414 - acc: 0.5853 - val_loss: 0.2410 - val_acc: 0.5869\n",
      ". theta fit =  [0.13272564 0.6762336  0.21327312]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 293s 204us/step - loss: -0.2420 - acc: 0.5872 - val_loss: -0.2430 - val_acc: 0.5869\n",
      ". theta fit =  [0.1302209  0.6737608  0.21084839]\n",
      "Epoch:  3\n",
      "[0.1302209  0.6737608  0.21084839]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 243s 169us/step - loss: 0.2430 - acc: 0.5863 - val_loss: 0.2431 - val_acc: 0.5871\n",
      ". theta fit =  [0.1302209  0.6737608  0.21084839]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 296s 206us/step - loss: -0.2440 - acc: 0.5874 - val_loss: -0.2450 - val_acc: 0.5871\n",
      ". theta fit =  [0.12776585 0.6713649  0.20840944]\n",
      "Epoch:  4\n",
      "[0.12776585 0.6713649  0.20840944]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 242s 168us/step - loss: 0.2446 - acc: 0.5868 - val_loss: 0.2448 - val_acc: 0.5847\n",
      ". theta fit =  [0.12776585 0.6713649  0.20840944]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 298s 207us/step - loss: -0.2455 - acc: 0.5849 - val_loss: -0.2462 - val_acc: 0.5847\n",
      ". theta fit =  [0.12538983 0.6691409  0.20594935]\n",
      "Epoch:  5\n",
      "[0.12538983 0.6691409  0.20594935]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 243s 169us/step - loss: 0.2458 - acc: 0.5863 - val_loss: 0.2459 - val_acc: 0.5881\n",
      ". theta fit =  [0.12538983 0.6691409  0.20594935]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 294s 204us/step - loss: -0.2463 - acc: 0.5884 - val_loss: -0.2469 - val_acc: 0.5881\n",
      ". theta fit =  [0.12307699 0.6673524  0.20347552]\n",
      "Epoch:  6\n",
      "[0.12307699 0.6673524  0.20347552]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 243s 169us/step - loss: 0.2466 - acc: 0.5860 - val_loss: 0.2467 - val_acc: 0.5858\n",
      ". theta fit =  [0.12307699 0.6673524  0.20347552]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 296s 205us/step - loss: -0.2469 - acc: 0.5866 - val_loss: -0.2474 - val_acc: 0.5858\n",
      ". theta fit =  [0.12084127 0.6668276  0.20099679]\n",
      "Epoch:  7\n",
      "[0.12084127 0.6668276  0.20099679]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 232s 161us/step - loss: 0.2472 - acc: 0.5848 - val_loss: 0.2473 - val_acc: 0.5845\n",
      ". theta fit =  [0.12084127 0.6668276  0.20099679]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 288s 200us/step - loss: -0.2474 - acc: 0.5845 - val_loss: -0.2478 - val_acc: 0.5845\n",
      ". theta fit =  [0.11885726 0.66815305 0.19856857]\n",
      "Epoch:  8\n",
      "[0.11885726 0.66815305 0.19856857]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 232s 161us/step - loss: 0.2476 - acc: 0.5829 - val_loss: 0.2477 - val_acc: 0.5848\n",
      ". theta fit =  [0.11885726 0.66815305 0.19856857]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 279s 194us/step - loss: -0.2477 - acc: 0.5849 - val_loss: -0.2481 - val_acc: 0.5848\n",
      ". theta fit =  [0.11690525 0.6697192  0.1961444 ]\n",
      "Epoch:  9\n",
      "[0.11690525 0.6697192  0.1961444 ]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 239s 166us/step - loss: 0.2479 - acc: 0.5809 - val_loss: 0.2482 - val_acc: 0.5785\n",
      ". theta fit =  [0.11690525 0.6697192  0.1961444 ]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 297s 207us/step - loss: -0.2481 - acc: 0.5791 - val_loss: -0.2485 - val_acc: 0.5785\n",
      ". theta fit =  [0.11549652 0.6717901  0.19378638]\n",
      "Epoch:  10\n",
      "[0.11549652 0.6717901  0.19378638]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 242s 168us/step - loss: 0.2482 - acc: 0.5776 - val_loss: 0.2484 - val_acc: 0.5745\n",
      ". theta fit =  [0.11549652 0.6717901  0.19378638]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 296s 206us/step - loss: -0.2482 - acc: 0.5757 - val_loss: -0.2487 - val_acc: 0.5745\n",
      ". theta fit =  [0.1139736  0.6738289  0.19135417]\n",
      "Epoch:  11\n",
      "[0.1139736  0.6738289  0.19135417]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 241s 167us/step - loss: 0.2484 - acc: 0.5742 - val_loss: 0.2487 - val_acc: 0.5798\n",
      ". theta fit =  [0.1139736  0.6738289  0.19135417]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 291s 202us/step - loss: -0.2486 - acc: 0.5803 - val_loss: -0.2491 - val_acc: 0.5798\n",
      ". theta fit =  [0.11222259 0.6759843  0.18894993]\n",
      "Epoch:  12\n",
      "[0.11222259 0.6759843  0.18894993]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 241s 167us/step - loss: 0.2486 - acc: 0.5684 - val_loss: 0.2487 - val_acc: 0.5763\n",
      ". theta fit =  [0.11222259 0.6759843  0.18894993]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 295s 205us/step - loss: -0.2486 - acc: 0.5774 - val_loss: -0.2490 - val_acc: 0.5763\n",
      ". theta fit =  [0.11078836 0.6782363  0.186579  ]\n",
      "Epoch:  13\n",
      "[0.11078836 0.6782363  0.186579  ]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 238s 165us/step - loss: 0.2488 - acc: 0.5594 - val_loss: 0.2490 - val_acc: 0.5499\n",
      ". theta fit =  [0.11078836 0.6782363  0.186579  ]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 291s 202us/step - loss: -0.2489 - acc: 0.5514 - val_loss: -0.2492 - val_acc: 0.5499\n",
      ". theta fit =  [0.11133295 0.6805637  0.1841914 ]\n",
      "Epoch:  14\n",
      "[0.11133295 0.6805637  0.1841914 ]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 240s 167us/step - loss: 0.2489 - acc: 0.5614 - val_loss: 0.2491 - val_acc: 0.5657\n",
      ". theta fit =  [0.11133295 0.6805637  0.1841914 ]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 206s 143us/step - loss: -0.2488 - acc: 0.5678 - val_loss: -0.2493 - val_acc: 0.5657\n",
      ". theta fit =  [0.11035521 0.68287146 0.18181613]\n",
      "Epoch:  15\n",
      "[0.11035521 0.68287146 0.18181613]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 161s 112us/step - loss: 0.2490 - acc: 0.5536 - val_loss: 0.2492 - val_acc: 0.5549\n",
      ". theta fit =  [0.11035521 0.68287146 0.18181613]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 200s 139us/step - loss: -0.2489 - acc: 0.5562 - val_loss: -0.2494 - val_acc: 0.5549\n",
      ". theta fit =  [0.10989808 0.68517196 0.17942458]\n",
      "Epoch:  16\n",
      "[0.10989808 0.68517196 0.17942458]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2491 - acc: 0.5505 - val_loss: 0.2495 - val_acc: 0.5346\n",
      ". theta fit =  [0.10989808 0.68517196 0.17942458]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 198s 138us/step - loss: -0.2491 - acc: 0.5377 - val_loss: -0.2497 - val_acc: 0.5346\n",
      ". theta fit =  [0.11031766 0.6874948  0.17715321]\n",
      "Epoch:  17\n",
      "[0.11031766 0.6874948  0.17715321]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 109us/step - loss: 0.2493 - acc: 0.5494 - val_loss: 0.2495 - val_acc: 0.5489\n",
      ". theta fit =  [0.11031766 0.6874948  0.17715321]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2491 - acc: 0.5516 - val_loss: -0.2497 - val_acc: 0.5489\n",
      ". theta fit =  [0.11013094 0.6898378  0.1748329 ]\n",
      "Epoch:  18\n",
      "[0.11013094 0.6898378  0.1748329 ]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2493 - acc: 0.5471 - val_loss: 0.2500 - val_acc: 0.5519\n",
      ". theta fit =  [0.11013094 0.6898378  0.1748329 ]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2495 - acc: 0.5547 - val_loss: -0.2502 - val_acc: 0.5519\n",
      ". theta fit =  [0.10974664 0.69218796 0.17259866]\n",
      "Epoch:  19\n",
      "[0.10974664 0.69218796 0.17259866]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2494 - acc: 0.5436 - val_loss: 0.2498 - val_acc: 0.5414\n",
      ". theta fit =  [0.10974664 0.69218796 0.17259866]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2493 - acc: 0.5451 - val_loss: -0.2500 - val_acc: 0.5414\n",
      ". theta fit =  [0.10920308 0.69453627 0.1703763 ]\n",
      "Epoch:  20\n",
      "[0.10920308 0.69453627 0.1703763 ]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2495 - acc: 0.5395 - val_loss: 0.2502 - val_acc: 0.5355\n",
      ". theta fit =  [0.10920308 0.69453627 0.1703763 ]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2496 - acc: 0.5397 - val_loss: -0.2503 - val_acc: 0.5355\n",
      ". theta fit =  [0.10905446 0.69690776 0.16828673]\n",
      "Epoch:  21\n",
      "[0.10905446 0.69690776 0.16828673]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2496 - acc: 0.5357 - val_loss: 0.2502 - val_acc: 0.5228\n",
      ". theta fit =  [0.10905446 0.69690776 0.16828673]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 136us/step - loss: -0.2495 - acc: 0.5285 - val_loss: -0.2503 - val_acc: 0.5228\n",
      ". theta fit =  [0.10926203 0.69928455 0.16618018]\n",
      "Epoch:  22\n",
      "[0.10926203 0.69928455 0.16618018]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2497 - acc: 0.5359 - val_loss: 0.2503 - val_acc: 0.5326\n",
      ". theta fit =  [0.10926203 0.69928455 0.16618018]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2495 - acc: 0.5380 - val_loss: -0.2504 - val_acc: 0.5326\n",
      ". theta fit =  [0.10925491 0.7016416  0.16398746]\n",
      "Epoch:  23\n",
      "[0.10925491 0.7016416  0.16398746]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2497 - acc: 0.5336 - val_loss: 0.2505 - val_acc: 0.5209\n",
      ". theta fit =  [0.10925491 0.7016416  0.16398746]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2496 - acc: 0.5267 - val_loss: -0.2506 - val_acc: 0.5209\n",
      ". theta fit =  [0.10971641 0.7040074  0.16189456]\n",
      "Epoch:  24\n",
      "[0.10971641 0.7040074  0.16189456]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2498 - acc: 0.5339 - val_loss: 0.2505 - val_acc: 0.5373\n",
      ". theta fit =  [0.10971641 0.7040074  0.16189456]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2496 - acc: 0.5431 - val_loss: -0.2506 - val_acc: 0.5373\n",
      ". theta fit =  [0.10916551 0.70637417 0.15983187]\n",
      "Epoch:  25\n",
      "[0.10916551 0.70637417 0.15983187]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 159s 110us/step - loss: 0.2498 - acc: 0.5295 - val_loss: 0.2507 - val_acc: 0.5322\n",
      ". theta fit =  [0.10916551 0.70637417 0.15983187]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2496 - acc: 0.5393 - val_loss: -0.2508 - val_acc: 0.5322\n",
      ". theta fit =  [0.10842535 0.70873106 0.15808176]\n",
      "Epoch:  26\n",
      "[0.10842535 0.70873106 0.15808176]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2498 - acc: 0.5244 - val_loss: 0.2508 - val_acc: 0.5245\n",
      ". theta fit =  [0.10842535 0.70873106 0.15808176]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2497 - acc: 0.5319 - val_loss: -0.2509 - val_acc: 0.5245\n",
      ". theta fit =  [0.10830743 0.711083   0.15611254]\n",
      "Epoch:  27\n",
      "[0.10830743 0.711083   0.15611254]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 146s 102us/step - loss: 0.2498 - acc: 0.5220 - val_loss: 0.2508 - val_acc: 0.5137\n",
      ". theta fit =  [0.10830743 0.711083   0.15611254]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2498 - acc: 0.5221 - val_loss: -0.2509 - val_acc: 0.5137\n",
      ". theta fit =  [0.10811389 0.7134785  0.15497491]\n",
      "Epoch:  28\n",
      "[0.10811389 0.7134785  0.15497491]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2499 - acc: 0.5209 - val_loss: 0.2509 - val_acc: 0.5185\n",
      ". theta fit =  [0.10811389 0.7134785  0.15497491]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2497 - acc: 0.5285 - val_loss: -0.2510 - val_acc: 0.5185\n",
      ". theta fit =  [0.1075061  0.7158437  0.15367714]\n",
      "Epoch:  29\n",
      "[0.1075061  0.7158437  0.15367714]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2498 - acc: 0.5177 - val_loss: 0.2510 - val_acc: 0.5093\n",
      ". theta fit =  [0.1075061  0.7158437  0.15367714]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2496 - acc: 0.5182 - val_loss: -0.2511 - val_acc: 0.5093\n",
      ". theta fit =  [0.10821744 0.7182173  0.15216094]\n",
      "Epoch:  30\n",
      "[0.10821744 0.7182173  0.15216094]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2498 - acc: 0.5188 - val_loss: 0.2512 - val_acc: 0.5093\n",
      ". theta fit =  [0.10821744 0.7182173  0.15216094]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 128us/step - loss: -0.2496 - acc: 0.5197 - val_loss: -0.2512 - val_acc: 0.5093\n",
      ". theta fit =  [0.10826866 0.7205886  0.15124524]\n",
      "Epoch:  31\n",
      "[0.10826866 0.7205886  0.15124524]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2498 - acc: 0.5182 - val_loss: 0.2511 - val_acc: 0.5117\n",
      ". theta fit =  [0.10826866 0.7205886  0.15124524]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 190s 132us/step - loss: -0.2496 - acc: 0.5223 - val_loss: -0.2512 - val_acc: 0.5117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  [0.1077725  0.7229719  0.15037921]\n",
      "Epoch:  32\n",
      "[0.1077725  0.7229719  0.15037921]\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2498 - acc: 0.5168 - val_loss: 0.2513 - val_acc: 0.5088\n",
      ". theta fit =  [0.1077725  0.7229719  0.15037921]\n",
      "<tf.Variable 'thetaX:0' shape=(3,) dtype=float32_ref>\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1439000/1440000 [============================>.] - ETA: 0s - loss: -0.2495 - acc: 0.5207"
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=(3,),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 1e-6 #smaller learning rate yields better precision\n",
    "epochs = 200 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "        \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "    print(theta0)   \n",
    "    theta_prime = theta0\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0,\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.12, 0, len(fit_vals), label = 'alphaS Truth')\n",
    "plt.hlines(0.6, 0, len(fit_vals), label = 'aLund Truth')\n",
    "plt.hlines(0.12, 0, len(fit_vals), label = 'probStoUD Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"3D_fit \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"3D_fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
