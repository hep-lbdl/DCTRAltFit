{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses the original DCTR model (i.e. model expects “empty particles” to have 0 in every entry). As opposerd to the, modified DCTR that was trained to allow non-zero inputs for theta on empty particles, the fit appears less noisy and more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small = X_train[0:int(0.8*10**5)]\n",
    "Y_train_small = Y_train[0:int(0.8*10**5)]\n",
    "X_test_small = X_test[0:int(0.2*10**5)]\n",
    "Y_test_small = Y_test[0:int(0.2*10**5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_probStoUD.h5') #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, val]\n",
    "    \n",
    "    # zip theta_prime to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0, #checks if pT != 0, which means we have a particle\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-16-106adffad1cd>:19: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      " 137000/1440000 [=>............................] - ETA: 3:56 - loss: 0.2586 - acc: 0.5080"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 33) #iterating across possible alphaS values\n",
    "vlvals = []\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas,lvals, label = lvals)\n",
    "plt.plot(thetas,vlvals, label = vlvals)\n",
    "plt.vlines(, ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"MSE for alphaS altFit SUCCESS, proper method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.217\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_14[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch:  0\n",
      "WARNING:tensorflow:From <ipython-input-15-24940b61cb9f>:41: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training g\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2327 - acc: 0.5773 - val_loss: 0.2298 - val_acc: 0.5810\n",
      ". theta fit =  0.12\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2301 - acc: 0.5822 - val_loss: -0.2304 - val_acc: 0.5810\n",
      ". theta fit =  0.12071699\n",
      "Epoch:  1\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2305 - acc: 0.5816 - val_loss: 0.2299 - val_acc: 0.5829\n",
      ". theta fit =  0.12071699\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2305 - acc: 0.5837 - val_loss: -0.2309 - val_acc: 0.5829\n",
      ". theta fit =  0.12187422\n",
      "Epoch:  2\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 109us/step - loss: 0.2306 - acc: 0.5834 - val_loss: 0.2304 - val_acc: 0.5822\n",
      ". theta fit =  0.12187422\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2308 - acc: 0.5833 - val_loss: -0.2315 - val_acc: 0.5822\n",
      ". theta fit =  0.12310385\n",
      "Epoch:  3\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2312 - acc: 0.5843 - val_loss: 0.2308 - val_acc: 0.5837\n",
      ". theta fit =  0.12310385\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 196s 136us/step - loss: -0.2312 - acc: 0.5854 - val_loss: -0.2319 - val_acc: 0.5837\n",
      ". theta fit =  0.12435214\n",
      "Epoch:  4\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2320 - acc: 0.5847 - val_loss: 0.2325 - val_acc: 0.5829\n",
      ". theta fit =  0.12435214\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2327 - acc: 0.5843 - val_loss: -0.2334 - val_acc: 0.5829\n",
      ". theta fit =  0.12559955\n",
      "Epoch:  5\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2329 - acc: 0.5852 - val_loss: 0.2330 - val_acc: 0.5837\n",
      ". theta fit =  0.12559955\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2332 - acc: 0.5856 - val_loss: -0.2340 - val_acc: 0.5837\n",
      ". theta fit =  0.12685455\n",
      "Epoch:  6\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 160s 111us/step - loss: 0.2337 - acc: 0.5857 - val_loss: 0.2350 - val_acc: 0.5799\n",
      ". theta fit =  0.12685455\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 136us/step - loss: -0.2351 - acc: 0.5822 - val_loss: -0.2360 - val_acc: 0.5799\n",
      ". theta fit =  0.12809907\n",
      "Epoch:  7\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2345 - acc: 0.5858 - val_loss: 0.2347 - val_acc: 0.5844\n",
      ". theta fit =  0.12809907\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 197s 137us/step - loss: -0.2348 - acc: 0.5866 - val_loss: -0.2357 - val_acc: 0.5844\n",
      ". theta fit =  0.12934646\n",
      "Epoch:  8\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2354 - acc: 0.5858 - val_loss: 0.2355 - val_acc: 0.5848\n",
      ". theta fit =  0.12934646\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 195s 135us/step - loss: -0.2356 - acc: 0.5872 - val_loss: -0.2364 - val_acc: 0.5848\n",
      ". theta fit =  0.1305986\n",
      "Epoch:  9\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2362 - acc: 0.5861 - val_loss: 0.2365 - val_acc: 0.5846\n",
      ". theta fit =  0.1305986\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 194s 135us/step - loss: -0.2366 - acc: 0.5863 - val_loss: -0.2374 - val_acc: 0.5846\n",
      ". theta fit =  0.13185368\n",
      "Epoch:  10\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2371 - acc: 0.5858 - val_loss: 0.2375 - val_acc: 0.5836\n",
      ". theta fit =  0.13185368\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 193s 134us/step - loss: -0.2375 - acc: 0.5863 - val_loss: -0.2383 - val_acc: 0.5836\n",
      ". theta fit =  0.13311931\n",
      "Epoch:  11\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2379 - acc: 0.5865 - val_loss: 0.2383 - val_acc: 0.5847\n",
      ". theta fit =  0.13311931\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 192s 133us/step - loss: -0.2383 - acc: 0.5871 - val_loss: -0.2392 - val_acc: 0.5847\n",
      ". theta fit =  0.13439102\n",
      "Epoch:  12\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2387 - acc: 0.5864 - val_loss: 0.2391 - val_acc: 0.5847\n",
      ". theta fit =  0.13439102\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 192s 134us/step - loss: -0.2392 - acc: 0.5866 - val_loss: -0.2402 - val_acc: 0.5847\n",
      ". theta fit =  0.13566406\n",
      "Epoch:  13\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2397 - acc: 0.5866 - val_loss: 0.2401 - val_acc: 0.5838\n",
      ". theta fit =  0.13566406\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 191s 132us/step - loss: -0.2401 - acc: 0.5866 - val_loss: -0.2412 - val_acc: 0.5838\n",
      ". theta fit =  0.13693102\n",
      "Epoch:  14\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2406 - acc: 0.5865 - val_loss: 0.2411 - val_acc: 0.5854\n",
      ". theta fit =  0.13693102\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 191s 133us/step - loss: -0.2411 - acc: 0.5880 - val_loss: -0.2422 - val_acc: 0.5854\n",
      ". theta fit =  0.1381829\n",
      "Epoch:  15\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2416 - acc: 0.5867 - val_loss: 0.2424 - val_acc: 0.5851\n",
      ". theta fit =  0.1381829\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 191s 133us/step - loss: -0.2422 - acc: 0.5877 - val_loss: -0.2435 - val_acc: 0.5851\n",
      ". theta fit =  0.13942409\n",
      "Epoch:  16\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2425 - acc: 0.5866 - val_loss: 0.2432 - val_acc: 0.5840\n",
      ". theta fit =  0.13942409\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2430 - acc: 0.5875 - val_loss: -0.2441 - val_acc: 0.5840\n",
      ". theta fit =  0.14064932\n",
      "Epoch:  17\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 104us/step - loss: 0.2434 - acc: 0.5864 - val_loss: 0.2440 - val_acc: 0.5839\n",
      ". theta fit =  0.14064932\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2438 - acc: 0.5868 - val_loss: -0.2449 - val_acc: 0.5839\n",
      ". theta fit =  0.14189446\n",
      "Epoch:  18\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2441 - acc: 0.5864 - val_loss: 0.2448 - val_acc: 0.5832\n",
      ". theta fit =  0.14189446\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2446 - acc: 0.5864 - val_loss: -0.2456 - val_acc: 0.5832\n",
      ". theta fit =  0.14313978\n",
      "Epoch:  19\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 104us/step - loss: 0.2448 - acc: 0.5864 - val_loss: 0.2455 - val_acc: 0.5832\n",
      ". theta fit =  0.14313978\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2451 - acc: 0.5878 - val_loss: -0.2462 - val_acc: 0.5832\n",
      ". theta fit =  0.14437863\n",
      "Epoch:  20\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2455 - acc: 0.5858 - val_loss: 0.2461 - val_acc: 0.5829\n",
      ". theta fit =  0.14437863\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2458 - acc: 0.5864 - val_loss: -0.2469 - val_acc: 0.5829\n",
      ". theta fit =  0.14561895\n",
      "Epoch:  21\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2461 - acc: 0.5850 - val_loss: 0.2468 - val_acc: 0.5823\n",
      ". theta fit =  0.14561895\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2464 - acc: 0.5865 - val_loss: -0.2475 - val_acc: 0.5823\n",
      ". theta fit =  0.14685352\n",
      "Epoch:  22\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2468 - acc: 0.5847 - val_loss: 0.2473 - val_acc: 0.5823\n",
      ". theta fit =  0.14685352\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2469 - acc: 0.5868 - val_loss: -0.2480 - val_acc: 0.5823\n",
      ". theta fit =  0.14808282\n",
      "Epoch:  23\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2473 - acc: 0.5839 - val_loss: 0.2480 - val_acc: 0.5795\n",
      ". theta fit =  0.14808282\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2474 - acc: 0.5854 - val_loss: -0.2485 - val_acc: 0.5795\n",
      ". theta fit =  0.14928076\n",
      "Epoch:  24\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2478 - acc: 0.5834 - val_loss: 0.2484 - val_acc: 0.5805\n",
      ". theta fit =  0.14928076\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2479 - acc: 0.5857 - val_loss: -0.2489 - val_acc: 0.5805\n",
      ". theta fit =  0.15047863\n",
      "Epoch:  25\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2482 - acc: 0.5815 - val_loss: 0.2490 - val_acc: 0.5801\n",
      ". theta fit =  0.15047863\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2484 - acc: 0.5857 - val_loss: -0.2495 - val_acc: 0.5801\n",
      ". theta fit =  0.15168029\n",
      "Epoch:  26\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2486 - acc: 0.5794 - val_loss: 0.2494 - val_acc: 0.5754\n",
      ". theta fit =  0.15168029\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2488 - acc: 0.5824 - val_loss: -0.2498 - val_acc: 0.5754\n",
      ". theta fit =  0.15285657\n",
      "Epoch:  27\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2489 - acc: 0.5768 - val_loss: 0.2496 - val_acc: 0.5738\n",
      ". theta fit =  0.15285657\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2489 - acc: 0.5813 - val_loss: -0.2499 - val_acc: 0.5738\n",
      ". theta fit =  0.15401563\n",
      "Epoch:  28\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2492 - acc: 0.5728 - val_loss: 0.2500 - val_acc: 0.5639\n",
      ". theta fit =  0.15401563\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2492 - acc: 0.5721 - val_loss: -0.2502 - val_acc: 0.5639\n",
      ". theta fit =  0.1551214\n",
      "Epoch:  29\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2494 - acc: 0.5682 - val_loss: 0.2503 - val_acc: 0.5603\n",
      ". theta fit =  0.1551214\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2494 - acc: 0.5678 - val_loss: -0.2505 - val_acc: 0.5603\n",
      ". theta fit =  0.15621948\n",
      "Epoch:  30\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2495 - acc: 0.5618 - val_loss: 0.2504 - val_acc: 0.5543\n",
      ". theta fit =  0.15621948\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2495 - acc: 0.5645 - val_loss: -0.2505 - val_acc: 0.5543\n",
      ". theta fit =  0.15722224\n",
      "Epoch:  31\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2496 - acc: 0.5567 - val_loss: 0.2504 - val_acc: 0.5507\n",
      ". theta fit =  0.15722224\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2494 - acc: 0.5622 - val_loss: -0.2506 - val_acc: 0.5507\n",
      ". theta fit =  0.15813297\n",
      "Epoch:  32\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2496 - acc: 0.5468 - val_loss: 0.2505 - val_acc: 0.5436\n",
      ". theta fit =  0.15813297\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 190s 132us/step - loss: -0.2494 - acc: 0.5559 - val_loss: -0.2506 - val_acc: 0.5436\n",
      ". theta fit =  0.15890452\n",
      "Epoch:  33\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2496 - acc: 0.5406 - val_loss: 0.2506 - val_acc: 0.5382\n",
      ". theta fit =  0.15890452\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2494 - acc: 0.5515 - val_loss: -0.2507 - val_acc: 0.5382\n",
      ". theta fit =  0.1596066\n",
      "Epoch:  34\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2495 - acc: 0.5339 - val_loss: 0.2507 - val_acc: 0.5109\n",
      ". theta fit =  0.1596066\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 190s 132us/step - loss: -0.2495 - acc: 0.5224 - val_loss: -0.2507 - val_acc: 0.5109\n",
      ". theta fit =  0.15936692\n",
      "Epoch:  35\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2495 - acc: 0.5348 - val_loss: 0.2508 - val_acc: 0.5190\n",
      ". theta fit =  0.15936692\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2493 - acc: 0.5346 - val_loss: -0.2508 - val_acc: 0.5190\n",
      ". theta fit =  0.1596653\n",
      "Epoch:  36\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2494 - acc: 0.5314 - val_loss: 0.2509 - val_acc: 0.5205\n",
      ". theta fit =  0.1596653\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2490 - acc: 0.5369 - val_loss: -0.2509 - val_acc: 0.5205\n",
      ". theta fit =  0.15987948\n",
      "Epoch:  37\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2493 - acc: 0.5298 - val_loss: 0.2508 - val_acc: 0.5186\n",
      ". theta fit =  0.15987948\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2492 - acc: 0.5341 - val_loss: -0.2508 - val_acc: 0.5186\n",
      ". theta fit =  0.16013739\n",
      "Epoch:  38\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2493 - acc: 0.5274 - val_loss: 0.2509 - val_acc: 0.5219\n",
      ". theta fit =  0.16013739\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2488 - acc: 0.5383 - val_loss: -0.2509 - val_acc: 0.5219\n",
      ". theta fit =  0.16033578\n",
      "Epoch:  39\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2492 - acc: 0.5270 - val_loss: 0.2511 - val_acc: 0.5275\n",
      ". theta fit =  0.16033578\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2489 - acc: 0.5440 - val_loss: -0.2511 - val_acc: 0.5275\n",
      ". theta fit =  0.16058363\n",
      "Epoch:  40\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2491 - acc: 0.5256 - val_loss: 0.2507 - val_acc: 0.5160\n",
      ". theta fit =  0.16058363\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2489 - acc: 0.5323 - val_loss: -0.2507 - val_acc: 0.5160\n",
      ". theta fit =  0.16059904\n",
      "Epoch:  41\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2490 - acc: 0.5243 - val_loss: 0.2509 - val_acc: 0.5201\n",
      ". theta fit =  0.16059904\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2487 - acc: 0.5378 - val_loss: -0.2509 - val_acc: 0.5201\n",
      ". theta fit =  0.160622\n",
      "Epoch:  42\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2489 - acc: 0.5264 - val_loss: 0.2511 - val_acc: 0.5176\n",
      ". theta fit =  0.160622\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2488 - acc: 0.5362 - val_loss: -0.2511 - val_acc: 0.5176\n",
      ". theta fit =  0.1604888\n",
      "Epoch:  43\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2489 - acc: 0.5262 - val_loss: 0.2511 - val_acc: 0.5028\n",
      ". theta fit =  0.1604888\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2484 - acc: 0.5223 - val_loss: -0.2511 - val_acc: 0.5028\n",
      ". theta fit =  0.159978\n",
      "Epoch:  44\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2488 - acc: 0.5293 - val_loss: 0.2512 - val_acc: 0.5117\n",
      ". theta fit =  0.159978\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2486 - acc: 0.5292 - val_loss: -0.2512 - val_acc: 0.5117\n",
      ". theta fit =  0.1597957\n",
      "Epoch:  45\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2487 - acc: 0.5325 - val_loss: 0.2513 - val_acc: 0.5152\n",
      ". theta fit =  0.1597957\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2484 - acc: 0.5359 - val_loss: -0.2513 - val_acc: 0.5152\n",
      ". theta fit =  0.15998474\n",
      "Epoch:  46\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2486 - acc: 0.5317 - val_loss: 0.2513 - val_acc: 0.5019\n",
      ". theta fit =  0.15998474\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2483 - acc: 0.5235 - val_loss: -0.2513 - val_acc: 0.5019\n",
      ". theta fit =  0.15967296\n",
      "Epoch:  47\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2485 - acc: 0.5346 - val_loss: 0.2514 - val_acc: 0.5151\n",
      ". theta fit =  0.15967296\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2481 - acc: 0.5380 - val_loss: -0.2514 - val_acc: 0.5151\n",
      ". theta fit =  0.15960507\n",
      "Epoch:  48\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2484 - acc: 0.5363 - val_loss: 0.2512 - val_acc: 0.5200\n",
      ". theta fit =  0.15960507\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2479 - acc: 0.5429 - val_loss: -0.2512 - val_acc: 0.5200\n",
      ". theta fit =  0.15965728\n",
      "Epoch:  49\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2483 - acc: 0.5348 - val_loss: 0.2518 - val_acc: 0.5268\n",
      ". theta fit =  0.15965728\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2481 - acc: 0.5489 - val_loss: -0.2518 - val_acc: 0.5268\n",
      ". theta fit =  0.16015965\n",
      "Epoch:  50\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2482 - acc: 0.5322 - val_loss: 0.2515 - val_acc: 0.5084\n",
      ". theta fit =  0.16015965\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2480 - acc: 0.5307 - val_loss: -0.2515 - val_acc: 0.5084\n",
      ". theta fit =  0.15959324\n",
      "Epoch:  51\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2482 - acc: 0.5361 - val_loss: 0.2521 - val_acc: 0.5154\n",
      ". theta fit =  0.15959324\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 179s 125us/step - loss: -0.2477 - acc: 0.5401 - val_loss: -0.2521 - val_acc: 0.5154\n",
      ". theta fit =  0.159684\n",
      "Epoch:  52\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2480 - acc: 0.5369 - val_loss: 0.2515 - val_acc: 0.5098\n",
      ". theta fit =  0.159684\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2480 - acc: 0.5321 - val_loss: -0.2515 - val_acc: 0.5098\n",
      ". theta fit =  0.159615\n",
      "Epoch:  53\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2480 - acc: 0.5372 - val_loss: 0.2516 - val_acc: 0.5234\n",
      ". theta fit =  0.159615\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 180s 125us/step - loss: -0.2477 - acc: 0.5489 - val_loss: -0.2516 - val_acc: 0.5234\n",
      ". theta fit =  0.15991238\n",
      "Epoch:  54\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2479 - acc: 0.5361 - val_loss: 0.2518 - val_acc: 0.5053\n",
      ". theta fit =  0.15991238\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2476 - acc: 0.5317 - val_loss: -0.2518 - val_acc: 0.5053\n",
      ". theta fit =  0.15974288\n",
      "Epoch:  55\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2478 - acc: 0.5373 - val_loss: 0.2519 - val_acc: 0.5169\n",
      ". theta fit =  0.15974288\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 128us/step - loss: -0.2473 - acc: 0.5433 - val_loss: -0.2519 - val_acc: 0.5169\n",
      ". theta fit =  0.15986218\n",
      "Epoch:  56\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2477 - acc: 0.5376 - val_loss: 0.2519 - val_acc: 0.5071\n",
      ". theta fit =  0.15986218\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2474 - acc: 0.5342 - val_loss: -0.2519 - val_acc: 0.5071\n",
      ". theta fit =  0.15937963\n",
      "Epoch:  57\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2476 - acc: 0.5413 - val_loss: 0.2520 - val_acc: 0.5172\n",
      ". theta fit =  0.15937963\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 127us/step - loss: -0.2474 - acc: 0.5432 - val_loss: -0.2520 - val_acc: 0.5172\n",
      ". theta fit =  0.15962008\n",
      "Epoch:  58\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2475 - acc: 0.5397 - val_loss: 0.2521 - val_acc: 0.5136\n",
      ". theta fit =  0.15962008\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 182s 127us/step - loss: -0.2470 - acc: 0.5425 - val_loss: -0.2521 - val_acc: 0.5136\n",
      ". theta fit =  0.15948965\n",
      "Epoch:  59\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2474 - acc: 0.5419 - val_loss: 0.2524 - val_acc: 0.5129\n",
      ". theta fit =  0.15948965\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2467 - acc: 0.5421 - val_loss: -0.2524 - val_acc: 0.5129\n",
      ". theta fit =  0.15926488\n",
      "Epoch:  60\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2473 - acc: 0.5446 - val_loss: 0.2527 - val_acc: 0.5242\n",
      ". theta fit =  0.15926488\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 127us/step - loss: -0.2465 - acc: 0.5549 - val_loss: -0.2527 - val_acc: 0.5242\n",
      ". theta fit =  0.15964785\n",
      "Epoch:  61\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 107us/step - loss: 0.2471 - acc: 0.5430 - val_loss: 0.2523 - val_acc: 0.5128\n",
      ". theta fit =  0.15964785\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2466 - acc: 0.5432 - val_loss: -0.2523 - val_acc: 0.5128\n",
      ". theta fit =  0.1595467\n",
      "Epoch:  62\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 107us/step - loss: 0.2471 - acc: 0.5434 - val_loss: 0.2525 - val_acc: 0.5175\n",
      ". theta fit =  0.1595467\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 128us/step - loss: -0.2465 - acc: 0.5482 - val_loss: -0.2525 - val_acc: 0.5175\n",
      ". theta fit =  0.15955894\n",
      "Epoch:  63\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2470 - acc: 0.5433 - val_loss: 0.2528 - val_acc: 0.5183\n",
      ". theta fit =  0.15955894\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2467 - acc: 0.5489 - val_loss: -0.2528 - val_acc: 0.5183\n",
      ". theta fit =  0.15980032\n",
      "Epoch:  64\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2469 - acc: 0.5432 - val_loss: 0.2529 - val_acc: 0.5118\n",
      ". theta fit =  0.15980032\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2464 - acc: 0.5433 - val_loss: -0.2529 - val_acc: 0.5118\n",
      ". theta fit =  0.15968671\n",
      "Epoch:  65\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2468 - acc: 0.5443 - val_loss: 0.2528 - val_acc: 0.5132\n",
      ". theta fit =  0.15968671\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 177s 123us/step - loss: -0.2460 - acc: 0.5457 - val_loss: -0.2528 - val_acc: 0.5132\n",
      ". theta fit =  0.15956043\n",
      "Epoch:  66\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2467 - acc: 0.5458 - val_loss: 0.2525 - val_acc: 0.5151\n",
      ". theta fit =  0.15956043\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 177s 123us/step - loss: -0.2461 - acc: 0.5473 - val_loss: -0.2524 - val_acc: 0.5151\n",
      ". theta fit =  0.15939218\n",
      "Epoch:  67\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2465 - acc: 0.5464 - val_loss: 0.2536 - val_acc: 0.5133\n",
      ". theta fit =  0.15939218\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2461 - acc: 0.5446 - val_loss: -0.2536 - val_acc: 0.5133\n",
      ". theta fit =  0.15928876\n",
      "Epoch:  68\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2465 - acc: 0.5479 - val_loss: 0.2534 - val_acc: 0.5096\n",
      ". theta fit =  0.15928876\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 179s 124us/step - loss: -0.2459 - acc: 0.5440 - val_loss: -0.2534 - val_acc: 0.5096\n",
      ". theta fit =  0.15904684\n",
      "Epoch:  69\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2464 - acc: 0.5486 - val_loss: 0.2534 - val_acc: 0.5142\n",
      ". theta fit =  0.15904684\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 179s 124us/step - loss: -0.2455 - acc: 0.5493 - val_loss: -0.2534 - val_acc: 0.5142\n",
      ". theta fit =  0.15903133\n",
      "Epoch:  70\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2463 - acc: 0.5496 - val_loss: 0.2532 - val_acc: 0.5097\n",
      ". theta fit =  0.15903133\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 182s 126us/step - loss: -0.2458 - acc: 0.5450 - val_loss: -0.2532 - val_acc: 0.5097\n",
      ". theta fit =  0.15880732\n",
      "Epoch:  71\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2462 - acc: 0.5509 - val_loss: 0.2534 - val_acc: 0.5215\n",
      ". theta fit =  0.15880732\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 179s 125us/step - loss: -0.2456 - acc: 0.5570 - val_loss: -0.2534 - val_acc: 0.5215 -0.2456\n",
      ". theta fit =  0.15918052\n",
      "Epoch:  72\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2461 - acc: 0.5491 - val_loss: 0.2537 - val_acc: 0.5163\n",
      ". theta fit =  0.15918052\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 128us/step - loss: -0.2454 - acc: 0.5524 - val_loss: -0.2537 - val_acc: 0.5163\n",
      ". theta fit =  0.15923464\n",
      "Epoch:  73\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 107us/step - loss: 0.2460 - acc: 0.5497 - val_loss: 0.2539 - val_acc: 0.5129\n",
      ". theta fit =  0.15923464\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 180s 125us/step - loss: -0.2455 - acc: 0.5484 - val_loss: -0.2538 - val_acc: 0.5129\n",
      ". theta fit =  0.1590381\n",
      "Epoch:  74\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2459 - acc: 0.5513 - val_loss: 0.2537 - val_acc: 0.5215\n",
      ". theta fit =  0.1590381\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 125us/step - loss: -0.2451 - acc: 0.5596 - val_loss: -0.2537 - val_acc: 0.5215\n",
      ". theta fit =  0.15939511\n",
      "Epoch:  75\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2458 - acc: 0.5508 - val_loss: 0.2536 - val_acc: 0.5151\n",
      ". theta fit =  0.15939511\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 181s 126us/step - loss: -0.2454 - acc: 0.5509 - val_loss: -0.2536 - val_acc: 0.5151\n",
      ". theta fit =  0.15934335\n",
      "Epoch:  76\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 107us/step - loss: 0.2457 - acc: 0.5499 - val_loss: 0.2541 - val_acc: 0.5189\n",
      ". theta fit =  0.15934335\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 182s 126us/step - loss: -0.2451 - acc: 0.5543 - val_loss: -0.2541 - val_acc: 0.5189\n",
      ". theta fit =  0.15923668\n",
      "Epoch:  77\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2456 - acc: 0.5523 - val_loss: 0.2543 - val_acc: 0.5128\n",
      ". theta fit =  0.15923668\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2447 - acc: 0.5512 - val_loss: -0.2543 - val_acc: 0.5128\n",
      ". theta fit =  0.15912984\n",
      "Epoch:  78\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2454 - acc: 0.5531 - val_loss: 0.2534 - val_acc: 0.5161\n",
      ". theta fit =  0.15912984\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2450 - acc: 0.5534 - val_loss: -0.2534 - val_acc: 0.5161\n",
      ". theta fit =  0.15914768\n",
      "Epoch:  79\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2454 - acc: 0.5526 - val_loss: 0.2540 - val_acc: 0.5196\n",
      ". theta fit =  0.15914768\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2445 - acc: 0.5581 - val_loss: -0.2540 - val_acc: 0.5196\n",
      ". theta fit =  0.15926658\n",
      "Epoch:  80\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2452 - acc: 0.5520 - val_loss: 0.2543 - val_acc: 0.5175\n",
      ". theta fit =  0.15926658\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 183s 127us/step - loss: -0.2448 - acc: 0.5564 - val_loss: -0.2543 - val_acc: 0.5175\n",
      ". theta fit =  0.15919794\n",
      "Epoch:  81\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 156s 108us/step - loss: 0.2451 - acc: 0.5542 - val_loss: 0.2540 - val_acc: 0.5131\n",
      ". theta fit =  0.15919794\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 183s 127us/step - loss: -0.2446 - acc: 0.5521 - val_loss: -0.2540 - val_acc: 0.5131\n",
      ". theta fit =  0.15908606\n",
      "Epoch:  82\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2451 - acc: 0.5540 - val_loss: 0.2549 - val_acc: 0.5146\n",
      ". theta fit =  0.15908606\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2443 - acc: 0.5557 - val_loss: -0.2549 - val_acc: 0.5146\n",
      ". theta fit =  0.1590661\n",
      "Epoch:  83\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2450 - acc: 0.5549 - val_loss: 0.2553 - val_acc: 0.5177\n",
      ". theta fit =  0.1590661\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2440 - acc: 0.5584 - val_loss: -0.2553 - val_acc: 0.5177\n",
      ". theta fit =  0.15921664\n",
      "Epoch:  84\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2449 - acc: 0.5542 - val_loss: 0.2547 - val_acc: 0.5223\n",
      ". theta fit =  0.15921664\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2441 - acc: 0.5617 - val_loss: -0.2547 - val_acc: 0.5223\n",
      ". theta fit =  0.15943253\n",
      "Epoch:  85\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 109us/step - loss: 0.2447 - acc: 0.5538 - val_loss: 0.2551 - val_acc: 0.5146\n",
      ". theta fit =  0.15943253\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 184s 128us/step - loss: -0.2437 - acc: 0.5565 - val_loss: -0.2551 - val_acc: 0.5146\n",
      ". theta fit =  0.15937848\n",
      "Epoch:  86\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-24940b61cb9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_loss_wrapper_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyinputs_fit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m#Now, fix g and train \\theta.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 5e-7 #smaller learning rate yields better precision\n",
    "epochs = 60 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "        \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "        \n",
    "    theta_prime = [0.1365, 0.68, theta0]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0,\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.16, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"alphaS Fit (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"alphaS Fit (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
