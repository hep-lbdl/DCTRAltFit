{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses the original DCTR model (i.e. model expects “empty particles” to have 0 in every entry). As opposerd to the, modified DCTR that was trained to allow non-zero inputs for theta on empty particles, the fit appears less noisy and more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small = X_train[0:int(0.8*10**5)]\n",
    "Y_train_small = Y_train[0:int(0.8*10**5)]\n",
    "X_test_small = X_test[0:int(0.2*10**5)]\n",
    "Y_test_small = Y_test[0:int(0.2*10**5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_probStoUD.h5') #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, val]\n",
    "    \n",
    "    # zip theta_prime to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0, #checks if pT != 0, which means we have a particle\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-14-106adffad1cd>:19: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 154s 107us/step - loss: 0.2429 - acc: 0.5197 - val_loss: 0.2378 - val_acc: 0.5271\n",
      "trainnig theta = : 0.10625000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2379 - acc: 0.5280 - val_loss: 0.2370 - val_acc: 0.5290\n",
      "trainnig theta = : 0.1125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2380 - acc: 0.5294 - val_loss: 0.2376 - val_acc: 0.5294\n",
      "trainnig theta = : 0.11875000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2388 - acc: 0.5298 - val_loss: 0.2386 - val_acc: 0.5307\n",
      "trainnig theta = : 0.125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2398 - acc: 0.5301 - val_loss: 0.2396 - val_acc: 0.5302\n",
      "trainnig theta = : 0.13125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 146s 101us/step - loss: 0.2408 - acc: 0.5301 - val_loss: 0.2407 - val_acc: 0.5302\n",
      "trainnig theta = : 0.1375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 146s 101us/step - loss: 0.2418 - acc: 0.5302 - val_loss: 0.2419 - val_acc: 0.5306\n",
      "trainnig theta = : 0.14375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2428 - acc: 0.5303 - val_loss: 0.2426 - val_acc: 0.5309\n",
      "trainnig theta = : 0.15\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2438 - acc: 0.5305 - val_loss: 0.2437 - val_acc: 0.5306\n",
      "trainnig theta = : 0.15625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2447 - acc: 0.5305 - val_loss: 0.2446 - val_acc: 0.5305\n",
      "trainnig theta = : 0.1625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2456 - acc: 0.5304 - val_loss: 0.2456 - val_acc: 0.5310\n",
      "trainnig theta = : 0.16875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2463 - acc: 0.5308 - val_loss: 0.2462 - val_acc: 0.5309\n",
      "trainnig theta = : 0.175\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2469 - acc: 0.5309 - val_loss: 0.2469 - val_acc: 0.5309\n",
      "trainnig theta = : 0.18125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2474 - acc: 0.5308 - val_loss: 0.2474 - val_acc: 0.5304\n",
      "trainnig theta = : 0.1875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2479 - acc: 0.5307 - val_loss: 0.2480 - val_acc: 0.5312\n",
      "trainnig theta = : 0.19374999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 146s 101us/step - loss: 0.2484 - acc: 0.5310 - val_loss: 0.2485 - val_acc: 0.5304\n",
      "trainnig theta = : 0.2\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2490 - acc: 0.5309 - val_loss: 0.2490 - val_acc: 0.5307\n",
      "trainnig theta = : 0.20625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 146s 102us/step - loss: 0.2496 - acc: 0.5310 - val_loss: 0.2496 - val_acc: 0.5308\n",
      "trainnig theta = : 0.2125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2501 - acc: 0.5312 - val_loss: 0.2502 - val_acc: 0.5299\n",
      "trainnig theta = : 0.21875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2505 - acc: 0.5310 - val_loss: 0.2506 - val_acc: 0.5305\n",
      "trainnig theta = : 0.22499999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2507 - acc: 0.5311 - val_loss: 0.2508 - val_acc: 0.5299\n",
      "trainnig theta = : 0.23124999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2507 - acc: 0.5303 - val_loss: 0.2509 - val_acc: 0.5293\n",
      "trainnig theta = : 0.2375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2506 - acc: 0.5305 - val_loss: 0.2507 - val_acc: 0.5289\n",
      "trainnig theta = : 0.24375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2505 - acc: 0.5302 - val_loss: 0.2506 - val_acc: 0.5281\n",
      "trainnig theta = : 0.25\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2504 - acc: 0.5290 - val_loss: 0.2505 - val_acc: 0.5281\n",
      "trainnig theta = : 0.25625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2504 - acc: 0.5281 - val_loss: 0.2506 - val_acc: 0.5255\n",
      "trainnig theta = : 0.26249999999999996\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2504 - acc: 0.5263 - val_loss: 0.2506 - val_acc: 0.5241\n",
      "trainnig theta = : 0.26875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2504 - acc: 0.5213 - val_loss: 0.2507 - val_acc: 0.5131\n",
      "trainnig theta = : 0.275\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2505 - acc: 0.5177 - val_loss: 0.2507 - val_acc: 0.5148\n",
      "trainnig theta = : 0.28125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2505 - acc: 0.5121 - val_loss: 0.2507 - val_acc: 0.5120\n",
      "trainnig theta = : 0.2875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2507 - acc: 0.5081 - val_loss: 0.2509 - val_acc: 0.5033\n",
      "trainnig theta = : 0.29374999999999996\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2508 - acc: 0.5045 - val_loss: 0.2511 - val_acc: 0.5006\n",
      "trainnig theta = : 0.3\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2509 - acc: 0.5018 - val_loss: 0.2513 - val_acc: 0.4984\n",
      "[[0.2428888663235638], [0.23788699275917477], [0.23799165116830004], [0.2388010927475989], [0.23976645410681766], [0.24078703738955987], [0.2418350588219861], [0.24283174655089776], [0.24378538507347305], [0.24469712370385727], [0.2455548577114112], [0.2462658716365695], [0.24689753678523832], [0.24743450504417222], [0.2479256354479326], [0.24844550596964027], [0.2489914854988456], [0.24955628365278243], [0.2500831688961221], [0.25049273333408767], [0.25071119154906935], [0.25073768890773257], [0.2506208524418374], [0.25045201768063835], [0.25040614166193537], [0.25040991774035826], [0.25041929175042443], [0.2504284498592218], [0.25047120654748545], [0.25054154827569924], [0.25065493007294004], [0.2507578877214756], [0.25092128553531234]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 33) #iterating across possible alphaS values\n",
    "vlvals = []\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals, label = 'lvals')\n",
    "plt.plot(thetas,vlvals, label = 'vlvals')\n",
    "plt.vlines(0.275, ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"probStuUD Vs Loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.217\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 5e-7 #smaller learning rate yields better precision\n",
    "epochs = 60 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "        \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "        \n",
    "    theta_prime = [0.1365, 0.68, theta0]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0,\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.2750, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
