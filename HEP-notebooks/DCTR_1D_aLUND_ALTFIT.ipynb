{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses the original DCTR model (i.e. model expects “empty particles” to have 0 in every entry). As opposerd to the, modified DCTR that was trained to allow non-zero inputs for theta on empty particles, the fit appears less noisy and more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_aLund.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small = X_train[0:int(0.8*10**5)]\n",
    "Y_train_small = Y_train[0:int(0.8*10**5)]\n",
    "X_test_small = X_test[0:int(0.2*10**5)]\n",
    "Y_test_small = Y_test[0:int(0.2*10**5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_aLund.h5') #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    theta_prime = [0.1365, val, 0.217]\n",
    "    \n",
    "    # zip theta_prime to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0, #checks if pT != 0, which means we have a particle\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, \n",
    "                                                   concat_input_and_params, \n",
    "                                                   concat_input_and_params], \n",
    "                                                  axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.5\n",
      "WARNING:tensorflow:From <ipython-input-14-e68042f569ce>:20: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 146s 102us/step - loss: 0.2504 - acc: 0.5091 - val_loss: 0.2496 - val_acc: 0.5112\n",
      "trainnig theta = : 0.5125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 146s 102us/step - loss: 0.2498 - acc: 0.5101 - val_loss: 0.2497 - val_acc: 0.5104\n",
      "trainnig theta = : 0.525\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 145s 101us/step - loss: 0.2499 - acc: 0.5108 - val_loss: 0.2499 - val_acc: 0.5110\n",
      "trainnig theta = : 0.5375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 144s 100us/step - loss: 0.2500 - acc: 0.5111 - val_loss: 0.2501 - val_acc: 0.5123\n",
      "trainnig theta = : 0.55\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 145s 101us/step - loss: 0.2502 - acc: 0.5112 - val_loss: 0.2503 - val_acc: 0.5096\n",
      "trainnig theta = : 0.5625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 144s 100us/step - loss: 0.2503 - acc: 0.5117 - val_loss: 0.2503 - val_acc: 0.5107\n",
      "trainnig theta = : 0.575\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 142s 98us/step - loss: 0.2504 - acc: 0.5117 - val_loss: 0.2504 - val_acc: 0.5128\n",
      "trainnig theta = : 0.5875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 141s 98us/step - loss: 0.2504 - acc: 0.5121 - val_loss: 0.2505 - val_acc: 0.5117\n",
      "trainnig theta = : 0.6\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 141s 98us/step - loss: 0.2504 - acc: 0.5124 - val_loss: 0.2505 - val_acc: 0.5119\n",
      "trainnig theta = : 0.6125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 141s 98us/step - loss: 0.2504 - acc: 0.5121 - val_loss: 0.2504 - val_acc: 0.5120\n",
      "trainnig theta = : 0.625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 142s 99us/step - loss: 0.2503 - acc: 0.5127 - val_loss: 0.2503 - val_acc: 0.5124\n",
      "trainnig theta = : 0.6375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 142s 98us/step - loss: 0.2501 - acc: 0.5125 - val_loss: 0.2503 - val_acc: 0.5110\n",
      "trainnig theta = : 0.65\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 141s 98us/step - loss: 0.2499 - acc: 0.5126 - val_loss: 0.2500 - val_acc: 0.5122\n",
      "trainnig theta = : 0.6625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 142s 99us/step - loss: 0.2497 - acc: 0.5126 - val_loss: 0.2498 - val_acc: 0.5106\n",
      "trainnig theta = : 0.675\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2495 - acc: 0.5127 - val_loss: 0.2495 - val_acc: 0.5118\n",
      "trainnig theta = : 0.6875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2493 - acc: 0.5124 - val_loss: 0.2494 - val_acc: 0.5102\n",
      "trainnig theta = : 0.7\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2491 - acc: 0.5126 - val_loss: 0.2492 - val_acc: 0.5106\n",
      "trainnig theta = : 0.7125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2489 - acc: 0.5122 - val_loss: 0.2490 - val_acc: 0.5088\n",
      "trainnig theta = : 0.725\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 105us/step - loss: 0.2488 - acc: 0.5116 - val_loss: 0.2488 - val_acc: 0.5122\n",
      "trainnig theta = : 0.7375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2486 - acc: 0.5118 - val_loss: 0.2486 - val_acc: 0.5088\n",
      "trainnig theta = : 0.75\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2484 - acc: 0.5104 - val_loss: 0.2485 - val_acc: 0.5100\n",
      "trainnig theta = : 0.7625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 107us/step - loss: 0.2482 - acc: 0.5094 - val_loss: 0.2483 - val_acc: 0.5067\n",
      "trainnig theta = : 0.775\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2481 - acc: 0.5072 - val_loss: 0.2481 - val_acc: 0.5065\n",
      "trainnig theta = : 0.7875000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 159s 111us/step - loss: 0.2479 - acc: 0.5064 - val_loss: 0.2480 - val_acc: 0.5051\n",
      "trainnig theta = : 0.8\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2478 - acc: 0.5056 - val_loss: 0.2478 - val_acc: 0.5027\n",
      "trainnig theta = : 0.8125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 159s 110us/step - loss: 0.2476 - acc: 0.5051 - val_loss: 0.2477 - val_acc: 0.5041\n",
      "trainnig theta = : 0.825\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 159s 110us/step - loss: 0.2475 - acc: 0.5058 - val_loss: 0.2476 - val_acc: 0.5009\n",
      "trainnig theta = : 0.8375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 157s 109us/step - loss: 0.2475 - acc: 0.5049 - val_loss: 0.2476 - val_acc: 0.4972\n",
      "trainnig theta = : 0.8500000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2474 - acc: 0.5044 - val_loss: 0.2475 - val_acc: 0.4996\n",
      "trainnig theta = : 0.8625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 109us/step - loss: 0.2473 - acc: 0.5036 - val_loss: 0.2474 - val_acc: 0.5031\n",
      "trainnig theta = : 0.875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 159s 110us/step - loss: 0.2473 - acc: 0.5043 - val_loss: 0.2474 - val_acc: 0.4995\n",
      "trainnig theta = : 0.8875\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 158s 110us/step - loss: 0.2473 - acc: 0.5034 - val_loss: 0.2474 - val_acc: 0.5031\n",
      "trainnig theta = : 0.9\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 153s 106us/step - loss: 0.2472 - acc: 0.5038 - val_loss: 0.2473 - val_acc: 0.5009\n",
      "[[0.2504484974882669], [0.249806909987496], [0.24987572626107268], [0.2500329843618804], [0.2501934778669642], [0.2503183476626873], [0.25039107889557877], [0.25043276898148986], [0.250427457338406], [0.25037478986713624], [0.2502644800270597], [0.25011844324568905], [0.24994403161108494], [0.24970000446256663], [0.24945057873717613], [0.24925542730424138], [0.2490929631723298], [0.24893844239413737], [0.24876894196495414], [0.24858304278718102], [0.24840504945152336], [0.24822192373168137], [0.248050982474039], [0.24789440115499828], [0.24776351629859872], [0.24763297740783957], [0.2475363220812546], [0.24745593806728722], [0.24739353576054177], [0.24733726281879675], [0.2472943143505189], [0.247258811402652], [0.24723204233580165]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "thetas = np.linspace(0.50, 0.90, 33) #iterating across possible alphaS values\n",
    "vlvals = []\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4c5a95c5d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEMCAYAAAD5zKAAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xd0FdXax/Hvkx5SIAmhJYGE3iEQEqqiohQRUHrvqIiovHov1qtevYKo2FBRQIpCQBQFRUAQEEFKkN47BKkBQk1I2e8fc4BDDyQnJ+X5rHVWzszeM+eZ67r+nJk9e8QYg1JKKZXVXJxdgFJKqbxJA0YppZRDaMAopZRyCA0YpZRSDqEBo5RSyiE0YJRSSjmEBoxSSimH0IBRSinlEBowSimlHMLN2QU4U+HChU14eLizy1BKqVxl9erVx40xwbfrl68DJjw8nLi4OGeXoZRSuYqI7MtIP71EppRSyiE0YJRSSjmEBoxSSimHyNf3YJRS6lZSUlKIj48nKSnJ2aU4hZeXF6Ghobi7u9/V9g4NGBFpBnwEuAJjjDHDrmkfAvQDUoFjQB9jzD5bWxqwwdZ1vzGmlW19BBALBAGrge7GmIsi0gsYARy0bfOpMWaMAw9PKZXHxcfH4+fnR3h4OCLi7HKylTGGhIQE4uPjiYiIuKt9OOwSmYi4AqOA5kBloLOIVL6m2xogyhhTHZgOvGvXdsEYU9P2aWW3fjgw0hhTFjgJ9LVrm2q3jYaLUipTkpKSCAoKynfhAiAiBAUFZerszZH3YKKBncaY3caYi1hnHa3tOxhjFhpjztsWlwOht9qhWP+U78cKI4AJQJssrVoppezkx3C5JLPH7shLZCHAAbvleCDmFv37Ar/aLXuJSBzW5bNhxpgfsS6LnTLGpNrtM8Rum7Yicg+wHXjOGGP/+wCIyABgAEDJkiXv7IjU1dJSYN8yOHMY0lMgPdX6pKVe+Z6eAulp4F4AanYBn8LOrloplU1yxE1+EekGRAH32q0uZYw5KCKlgd9FZAOQeIvdzAKmGGOSReRxrLOb+6/tZIz5EvgSICoqymTVMeQbaSmwezFs/hG2/gwXTmZ820XDIOZxqP80FAh0XI1K5SG+vr6cPXv2jrfbu3cvLVu2ZOPGjQ6oKmMcGTAHgTC75VCu3IC/TESaAC8D9xpjki+tN8YctP3dLSKLgEjge6CQiLjZzmIu79MYk2C32zFcfT8nS124mMa2I2eoGVbIUT+Rs6RehN2LbKHyCySdAg8/qNAcKreGIpXA1R1c3G7+SdgJi4fDnyNh5VdQ9wmo9xR4Bzj76JRSDuLIezCrgHIiEiEiHkAnYKZ9BxGJBEYDrYwxR+3WB4iIp+17YaABsNkYY4CFQDtb157AT7Z+xe123QrY4pCjAkb/sYvHPlvK6aQUR/2E86WnwbY5MOMJGFEWJreHLbOgfDPoHAsv7MQ89iV7gu9j3fkgtiUFsC+lIEfS/TmFL0muPhh3b3DzABcXCC4P7cbCwL+g7APwxwj4sAYsGg5JtzoxVUoBdOrUiV9++eXycq9evZg+fTp79+6lUaNG1KpVi1q1arFs2bLrtt20aRPR0dHUrFmT6tWrs2PHjmyp2WFnMMaYVBEZBMzFGqY8zhizSUTeBOKMMTOxhhX7At/ZbiZdGo5cCRgtIulYITjMGLPZtut/A7Ei8hbWKLSxtvWDRaQV1j2bE0AvRx1bdEQg6QZW7z3JfRWLOOpnnOfgavj5OTi0DrwKQsWHoUobKN2YYxdg6c7j/PnjNpbuPM6hxFuPMPFwc8HLzYUAHw/uq1CEltWLU6vdeFyOboJF78Ci/8Hyz6D+IIh5Ajz9sucYVZ7SuHFjABYtWuSw33hj1iY2/3M6S/dZuYQ//3mkSob6duzYkWnTpvHwww9z8eJFFixYwOeff44xht9++w0vLy927NhB586dr5tj8YsvvuCZZ56ha9euXLx4kbS0tCw9jptx6D0YY8xsYPY1616z+97kJtstA6rdpG031gi1a9e/CLyYmXozKjIsAHdXYfmehLwVMEmJ8Ptb1iUs3yLw2BjOlX2YlfvP8uf24yydvYKth88AUKiAO/XLBPFUmcIUL+hFUko6SSlpJKdaf5NS00hOSb/89+CpC0xZuZ/xy/ZSzN+LFtWK83C9T4ls9Dwui4dbv/vXZ9D4RajTzzrrUUpd1rx5c5555hmSk5OZM2cO99xzD97e3iQmJjJo0CDWrl2Lq6sr27dvv27bevXq8fbbbxMfH89jjz1GuXLlsqXmHHGTP7fx9nClRmghVu454exSsoYxsGkGzHkRzh4hLaofswr3ZfJfp1gzdREpaQYPNxeiwwP5d7MQGpYtTOUS/ri63NkQxrPJqSzYcoRf1h/imxX7GLd0DyUKetGi2mu0f+QJym8aifz6gnWvp/WnEFjaQQes1J3L6JmGo3h5edG4cWPmzp3L1KlT6dSpEwAjR46kaNGirFu3jvT0dLy8vK7btkuXLsTExPDLL7/QokULRo8ezf33XzcGKstpwNyl6IhAvvxjN+cvplLAIxf/z3hiD8x+HnbOJ61odWZVeo/h6wtwKHE35Yr40q9RaRqWLUztUgF4ubtm6qd8Pd1oXTOE1jVDOJOUwnxb2Ez8ax9j0tIJKTiYYZUb03DXB8jnDaDJ61Cnv57NKGXTsWNHxowZQ1xcHOPHjwcgMTGR0NBQXFxcmDBhwg0vf+3evZvSpUszePBg9u/fz/r16zVgcrKY0kF8tmgXf+87RcNyufDZjtSLsOxj+GME6eLKovAhPL83mhP70qlbugDvPFaNe8sHO+whMz8vdx6NDOXRyFASL6Qwf/MRpq+Op/vf5WkZ/hnveY3D69d/weaZ0PoTPZtRCnjooYfo3r07rVu3xsPDA4CBAwfStm1bJk6cSLNmzfDx8bluu2nTpjFp0iTc3d0pVqwYL730UrbUK9bArPwpKirK3O0Lx84mp1LjjXkMbFyG/3uoQhZX5kBnj0H8Spj/BhzfxsaCjXkyoQPxaYVoWrkYj99bmsiSzhk6bIxhWtwB3pi1GXcXYWKtHdTYNNx6YLPJ63o2o27KUTf5t2zZQqVKlbJ0n7nNjf43EJHVxpio222rZzB3ydfTjaol/FmRU+/DpKdBwi44sgEOX/pshLOHAUhwK8q/Ul5gyfHaPFYrhP73lKZMsK9TSxYROtYpSXREEM9OXUvrZRH0qTaWl9K+wO3y2cynEHh3E+8ppbKXBkwmREcEMuGvfSSlpGX6/kSWOP0P/PGeNbz4yCZIvWCtd3GD4IqYMo1ZnRzGhxs82Goq0bZhWd5pEEER/+tvCjpTRGEfpj9Rj09+38mnv+9gXsGnmNTgISLi3obP68N9L1sPeQZE6BmNUjmYBkwmxEQE8dWSPaw7cIqY0kHOLcYY+GEAHFgJYdEQ1RuKVYOiVSG4AknGjdd+2si0tfHcVyGY+R1rUqiAh3NrvgV3VxeGPFiee8sH89zUtTzwexj/rvct/RM/xGXeyzDvZfDwtY6vWLUrnyKVwT1nBaZS+ZUGTCbUCQ9EBFbsOeH8gFnzDexdAi0/tMLFzqHECzwx6S/WxScy+P6yPNukPC53OMTYWWqXCmD2M414Y+Ym3lkWzy8hzzGqwwuEJe24culvXSys+sraQFyhcHkrbGIeh9DbXiZWSjmIBkwmFCzgTsVi/s5/HubsUZj3CpSsD7V6XtW0YncCT03+m6SUdEZ3r03TKsWcVOTd8/V0Y0T7GtxfsQgvztjAQ1PS+bxbMxq36GF1SE+HU3uvvte08zfYtxSe/lvPaJRyEr2AnUkxEYGs3neSlLR05xUxZyiknIdHPrp8T8IYw4Rle+k6ZgX+3u78+FT9XBku9ppXK86cZ+6hdLAP/SfGMWvdP1aDi4s1jLlya7j/FegSCx0mwumDEDfOuUUrlY9pwGRSdEQgF1LS2HDQSRM2bp8HG7+HRs9bE0oCSSlp/N936/jPzE00rlCEH59qQNkieWOOr2IFvZgyoC41wwoxOHYNU1buv3HHiHugdGNY8j4k3/lU50rlVHv37qVq1ap3te2iRYto2bJlFld0cxowmRQdYb3XZMVuJ1wmSz4LvwyB4IrQ8DkADp66QLsvlvHD3wd5tkk5vuxeG38v9+yvzYH8vdyZ2CeGe8sH8+IPG/hi8a4bd7z/NTh/HFZ8nr0FKqUADZhMK+zrSZlgH1buSbh956y28G1IPACPfAxuHvy9/ySPfPIn+46f56seUbnqZv6d8vZw5cvuUTxSowTDft3K8Dlbue6h4dDaUOFhWPrJnb0YTakcYujQoYwaNery8uuvv8706dMvL9etW5dNmzZdXm7cuDFxcXGsXLmSevXqERkZSf369dm2bdt1+168eDE1a9akZs2aREZGcubMmSyvX2/yZ4GY0kHMWvsPaenmjieAvGsHV8OKLyCqL5SMYd2BU/Qcu5JAXw/G9arj9Icms4OHmwsfdqyJv5cbny/aReKFFP7buurV/wzufxk+bwBLP4Ym/3FesSr3+3WoNYgkKxWrBs2H3bS5Y8eOPPvsszz11FOANeXL6NGjL89DdmkK/zfeeINDhw5x6NAhoqKiOH36NEuWLMHNzY358+fz0ksv8f3331+17/fee49Ro0bRoEEDzp49e8NJMjNLz2CyQExEIGeSU9lyKGvfFXFTaSkwczD4FoUm/2HjwUS6j11BIR93pvSvmy/C5RJXF+GtNlUZ2LgMk1fs55nYNVxMtRtwUbQKVGtnhfHZozffkVI5UGRkJEePHuWff/5h3bp1BAQEEBZ25UXBHTp0uHxGM23aNNq1s97FmJiYSPv27alatSrPPffcVWc5lzRo0IAhQ4bw8ccfc+rUKdzcsv58Q89gssCl+zDLdydQNaSg439w2SdwZCN0/Jatp4TuY5fj6+nG5H51KVHI2/G/n8OICP9qVhF/b3eG/bqVs8mpfN61Nt4ettkVGr8IG3+wbvg3H+7cYlXudYszDUdq374906dP5/Dhw3Ts2PGqtpCQEIKCgli/fj1Tp07liy++AODVV1/lvvvuY8aMGezdu/fyXG32hg4dysMPP8zs2bNp0KABc+fOpWLFillau57BZIHiBb0pGVgge56HSdhlvdu+0iPsCLyXrl+twMPNhSkD6hIWWMDxv5+DPXFvGd55rBqLtx+jx7gVV15pHVQGIrtaQ5ZPHXBukUrdoY4dOxIbG8v06dNp3779DdvfffddEhMTqV69OmCdwYSEhABcvpx2rV27dlGtWjX+/e9/U6dOHbZu3ZrltWvAZJGYiEBW7j1BeroDZ6c2xnqVsasH+2Jep8uYFYgIk/vXpVTQ9VN050edo0vySedI1h44RY+xKzl/MdVquPff1t/FegajcpcqVapw5swZQkJCKF68+HXt7dq1IzY2lg4dOlxe969//YsXX3yRyMhIUlNTb7jfDz/8kKpVq1K9enXc3d1p3rx5lteu0/Xf5XT91/ou7gAvTF/P3GfvoUIxBz1zsuZb+GkgCY2H8fCy8qSkpRM7oC7liuaNZ1yy0pyNhxn47WoaVyjCl91r4+bqYt2kXfklPLUSCpd1dokqC+l0/Y6Tmen69Qwmi9S1zUW2wlHDlc8eg3kvk1wimtZ/lSMpNY1v+sVouNxEs6rFeLN1VX7fepSXZmywhjA3GgJuXrDof84uT6l8QQMmi4QGeFO8oJdj3g9jDPz6AubiOfqe6Mbp5DS+6RtDpeL+Wf9beUi3uqUY/EA5psXF88Fv28G3CNR9wpr5IKuHmyqlruPQgBGRZiKyTUR2isjQG7QPEZHNIrJeRBaISCm7tjQRWWv7zLRbHyEiK2z7nCoiHrb1nrblnbb2cEce2w2OhZiIQFbsPnH9A3+ZYQzMeRE2zWCsawfWXSjGpL4x2TNaLQ94rkk5OtUJ45PfdzLpr71Q/2nwKgi/v+3s0pTK8xwWMCLiCowCmgOVgc4iUvmabmuAKGNMdWA68K5d2wVjTE3bp5Xd+uHASGNMWeAk0Ne2vi9w0rZ+pK1ftoqOCOL42WT2HD+XNTs0Bn57FVZ8zvfujzAyqSXj+9ShRlihrNl/PiBiPSfTpFIRXpu5iTm7kqD+YNj+KxxY5ezylMrTHHkGEw3sNMbsNsZcBGKB1vYdjDELjTHnbYvLgdBb7VBEBLgfK4wAJgBtbN9b25axtT9g659tYkpbz8NkyXBlY2DBm7DsE350b8ErSV0Z1yua2qUCM7/vfMbN1YVPOtciMqwQg2PXsqpoB/AJht//6+zSlMrTHBkwIYD9QwfxtnU30xf41W7ZS0TiRGS5iFwKkSDglDHm0rg7+31e/j1be6Ktf7YpXdiHwr6eWXMfZtE78OcH/OzelJeSujO+d7TzX2qWi3l7uDK2Zx3CArzpM2ULR2o8BXsWw+7Fzi5NqTwrR9zkF5FuQBQwwm51KdswuC7AhyJSJot+a4AtuOKOHTuWFbu037f1PExmA2bxCFg8nF/dm/CvpF6M6x2j4ZIFAnw8mNAnmgIerrRfVZFUvxDrLCYfD9VXOVtCQsLlCSmLFStGSEjI5eWLFy9maB8//PDDVQ9RNmzYkLVr1zqq5Ks4MmAOAmF2y6G2dVcRkSbAy0ArY0zypfXGmIO2v7uBRUAkkAAUEpFLU9zY7/Py79naC9r6X8UY86UxJsoYExUcHJyZ47uh6IhADp66wIET52/f+Ub+HAkL3+I39/t4PqkvY3vFXB4CrTIvNKAAE/pEczLFhQ9THoX4VbB6vLPLUuqGgoKCWLt2LWvXruWJJ57gueeeu7zs4eEBWC8XTE+/+QsPrw2Y7OTIgFkFlLON+vIAOgEz7TuISCQwGitcjtqtDxART9v3wkADYLOxhmctBNrZuvYEfrJ9n2lbxtb+u3HCU6SZug+z7FOY/zq/u9/DM0n9GdMrhnplNFyyWsVi/nzVI4qvztRnrUctzJyhcGSzs8tSKsN27txJ5cqV6dq1K1WqVOHAgQMUKnRl8E9sbCz9+vVjyZIlzJ49m+eee46aNWuyd+/ey+3R0dFUqFCBZcuWOaxOh012aYxJFZFBwFzAFRhnjNkkIm8CccaYmViXxHyB72z34/fbRoxVAkaLSDpWCA4zxlz6N8C/gVgReQtrFNpY2/qxwCQR2QmcwAq0bFe+iB+FCrizYk8CbWvfcszC1VaMhnkv84dbA55OelzDxcHqlg7ig4616De5P4v9XsZnem/ovxA88vd8burWbjRpZGZkZuaBrVu3MnHiRKKiom46HUyjRo1o0aIF7dq1o02bNpfXG2NYuXIlM2fO5M0332TOnDl3XcetOHQ2ZWPMbGD2Netes/ve5CbbLQOq3aRtN9YItWvXJwHXzwSXzVxchDrhd3gfZtVY+PVfLHWry8DkJ/mqZ13qlynsuCIVAA9XL86CLZV5cv0TTEh5B5nzb2j1ibPLUipDypQpQ1TUbWdruaHHHnsMgNq1a18+q3EEna7fAWIiAvlt8xGOnE6iqP9tXuKzbir8MoTlbnV4PGkQo3vVo35ZDZfs8krLyjTZfozpnh1o//dEiLjXen+MUjeQ1XOdZYaPz5UJbl1cXK56wDspKemW23p6egLg6up607OfrJAjRpHlNZfeD3Pb4cr7V2B+GsQ6t2r0SxrMFz3r0UDDJVsF+njwn0cq8+KJhzlSsAbMehZO7HZ2WUrdERcXFwICAtixYwfp6enMmDHjcpufn59DXoecobqc8qt5XOXi/vh6urFi940nvjTGsH7jek5P6Mi+1AAGJA3m8571aFhOw8UZWtUoQcMKxel6sj/p4gLT+0BqxoaAKpVTDB8+nKZNm1K/fn1CQ6/c/+3cuTP/+9//rrrJn110uv4smq7/Wj3HreSfUxf4bci9l9elpKUze8MhJi/ZzOvHhhDicpzY6l/T8v578+WbKHOSg6cu8OAHi3m8yGaeOf4G1BsETXW+stxCp+t3nMxM16/3YBwkpnQg787ZRsLZZFxdhMkr9zNx2T6OnD7PJJ9PqOASz8VO0xhQ8UFnl6qAkELe/KtpBV6flcYjlbpQ+q9PIeIeKN/U2aUplWtpwDhIjO0+zNNT1vD3/pMkpaTTsGxhppaZS6ktK6DZcLw0XHKU7vXC+WndP3Te+zBLg9fhNuMJeHIp+JdwdmlK5Up6D8ZBqoUUws/Ljbh9J2ldI4S5z97DN3X2UGrLaKjdG2Ied3aJ6hquLsLwttU5cdGFd33/DanJ8H1/SE9zdmnKifLzbYTMHrsGjIN4uLnwy9ON+Gvo/QxvV50KKVtg5tMQ3ghajIDsnehZZVD5on4MbFyWL7e4sbn2f2Dfn/DHiNtvqPIkLy8vEhIS8mXIGGNISEjAy+s2j1rcgl4ic6CSQbanwk/th9gu4B8CHSaCq7tzC1O3NPC+Mvyy4RD915ZjUdUOuC8eDuENrY/KV0JDQ4mPjyerJ8bNLby8vK4akXanNGAcLfksTOlsDXvtNQ0K6PtccjpPN1eGt61Guy/+YoTrAF4KWA0/PgkDl4OHz+13oPIMd3d3IiIinF1GrqWXyBwpPR1mPA5HN0P7ryG4vLMrUhlUu1QgPeqW4quVR9lWd5h1Frrwf84uS6lcRQPGkRa8AVt/hqbvQNkHnF2NukMvNKtIMX8vnl7qSVqtXrD8Mzj4t7PLUirX0IBxBGNg/huw9EMdMZaL+Xq68Vabqmw/cpYvPXqCTxGYNRjSHDd3k1J5iQZMVktPh9kvwJ8fQK2e8PD7OmIsF3ugUlEeqVGCkUuOcLjhm3B4Aywf5eyylMoVNGCyUloq/PQUrPrKmmrkkY/AxdXZValMerVlJbzcXXh2XRimQgtY+A6c2OPsspTK8TRgskpqMkzvDesmw30vw0Nv6ZlLHlHEz4uhzSuxfM9Jfg79P3Bxg5+ftS6FKqVuSgMmK1w8bw1F3jLTuqF/7780XPKYTnXCiCoVwKsLT3C20cuwexGsi3V2WUrlaBowmZWUCN+0hV2/wyMfQ72Bzq5IOYCLi/DOY9U4l5zKfw7GQFgMzH0Jzh13dmlK5VgaMJlx/gRMaAXxK6HdWKjd09kVKQcqV9SPx+8pw/drD/F3jTcg+YwVMkqpG9KAuVtnDsPXLeDoFug0Gaq2dXZFKhsMur8s4UEFGLIwidT6z8L6qbBzvrPLUipH0oC5Gyf3wbhmkHgAuk3Xd4bkI17urrz9aDX2Jpznk5RWEFQOfn4OLp5zdmlK5TgaMHdj4/dw4ST0+Ml6KZXKVxqULcxjtUIYtSSeAw11GhmlbsahASMizURkm4jsFJGhN2gfIiKbRWS9iCwQkVLXtPuLSLyIfGq3rqOt/yYRGW63vpeIHBORtbZPP4cdWMPnrBdRhd72jaEqj3q5RSX8vNx4bnkBzKVpZP5Z4+yylMpRHBYwIuIKjAKaA5WBziJS+Zpua4AoY0x1YDrw7jXt/wX+sNtnEDACeMAYUwUoJiL2k3xNNcbUtH3GZO0R2RGBgnc/hbXK/YJ8PXmpRSXi9p1kemB/axqZmYOtWbOVUoBjz2CigZ3GmN3GmItALNDavoMxZqEx5rxtcTlw+d/aIlIbKArMs9ukNLDDGHPp5QzzAb27rpyiXe1Q6pYO5M35Bzl1/ztweL313p+L52+/sVL5gCMDJgQ4YLccb1t3M32BXwFExAV4H3j+mj47gQoiEi4ibkAbIMyuva3t8tl0EQnjBkRkgIjEiUhcfn2JkMoaIsLbj1YjOSWdV7dFWFMD7ZwP3zwGF045uzylnC5H3OQXkW5AFNblL4CBwGxjTLx9P2PMSeBJYCqwBNgLXHph+iwg3Ha57Tdgwo1+yxjzpTEmyhgTFRwcnNWHovKZMsG+PHVfWWat+4dFvi2g3TiIj4MJLeHsUWeXp5RTOTJgDnL12UWobd1VRKQJ8DLQyhiTbFtdDxgkInuB94AeIjIMwBgzyxgTY4ypB2wDttvWJ9htPwaonfWHpNT1nmhcmjLBPrzy40bOlWsFnWPh+E5rKPup/c4uTymncWTArALKiUiEiHgAnYCZ9h1EJBIYjRUul/9zzxjT1RhT0hgTjnWZbKIxZqhtmyK2vwFYZzpjbMvF7XbdCtjiqANTyp6nmyvD2lbnn1MXeGH6OkzZB6DHj9Y0MuOawbFtzi5RKadwWMAYY1KBQcBcrH/ZTzPGbBKRN0Wkla3bCMAX+M42tHjmTXZn7yMR2QwsBYYZY7bb1g+2DV1eBwwGemXl8Sh1K3XCAxnavCKzNxzms0W7oGRd6P0LpKVYIaNvwlT5kJh8POV4VFSUiYuLc3YZKo8wxvBM7Fpmrf+Hcb3qcF+FIpCwCya1seat6xwLEY2cXWae1LhxYwAWLVrk1DryCxFZbYy57YOAOeImv1J5gYgwvG11KhXzZ/CUNew5fg6CykCfudZzU9+0ha2znV2mUtlGA0apLOTt4cro7rVxcxEGTIzjbHIq+JeA3r9C0SowtRusn+bsMpXKFhowSmWxsMACjOpSi93Hz/F/09aSnm6gQCD0nAml6sOMx2HTDGeXqZTDacAo5QD1yxbmxeYVmbvpCKMW7rRWevpBl6kQGg3f94Ntc5xbpFIOpgGjlIP0bRjBo5EhfDB/Owu2HLFWevhA12lQrBpM6wG7Fjq3SKUcSANGKQcRsV6zXKWEP8/GrmXXsbNWg1dB6PaDNQAgtgvsX+7cQpVyEA0YpRzIy92V0d2j8HBzYcDEOM4kpVgNBQKt9wn5l4Bv2+tzMipP0oBRysFCCnnzaZda7E04z3NT11k3/QF8i0CPmeBdyJog88gm5xaqVBbTgFEqG9QrE8SrD1di/pYjvP+b3dQxBUOskHHzholtrDnMlMojNGCUyiY964fTqU4YoxbuYvTiXVcaAiOsy2UmHSa2gpP7nFekUllIA0apbHLp/TEtqxfnnV+3Mn7pniuNweWtCTIvnrNC5vQ/zitUqSyiAaNUNnJ1EUZ2rMlDlYvy+qzNxK60m86/WDVrdNm5BJjY2pq/TKlcTANGqWzm7urCJ10iubd8MC/O2MCMNXbv1QutbT2MeXKv9cR/errT6lQqszRglHICTzdrzrK6EUH837R1zN5w6EpjeANo+j/YMQ+WjnRekUplkgaMUk7i5e5jNyn3AAAgAElEQVTKmJ5R1CoZwOApa5i/+ciVxjr9oGo7+P0t2POH84pUKhM0YJRyIh9PN8b1rkPlEv4M/PZvluw4ZjWIwCMfQVBZmN4Xzhx2bqFK3QUNGKWczN/LnYl9oilTxJf+E+NYvjvBavD0hQ4T4eJZmN4H0lKdW6hSd0gDRqkcoFABD77pG01oQAH6jl/F6n0nrYYilaDlh7BvKSx8y7lFKnWHNGCUyiGCfD2Z3C+GYD9Pen29knUHTlkNNTpC7d7w50id4l/lKhowSuUgRfy9+LZ/XQoVcKfbmBWs2W87k2k2DIrXgBkDrCHMSuUCGjBK5TAhhbyZOqAegb4edB+7ktX7ToC7F7SfAAb4rhekJju7TKVuy6EBIyLNRGSbiOwUkaE3aB8iIptFZL2ILBCRUte0+4tIvIh8areuo63/JhEZbrfeU0Sm2n5rhYiEO/LYlHKkEoW8iR1Ql2A/T3qMXcmqvSesOcse/QL+WQNzX3J2iUrdlsMCRkRcgVFAc6Ay0FlEKl/TbQ0QZYypDkwH3r2m/b/A5YcARCQIGAE8YIypAhQTkQdszX2Bk8aYssBIYDhK5WLFC1ohU7SgFz3HrWTF7gSo2ALqD4ZVY2D9d84uUalbcuQZTDSw0xiz2xhzEYgFWtt3MMYsNMacty0uB0IvtYlIbaAoMM9uk9LADmOM7WEB5gNtbd9bAxNs36cDD4iIZOHxKJXtivp7Edu/LiUKedPr61X8tSsBHngNStaHWc/AsW2334lSTpKhgBGRMiLiafveWEQGi0ih22wWAhywW463rbuZvsCvtt9wAd4Hnr+mz06ggoiEi4gb0AYIu/b3jDGpQCIQdLtjUyqnK+LvxZT+dQkL9Kb3+JUs3ZMI7caBRwGY2g2SEp1dolI3lNEzmO+BNBEpC3yJ9S/1yVlVhIh0A6KwLn8BDARmG2Pi7fsZY04CTwJTgSXAXiDtDn9rgIjEiUjcsWPHbr+BUjlAsJ8nk/vXJTzIhz7jV/HHYTdoPx5O7Ibv+0P6Hf3fQKlskdGASbedFTwKfGKMeQEofpttDnLl7AKsy18Hr+0kIk2Al4FWxphLQ2PqAYNEZC/wHtBDRIYBGGNmGWNijDH1gG3A9mt/z3Z2UxBIuPb3jDFfGmOijDFRwcHBtz9ypXKIwr5WyJQO9qXfxDgWJZeH5sNhx1xY+Lazy1PqOhkNmBQR6Qz0BH62rXO/zTargHIiEiEiHkAnYKZ9BxGJBEZjhcvRS+uNMV2NMSWNMeFYl8kmGmOG2rYpYvsbgHWmM8a22UxbfQDtgN+NMSaDx6dUrhDo48HkfjGUK+LLgImrWej7CNTuBUveh43fO7s8pa6S0YDpjXVW8bYxZo+IRACTbrWB7YxnEDAX2AJMM8ZsEpE3RaSVrdsIwBf4TkTWisjMm+zO3kcishlYCgwzxlw6gxkLBInITmAIcN2waKXyggAfDyb3q0uFYn48/u3frKg4FMLqwo9PwaF1zi5PqcvkTv8j33bmEGaMWe+YkrJPVFSUiYuLc3YZSt2VE+cu0mH0XxxJTGJatzJUmtUKxAX6LwTf/HX5t3HjxgAsWrTIqXXkFyKy2hgTdbt+GR1Ftsj20GMg8DfwlYh8kNkilVJ3L9DHg0l9o/H3dqdb7B7im46Bc8dgWg9Ivejs8pTK8CWygsaY08BjWPdDYoAmjitLKZURxQt6M6lvNAAdZyVx8sEPYP8ymKNXiJXzZTRg3ESkONCBKzf5lVI5QOlgX8b3jibxQgodloaSFP00xI2FuHHOLk3lcxkNmDexbtbvMsasEpHSwA7HlaWUuhPVQgvyVY8o9p04T5fdD5FaugnMfgH2LXN2aSofy1DAGGO+M8ZUN8Y8aVvebYxpe7vtlFLZp16ZID7tHMnag2d4Knkg6YXCYWp3OHXgttsq5QgZvckfKiIzROSo7fO9iITefkulVHZ6qEoxhretztxdSbzl9yom7SLEdoGL55xdmsqHMnqJ7GusBxlL2D6zbOuUUjlM+6gwXnm4EuO2uTOh+MuYwxtgel9IS3V2aSqfyWjABBtjvjbGpNo+44H8NdBeqVykX6PSDGxchte3hjE//P9g+6/w679AJ7dQ2SijAZMgIt1ExNX26cYN5vlSSuUcLzStQOfokvTfWouN4b2skWV/jnR2WSofyWjA9MEaonwYOIQ111cvB9WklMoCIsJbbaryQMUitNn+IEdLtYIFb8C6WGeXpvKJjI4i22eMaWWMCTbGFDHGtOHKi76UUjmUq4vwUedIyhbxp9nejpwvUR9+egp2LXR2aSofyMwbLYdkWRVKKYfx9XRjTM8oXNy9aHtyIKmB5azhy4c3OLs0lcdlJmD0dcRK5RKhAQX4qkdtdp1xY5C8iPH0hW/bQ2L87TdW6i5lJmB0OIpSuUhkyQDea1+DOQfceC/4f5iL5+CbdnDhlLNLU3nULQNGRM6IyOkbfM5gPQ+jlMpFWtUowbNNyjFqsyczKw6HhJ0wtRukJt9+Y6Xu0C0DxhjjZ4zxv8HHzxjjll1FKqWyzjMPlKNVjRI8s6Ig66L+B3uXwI9PQnq6s0tTeUxmLpEppXIhEeHddtWJLFmIjn+FcajOUOt1y3Nf0gcxVZbSgFEqH/Jyd+XL7lEE+XjSZm0U5yL7w4rP4ZcheiajsowGjFL5VLCfJ2N6RnE2OY2O+1qRUu8Z6x0yMx6HtBRnl6fyAA0YpfKxSsX9+aRLJJsPneGpI61Iv/812DANpvWElCRnl6dyOQ0YpfK5+ysW5dWWlZm3+QhvnmqGaT4Ctv0CUzrqNP8qUxwaMCLSTES2ichOEbnuJeEiMkRENovIehFZICKlrmn3F5F4EfnUbl1nEdlg22aOiBS2rX9dRA6KyFrbp4Ujj02pvKR3gwj6Noxg/LK9jL3YBNp8Dnv+gEmP6nMy6q45LGBExBUYBTQHKgOdRaTyNd3WAFHGmOrAdODda9r/C/xht0834CPgPts264FBdv1HGmNq2j6zs/SAlMrjXm5RiRbVivHWL1v42aUxtB8PB/+GCS3h7DEnV6dyI0eewUQDO22vV74IxAKt7TsYYxYaY87bFpcDl9+SKSK1gaLAPLtNxPbxEREB/IF/HHcISuUfLi7CBx1qElUqgCFT17HSuxF0iYXjO+Hr5pB40NklqlzGkQETAti/DDzetu5m+gK/AoiIC/A+8Lx9B2NMCvAksAErWCoDY+26DLJdOhsnIgGZPgKl8hkvd1e+6hFFaKA3/SfGsdM/Brr/AGePwLhmcGK3s0tUuUiOuMlve4FZFDDCtmogMNsYE39NP3esgInEmqpmPfCirflzoAxQE+udNe/f5LcGiEiciMQdO6an/UpdK8DHgwm9o3F3FXqOW8XRwFrQcyZcPGuFTMIuZ5eocglHBsxBIMxuOdS27ioi0gR4GWhljLk0IVI9rLORvcB7QA8RGYYVHhhjdhljDDANqG9bd8QYk2aMSQe+wrpEdx1jzJfGmChjTFRwsL71WakbCQsswLhedThx7iJ9xq/iXFA16P2r9XzMlE56419liCMDZhVQTkQiRMQD6ATMtO8gIpHAaKxwOXppvTGmqzGmpDEmHOsy2URjzFCsgKosIpeS4UFgi21fxe12/Siw0TGHpVT+UD20EKO6RrL5n9M8NflvUoPKQ8dJ1mWy6X0gLdXZJaoczmEBY4xJxRrhNRcrBKYZYzaJyJsi0srWbQTgC3xnG1o88ya7u7TPf4A3gD9EZD3WGc3/bM3vXhq+DNwHPJf1R6VU/nJ/xaK81aYai7Yd45UfN2JKNYCH34ddC+C3V51dnsrhHDojsm2o8Oxr1r1m971JBvYxHhhvt/wF8MUN+nXPRKlKqZvoElOSg6fOM2rhLkIKefP0A73g6FZY/hkEV4TaPZ1dosqhdMp9pdRtPf9QBQ6dSuL937YTGujNow+9Bce3wy//B0FlIbyBs0tUOVCOGEWmlMrZRIRhbatTr3QQ/56+gZX7T0O7cRAQbr2w7OReZ5eociANGKVUhni4ufBFt9qEBnozYFIce865Q5epYNJhcidIOu3sElUOowGjlMqwggXc+bpXHQToM34VJ73CoMNE63LZD/0hPc3ZJaocRANGKXVHSgX58GWPKA6evMDj36wmuWRDaPEubJ8D8193dnkqB9GAUUrdsTrhgYxoX52Ve07w4vcbMFF9oU4/WPYxrJ3s7PJUDqGjyJRSd6V1zRD2Hj/PyPnbCS/sw+Bmw+D4Dpj1DASWhpJ1nV2icjI9g1FK3bXBD5TlscgQPvhtOz9tOGpN8V8wFGK7wok9zi5POZkGjFLqrokI77StRnREIC98t564o0CX78Ckwbft4fwJZ5eonEgDRimVKZ5urozuVpuQAG8GTFrNPikOnSbDqX0wtTukJt9+JypP0oBRSmVagI8H43rVId0Yeo9fRWJwHWj9Gez7E2YOBmOcXaJyAg0YpVSWiCjsw+hutTlw4jwDJsWRVOkxuO8VWB8Li4Y5uzzlBBowSqksE1M6iPfa12DFnhMMnrKG1AZDoGZXWDwM1k5xdnkqm2nAKKWyVOuaIbzWsjLzNh/h5R83YVqOhIh7YObTsGeJs8tT2UgDRimV5fo0jGDQfWWZGneAEfP3QIdJEFQGpnaFY9ucXZ7KJhowSimH+L+HytM5uiSfLdrFmLgT0GUauHpaw5fPHnN2eSobaMAopRxCRHirTVWaVy3GW79s4Yc9rtAlFs4ehSmdIOWCs0tUDqYBo5RyGFcX4cNONalfJogXpq/n9zOh0HYMHFwNPwyA9HRnl6gcSANGKeVQnm6ujO5em0rF/Rj47d/EedeHpm/DlpkQ20XfI5OHacAopRzOz8ud8b2jKV7Qmz7jV7E1vBs0fxd2zIMxTSBhl7NLVA6gAaOUyhaFfT2Z2Ccabw9XeoxbxYFy3aHHj3DuGHx1H+yc7+wSVRbTgFFKZZuwwAJM7BNDUkoa3ceu4GjhaBiwEAqGWaPLln6k08rkIQ4NGBFpJiLbRGSniAy9QfsQEdksIutFZIGIlLqm3V9E4kXkU7t1nUVkg22bOSJS2LY+UER+E5Edtr8Bjjw2pdTdqVDMj6971+HI6WS6fLWCo27FoO88qPQI/Paa9eplHWGWJzgsYETEFRgFNAcqA51FpPI13dYAUcaY6sB04N1r2v8L/GG3TzfgI+A+2zbrgUG25qHAAmNMOWCBbVkplQPVLhXI173rcPDkBStkkl2h/QS4/xXYMB3GNYPEeGeXqTLJkWcw0cBOY8xuY8xFIBZobd/BGLPQGHPetrgcCL3UJiK1gaLAPLtNxPbxEREB/IF/bG2tgQm27xOANll7OEqprFS3dNDVIXM2Ge55ATpPsW76f9kY9v3l7DJVJjgyYEKAA3bL8bZ1N9MX+BVARFyA94Hn7TsYY1KAJ4ENWMFSGRhray5qjDlk+34YK5yuIyIDRCROROKOHdOniZVyputC5kwSVGgO/ReApz9MeATWfOPsMtVdyhE3+UWkGxAFjLCtGgjMNsbEX9PPHStgIoESWJfIXrx2f8YYA9zwTqEx5ktjTJQxJio4ODjrDkIpdVduGDLBFaD/7xDeEH4aBBt/cHaZ6i44MmAOAmF2y6G2dVcRkSbAy0ArY8ylV9/VAwaJyF7gPaCHiAwDagIYY3bZQmQaUN+2zRERKW7bZ3HgaJYfkVLKIW4YMt6FrMtlJevCjMdh92Jnl6nukCMDZhVQTkQiRMQD6ATMtO8gIpHAaKxwuRwIxpiuxpiSxphwrMtkE40xQ7ECqrKIXDr1eBDYYvs+E+hp+94T+Mkxh6WUcoQbhoy7txUygWUgtiscWu/sMtUdcFjAGGNSsUZ4zcUKgWnGmE0i8qaItLJ1GwH4At+JyFoRmXmT3V3a5z/AG8AfIrIe64zmf7bmYcCDIrIDaGJbVkrlIjc+kwmAbt+Dlz982w5O7nV2mSqDxOTjh5qioqJMXFycs8tQSl1j+e4Een+9ipAAbyb3j6GInxcc3QrjmkKBIOu5GZ/Cl/s3btwYgEWLFjmn4HxGRFYbY6Ju1y9H3ORXSil79mcyHUcvZ/exs1CkovVOmdMHraf+k886u0x1GxowSqkcqW7pIL7pF03ihRTajFrK0p3HoWQMtPsaDq2FaT0gLcXZZapb0IBRSuVYtUsF8tNTDShe0Jse41Yyafk+qNgCWn4IuxZYQ5j1nTI5lpuzC1BKqVsJCyzA9Cfr8UzsWl79cSM7j5zh1ZbdcTt7BBa+DX43fKZa5QB6BqOUyvH8vNz5qkcU/RtFMOGvffQev4rEOs9CnX6w9CPahepjbzmRnsEopXIFVxfh5YcrU7aIL6/8uJFHP1/GuO7/IfzsEQYxi8r+56y5y0rWBRFnl6vQMxilVC7TsU5JJvWN4eS5i7T+fDl/RQ5nyv4i1Ak4A183g9GN4O9JOuV/DqABo5TKdeqWDuLHpxoQ7OdJ9/HreO9sc9r/VQVajoS0VJg5CD6oDL/9B07td3a5+ZYGjFIqVyoV5MMPA+vToGxhEko35WBYE1Ije8HAv6DnLAhvAMs+ho9qWNPM7F6sb8vMZhowSqlcy9/LnbE9o/A7FMeZ4lH0nRDH6eRUiLgHOn4Dz6yHBs/AvmUwsZX1IrOzOiAgu2jAKKVyNTdXF4L2LSRo91yW7jzOY58tY1/COauxUBg0eR2GbLYunx1aB189YE07oxxOA0YplSf4HV3PpL4xHD+bTOtRS/lrV8KVRndviOoDvWdDWjKMfQh2LXResfmEBoxSKs+oVyaIHwc2IMjHg+5jVzBl5TU3+ENqQb8FUDDEmpl59YQb70hlCQ0YpVSeEl7YhxlPNaBB2cK8+MMG3py1mdQ0u+lkCoVBn7nWfZpZg2H+6zrdjINowCil8pxLN//7NIhg3NI91s3/JLuJMb38rZmZa/eGP0fC9N763IwDaMAopfIkN1cXXnukMu88Vu36m/8Aru7Wjf+H3oLNP8GER+DsMecVnAdpwCil8rTO0SUv3/xv8dESxv6558olMxGo/zR0nASHN8KYB+DYNucWnIdowCil8rx6ZYKYNaghdSIC+e/Pm2n16VLWHjh1pUOlR6D3L9ZlsjEPWjMAHFil92YySQNGKZUvhAUW4Otedfi8ay0SziXz6GdLeeXHDSResN2bCakN/RdAWB3461MY2wRGVoafh8DOBZB60bkHkAvpbMpKqXxDRGherTiNygfzwbztjF+2hzkbj/Bqy0q0qlECKVQSun0PF07C9nmwdRasmwJxY8GzIJRvChUfhrJNwNPX2YeT44nJx3PzREVFmbi4OGeXoZTKpMaNGwOwaNGiO9pu48FEXp6xgXXxiTQoG8R/W1eldPA1wZFywXooc+vPsO1XuHACXD2toLn/FQgqkzUHkYuIyGpjTNTt+jn0EpmINBORbSKyU0SG3qB9iIhsFpH1IrJAREpd0+4vIvEi8qlt2U9E1tp9jovIh7a2XiJyzK6tnyOPTSmV+1UNKcgPAxvw3zZVWR+fSLMPl/DBb9u5cDHtSid3b+s1zW0+g+d3QM+foXYv2D4XRkXDL8/r6LObcFjAiIgrMApoDlQGOotI5Wu6rQGijDHVgenAu9e0/xf449KCMeaMMabmpQ+wD/jBrv9Uu/YxWXxISqk8yNVF6F63FAv+716aVyvGxwt20OjdhXy9dA9JKWnXdHaDiEbQ4l14Zi3U6glx4+DjmrB4BFw8d+MfyacceQYTDew0xuw2xlwEYoHW9h2MMQuNMedti8uB0EttIlIbKArMu9HORaQ8UARY4oDalVL5TBE/Lz7qFMl3T9SjTLAPb8zazH3vLWLyiv2kpN1gNJlvEWj5ATy1Ako3hoVvwce1rOln0lKzu/wcyZEBEwIcsFuOt627mb7ArwAi4gK8Dzx/i/6dsM5Y7G8itbVdbpsuImF3V7ZSKj+rEx5I7IC6fNsvhuIFvXhpxgbuf38R38UduHrKmUsKl4NO31rTzxQqaU0/80UD2DYn379/JkcMUxaRbkAUMMK2aiAw2xgTf4vNOgFT7JZnAeG2y22/ATecxU5EBohInIjEHTum102VUtcTERqULcz3T9bn6951KOjtzgvT1/PQyD/4ae1B0tNvEBwl60LfedBhEqSlwJSOML4lrPkm375V02GjyESkHvC6MaapbflFAGPMO9f0awJ8AtxrjDlqW/ct0AhIB3wBD+AzY8xQW3sN4DtjTPmb/LYrcMIYU/BWNeooMqXyhrsdRZZRxhjmbT7CB/O2s+3IGSoU9WPQ/WVpWqUYHm43+O/0tBRYPR6WvA9nDlnrAsKtCTYj7oXwRuBX1CG1ZoeMjiJz5HMwq4ByIhIBHMQ64+hi30FEIoHRQLNL4QJgjOlq16cX1kAA+1Fonbn67AURKW6Msf2TpBWwJesORSmVn4kITasU48FKRfllwyFGzt/O01PWUNjXg3a1w+hUJ4zwwj5XNnB1h+j+UKcfHN0Ce/6wPpt+gr8nWn2CK9oC5x4IbwjeAc45OAdyWMAYY1JFZBAwF3AFxhljNonIm0CcMWYm1iUxX+A7EQHYb4xplYHddwBaXLNusIi0AlKBE0CvrDkSpZSyuLgIj9QoQYtqxfljxzGmrNjPV0t288XiXTQoG0SnOiV5qEpRPN1crQ1EoGhl61P3CUhPs96qeSlw1nwDK78EcYWwGCj/EJRrCkUqWdvmcvqgpV4iUyrXc/Qlsls5cjqJ7+IOMGXlAQ6eukCgjwftaofSqU7Y9Q9tXiv1IhyMg12/w/Y5cHiDtb5gGJR7yJo5IOIe61mcHCSjl8g0YDRglMr1nBkwl6SnG5bsPM6UFfuZv+UIqemGuqUDaVc7jKZViuLn5X77nZz+B3bMs6ap2b0IUs6Bm5cVMuUegqJVoFAp8CsGLq4OP6ab0YDJAA0YpfKGnBAw9o6eTuK71fFMizvAvoTzeLm78GDlYjwaWYJG5YJxd83AAN7UZNj7pzVjwI65cHLvlTYXdygYag2LDihl/S1UyvoULgcFAh12bKABkyEaMErlDTktYC4xxvD3/lP8uOYgP6//h5PnUwj08eCR6sVpExlCzbBCSEbutRhjBcyJXdaQ51P74eS+K9/PHb3SV1yhzH1Qrb01X5qnX5YflwZMBmjAKJU35NSAsXcxNZ0/th9jxtqD/Lb5CBdT0wkPKkCbyBBaVi9BmWCfjIXNDXd+HhIPWKGzbyls/AES91uX18o3tcKm7IPg7pUlx6IBkwEaMErlDbkhYOydTkphzsbD/LjmIH/tTsAYCCnkzT3lC9OoXDANyhSmYIEM3LO5mfR0iF8FG76DTTPg/HHw9LderFa1rfUsjuvdDyLWgMkADRil8obcFjD2DiVeYMGWoyzZcYxlOxM4k5yKi0D10ELcUz6Ye8oVpmZYIdwyct/mRtJSYc9i2Pg9bJkFyafBJxiaDYNq7e5qlxowGaABo1TekJsDxl5KWjrrDpzijx3HWbLjGOsOnCLdgJ+nG/XKBBEdEUjtUgFUKVHwxjMI3PYHkqxRahunQ8wTUKr+XdWZE57kV0opdQfcXV2ICg8kKjyQIQ+WJ/F8Ckt3WWGzZMdx5m0+AoCnmwvVQwtSq1QAtUsGULtUAEG+nhn4AS+o3Mr6ZAMNGKWUyqEKFnCnRbXitKhWHLAe6vx730lW7ztJ3L6TjPtzD6PTdgMQUdiHWiUDqBlWkPJF/ahQzI9CBTycWb4GjFJK5RZF/b1oXq04zW2Bk5SSxoaDiay2hc6ibUf5/u8rk9AX8fOkQjE/yhXxo0IxX8oX9aNcUT98PbPnX/0aMEoplUt5ubtSJzyQOuHWg5XGGA4lJrH9yBm2HznDtsNn2X7kDJNX7iMp5cq7bEIDvHmhaQVa17zVK7oyTwNGKZXr5fab+1lFRChRyJsShbxpXKHI5fVp6Yb4k+fZdvgMO46eZdvhMwRn5J5NJmnAKKVUHufqIpQK8qFUkA8PVcm+380Rb7RUSimV92jAKKWUcggNGKWUUg6hAaOUUsohNGCUUko5hAaMUkoph9CAUUop5RAaMEoppRwiX0/XLyLHgH13uXlh4HgWlpNVtK47o3XduZxam9Z1ZzJTVyljTPD/t3d/IVKVcRjHv09KCpG1akHk/0hMIZQgIggjQjejTI3QCLSsoKCLootECBGirvpHQVREIpiUV0YFWWkXoheWmim4bquga1BYXkRiIb8uzrt51J11Zt0z79l6PjDsO+fM2fPsb8/O78x52ZmLPeh/3WAuhaRdzXweQrs5V2ucq3V1zeZcrWlHLl8iMzOzSrjBmJlZJdxgBu/d3AEacK7WOFfr6prNuVpTeS7PwZiZWSX8CsbMzCrhBtMPSZ2SDkrqlvRCP+tXSPpV0p50e7y0brmkQ+m2vEa5zpSWb25nrvSYhyQdkLRf0obS8mz1ukiubPWS9Fpp312STpbW5Ty+BsqVs16TJG2VtFvSD5IWlNatStsdlDS/DrkkTZF0qlSvd9qca7Kkr1OmbZImlNYN7fEVEb6VbsAI4CdgGnA5sBeYed5jVgBv9bPtWKAnfe1I447cudK6PzLW60Zgd18tgGtrUq9+c+Wu13mPfwb4oA71apQrd70o5hKeSuOZwJHSeC8wCpiavs+IGuSaAvyYsV6fAMvT+C5gfVXHl1/BXOhWoDsieiLiL2AjsLDJbecDWyLit4j4HdgCdNYgV5WayfUE8HaqCRHxS1qeu16NclWp1d/jMuCjNM5dr0a5qtRMrgDGpPFVwPE0XghsjIjTEXEY6E7fL3euKjWTaybwTRpvLa0f8uPLDeZC1wNHS/ePpWXnW5JeYm6SNLHFbdudC2C0pF2Sdkp6YIgyNZtrOjBd0va0/84Wts2RC/LWCyguZVCcefc9GeSuV6NckLdea4BHJB0DPqd4ddXstjlyAUxNl86+lXTHEGVqNtdeYHEaLwKulDSuyW1b4gYzOLniL+gAAAL1SURBVJ8CUyLiZoouvy5znj4D5ZocxX/tPgy8LumGNuYaSXE56k6KM9/3JF3dxv03MlCunPXqsxTYFBFnMux7IP3lylmvZcCHETEBWACsl1SH57ZGuX4GJkXEHOA5YIOkMQN8n6H2PDBX0m5gLtALVHKM1eGXUDe9QPnMf0Ja9q+IOBERp9Pd94Fbmt02Uy4iojd97QG2AXPalYviTGhzRPydLlV0UTyxZ63XALly16vPUs69DJW7Xo1y5a7XSuDjtP8dwGiK99nKXa9+c6VLdifS8u8o5kymtytXRByPiMWpwa1Oy042+TO1poqJpuF8ozir7aG4BNA3STbrvMdcVxovAnbG2UmywxQTZB1pPLYGuTqAUWk8HjjEABO4FeTqBNaV9n8UGFeDejXKlbVe6XEzgCOk/1Wrw/E1QK7cx9cXwIo0volirkPALM6d5O9h6Cb5LyXXNX05KCbje9t83I8HLkvjl4C1VR1fl/wD/RdvFC9nuyjOLFanZWuB+9P4ZWB/+uVtBWaUtn2MYjKxG3i0DrmA24F9afk+YGWbcwl4FTiQ9r+0JvXqN1fueqX7a4BX+tk2W70a5cpdL4pJ6+1p/3uAeaVtV6ftDgL31CEXsCT9ne4Bvgfua3OuBylOAroornSMqur48n/ym5lZJTwHY2ZmlXCDMTOzSrjBmJlZJdxgzMysEm4wZmZWCTcYMzOrhBuMmZlVwg3GrGYkjZD0RvqMmn2SpuXOZDYYbjBm9bMK6ImIWcCbwNOZ85gNysjcAczsLElXAIsiou+NSg8D92aMZDZobjBm9XI3MFHSnnR/LPBVxjxmg+ZLZGb1Mht4MSJmR8Rs4EuKN0U0G3bcYMzqpQP4E0DSSGAexQfJmQ07bjBm9dIF3JbGzwKfRfFhaGbDjt+u36xGJHVQfFDVeGAH8GREnMqbymxw3GDMzKwSvkRmZmaVcIMxM7NKuMGYmVkl3GDMzKwSbjBmZlYJNxgzM6uEG4yZmVXCDcbMzCrxD+EI8mlcSQVwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals, label = 'lvals')\n",
    "plt.plot(thetas,vlvals, label = 'vlvals')\n",
    "plt.vlines(0.80, ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"aLund vs. Loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.68\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 5e-7 #smaller learning rate yields better precision\n",
    "epochs = 60 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "        \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "        \n",
    "    theta_prime = [0.1365, theta0, 0.217]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0,\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.80, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"aLund Fit \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"aLund Fit\\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
