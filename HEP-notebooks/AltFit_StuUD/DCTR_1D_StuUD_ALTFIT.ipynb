{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTR Alternative Fitting Algorithm for aLund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:04:26.698048Z",
     "start_time": "2020-06-02T01:04:26.691728Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:04:29.090651Z",
     "start_time": "2020-06-02T01:04:27.596632Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:04:29.270679Z",
     "start_time": "2020-06-02T01:04:29.263876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:04:36.696497Z",
     "start_time": "2020-06-02T01:04:36.687419Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:04:39.463379Z",
     "start_time": "2020-06-02T01:04:39.455442Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:04:46.954294Z",
     "start_time": "2020-06-02T01:04:46.948977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:04:48.653700Z",
     "start_time": "2020-06-02T01:04:48.645252Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_probStoUD.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:07:18.283339Z",
     "start_time": "2020-06-02T01:04:49.588352Z"
    }
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:07:19.319811Z",
     "start_time": "2020-06-02T01:07:18.286371Z"
    }
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:07:20.171164Z",
     "start_time": "2020-06-02T01:07:19.324059Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:07:21.353359Z",
     "start_time": "2020-06-02T01:07:20.175392Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100, 100, 128)\n",
    "F_sizes = (100, 100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, Phi_sizes=Phi_sizes, F_sizes=F_sizes, summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights(\n",
    "    './saved_models/DCTR_ee_dijets_1D_probStoUD.h5')  #ORIGINAL DCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:07:21.363031Z",
     "start_time": "2020-06-02T01:07:21.357413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:07:48.435879Z",
     "start_time": "2020-06-02T01:07:48.254262Z"
    }
   },
   "outputs": [],
   "source": [
    "model = PFN(input_dim=4,\n",
    "            Phi_sizes=Phi_sizes,\n",
    "            F_sizes=F_sizes,\n",
    "            output_dim=1,\n",
    "            output_act='sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs, val=0):\n",
    "    x = inputs  #x.shape = (?,?,4)\n",
    "    #Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(\n",
    "        x, np.arange(51),\n",
    "        axis=1)  # Axis corressponds to (max) number of particles in each event\n",
    "\n",
    "    theta_prime = [0.1365, 0.68, val]  # [alphaS, aLund, probStuUD]\n",
    "\n",
    "    # zip theta_prime to each input particle (but not to the padded rows)\n",
    "    #checks if pT != 0, which means we have a particle\n",
    "    concat_input_and_params = tf.where(\n",
    "        K.abs(x[..., 0]) > 0, K.ones_like(x[..., 0]), K.zeros_like(x[..., 0]))\n",
    "\n",
    "    concat_input_and_params = theta_prime * K.stack([concat_input_and_params, \n",
    "                                                     concat_input_and_params,\n",
    "                                                     concat_input_and_params],\n",
    "                                                    axis=-1)\n",
    "\n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data)  # NN reweight\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        #t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "\n",
    "        # Categorical Cross-Entropy Loss\n",
    "\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        t_loss = -((y_true) * K.log(y_pred) + w *\n",
    "                   (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T04:05:35.215419Z",
     "start_time": "2020-06-02T01:07:49.115788Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From <ipython-input-13-609327681ebf>:28: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 134s 149us/step - loss: 0.6957 - acc: 0.5129 - val_loss: 0.6751 - val_acc: 0.5193\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 131s 146us/step - loss: 0.6704 - acc: 0.5238 - val_loss: 0.6671 - val_acc: 0.5262\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 129s 144us/step - loss: 0.6666 - acc: 0.5275 - val_loss: 0.6646 - val_acc: 0.5277\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 128s 142us/step - loss: 0.6651 - acc: 0.5285 - val_loss: 0.6642 - val_acc: 0.5280\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 128s 142us/step - loss: 0.6641 - acc: 0.5286 - val_loss: 0.6645 - val_acc: 0.5274\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 127s 142us/step - loss: 0.6630 - acc: 0.5293 - val_loss: 0.6629 - val_acc: 0.5293\n",
      "Epoch 7/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6625 - acc: 0.5300 - val_loss: 0.6631 - val_acc: 0.5280\n",
      "Epoch 8/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6621 - acc: 0.5301 - val_loss: 0.6620 - val_acc: 0.5288\n",
      "Epoch 9/50\n",
      "900000/900000 [==============================] - 128s 142us/step - loss: 0.6618 - acc: 0.5304 - val_loss: 0.6617 - val_acc: 0.5298\n",
      "Epoch 10/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6616 - acc: 0.5304 - val_loss: 0.6611 - val_acc: 0.5296\n",
      "Epoch 11/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6614 - acc: 0.5304 - val_loss: 0.6611 - val_acc: 0.5298\n",
      "Epoch 12/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6613 - acc: 0.5308 - val_loss: 0.6612 - val_acc: 0.5293\n",
      "Epoch 13/50\n",
      "900000/900000 [==============================] - 128s 142us/step - loss: 0.6612 - acc: 0.5308 - val_loss: 0.6610 - val_acc: 0.5299\n",
      "Epoch 14/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6611 - acc: 0.5308 - val_loss: 0.6614 - val_acc: 0.5291\n",
      "Epoch 15/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6610 - acc: 0.5308 - val_loss: 0.6613 - val_acc: 0.5298\n",
      "Epoch 16/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6610 - acc: 0.5308 - val_loss: 0.6612 - val_acc: 0.5302\n",
      "Epoch 17/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6609 - acc: 0.5308 - val_loss: 0.6610 - val_acc: 0.5302\n",
      "Epoch 18/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6609 - acc: 0.5311 - val_loss: 0.6611 - val_acc: 0.5300\n",
      "Epoch 19/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6608 - acc: 0.5310 - val_loss: 0.6615 - val_acc: 0.5299\n",
      "Epoch 20/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6608 - acc: 0.5312 - val_loss: 0.6612 - val_acc: 0.5294\n",
      "Epoch 21/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6607 - acc: 0.5313 - val_loss: 0.6610 - val_acc: 0.5293\n",
      "Epoch 22/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6607 - acc: 0.5314 - val_loss: 0.6608 - val_acc: 0.5296\n",
      "Epoch 23/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6606 - acc: 0.5315 - val_loss: 0.6608 - val_acc: 0.5301\n",
      "Epoch 24/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6605 - acc: 0.5314 - val_loss: 0.6610 - val_acc: 0.5297\n",
      "Epoch 25/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6605 - acc: 0.5314 - val_loss: 0.6609 - val_acc: 0.5299\n",
      "Epoch 26/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6605 - acc: 0.5314 - val_loss: 0.6615 - val_acc: 0.5300\n",
      "Epoch 27/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6604 - acc: 0.5316 - val_loss: 0.6609 - val_acc: 0.5298\n",
      "trainnig theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 131s 146us/step - loss: 0.6709 - acc: 0.5316 - val_loss: 0.6711 - val_acc: 0.5300\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6708 - acc: 0.5316 - val_loss: 0.6712 - val_acc: 0.5301\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 127s 142us/step - loss: 0.6707 - acc: 0.5319 - val_loss: 0.6714 - val_acc: 0.5294\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6707 - acc: 0.5318 - val_loss: 0.6712 - val_acc: 0.5301\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6706 - acc: 0.5322 - val_loss: 0.6715 - val_acc: 0.5300\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6706 - acc: 0.5319 - val_loss: 0.6711 - val_acc: 0.5301\n",
      "trainnig theta = : 0.15\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 131s 145us/step - loss: 0.6805 - acc: 0.5313 - val_loss: 0.6809 - val_acc: 0.5300\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 129s 143us/step - loss: 0.6804 - acc: 0.5315 - val_loss: 0.6807 - val_acc: 0.5302\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6803 - acc: 0.5319 - val_loss: 0.6807 - val_acc: 0.5300\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6803 - acc: 0.5320 - val_loss: 0.6808 - val_acc: 0.5299\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6802 - acc: 0.5316 - val_loss: 0.6811 - val_acc: 0.5301\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6802 - acc: 0.5317 - val_loss: 0.6807 - val_acc: 0.5304\n",
      "Epoch 7/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6802 - acc: 0.5320 - val_loss: 0.6809 - val_acc: 0.5300\n",
      "Epoch 8/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6801 - acc: 0.5325 - val_loss: 0.6808 - val_acc: 0.5302\n",
      "Epoch 9/50\n",
      "900000/900000 [==============================] - 126s 139us/step - loss: 0.6801 - acc: 0.5322 - val_loss: 0.6808 - val_acc: 0.5298\n",
      "Epoch 10/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6800 - acc: 0.5324 - val_loss: 0.6816 - val_acc: 0.5286\n",
      "Epoch 11/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6799 - acc: 0.5325 - val_loss: 0.6810 - val_acc: 0.5296\n",
      "trainnig theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 130s 144us/step - loss: 0.6874 - acc: 0.5322 - val_loss: 0.6879 - val_acc: 0.5301\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 128s 142us/step - loss: 0.6873 - acc: 0.5323 - val_loss: 0.6879 - val_acc: 0.5301\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 128s 142us/step - loss: 0.6873 - acc: 0.5324 - val_loss: 0.6881 - val_acc: 0.5294\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6872 - acc: 0.5326 - val_loss: 0.6880 - val_acc: 0.5294\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6871 - acc: 0.5329 - val_loss: 0.6882 - val_acc: 0.5296\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6871 - acc: 0.5329 - val_loss: 0.6882 - val_acc: 0.5295\n",
      "trainnig theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 130s 144us/step - loss: 0.6920 - acc: 0.5322 - val_loss: 0.6926 - val_acc: 0.5298\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6920 - acc: 0.5324 - val_loss: 0.6927 - val_acc: 0.5298\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6919 - acc: 0.5325 - val_loss: 0.6927 - val_acc: 0.5297\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 125s 138us/step - loss: 0.6919 - acc: 0.5325 - val_loss: 0.6927 - val_acc: 0.5295\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6918 - acc: 0.5330 - val_loss: 0.6931 - val_acc: 0.5291\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 124s 138us/step - loss: 0.6917 - acc: 0.5330 - val_loss: 0.6929 - val_acc: 0.5295\n",
      "trainnig theta = : 0.22499999999999998\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 129s 143us/step - loss: 0.6958 - acc: 0.5319 - val_loss: 0.6965 - val_acc: 0.5282\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6958 - acc: 0.5322 - val_loss: 0.6967 - val_acc: 0.5289\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 124s 138us/step - loss: 0.6957 - acc: 0.5325 - val_loss: 0.6965 - val_acc: 0.5292\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 124s 138us/step - loss: 0.6956 - acc: 0.5329 - val_loss: 0.6964 - val_acc: 0.5292\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 125s 138us/step - loss: 0.6956 - acc: 0.5330 - val_loss: 0.6966 - val_acc: 0.5277\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 125s 138us/step - loss: 0.6955 - acc: 0.5334 - val_loss: 0.6966 - val_acc: 0.5279\n",
      "Epoch 7/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6955 - acc: 0.5335 - val_loss: 0.6967 - val_acc: 0.5274\n",
      "Epoch 8/50\n",
      "900000/900000 [==============================] - 125s 138us/step - loss: 0.6953 - acc: 0.5339 - val_loss: 0.6968 - val_acc: 0.5274\n",
      "Epoch 9/50\n",
      "900000/900000 [==============================] - 124s 138us/step - loss: 0.6952 - acc: 0.5342 - val_loss: 0.6969 - val_acc: 0.5278\n",
      "trainnig theta = : 0.25\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 129s 144us/step - loss: 0.6943 - acc: 0.5310 - val_loss: 0.6950 - val_acc: 0.5266\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6942 - acc: 0.5310 - val_loss: 0.6950 - val_acc: 0.5256\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6942 - acc: 0.5307 - val_loss: 0.6952 - val_acc: 0.5247\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6941 - acc: 0.5312 - val_loss: 0.6952 - val_acc: 0.5235\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 124s 138us/step - loss: 0.6940 - acc: 0.5309 - val_loss: 0.6952 - val_acc: 0.5254\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 125s 138us/step - loss: 0.6939 - acc: 0.5327 - val_loss: 0.6953 - val_acc: 0.5241\n",
      "trainnig theta = : 0.275\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 131s 145us/step - loss: 0.6945 - acc: 0.5234 - val_loss: 0.6952 - val_acc: 0.5193\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6944 - acc: 0.5222 - val_loss: 0.6951 - val_acc: 0.5089\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6943 - acc: 0.5213 - val_loss: 0.6952 - val_acc: 0.5157\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 126s 140us/step - loss: 0.6942 - acc: 0.5211 - val_loss: 0.6953 - val_acc: 0.5133\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6941 - acc: 0.5227 - val_loss: 0.6952 - val_acc: 0.5127\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6940 - acc: 0.5221 - val_loss: 0.6956 - val_acc: 0.5106\n",
      "Epoch 7/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6939 - acc: 0.5226 - val_loss: 0.6955 - val_acc: 0.5113\n",
      "trainnig theta = : 0.3\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/50\n",
      "900000/900000 [==============================] - 131s 145us/step - loss: 0.6959 - acc: 0.5048 - val_loss: 0.6966 - val_acc: 0.4905\n",
      "Epoch 2/50\n",
      "900000/900000 [==============================] - 129s 143us/step - loss: 0.6957 - acc: 0.5026 - val_loss: 0.6968 - val_acc: 0.4905\n",
      "Epoch 3/50\n",
      "900000/900000 [==============================] - 128s 142us/step - loss: 0.6956 - acc: 0.5044 - val_loss: 0.6968 - val_acc: 0.4919\n",
      "Epoch 4/50\n",
      "900000/900000 [==============================] - 128s 142us/step - loss: 0.6955 - acc: 0.5030 - val_loss: 0.6968 - val_acc: 0.4914\n",
      "Epoch 5/50\n",
      "900000/900000 [==============================] - 127s 141us/step - loss: 0.6954 - acc: 0.5061 - val_loss: 0.6971 - val_acc: 0.4939\n",
      "Epoch 6/50\n",
      "900000/900000 [==============================] - 125s 139us/step - loss: 0.6953 - acc: 0.5078 - val_loss: 0.6970 - val_acc: 0.4858\n",
      "[0.6604026738140318, 0.6706149539020326, 0.6798833249012629, 0.6870796509583791, 0.6917402934365803, 0.6951920788817936, 0.693850998878479, 0.6939198243618011, 0.6952524294455846]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 9)  #iterating across possible alphaS values\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam',\n",
    "                        loss=my_loss_wrapper(myinputs, theta),\n",
    "                        metrics=['accuracy'])\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs=50,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[earlystopping])\n",
    "    lvals += [np.min(history.history['loss'])]\n",
    "    vlvals += [np.min(history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T04:13:02.367810Z",
     "start_time": "2020-06-02T04:13:01.881893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f97e01b1b50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.vlines(0.275, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"probStuUD Vs Loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T03:26:09.171164Z",
     "start_time": "2020-05-31T03:25:59.055Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.217\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T03:26:09.172244Z",
     "start_time": "2020-05-31T03:25:59.057Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 5e-7 #smaller learning rate yields better precision\n",
    "epochs = 60 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "        \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "        \n",
    "    theta_prime = [0.1365, 0.68, theta0]\n",
    "    \n",
    "    # Add MC params to each input particle (but not to the padded rows)\n",
    "    concat_input_and_params = tf.where(K.abs(x[...,0])>0,\n",
    "                                   K.ones_like(x[...,0]),\n",
    "                                   K.zeros_like(x[...,0]))\n",
    "    \n",
    "    concat_input_and_params = theta_prime*K.stack([concat_input_and_params, concat_input_and_params, concat_input_and_params], axis = -1)\n",
    "    \n",
    "    data = K.concatenate([x, concat_input_and_params], -1)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T03:26:09.173339Z",
     "start_time": "2020-05-31T03:25:59.058Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.2750, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"probStuUD (start = 0.12) \\nN = {:.0e}, learning_rate = {:.0e}\".format(len(X_default), lr))\n",
    "plt.savefig(\"probStuUD Fit \\nN = {:.0e}, learning_rate = {:.0e}.png\".format(len(X_default), 5e-7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
