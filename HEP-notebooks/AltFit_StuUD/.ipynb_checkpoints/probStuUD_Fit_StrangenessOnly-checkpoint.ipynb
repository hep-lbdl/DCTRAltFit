{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:17.571858Z",
     "start_time": "2020-06-02T15:56:17.564059Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:20.028697Z",
     "start_time": "2020-06-02T15:56:17.576634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.utils import remap_pids\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:20.052471Z",
     "start_time": "2020-06-02T15:56:20.043184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global plot settings\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22) \n",
    "rc('xtick', labelsize=15) \n",
    "rc('ytick', labelsize=15) \n",
    "rc('legend', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:20.062742Z",
     "start_time": "2020-06-02T15:56:20.056665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:20.075257Z",
     "start_time": "2020-06-02T15:56:20.067226Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:20.230469Z",
     "start_time": "2020-06-02T15:56:20.223446Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    #remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:21.092649Z",
     "start_time": "2020-06-02T15:56:21.088089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = '/data0/users/aandreassen/zenodo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:21.264507Z",
     "start_time": "2020-06-02T15:56:21.258995Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = np.load(data_dir + '1D_probStoUD_train.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:56:46.162555Z",
     "start_time": "2020-06-02T15:56:21.648871Z"
    }
   },
   "outputs": [],
   "source": [
    "X = dataset['X']\n",
    "Y = dataset['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T15:59:34.909304Z",
     "start_time": "2020-06-02T15:56:46.166317Z"
    }
   },
   "outputs": [],
   "source": [
    "X = preprocess_data(X)\n",
    "Y = to_categorical(Y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:08:57.249812Z",
     "start_time": "2020-06-02T15:59:34.913934Z"
    }
   },
   "outputs": [],
   "source": [
    "# strange hadrons particle IDs\n",
    "strange_PIDs = np.array([130, 310, 311, 321, 3122, 3222, 3212, 3112])\n",
    "\n",
    "# add anti-particles\n",
    "strange_PIDs = np.concatenate([strange_PIDs,-strange_PIDs])\n",
    "X_strangeness = []\n",
    "#More preprocessing: zipping data points with no particle with parameters\n",
    "\n",
    "for event in X:\n",
    "    probStoUD = event[0][6]\n",
    "    PIDs = event[:, 3]\n",
    "    number_strange_hadrons = 0\n",
    "    for PID in PIDs:\n",
    "        number_strange_hadrons += PID in strange_PIDs\n",
    "    X_strangeness.append([number_strange_hadrons, probStoUD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:08:58.435184Z",
     "start_time": "2020-06-02T16:08:57.254325Z"
    }
   },
   "outputs": [],
   "source": [
    "X_strangeness = np.array(X_strangeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:08:58.667185Z",
     "start_time": "2020-06-02T16:08:58.463610Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_strangeness, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:08:58.799339Z",
     "start_time": "2020-06-02T16:08:58.671695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((2,))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs = inputs, outputs = outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:09:55.186026Z",
     "start_time": "2020-06-02T16:08:58.804536Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6919 - acc: 0.5161 - val_loss: 0.6912 - val_acc: 0.5229\n",
      "Epoch 2/200\n",
      "1440000/1440000 [==============================] - 2s 1us/step - loss: 0.6905 - acc: 0.5247 - val_loss: 0.6903 - val_acc: 0.5237\n",
      "Epoch 3/200\n",
      "1440000/1440000 [==============================] - 2s 1us/step - loss: 0.6899 - acc: 0.5272 - val_loss: 0.6900 - val_acc: 0.5262\n",
      "Epoch 4/200\n",
      "1440000/1440000 [==============================] - 2s 1us/step - loss: 0.6898 - acc: 0.5275 - val_loss: 0.6899 - val_acc: 0.5256\n",
      "Epoch 5/200\n",
      "1440000/1440000 [==============================] - 2s 1us/step - loss: 0.6897 - acc: 0.5279 - val_loss: 0.6899 - val_acc: 0.5261\n",
      "Epoch 6/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6897 - acc: 0.5276 - val_loss: 0.6898 - val_acc: 0.5268\n",
      "Epoch 7/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6897 - acc: 0.5273 - val_loss: 0.6900 - val_acc: 0.5246\n",
      "Epoch 8/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6897 - acc: 0.5275 - val_loss: 0.6898 - val_acc: 0.5265\n",
      "Epoch 9/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6897 - acc: 0.5272 - val_loss: 0.6899 - val_acc: 0.5263\n",
      "Epoch 10/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6897 - acc: 0.5275 - val_loss: 0.6898 - val_acc: 0.5265\n",
      "Epoch 11/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6897 - acc: 0.5275 - val_loss: 0.6898 - val_acc: 0.5267\n",
      "Epoch 12/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5278 - val_loss: 0.6899 - val_acc: 0.5261\n",
      "Epoch 13/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5277 - val_loss: 0.6899 - val_acc: 0.5254\n",
      "Epoch 14/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5277 - val_loss: 0.6898 - val_acc: 0.5258\n",
      "Epoch 15/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5274 - val_loss: 0.6898 - val_acc: 0.5266\n",
      "Epoch 16/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5280 - val_loss: 0.6899 - val_acc: 0.5268\n",
      "Epoch 17/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5276 - val_loss: 0.6899 - val_acc: 0.5268\n",
      "Epoch 18/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5279 - val_loss: 0.6899 - val_acc: 0.5267\n",
      "Epoch 19/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5279 - val_loss: 0.6898 - val_acc: 0.5268\n",
      "Epoch 20/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5278 - val_loss: 0.6898 - val_acc: 0.5260\n",
      "Epoch 21/200\n",
      "1440000/1440000 [==============================] - 2s 1us/step - loss: 0.6896 - acc: 0.5279 - val_loss: 0.6898 - val_acc: 0.5264\n",
      "Epoch 22/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6896 - acc: 0.5277 - val_loss: 0.6902 - val_acc: 0.5262\n",
      "Epoch 23/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6897 - acc: 0.5275 - val_loss: 0.6900 - val_acc: 0.5263\n",
      "Epoch 24/200\n",
      "1440000/1440000 [==============================] - 2s 1us/step - loss: 0.6896 - acc: 0.5277 - val_loss: 0.6898 - val_acc: 0.5261\n",
      "Epoch 25/200\n",
      "1440000/1440000 [==============================] - 2s 1us/step - loss: 0.6896 - acc: 0.5283 - val_loss: 0.6898 - val_acc: 0.5263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f23e44e8fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience = 10,\n",
    "                              restore_best_weights=True)\n",
    "dctr_model.fit(X_train, Y_train, \n",
    "               epochs=200, batch_size = 10000, \n",
    "               validation_data = (X_val, Y_val), \n",
    "               verbose = 1, \n",
    "               callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:09:55.195518Z",
     "start_time": "2020-06-02T16:09:55.190583Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_alphaS.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:20:00.421930Z",
     "start_time": "2020-06-02T16:09:55.200491Z"
    }
   },
   "outputs": [],
   "source": [
    "X_0_jet = default_dataset['jet']\n",
    "X_0 = []\n",
    "for event in X_0_jet:\n",
    "    PIDs = event[:, 3]\n",
    "    number_strange_hadrons = 0\n",
    "    for PID in PIDs:\n",
    "        number_strange_hadrons += PID in strange_PIDs\n",
    "    X_0.append([number_strange_hadrons])\n",
    "X_0 = np.array(X_0)\n",
    "\n",
    "X_1_jet = unknown_dataset['jet']\n",
    "X_1 = []\n",
    "for event in X_1_jet:\n",
    "    PIDs = event[:, 3]\n",
    "    number_strange_hadrons = 0\n",
    "    for PID in PIDs:\n",
    "        number_strange_hadrons += PID in strange_PIDs\n",
    "    X_1.append([number_strange_hadrons])\n",
    "X_1 = np.array(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:20:00.597112Z",
     "start_time": "2020-06-02T16:20:00.426587Z"
    }
   },
   "outputs": [],
   "source": [
    "labels0 = np.zeros(len(X_0))\n",
    "labels1 = np.ones(len(X_1))\n",
    "\n",
    "xvals = np.concatenate([X_0,X_1])\n",
    "yvals = np.concatenate([labels0,labels1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals, yvals, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:20:00.608140Z",
     "start_time": "2020-06-02T16:20:00.601599Z"
    }
   },
   "outputs": [],
   "source": [
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr_model(d)\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T16:20:00.700456Z",
     "start_time": "2020-06-02T16:20:00.613248Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "earlystopping = EarlyStopping(patience = 10,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0):\n",
    "    x  = inputs\n",
    "    x = K.squeeze(x, axis = 1)\n",
    "    x = K.gather(x, np.arange(500))\n",
    "\n",
    "    theta = 0. #starting value\n",
    "    #theta0 = tf.constant(val, dtype= tf.float32)#target value\n",
    "    \n",
    "    #creating tensor with same shape as inputs, with val in every entry\n",
    "    theta0_stack = K.constant(val, dtype=tf.float32, shape = x.shape)\n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.stack((x, theta0_stack), axis=-1) \n",
    "    \n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        #t_loss = y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        \n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:47:33.981216Z",
     "start_time": "2020-06-02T16:20:00.705038Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : 0.1\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6658 - acc: 0.5195 - val_loss: 0.6649 - val_acc: 0.5180\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 7us/step - loss: 0.6654 - acc: 0.5200 - val_loss: 0.6649 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 7us/step - loss: 0.6654 - acc: 0.5196 - val_loss: 0.6649 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6654 - acc: 0.5199 - val_loss: 0.6651 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6654 - acc: 0.5195 - val_loss: 0.6649 - val_acc: 0.5180\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6653 - acc: 0.5202 - val_loss: 0.6651 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6653 - acc: 0.5195 - val_loss: 0.6650 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6653 - acc: 0.5201 - val_loss: 0.6649 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 7us/step - loss: 0.6653 - acc: 0.5200 - val_loss: 0.6649 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 7us/step - loss: 0.6653 - acc: 0.5199 - val_loss: 0.6649 - val_acc: 0.5180\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 7us/step - loss: 0.6653 - acc: 0.5197 - val_loss: 0.6649 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 7us/step - loss: 0.6653 - acc: 0.5196 - val_loss: 0.6649 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6653 - acc: 0.5195 - val_loss: 0.6649 - val_acc: 0.5180\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 7us/step - loss: 0.6653 - acc: 0.5200 - val_loss: 0.6649 - val_acc: 0.5180\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 7us/step - loss: 0.6653 - acc: 0.5192 - val_loss: 0.6649 - val_acc: 0.5203\n",
      "testing theta = : 0.10625000000000001\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5200 - val_loss: 0.6685 - val_acc: 0.5180\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5202 - val_loss: 0.6687 - val_acc: 0.5180\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5198 - val_loss: 0.6685 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5203 - val_loss: 0.6684 - val_acc: 0.5180\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5200 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5205 - val_loss: 0.6685 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5204 - val_loss: 0.6684 - val_acc: 0.5180\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5201 - val_loss: 0.6684 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5208 - val_loss: 0.6684 - val_acc: 0.5180\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5197 - val_loss: 0.6686 - val_acc: 0.5180\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5201 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5203 - val_loss: 0.6685 - val_acc: 0.5180\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5200 - val_loss: 0.6685 - val_acc: 0.5180\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5197 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5202 - val_loss: 0.6685 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5200 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5199 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5207 - val_loss: 0.6684 - val_acc: 0.5180\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5202 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5198 - val_loss: 0.6685 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5200 - val_loss: 0.6687 - val_acc: 0.5180\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5200 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5201 - val_loss: 0.6685 - val_acc: 0.5203\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5201 - val_loss: 0.6684 - val_acc: 0.5180\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5200 - val_loss: 0.6684 - val_acc: 0.5180\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5203 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5201 - val_loss: 0.6685 - val_acc: 0.5203\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6688 - acc: 0.5200 - val_loss: 0.6684 - val_acc: 0.5203\n",
      "testing theta = : 0.1125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5204 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5201 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5202 - val_loss: 0.6712 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5203 - val_loss: 0.6714 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5203 - val_loss: 0.6712 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5210 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5202 - val_loss: 0.6715 - val_acc: 0.5180\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5207 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5205 - val_loss: 0.6712 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5208 - val_loss: 0.6712 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5202 - val_loss: 0.6712 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5207 - val_loss: 0.6714 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5202 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5203 - val_loss: 0.6713 - val_acc: 0.5203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5204 - val_loss: 0.6712 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5204 - val_loss: 0.6714 - val_acc: 0.5180\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5208 - val_loss: 0.6713 - val_acc: 0.5180\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5207 - val_loss: 0.6714 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5201 - val_loss: 0.6714 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5208 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5201 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5207 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5202 - val_loss: 0.6713 - val_acc: 0.5180\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5208 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6716 - acc: 0.5207 - val_loss: 0.6713 - val_acc: 0.5203\n",
      "testing theta = : 0.11875000000000001\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6739 - acc: 0.5207 - val_loss: 0.6737 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5205 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5209 - val_loss: 0.6737 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5205 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5209 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5204 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5208 - val_loss: 0.6737 - val_acc: 0.5180\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5204 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5207 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5210 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5208 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5211 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5207 - val_loss: 0.6736 - val_acc: 0.5180\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5208 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5208 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5204 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5206 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5211 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5207 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5210 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5207 - val_loss: 0.6737 - val_acc: 0.5203\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5209 - val_loss: 0.6736 - val_acc: 0.5180\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5206 - val_loss: 0.6737 - val_acc: 0.5203\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6739 - acc: 0.5208 - val_loss: 0.6736 - val_acc: 0.5203\n",
      "testing theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6753 - acc: 0.5212 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5209 - val_loss: 0.6751 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5209 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5207 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5211 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5207 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5209 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5210 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5208 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5212 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5208 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5209 - val_loss: 0.6751 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5208 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5212 - val_loss: 0.6751 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5209 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5208 - val_loss: 0.6751 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6753 - acc: 0.5211 - val_loss: 0.6750 - val_acc: 0.5203\n",
      "testing theta = : 0.13125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6764 - acc: 0.5212 - val_loss: 0.6763 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6765 - acc: 0.5211 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5211 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5210 - val_loss: 0.6762 - val_acc: 0.5180\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5208 - val_loss: 0.6762 - val_acc: 0.5203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6765 - acc: 0.5211 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5210 - val_loss: 0.6763 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5212 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5211 - val_loss: 0.6763 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5211 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5209 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5210 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5210 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5210 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6765 - acc: 0.5212 - val_loss: 0.6762 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6764 - acc: 0.5211 - val_loss: 0.6762 - val_acc: 0.5180\n",
      "testing theta = : 0.1375\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6781 - acc: 0.5211 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6781 - acc: 0.5211 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6780 - acc: 0.5211 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6781 - acc: 0.5212 - val_loss: 0.6779 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6781 - acc: 0.5211 - val_loss: 0.6779 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6781 - acc: 0.5211 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6781 - acc: 0.5210 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6780 - acc: 0.5212 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6781 - acc: 0.5212 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6781 - acc: 0.5210 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6781 - acc: 0.5212 - val_loss: 0.6779 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6781 - acc: 0.5210 - val_loss: 0.6779 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6781 - acc: 0.5212 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6781 - acc: 0.5212 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6781 - acc: 0.5211 - val_loss: 0.6778 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6781 - acc: 0.5210 - val_loss: 0.6779 - val_acc: 0.5203\n",
      "testing theta = : 0.14375\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5211 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5213 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6795 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5211 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5211 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5211 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5211 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5211 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 32/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "Epoch 33/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 34/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 35/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 36/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5211 - val_loss: 0.6795 - val_acc: 0.5203\n",
      "Epoch 37/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 38/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 39/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 40/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 41/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5211 - val_loss: 0.6794 - val_acc: 0.5203\n",
      "Epoch 42/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6796 - acc: 0.5212 - val_loss: 0.6793 - val_acc: 0.5203\n",
      "testing theta = : 0.15\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6810 - acc: 0.5212 - val_loss: 0.6809 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5211 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5211 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5211 - val_loss: 0.6809 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5211 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5212 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5211 - val_loss: 0.6809 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5212 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5212 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5212 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5212 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5212 - val_loss: 0.6808 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6810 - acc: 0.5211 - val_loss: 0.6809 - val_acc: 0.5203\n",
      "testing theta = : 0.15625\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5210 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5211 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5212 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6826 - acc: 0.5210 - val_loss: 0.6825 - val_acc: 0.5203\n",
      "testing theta = : 0.1625\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6843 - acc: 0.5211 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6844 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6843 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5210 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5213 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5211 - val_loss: 0.6843 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5211 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5210 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6843 - val_acc: 0.5203\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6843 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6843 - val_acc: 0.5203\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6843 - acc: 0.5212 - val_loss: 0.6842 - val_acc: 0.5203\n",
      "testing theta = : 0.16875\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6862 - acc: 0.5213 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6862 - acc: 0.5211 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6861 - acc: 0.5210 - val_loss: 0.6860 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5211 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6861 - acc: 0.5211 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6860 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6861 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6861 - acc: 0.5211 - val_loss: 0.6860 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6861 - acc: 0.5212 - val_loss: 0.6860 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6861 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6861 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6862 - acc: 0.5211 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6861 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6861 - acc: 0.5212 - val_loss: 0.6861 - val_acc: 0.5203\n",
      "testing theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5211 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6876 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6876 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6877 - acc: 0.5212 - val_loss: 0.6877 - val_acc: 0.5203\n",
      "testing theta = : 0.18125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6889 - acc: 0.5211 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6889 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.5212 - val_loss: 0.6888 - val_acc: 0.5203\n",
      "testing theta = : 0.1875\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6895 - acc: 0.5211 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6895 - acc: 0.5212 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "testing theta = : 0.19374999999999998\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6898 - acc: 0.5212 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5212 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5214 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5213 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5212 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5213 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5212 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5208 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5212 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5206 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5212 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5207 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 7s 8us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5210 - val_loss: 0.6898 - val_acc: 0.5180\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5209 - val_loss: 0.6898 - val_acc: 0.5203\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.5212 - val_loss: 0.6897 - val_acc: 0.5203\n",
      "testing theta = : 0.2\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6896 - acc: 0.5207 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6896 - acc: 0.5213 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6896 - acc: 0.5207 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6896 - acc: 0.5210 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6896 - acc: 0.5211 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6896 - acc: 0.5206 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6896 - acc: 0.5210 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6896 - acc: 0.5211 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6896 - acc: 0.5209 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6896 - acc: 0.5209 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6896 - acc: 0.5208 - val_loss: 0.6896 - val_acc: 0.5203\n",
      "testing theta = : 0.20625\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.6894 - acc: 0.5210 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5209 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5205 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5209 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5208 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5206 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5210 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5208 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5207 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5208 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5209 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5208 - val_loss: 0.6895 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5206 - val_loss: 0.6895 - val_acc: 0.5180\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5207 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5209 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5208 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 8s 8us/step - loss: 0.6894 - acc: 0.5203 - val_loss: 0.6895 - val_acc: 0.5180\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5206 - val_loss: 0.6895 - val_acc: 0.5180\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5208 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5206 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6894 - acc: 0.5207 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "testing theta = : 0.2125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.6892 - acc: 0.5203 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5206 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5206 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5205 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5203 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5201 - val_loss: 0.6893 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5207 - val_loss: 0.6893 - val_acc: 0.5180\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5199 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5200 - val_loss: 0.6893 - val_acc: 0.5180\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5208 - val_loss: 0.6893 - val_acc: 0.5180\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5199 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5201 - val_loss: 0.6893 - val_acc: 0.5180\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5207 - val_loss: 0.6893 - val_acc: 0.5180\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5200 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6893 - val_acc: 0.5180\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5203 - val_loss: 0.6893 - val_acc: 0.5203\n",
      "testing theta = : 0.21875\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.6892 - acc: 0.5205 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5199 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5203 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5197 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5194 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5206 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5202 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5206 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5200 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5199 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5199 - val_loss: 0.6893 - val_acc: 0.5180\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5205 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5201 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5196 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5203 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5206 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5201 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5201 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5204 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5201 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5200 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5206 - val_loss: 0.6892 - val_acc: 0.5180\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5202 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5195 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6892 - acc: 0.5197 - val_loss: 0.6892 - val_acc: 0.5203\n",
      "testing theta = : 0.22499999999999998\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.6889 - acc: 0.5202 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5201 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5190 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5202 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5196 - val_loss: 0.6890 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5194 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5197 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5199 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5199 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5199 - val_loss: 0.6890 - val_acc: 0.5203\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5199 - val_loss: 0.6890 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5195 - val_loss: 0.6890 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5200 - val_loss: 0.6891 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5194 - val_loss: 0.6890 - val_acc: 0.5180\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5201 - val_loss: 0.6890 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.5195 - val_loss: 0.6890 - val_acc: 0.5203\n",
      "testing theta = : 0.23124999999999998\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.6886 - acc: 0.5190 - val_loss: 0.6887 - val_acc: 0.5203\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5193 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5189 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5191 - val_loss: 0.6887 - val_acc: 0.5203\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5191 - val_loss: 0.6887 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5198 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5189 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5188 - val_loss: 0.6887 - val_acc: 0.5203\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5191 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5191 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5186 - val_loss: 0.6887 - val_acc: 0.5203\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5189 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5185 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5194 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5188 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5193 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5188 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5193 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5189 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5192 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5186 - val_loss: 0.6887 - val_acc: 0.5203\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5187 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5187 - val_loss: 0.6887 - val_acc: 0.5203\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5189 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5190 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.5194 - val_loss: 0.6887 - val_acc: 0.5180\n",
      "testing theta = : 0.2375\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.6885 - acc: 0.5192 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5195 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5189 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5187 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5188 - val_loss: 0.6886 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5186 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5185 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5188 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5186 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5187 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5188 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5186 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5188 - val_loss: 0.6886 - val_acc: 0.5203\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5192 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5185 - val_loss: 0.6886 - val_acc: 0.5203\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6885 - acc: 0.5188 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "testing theta = : 0.24375\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 10s 12us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5190 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5184 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5187 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5187 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5184 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5184 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5188 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5183 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5189 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5190 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "testing theta = : 0.25\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 10s 12us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5187 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5187 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5203\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5188 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5203\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5188 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5187 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5187 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5187 - val_loss: 0.6886 - val_acc: 0.5180\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5188 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5191 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 31/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 32/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5188 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 33/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5186 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "testing theta = : 0.25625\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 11s 12us/step - loss: 0.6883 - acc: 0.5179 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5173 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6884 - acc: 0.5183 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5176 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5184 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5172 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5187 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5182 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5184 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5178 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5178 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5184 - val_loss: 0.6885 - val_acc: 0.5180\n",
      "testing theta = : 0.26249999999999996\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 11s 12us/step - loss: 0.6882 - acc: 0.5140 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5114 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5102 - val_loss: 0.6884 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5109 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5108 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5098 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5129 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5095 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5097 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5126 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5092 - val_loss: 0.6884 - val_acc: 0.5180\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5135 - val_loss: 0.6884 - val_acc: 0.5001\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6882 - acc: 0.5090 - val_loss: 0.6884 - val_acc: 0.5180\n",
      "testing theta = : 0.26875\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 11s 12us/step - loss: 0.6881 - acc: 0.5045 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6881 - acc: 0.5012 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5008 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5002 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5012 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5009 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5004 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5021 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5011 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5023 - val_loss: 0.6883 - val_acc: 0.5146\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5019 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5002 - val_loss: 0.6883 - val_acc: 0.5001\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6881 - acc: 0.5010 - val_loss: 0.6883 - val_acc: 0.5001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : 0.275\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 11s 12us/step - loss: 0.6883 - acc: 0.5001 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.4999 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5020 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5008 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5003 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5000 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5001 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5000 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5003 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5000 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5017 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.4999 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6883 - acc: 0.5009 - val_loss: 0.6885 - val_acc: 0.5001\n",
      "testing theta = : 0.28125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 11s 12us/step - loss: 0.6887 - acc: 0.4981 - val_loss: 0.6889 - val_acc: 0.4857\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6887 - acc: 0.4929 - val_loss: 0.6889 - val_acc: 0.5001\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4920 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4918 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4932 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4924 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4927 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4918 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4927 - val_loss: 0.6889 - val_acc: 0.4943\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4935 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4935 - val_loss: 0.6889 - val_acc: 0.4943\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4934 - val_loss: 0.6889 - val_acc: 0.4943\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4936 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4944 - val_loss: 0.6888 - val_acc: 0.4912\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6886 - acc: 0.4923 - val_loss: 0.6888 - val_acc: 0.4943\n",
      "testing theta = : 0.2875\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 11s 13us/step - loss: 0.6888 - acc: 0.4878 - val_loss: 0.6890 - val_acc: 0.4798\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4876 - val_loss: 0.6890 - val_acc: 0.4942\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6888 - acc: 0.4879 - val_loss: 0.6890 - val_acc: 0.4912\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6888 - acc: 0.4856 - val_loss: 0.6890 - val_acc: 0.4912\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4861 - val_loss: 0.6890 - val_acc: 0.4943\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4879 - val_loss: 0.6890 - val_acc: 0.4822\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4886 - val_loss: 0.6890 - val_acc: 0.4798\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4875 - val_loss: 0.6891 - val_acc: 0.4822\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4892 - val_loss: 0.6890 - val_acc: 0.4935\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6888 - acc: 0.4887 - val_loss: 0.6890 - val_acc: 0.4799\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4881 - val_loss: 0.6890 - val_acc: 0.4935\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4885 - val_loss: 0.6890 - val_acc: 0.4935\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4887 - val_loss: 0.6890 - val_acc: 0.4933\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6888 - acc: 0.4890 - val_loss: 0.6890 - val_acc: 0.4933\n",
      "testing theta = : 0.29374999999999996\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 11s 13us/step - loss: 0.6889 - acc: 0.4829 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6889 - acc: 0.4827 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6889 - acc: 0.4819 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4813 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6889 - acc: 0.4811 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6889 - acc: 0.4816 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4805 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6889 - acc: 0.4821 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4817 - val_loss: 0.6892 - val_acc: 0.4911\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6889 - acc: 0.4814 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4803 - val_loss: 0.6892 - val_acc: 0.4943\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6889 - acc: 0.4822 - val_loss: 0.6892 - val_acc: 0.4933\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4836 - val_loss: 0.6892 - val_acc: 0.4868\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4816 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4819 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4812 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4814 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4810 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4815 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4820 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4813 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4803 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4805 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4810 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4817 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4822 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 27/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4812 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 28/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4812 - val_loss: 0.6892 - val_acc: 0.4799\n",
      "Epoch 29/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4810 - val_loss: 0.6892 - val_acc: 0.4820\n",
      "Epoch 30/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4809 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 31/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4815 - val_loss: 0.6892 - val_acc: 0.4798\n",
      "Epoch 32/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4798 - val_loss: 0.6892 - val_acc: 0.4820\n",
      "Epoch 33/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6889 - acc: 0.4806 - val_loss: 0.6892 - val_acc: 0.4866\n",
      "testing theta = : 0.3\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/100\n",
      "900000/900000 [==============================] - 12s 13us/step - loss: 0.6891 - acc: 0.4791 - val_loss: 0.6894 - val_acc: 0.4820\n",
      "Epoch 2/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4791 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 3/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4803 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 4/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4796 - val_loss: 0.6894 - val_acc: 0.4912\n",
      "Epoch 5/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4793 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 6/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6891 - acc: 0.4792 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 7/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4790 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 8/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4793 - val_loss: 0.6894 - val_acc: 0.4820\n",
      "Epoch 9/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4795 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 10/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4800 - val_loss: 0.6894 - val_acc: 0.4808\n",
      "Epoch 11/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4802 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 12/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4795 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 13/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6891 - acc: 0.4794 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 14/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4797 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 15/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6891 - acc: 0.4793 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 16/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6891 - acc: 0.4796 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 17/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4791 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 18/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6891 - acc: 0.4792 - val_loss: 0.6894 - val_acc: 0.4820\n",
      "Epoch 19/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6891 - acc: 0.4793 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 20/100\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.6891 - acc: 0.4790 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 21/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6891 - acc: 0.4793 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 22/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6891 - acc: 0.4790 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 23/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4793 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 24/100\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.6891 - acc: 0.4788 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "Epoch 25/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4790 - val_loss: 0.6893 - val_acc: 0.4798\n",
      "Epoch 26/100\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.4794 - val_loss: 0.6893 - val_acc: 0.4799\n",
      "[0.6648594932754834, 0.6683996207515399, 0.6712354998124971, 0.6735827448964119, 0.6749821487400267, 0.6761670062608189, 0.6778106394410134, 0.6793406273259057, 0.6808287862936656, 0.6824593749311235, 0.6841939362221294, 0.6860422043005625, 0.6876483571198252, 0.6887981242934863, 0.6894826906919479, 0.689733476307657, 0.689562021361457, 0.6894284837113487, 0.6892624256677098, 0.6892068361904886, 0.6889859501520793, 0.6886664321356349, 0.6885729523830943, 0.6885003480977482, 0.6884948123163647, 0.6884746557805274, 0.6883510043223698, 0.6882848777704769, 0.6884645378258494, 0.6888425475027826, 0.6890215873387124, 0.6891602645979987, 0.6893166905972693]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.30, 33)\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=my_loss_wrapper(myinputs, theta),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train),\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              batch_size=500,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1,\n",
    "              callbacks=[earlystopping])\n",
    "    lvals += [np.min(model.history.history['loss'])]\n",
    "    vlvals += [np.min(model.history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(vlvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:56:20.706531Z",
     "start_time": "2020-06-02T17:56:20.350609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas, lvals, label = 'lvals')\n",
    "plt.plot(thetas, vlvals, label = 'vlvals')\n",
    "#lt.xlabel('$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"probStuUD Fit Strangeness Only\")\n",
    "plt.vlines(0.275, ymin = np.min(lvals), ymax = np.max(vlvals), label = 'Truth')\n",
    "plt.legend(loc='lower center')\n",
    "#plt.savefig(\"probStuUD_Fit_StrangenessOnly.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:57:26.788576Z",
     "start_time": "2020-06-02T17:57:26.777417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define labels for legends\n",
    "label_0 = r'probStoUD=0.217'\n",
    "\n",
    "label_1 = r'probStoUD=0.275'\n",
    "\n",
    "pythia_text = r'\\textsc{Pythia 8}' + '\\n' + r'$e^+e^- \\to Z \\to $ dijets' +'\\n'+ r\"anti-$k_{\\mathrm{T}}$, $R=0.8$\"\n",
    "def make_legend():\n",
    "    ax = plt.gca()\n",
    "    leg = ax.legend(frameon=False)\n",
    "    leg.set_title(pythia_text, prop={'size':14})\n",
    "    leg._legend_box.align = \"left\"\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:57:51.558307Z",
     "start_time": "2020-06-02T17:57:26.951012Z"
    }
   },
   "outputs": [],
   "source": [
    "X_0_inputs = np.concatenate([X_0, 0.275*np.ones_like(X_0)], axis = 1)\n",
    "\n",
    "f = dctr_model.predict(X_0_inputs)\n",
    "weights_1 = f[:,1]/f[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:57:52.250154Z",
     "start_time": "2020-06-02T17:57:51.563015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcgAAAErCAYAAABacRPGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3U93G9eZ5/Hfk9NrGaKSrSNB7jPLcUiq92NR3mTVaVJ6AxaZ7B1S6hcQibL3aUp+AxLp3mWRkPYbMEX1LGe6BSnrtEQo+znPLO4tqlgsAIWLP1UFfj/n4BCoW39ugQU8qPvX3F0AAOC8n9WdAQAAmogACQBACQIkAAAlCJAAAJQgQAIAUIIAWRMz65jZct35mKXLcI5YfFzHl9fCBkgz2zaz12bmZrZvZp1c2pqZHca0l2a2Vth2zcxO42N7xHHWzez1mHnrStqX9HKc7dokfqFUPsf4nu/Gx7aZbcblQ9//MfO0W7gmiv/33fg/f50dv2S7vVw+9+J1NLU85o65bGab8X3ZLOZ1yHbr8f3L8rY+Yt0L+43ntpb/zFxWXMcT53Vm13GV6zT+uNk2s2583s22q3QC7r6wD0lrklxSZ0C6S9ocsu1ahWMsS9pNzJ9XXG+57vcy8fw6Vc5R0nrxPZTUlbQn6XCa74WkTUmnI9IvHGPYdpK2Jb2c4vvWLTnvfUndCu/jcu51R9Lrsms8Xt+nZde4pMP42Sh7vK77upr3g+u4mddxles05iG//FTSetVzWNg7SEly96P4dNCvhZ6kjQFp3dz2w45x4u47KfkbQ7VfOw3j7v2Kq+4W30N37yl8mIpm/V70JFXNtyTJ3Z9IOjKzsvym2FL4Us3bk7Q7Yruuu5/k8tWP25ztK/6C3lP44ng/YD89SXckrUi6mXtsafDnZWFxHSeb2XUcVb1O70i6Kummu19194OqJ7DQATI6kHSvuDDelh9p8IU66MujDhfyv2CWyhbGHyjFD3kj34v4xbgei88ntS7ppLDsOC4vFa/neyXFTUcxvRvz2XP3LXd/OuT4L939KP7462WPuH0xX/iI6/i8mV3HUeXr1N37Wdo4LkOAfK7yf8iapB0plHfnE+I/p9Kvr1jGPpNfg7HMfFehGHehDXkP92J6G96LI43+dTxUvPYu3N1ldzGDvrhiejc+JlIWPM1sc0RQTZa7q209ruNgHtfxPK7Tf5jWjhos++WxViwydfe+mR0p/JrL33av5W/DCwH0lqTn7n4SL4KHCgHY8vuOFd5ZkO0oFAdI0j1338it19HHu9hbkn7KHftu3FbxQyWFsvWnuXzlj3FH0s6wIqHY6OCZwgV4Y9CxS9a7G/e/l72P8csg+xAsSXpfVnwRPwzZBd8tWe+RpEMz25H0NJ//3P9s6HsxTn5m6ERDfh1XtCQNLdbr6uO1dI67Xy1ZvCYp6ddzJr6vx6nbj+LuvayxRdXiTK7jmWrldTzkOu3mvsPHez+nVSHb5IdCZe5e7nVHsXGCSiqtlavEVai4LlYOv1auolmFCvy4zW7u9a6k7fj83HYqVBiX7Ku0gYBKGhEp/EodWgGe5WHAsQ/z5xrXO6vUjueVPS9rkFD2XpUdZ69kvW3lKtjje9YtrDOwscQY+RnVuGGt7D0ctV1uHc+93o/vadVHV+HOYtA5evF/XuF//TK79krSXlfZ36Dtp/mI791Yx+E65joedZ3G97r43u0X/5cD9znLi74pj+JFofMBsJP/h+l88OyUXUzxos8HQC+kvy4cY00lrf/icZdLti224LpwoZV9ocQPV2mL3bJjlyxbVqHVb1ke4/LTsmMVlw84Tqd4nMJ7tRvfh3PHHvHFUjU/c/tiSbxWp/bFEvNzOCR9ZICM11TlVn8Tnvthwhdn2fXFdXyJrmMf8zrVgO/jssdlqIOUpBeSSjv7eigCONHHVk/5othV6axPztlD0juFC3+Qns5X2A/rT5Za9PVC0sPYR2jTzLrufuDVW9xd4B8rtlcHLJd0VpTxfsCxehrRQi+33WpJ2pG777j7TUlPVd4C8JxJ8zNF+aL0iUzaBzEWB265+50Js/JQsZpijGNn/c3Geii0Ccj65yWfP9fxxNp4HY9znfYUil1H5u0y1EHKQ13jiUJdY1krvOcKb/BWYXlZPUMVe3FfWb3CPcUGQWV5G3PfZ9uZ2Q2FX1dbCl8sB56r30zU0+gK8mHp7xXqgUa9Z2fHMbPl4pdXtCPptELd1Dj5GdU6uVNhnUFuafIvlmz7JeUaiuU+zFX3vyvp9iQZicdcTrhGH2qyxkLbCj9AJ2lswXV8Sa7jYdepmW176L6Sl70vXZXHgzOXIkBGzyVtmdmhLv7SOJC0W3KHWbwTrKof95eNYvFowAdnbDGPPX3sK/RE0pN4kexPoRXXwMrznGHpSxp+d33uOFmzbpVcqPFHQPY/KLv4s/dinPycaPgdfXeCu/A15fpfxf5k4/yC3vLQYKVXst2SKja2iS1ChzbWqmhNY/alk866Cowtls50JK1MIe9cx5fnOi69TuPd5268ccgfL/tOHx2kJylnbtNDHyv0S0e9Ubj4Xup8OX9Hoey/rO5iLffcC2mVGhsUt8vlo1gv6bnn2ZdIWeV/5VF9Bhx7WZPXvVyo+xnwvziN++loSH1AMW3Ae1E5P3HZSw0eQWlQg5ZRdT7bGlFPMsa1Wtb4Y13SfoVtN0uui0H1UUPrIOM1NrWRVUbku5tyLK5jruNh12nZ+xP3Xelauyx1kPLwC6KnUH9Y5iCul2+a3VcoHjnXJ6hCs/drVftGDigHL9619nL9hpZyeSz+Ul+S9FOV48ZjF5ty70q67yN+scX0LV18X3Z18Rdfv8pxyuqd4v6K/bEuvBdj5kcKv453in2x4vpj333HLj13NL1RZnZK9rWl3P871vPt5d+z3PvcsdA/N+uju+Hlv9iXNOIuRPMbMGNXie8f1/Glv46HXafv8+9PPM6WpPtVTsBiRL0U4gVQvN3O0pYlrXp559M1hQvntcI/4sRDEUJX4WJeVwiwO3F5R9Kb3C7eKxSJ7Ln7Ucl2jxSC967Cr5uTuK+sn9aywj/1UKF44igW3x6rUG/hFetLzcwVhmXKipVvKfxyzB8z6+N5oND386Cwj7W4fV+xYr9kne24fZbP5fj+ZcfJ+oEexeNlOjE/xf1deC/GyU9u3ewuXPpYPPOo7Es1fuGsx3N4qvN9T5cU+t0V6zkmEs/znsIPnq5y71lM7yrcQazkrrnTAbvreWgskp13Vke4rnDdHan8vd6TJHcv1s1PVczTD+6+krAt1/ElvI4Lxxh6ncaA25V0TbH0bUCgvbjtZQqQ85BrjXf2T7CPHYx3FS7eeXb6LWVm7u42ek1gtuKX4rnxN8fYlusYM0OAnLL4K+3QSwY6z37N+uQtTSfGFwsWAdcxZunS1EHO0aEGl+HfU2hNCwBouMvUzWMuYv3g+1hnkS/nzuZGG6vT9bTl6mSy5ts7VcvjgabgOsY8UMQKAEAJilgBAChBEeuU/fznP/fr16/XnQ0AaJWXL1/+t7v/ou585BEgp+z69es6Pp7Z1HkAsJDM7K9156EoKUCa2ecKnWKzjpddhU6n7xU60/fc/d+nlUkAAOatcoA0sy8k/VbSrxRGenmjMGzbsT6OyNBVGNXiMzP7rcL4gfvu/t00Mw0AwKyNDJAWplTKJv585O6vhqz+Q2HbTyTdNbPnCl0cCJQAgFYYGiDN7LZCUep9d/8w7s7jNs8kPTOz22b2yN0fjtoOAIC6DezmEe8c5e4PU4Jjkbv/4O4PzeyrSfcFAMCsDbyDdPc3Oj8jxVRQzAoAaIOZDRQQG/UAANBKsxxJZ2+G+wYAYKZGNdL5i6Tbc8oLAACNUaUf5AOFmbTzOgqzYe8rDA6Qd1PSXUn/NnHugCm7/uBPZ8/fPv51jTkB0HSjAuS+uz8rLjSzr9z9twO2eSXpILZW/XHSDAIAUIehdZBlwTE6rbDvKusAANBIqY10rk5pHQAAGik1QH5mZr8clGhmVyR9lrhvAABqlzSbh7s/MLP/MrN9ST9J6ikMWL4s6ZakdUkrU8slAABzNsl8kKuSnkraUZi1Q5JMoWXrqrv/fcK8AQBQm+QA6e59he4cMrNfxWXDZvoAAKA1JhpJx8yuxCHlPAuOcTJlAABaLTlAmtljhXrHA4VBAzIfzOzrSTMGAECdkgKkmf0+Pr3q7kuSjrI0d3/j7t8yrRUAoM1S6yD77v5N7rWXrMNAAQCA1kotYn1XYZ0bifsGAKB2qQHynwqv7dwLs+tioAAAQIulFrHumdlPkv6gMDi5S2eBcUPSphgoAADQYqkj6bwxsy2FgQKWJbnZ2U3kkaQvGSgAANBmkwwUcCJp1cxuKARJSTpx9zdTyRkAADWaZKg5SeFuUhJBEQCwUFL7Qf6nmf3PkuX3zez3ZnZsZv938uwBAFCP1DvIt5JOzGzX3f81W5ibYPkbM/uvcXZoZh2Fxj19SXck7bn7US59W2HWkK6ko1jEO7O0KukAgMWVGiD/TWF4uRdmtiFp3d3/d2GdgzH3+dDddyTJzI4kvTazq+7ej9NqPcoClJkdKgRRzSKtSjoAYLGl9oN0d++5+6qkZ5JemdkfCuu8HnOfm2a2Fnfei8u68e9a4e6tl607o7Qq6QCABZYaIJeyJ+7+RGFQgC8H1U1WtJIVqZpZFhizoNQrrNuXdGcWafH4Q9MBAIsvNUCeCxQld5OPJH0yzg5zd41SKL7diXNOdkpWf6dwdzmLNFVIBwAsuNQAuWFmj8zsczO7ki3M301K2h13p2bWjQ1jugqDEEi5u9USs0irkn6OmW3GlrvHf/vb38bZFADQUKmNdK4qBJFO/Hs2ak68E1wxs/vj7jRu+yQWsb40sxVJ70tWvRb/ziKtSvo57v5UMaCvrq6WzWwCAGiZ1KHmPkj6MGq1cfZpZp1YpCp375lZX9JDSYcqL/LsKdQLTjtNFdIBAAtuZIA0s88V5n98m1v2mwr73pH0XZVMxEYxhyrMCiKp4+5HZlYs8uwq9pOcdpokjUoHACy+KneQPyp02biVW5YFvkF3VB2NNx9kTyGg5nVzy47MbDnX7aKbG0RgFmlV0gEAC6xKgLytUOSYl7VaHcjMXlTNRCxSPYkNdPoKU2XdzwWk+5IexrrJW/G1ZphWJR0AsMDMffw2JWb2SayHHLbOjcs4s8fq6qofHx/XnQ0McP3Bn86ev3386xpzAiDPzF6OuvGat6RuHqOCY/QvKfsGAKAJhhaxmtkXKm/NWcWWpG8TtwUAoFaj6iAfSFqTNO4sFuM20gEAoFFGBci+pJspdYnjNNIBAKBpRtVB7kzQ0KbYbQMAgNYYegc5bnA0s08k3Y0vX0u6dK1YAQCLIXUs1lKxdeszSTKzPyoMMgAAQOskB8g4BN2uPg5anrckiTpIAEBrJQVIM7utMC7pnsIwcbck/RSTu5Lk7t9MI4MAANQh9Q5y3d0/y16Ymbv7v+dXMLPfFJcBANAWqRMmF/tFXstPnBxVGW0HAIBGSg2QRS8U5m7M+9WU9g0AwNylBshjM7ttZv9pZl/H1qsrZvZHM/vCzL7S+emxAABolaQ6SHd/ZWY3JD2V9H1cvCHpQNKRpFOFabIAAGil5G4ecRCBb3KvP0i6M41MAXViSiwA0vTqIC8oabQDAEBrzCxAKo6oAwBAG00yks5XCkWqg+aLXEvdNwAAdUsdSeexpGWF/pDvylaRtDpBvgAAqFXqHeQ7d/9y2ApmtpS4bwAAapdaB/l61Aru/iBx3wAA1C41QF4btYKZfZG4bwAAapcUIN39mZl9FUfNudCdIy7bmTh3AADUJLWRznVJdxVaqrqZXVhFkk+SMQAA6pTaSOeJpH2Fu8R+SbqJCZMBAC2WGiAP3X3oQABm9ihx3wAA1C61kc77USu4+/ej1gEAoKlSA2Q/1kMOZGZfJ+4bAIDapRaxuqR1M7sp6aUu3lEuSdqS9O0EeQMAoDapAfIg/u2pfGLkjqQbifsGAKB2qQGy5+5Dx1o1M1qxAgBaK7UOcqPCOgwUAABordSRdN6MmhDZ3d+kZQkAgPolBUgz+4skAiAAYGGlFrHuS+pOMyMAADTJJAMFXB22gpn9MXHfAADULrUV62uFfpDX4vNeyTprybkCAKBmqQHyR4W+jmWBUQoDBXySuG8AAGpHP0gAAEqk1kHer7AOs3kAAFor6Q7S3V9lz2N/yOxu8tjd/15cBwCAtkm9g5SZXYnFqH1JR/FxamZ/HjXTBwAATZc6UMAnCgOWH0q66e4/c/efSfpHST9I2h810g4AAE2W2kjnvqQNd/+QX+juPUlPzOxA0sP4AACgdVKLWD8Ug2NeDJSDuoAAANB4qQHSK6xDP0gAQGulBsirw+oYY9rPE/cNAEDtUgPkU0kHZvbP+UAZW7Z+pdBQ5w/TyCAAAHVI7Qf5wcy2JO1J+t7M8kWuJ5LuZv0hAQBoo9RWrNmEyF+a2Q1Jy3HxSepEyWbWkbQZX96S9MjdT3Lp2woNf7qSjmadViUdALC4kgNkJgbEC0HRzL5w9x/H2NWuu2/FbbuSXprZirv3zGxfuYBpZoeS7sTnU0+rkg4AWGzJI+kME+sld8ZYv6swbZakc91E1uOitcLdW8/M1maYViUdALDAUkfSuWJmfzGz/1f2UBh+bpxg0pG0W7L8WgxKxT6VfUl3ZpEWz29oOgBg8aXeQX4naV9hkPLPBjwqD1Ye79RWCouXFYay65Rs8k6hXnAWaaqQDgBYcKl1kIfu/mzYCmY21nRXhcYzmwqNYo7i80GWZpBWJf2cmMdNSfr000/H2RQA0FCpd5DvR63g7t+n7Di2Zt1w96w4s+xY12aYViX9HHd/6u6r7r76i1/8YtBqAIAWSQ2Q/VFTWpnZ14n73pW0kT+Wyos8ezNKG3VMAMAlkFrE6pLWzeympJe6eMe1JGlL0rfj7DT2O9x19358vRyLWYtFnl1Je7NIk6RR6QCAxZcaIA/i355Cp/6ijqQb4+zQzNYVRuF5H4tZuwqNgE4kHcVgmdVTdt39KD6fRVqVdADAAksNkD13Xx22gpm9qLqz2A9yvyQpq4e8L+lhXO9WfK0ZplVJBwAssNQAuTF6leoDBcSBAWxIej+3v4NZp1VJBwAstqRGOlXGW00dkxUAgCYYGCDN7MawOR9TmdkX094nAADTNjBAxjvArWkFtDg83SPRVQIA0AJDi1jd/RtJN83suZl9nnKAGBjvS3qmMDvG25T9AAAwTyMb6bj7s9giddfMViQdK4yR2pPUzwe8WCS7pNBFoyvprqSrCoHx3vSzDwDAbFRqxeruHyT9VpLM7FeS7kn6V0md2A3C46p9hUEDXkn6SdIWjXUAAG00djcPd3+lMWbqAACgjVL7QQKX2vUHfzp7/vbxr2vMCYBZSR2sHACAhUaABACgBAESAIASBEgAAEoQIAEAKEGABACgBAESAIASBEgAAErMLEAyrRUAoM1mEiDjoOU7s9g3AADzkDTUXAyAB5JuD1pFHwcwBwCgdVLHYv1O0r7CXWK/JN0kvUjNFAAAdUsNkIfu/mzYCmb2KHHfAADULrUO8v2oFdz9+8R9AwBQu9QA2Tez68NWMLOvE/cNAEDtUotYXdK6md2U9FIX7yiXJG1J+naCvAEAUJvUAHkQ//Yk3SpJ70i6kbhvAABqlxoge+6+OmwFM6MVKwCgtVLrIDcqrMNAAQCA1koKkO7+psJqgwYRAACg8UYWsZrZ55L67v42t+w3Ffa9ozCgAAAArVOlDvJHSa91vjFOFvh6A7ahkQ4AoNWqBMjbujicHI10AAALbWSAdPdXJYur1C/SSAcA0FpJ3Tzc/UP2PM7skd1NHrv73+M6VRryAADQSMnzQZrZdTP7i6RTSUfxcWpmfzazX04rgwAA1CEpQJrZDYWAuK9w93jV3X8m6TNJP0g6ineWAAC0UupIOpvu/llxYSxWfWJmTyU9jA8AAFontYj1eFiiu/c1uAsIAACNlxogfUrrAADQSKkB0iqsc5q4bwAAaje0DjIOM7dUknRqZs8lHap8LsiOuzMXJACgtUY10vlO0rIG1yeuDNrQzG66++9SMwYAQJ1GBciepA06/QMALptRdZCPCI4AgMtoaIAcMA4rAAALL3moOQAAFhkBEgCAEgRIAABKECABACiROlj5TJjZrqRDdz8qLN9W6HLSlXTk7iezTKuSDgBYbMkB0syuZJMjT8rM1hQGJFhXGJ0nn7av0N3kJL4+lHRnVmlV0gEAiy91Psi/SJpa/0h3P3L3JyofsWetcPfWiwF1VmlV0gEACy71DnJf0sY0M1ImBqVi0OxLumNmmnaawkTPA4+pMEk0AOASSG2k817S1WErmNkfE/ed1ylZ9k6hXnAWaaOOCQC4JFLvIF9LWjeza/F5adFocq4+KptJZJZpVdIBAJdAaoD8UeFOa9AsH0uSPkncd15xKi1JujbDtCrpF5jZpqRNSfr000+HrQoAaInUANlz99VhK5jZi8R95/VVXuTZm1HaqGOWcvenkp5K0urqqg9aDwDQHql1kPcrrPMocd9nYn/IYpFnVx/7Sk41bdQxk04CANBKSQEym+XDzK6Y2Rdm9nmWlj2f4kwgR2a2nHvdzQ0kMIu0KukAgAU3yUABjyVtSzqV9ELS72LSBzP72t2/HWNfy5LuKTTsWTKz57FfpBTuVh+aWVfSLZ2/e51FWpV0AMCCSwqQZvb7+PSqu38ws3/J0uIEy9+a2Vfu/l2V/cVO+SeSdkrS+rnlB7NOq5IOAFh8qXeQfXf/Jve6rGHKaeK+AQCoXWojnXcV1rmRuG8AAGqXGiD/qfDazr0wuy7ps8R9AwBQu9Qi1j0z+0nSHyS9UixijYFxQ6HT/MoU8gcAQC2SAqS7vzGzLYXO8cuSPA4CLoUBvb+c1lRYAADUIbmbR2x5umpmNxSCpCSdxFasAAC0WnKAzMSASFAEACyUiQNkrHfMpoI6pmgVALAIJhlJ53NJ30n6lT62YnUzO5K06e5/nUL+gNa6/uBPZ8/fPv51jTkBkCKpm0esd/xRoUHOqsLkyVcVhmV7qzCW6ZUp5REAgLlLvYPclrRS0iDnRNJWHFt1Vx/HZwUAoFVSBwroDWutGlu4Dpw/EQCApksOkFNaBwCARkoNkBpWxxjTvLDs69RjAQAwb8mzeUh6ZmaHkt4X0pYk3ZH03Mx+k1u+JanyHJEAANQpNUDuS+po+Hir+bQlSZ8kHgsAgLlLDZA9d18dZwMze5F4LAAA5i61DvJ+wjaPEo8FAMDcJQVId381j20AAKhLcitWAAAWGQESAIASBEgAAEoQIAEAKEGABACgxFQDpJldiRMoAwDQaqnzQT42sz+b2dfZmKxm9mdJLyU9MLPnBEoAQJuljqTzk6S9bMorM3ssqevu/5itEAcnZ+xVAEArpRaxXi3MB7muMEFy3ofEfQMAULvUAHkWHM3shqQbko4L67xLzRQAAHVLDZD5mTnWJb1x9/8orHMtcd8AANQutQ7yQ6xjNIWi1XXpbKLkO5IeSNqYSg4BAKhBUoB09x/MrCdpTdLNXH3kPYV5Il9IWpb0dhqZBABg3lLvIBWD4rPCsmcDVgcAoFWSAqSZfV6sc4yNddYU6if77v7dFPIHAEAtUhvpbBUXuPsbd3/m7t9K2jezrybLGgAA9ZnJWKzuTh9IAECrVSpiNbPbkjy3qGtm/0uhFWtRR9Kt+JdiVgBAK1Wtg+wptEq9p9ClwxWCYJn3kg7d/XeTZw8AgHpUCpCxxeobSd+b2bakGwRAYHLXH/zp7Pnbx7+uMScAilLqIPcU7igBAFhYYwdId//g7t9IZ/M/fmFmn2fp+ecAALRVcivWOMVVX9K+znf7yIahAwCgtVInTP59fHrV3a9JOsrSYn/Ib+kHCQBos9Sh5vpZMWvkJeucJu4bAIDapRaxVpnr8UbivgEAqF1qgPynwutzAwaY2XVJnyXuGwCA2qUWse6Z2U+S/iDplWIRawyMG5I2Ja1MIX8AANQidT7IN2a2Jempwgg7bnZ2E3kk6Ut3//t0sggAwPxNMh/kiaTVOM3Vclx8kps8GQCA1hoZIM3syrC7wdwwdABmgOHogHpUaaTzbOa5aDAz2zaz9fh3efQWAIBFUKWIdWOMwNCXdCzpsbv/NT1bzWBm+5IexeJkmdmhpDv15goAMA9V6yBfKUxjVcVNSSdmdtvd/yMtW42x5u4budc9M1tz96OBWwAAFkKVAHng7nfH2amZdSX9XlJrp8QyszVdnLWkr3AHSYBEY1FnCUxHlQD507g7dfeemZ0k5KdJOiXL3mnwRNFAbfJBEcB0mHvZMKpT2LHZV+7+3Ux2Pgdmtilpy91Xcsu2Jd0qFLtm627Gl/9D0v9JPOzPJf134rZNw7k0z6Kch8S5NNUk5/JLd//FNDMzqeR+kGXM7IWkK5J+q/YPVl5W53qtbEV3f6owaMJEzOzY3Vcn3U8TcC7NsyjnIXEuTbVI5yJNMB/kAG8UfkEsu/v3U973vPVVXsxarJcEACygqd5BuvvONPdXJ3c/MrOlwuKupL068gMAmK9p30EumqNCH9DujLt4TFxM2yCcS/MsynlInEtTLdK5zK6RziIws46khwoteW9Jep4NGgAAWGwESAAASlDEikvPzHbjwBCj1mv8uLxVzsXMNuOjY2ZdM9udV/6ANplqIx2MFvtS9hQa/BwNK7IdZ906VM1f7CcqSS8kLSn0L629QVcMJMuS1iUdjli30ePyjnMuCq2zdxUanPXUoPPIxOqN7Lq5pdx7P2D9xn5WxjmXpn5WpHPnkY0otjesTUaT/yeVuTuPOT0k7St0gcleH05j3Racy7Ykj4/XCo2daj+HfN4Vxt0dts5p4fXeqG0afC6bCkGyU3d+h+RxL/e8q9CvuvS6acFnZZxzaexnRdJu4Tx80DXU9P9J1QdFrPO15ud/RfWGFIeNs24dxslfX9JVSVfd/aa7t6ov6YhxeVvJ3fvu3q87H2XiWM6vs9fxeukp3B2XaexnJeFcmvxZ2cze11y+ugPWbez/ZBwEyDkZ50u26V/IKflr8hdyBYPG5R0XqcK4AAAGAklEQVT05dB4sQ5yPdZZNq0+NSsCLrowklXTPysa41wyDf6srHgsUo2BXyoZOKUF/5PKqIOcn3EGP2/6QOlj5y/WrbxXO7vLFAeMaLuj3B3AgZm9NrOVpnwpu/uJma0UFi9LKquLa/RnZcxzkdTcz0rhbnZL0s6Aa6bR/5NxECDnZ5wv2aZ/IY+bv0Z/IVdQeVzeNigptutLuqsGdfLOB4UYMI68vEFI0z8r45yL1PDPSrxzXFcoPXk0YLXG/0+qooh1fsb5km36F/JY+RvyhdwWCzMub+zWUZxIoKcw0XnjxJaTG+4+qHiu6Z+VMxXOpfGfFXfvufsThTvgl/GcilrzPxmFADk/43zJNv0LuXL+2vaFXCb+2i8bl3dUd4qmKhbvdZRrSNIwu5I2hqQ3/bOSN/Rcmv5ZyQfDGMj7CiONFbXpfzIUAXJOxvmSbfoXckL+2vSFLEkys+VC45V5j8s7NflziV9snVxaR+FcGlO8mon96Haz4sWyxkRN/6xkqpxL1MjPSmx4UzaF4YVA2Jb/SRUEyPka+CXbwi/kSufS5C/kmM9dSWuSduOXWOaeQkOEzH1J97KWn/F1Y4x5Lk/jaEDbCnc1jWtdaGbrkk4kvY8j/ixLWo1prfqsVD2XJn9WFO7+isG7q9DfsXX/k6oYi3WOhg1+Hr/cOu6+NWrdJkg4l2yEkJsKv6RbV9yC+Sj2Hcy542EautZ8VhLPpZGfldxoTX1JKwqd/w9iWmv+J+MgQAIAUIIiVgAAShAgAQAoQYAEAKAEARIAgBIESAAAShAgAQAoQYAEAKAEARJAqTg26L6ZvWzjZLfApAiQwBzFCYpfm5nHv5uF9O1h6XGdw1z6oJnp8+svm9npuBMjxxFcdhRGTykbfBpYaIykA8xZbvixjWyorgHpW4PG4TSzfUn3q8wTGPe3H4839rBlZvZS0qOyvA44VivH3QSKuIME5iwGqZ7CIOKD0vsaPoj4YdVJdOMcfitzGtNzrLtUoMkIkEA9DhRmZr8g3oUdj0hvxADWJUqDPtBGBEigHs+lsxkSitYUp6calN60Isw4jdO2BgR1oI3+oe4MAJeRu5+YWV9hhvkLwc7de2bWUwiUQ4NhvKPcUphaaElh2qEnMa0j6QeFufvu5+sRYwOfWwr1nTcVgvZqTL7p7ufm/4vBuhOPcUfn60A34z4kacfMNuLzvTZOcwRIBEigTk8VAsvZZMaF4tMDfZwbMEvv5NKzmemfuftKbtmume26+04MYCtmdlrYz7qkh4XtTiXdcPd+SYvXWwrBrhfXlULDnzuSlAvIdxXmMGzUHS6QgiJWoD7PJXUKwShffJql54tZ7xaCzzNJjwr7fSRpOwbTTLHOckuhnjPvWGGSW5Xc9XUKjXyOFYqCgYVFgARqEoNQX4Nbs2bpG2Xp8W5zWdJJYbt+3G61bLuorJHPkqR3A9Z/WXhdqQUt0GYUsQL1eqHQsGWnWHyaS78raSumv8+lZXeeyyVFoi8K6xbtSjo0s04sUu0q1C+W9rscsS9gIREggXrtS9qMAa5b0hl/VLoGdOAf1an/vcKd6d1Yn9iRtFK1b2VVZtadU/9LYOoIkECN3P0oBqh7Cq1Jy9L7A9JPpOQgdDeO0jPrFqZrGnxXCjQadZBA/bJBAwYVY75QaM16Lj0GxSOV9D2MA40PG9Wm2DhoWt7r/LitjOGK1iJAAvV7rtBPcVDXiH2FQFOWvqVQP9ktLF8vtEQtBqoT5bqXjJD1fbyg0FJWMY+3cmk05kFrESCBmsU6xKeD6v9it46DsvR4F7mi0Mhn28zWzWwz1y+xGwc270razWYHiftcjbOCZI/TOFPIcsm2O3GknKwP5X7Mwn6+G4q7byncnW7qYzEu0ErM5gFcMvHOble5UW7isiWFlrHPJN1mBBxcdjTSAS6fTUkv8wEw13eyF4tr1zT7BjxAo1HEClw+Rxow+EB0R6O7iQALjyJW4BKKd4nrutiIpqNQ30nfRVx6BEgAAEpQxAoAQAkCJAAAJQiQAACUIEACAFCCAAkAQAkCJAAAJf4/Tv6qPEhzTZgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_val = 3\n",
    "bins = np.linspace(0, clip_val, 101)\n",
    "plt.hist(np.clip(weights_1, 0, clip_val), bins = bins)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.title(\"Weights \" + label_0 + r' $\\rightarrow$ ' + label_1, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:57:52.261191Z",
     "start_time": "2020-06-02T17:57:52.255572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define default plot styles\n",
    "plot_style_0 = {'histtype':'step', 'color':'black', 'linewidth':2, 'linestyle':'--', 'density':True}\n",
    "plot_style_1 = {'alpha':0.5, 'density':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T17:57:53.439757Z",
     "start_time": "2020-06-02T17:57:52.265957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "bins = np.linspace(0,10,11)\n",
    "hist0 = plt.hist(X_0, bins = bins, label = label_0, **plot_style_1)\n",
    "hist1 = plt.hist(X_1, bins = bins, label = label_1, **plot_style_0)\n",
    "hist2 = plt.hist(X_0, bins = bins, label = label_0 + ' wgt.' , weights=weights_1, **plot_style_1)\n",
    "\n",
    "plt.xlabel('Number of Strange Hadrons')\n",
    "plt.ylabel('Jets per bin (normalized)')\n",
    "plt.xlim([0,15])\n",
    "make_legend()\n",
    "#plt.savefig(\"probStuUD_Fit_StrangenessOnly_Hist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.12\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),initializer = keras.initializers.Constant(value = theta_fit_init),trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = int(len(X_0)/50) #larger batch_size leads to better precision\n",
    "epochs = 20 #but requires more epochs to train\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs\n",
    "    x = K.squeeze(x, axis = 1)\n",
    "    x = K.gather(x, np.arange(batch_size))\n",
    "    theta = 0. #starting value\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "        \n",
    "    #creating tensor with same shape as inputs, with val in every entry \n",
    "    theta0_stack = K.ones_like(x,dtype=tf.float32)*theta0 \n",
    "    \n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.stack((x, theta0_stack), axis=-1) \n",
    "   \n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.16, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"N = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}\".format(len(X_0), batch_size, epochs*2))\n",
    "#plt.savefig(\":N = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}\".format(N, batch_size, epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
