{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:15.114446Z",
     "start_time": "2020-06-07T23:44:13.091033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import argrelmin, argrelmax\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:15.120674Z",
     "start_time": "2020-06-07T23:44:15.117161Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__)  # 1.15.0\n",
    "print(keras.__version__)  # 2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{(1-f(x_{T,i},\\theta'))}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it a $\\mu$ value in addition to $X_i\\sim\\mathcal{N}(\\mu, 1)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:15.142636Z",
     "start_time": "2020-06-07T23:44:15.122776Z"
    }
   },
   "outputs": [],
   "source": [
    "n_data_points = 10**6\n",
    "mu_min = -2\n",
    "mu_max = 2\n",
    "mu_values = np.random.uniform(mu_min, mu_max, n_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with this $\\mu$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:22.710741Z",
     "start_time": "2020-06-07T23:44:15.144513Z"
    }
   },
   "outputs": [],
   "source": [
    "X0 = [(np.random.normal(0, 1), mu)\n",
    "      for mu in mu_values]  # Note the zero in normal(0, 1)\n",
    "X1 = [(np.random.normal(mu, 1), mu) for mu in mu_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu$ values uniformly sampled in the specified range [mu_min, mu_max].\n",
    "\n",
    "For every value of $\\mu$ in mu_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,1)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,1)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:23.798585Z",
     "start_time": "2020-06-07T23:44:22.712748Z"
    }
   },
   "outputs": [],
   "source": [
    "Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "X = np.concatenate((X0, X1))\n",
    "Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:23.891046Z",
     "start_time": "2020-06-07T23:44:23.800546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((2, ))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs=inputs, outputs=outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:47.601685Z",
     "start_time": "2020-06-07T23:44:23.894159Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1600000 samples, validate on 400000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1600000/1600000 [==============================] - 4s 2us/step - loss: 0.5806 - val_loss: 0.5643\n",
      "Epoch 2/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5630 - val_loss: 0.5631\n",
      "Epoch 3/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5627 - val_loss: 0.5633\n",
      "Epoch 4/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5626 - val_loss: 0.5631\n",
      "Epoch 5/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5626 - val_loss: 0.5631\n",
      "Epoch 6/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5632\n",
      "Epoch 7/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5629\n",
      "Epoch 8/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5631\n",
      "Epoch 9/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5629\n",
      "Epoch 10/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5632\n",
      "Epoch 11/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 12/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5630\n",
      "Epoch 13/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5631\n",
      "Epoch 14/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5630\n",
      "Epoch 15/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 16/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5629\n",
      "Epoch 17/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5630\n",
      "Epoch 18/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 19/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 20/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5630\n",
      "Epoch 21/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 22/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 23/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5630\n",
      "Epoch 24/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 25/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 26/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5630\n",
      "Epoch 27/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 28/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5630\n",
      "Epoch 29/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5625 - val_loss: 0.5629\n",
      "Epoch 30/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 31/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n",
      "Epoch 32/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5624 - val_loss: 0.5629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd63364b278>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "dctr_model.fit(X_train,\n",
    "               Y_train,\n",
    "               epochs=200,\n",
    "               batch_size=10000,\n",
    "               validation_data=(X_test, Y_test),\n",
    "               callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:47.609279Z",
     "start_time": "2020-06-07T23:45:47.604419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"1d_gaussian_dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"1d_gaussian_dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"1d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"1d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:47.624068Z",
     "start_time": "2020-06-07T23:45:47.613018Z"
    }
   },
   "outputs": [],
   "source": [
    "# from NN (DCTR)\n",
    "def reweight(events, param):\n",
    "\n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    concat_input_and_params = K.ones(shape=events.shape) * param\n",
    "    #combining and reshaping into correct format:\n",
    "    model_inputs = K.concatenate((events, concat_input_and_params), axis=-1)\n",
    "\n",
    "    f = dctr_model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "# from analytical formula for normal distributions\n",
    "def analytical_reweight(events, param):\n",
    "    mu0 = 0.0\n",
    "    weights = K.exp(-(0.5 * (events - param)**2) + (0.5 * (events - mu0)**2))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:48.033378Z",
     "start_time": "2020-06-07T23:45:47.627016Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "theta0_param = 0  # this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = 1.  # this is the data (the target)\n",
    "\n",
    "theta0 = np.random.normal(theta0_param, 1, N)\n",
    "theta1 = np.random.normal(theta1_param, 1, N)\n",
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0, theta1])\n",
    "yvals = np.concatenate([labels0, labels1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals,\n",
    "                                                    yvals,\n",
    "                                                    test_size=0.5)\n",
    "X_train_theta, Y_train_theta = shuffle(xvals, yvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model: Scan\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:48.040254Z",
     "start_time": "2020-06-07T23:45:48.035447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njson_file = open(\\'1d_gaussian_dctr_model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\ndctr_model = keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\ndctr_model.load_weights(\"1d_gaussian_dctr_model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('1d_gaussian_dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"1d_gaussian_dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:48.117349Z",
     "start_time": "2020-06-07T23:45:48.042871Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1, ), dtype=tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs,\n",
    "                    val=0.,\n",
    "                    reweight_analytically=False,\n",
    "                    MSE_loss=True):\n",
    "    x = inputs\n",
    "    x = K.gather(x, np.arange(500))\n",
    "\n",
    "    theta_prime = val\n",
    "\n",
    "    if reweight_analytically:\n",
    "        # analytical reweight\n",
    "        weights = analytical_reweight(x, theta_prime)\n",
    "    else:\n",
    "        # NN (DCTR) reweight\n",
    "        weights = reweight(x, theta_prime)\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T00:16:35.164494Z",
     "start_time": "2020-06-07T23:45:48.120554Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : -2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0530 - acc: 0.6214 - val_loss: 0.0518 - val_acc: 0.6167\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0517 - acc: 0.6201 - val_loss: 0.0517 - val_acc: 0.6202\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0517 - acc: 0.6201 - val_loss: 0.0520 - val_acc: 0.6153\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0517 - acc: 0.6197 - val_loss: 0.0517 - val_acc: 0.6187\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0517 - acc: 0.6200 - val_loss: 0.0518 - val_acc: 0.6201\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0517 - acc: 0.6199 - val_loss: 0.0518 - val_acc: 0.6210\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0517 - acc: 0.6200 - val_loss: 0.0518 - val_acc: 0.6167\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0517 - acc: 0.6198 - val_loss: 0.0517 - val_acc: 0.6181\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0517 - acc: 0.6201 - val_loss: 0.0518 - val_acc: 0.6187\n",
      "testing theta = : -1.8\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0614 - acc: 0.6306 - val_loss: 0.0614 - val_acc: 0.6338\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0614 - acc: 0.6310 - val_loss: 0.0615 - val_acc: 0.6287\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6310 - val_loss: 0.0614 - val_acc: 0.6328\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6308 - val_loss: 0.0614 - val_acc: 0.6290\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6315 - val_loss: 0.0614 - val_acc: 0.6301\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6308 - val_loss: 0.0615 - val_acc: 0.6300\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6304 - val_loss: 0.0614 - val_acc: 0.6330\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6305 - val_loss: 0.0614 - val_acc: 0.6306\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0614 - acc: 0.6305 - val_loss: 0.0615 - val_acc: 0.6263\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6302 - val_loss: 0.0614 - val_acc: 0.6310\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6305 - val_loss: 0.0614 - val_acc: 0.6298\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6307 - val_loss: 0.0614 - val_acc: 0.6315\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6310 - val_loss: 0.0614 - val_acc: 0.6276\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6307 - val_loss: 0.0615 - val_acc: 0.6341\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0614 - acc: 0.6308 - val_loss: 0.0615 - val_acc: 0.6329\n",
      "testing theta = : -1.6\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6424 - val_loss: 0.0725 - val_acc: 0.6445\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6429 - val_loss: 0.0725 - val_acc: 0.6425\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6423 - val_loss: 0.0725 - val_acc: 0.6426\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6429 - val_loss: 0.0725 - val_acc: 0.6411\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6425 - val_loss: 0.0725 - val_acc: 0.6440\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6426 - val_loss: 0.0725 - val_acc: 0.6421\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6427 - val_loss: 0.0726 - val_acc: 0.6403\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6424 - val_loss: 0.0725 - val_acc: 0.6455\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6428 - val_loss: 0.0725 - val_acc: 0.6426\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6423 - val_loss: 0.0725 - val_acc: 0.6407\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0724 - acc: 0.6423 - val_loss: 0.0725 - val_acc: 0.6438\n",
      "testing theta = : -1.4\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0847 - acc: 0.6535 - val_loss: 0.0848 - val_acc: 0.6511\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0847 - acc: 0.6531 - val_loss: 0.0847 - val_acc: 0.6536\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0847 - acc: 0.6532 - val_loss: 0.0847 - val_acc: 0.6544\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0847 - acc: 0.6529 - val_loss: 0.0847 - val_acc: 0.6545\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0847 - acc: 0.6531 - val_loss: 0.0847 - val_acc: 0.6547\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0847 - acc: 0.6531 - val_loss: 0.0848 - val_acc: 0.6516\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0847 - acc: 0.6536 - val_loss: 0.0847 - val_acc: 0.6528\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0847 - acc: 0.6531 - val_loss: 0.0847 - val_acc: 0.6550\n",
      "testing theta = : -1.2\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6626 - val_loss: 0.0982 - val_acc: 0.6612\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6620 - val_loss: 0.0982 - val_acc: 0.6619\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6614 - val_loss: 0.0982 - val_acc: 0.6624\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6612 - val_loss: 0.0982 - val_acc: 0.6634\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6618 - val_loss: 0.0982 - val_acc: 0.6616\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6611 - val_loss: 0.0982 - val_acc: 0.6615\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6613 - val_loss: 0.0982 - val_acc: 0.6615\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6614 - val_loss: 0.0982 - val_acc: 0.6599\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6610 - val_loss: 0.0982 - val_acc: 0.6628\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6615 - val_loss: 0.0982 - val_acc: 0.6627\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6613 - val_loss: 0.0982 - val_acc: 0.6612\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0981 - acc: 0.6613 - val_loss: 0.0982 - val_acc: 0.6621\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6614 - val_loss: 0.0982 - val_acc: 0.6619\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0982 - acc: 0.6608 - val_loss: 0.0982 - val_acc: 0.6614\n",
      "testing theta = : -1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1132 - acc: 0.6707 - val_loss: 0.1133 - val_acc: 0.6724\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1132 - acc: 0.6710 - val_loss: 0.1133 - val_acc: 0.6731\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1132 - acc: 0.6714 - val_loss: 0.1132 - val_acc: 0.6728\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1132 - acc: 0.6712 - val_loss: 0.1133 - val_acc: 0.6727\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1132 - acc: 0.6715 - val_loss: 0.1132 - val_acc: 0.6717\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1132 - acc: 0.6712 - val_loss: 0.1132 - val_acc: 0.6714\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1132 - acc: 0.6710 - val_loss: 0.1132 - val_acc: 0.6721\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1132 - acc: 0.6716 - val_loss: 0.1133 - val_acc: 0.6727\n",
      "testing theta = : -0.7999999999999998\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1291 - acc: 0.6781 - val_loss: 0.1292 - val_acc: 0.6795\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1291 - acc: 0.6782 - val_loss: 0.1291 - val_acc: 0.6783\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1291 - acc: 0.6780 - val_loss: 0.1291 - val_acc: 0.6791\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1291 - acc: 0.6782 - val_loss: 0.1292 - val_acc: 0.6786\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1291 - acc: 0.6781 - val_loss: 0.1291 - val_acc: 0.6783\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1291 - acc: 0.6782 - val_loss: 0.1292 - val_acc: 0.6781\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1291 - acc: 0.6781 - val_loss: 0.1292 - val_acc: 0.6780\n",
      "testing theta = : -0.5999999999999999\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1463 - acc: 0.6849 - val_loss: 0.1463 - val_acc: 0.6856\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6851 - val_loss: 0.1464 - val_acc: 0.6860\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6852 - val_loss: 0.1463 - val_acc: 0.6843\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6851 - val_loss: 0.1463 - val_acc: 0.6850\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6852 - val_loss: 0.1463 - val_acc: 0.6850\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6852 - val_loss: 0.1463 - val_acc: 0.6855\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6851 - val_loss: 0.1463 - val_acc: 0.6858\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6851 - val_loss: 0.1463 - val_acc: 0.6851\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6853 - val_loss: 0.1463 - val_acc: 0.6849\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6853 - val_loss: 0.1463 - val_acc: 0.6856\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6852 - val_loss: 0.1463 - val_acc: 0.6847\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1462 - acc: 0.6850 - val_loss: 0.1463 - val_acc: 0.6845\n",
      "testing theta = : -0.3999999999999999\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1642 - acc: 0.6896 - val_loss: 0.1642 - val_acc: 0.6895\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1642 - acc: 0.6897 - val_loss: 0.1642 - val_acc: 0.6899\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1642 - acc: 0.6896 - val_loss: 0.1642 - val_acc: 0.6896\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1642 - acc: 0.6895 - val_loss: 0.1642 - val_acc: 0.6895\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1642 - acc: 0.6896 - val_loss: 0.1642 - val_acc: 0.6893\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1642 - acc: 0.6896 - val_loss: 0.1642 - val_acc: 0.6900\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1642 - acc: 0.6896 - val_loss: 0.1642 - val_acc: 0.6898\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1642 - acc: 0.6897 - val_loss: 0.1642 - val_acc: 0.6890\n",
      "testing theta = : -0.19999999999999996\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1817 - acc: 0.6915 - val_loss: 0.1817 - val_acc: 0.6914\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6918 - val_loss: 0.1817 - val_acc: 0.6910\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6915 - val_loss: 0.1817 - val_acc: 0.6912\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6917 - val_loss: 0.1817 - val_acc: 0.6914\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6916 - val_loss: 0.1817 - val_acc: 0.6913\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6917 - val_loss: 0.1817 - val_acc: 0.6914\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6917 - val_loss: 0.1817 - val_acc: 0.6910\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6917 - val_loss: 0.1817 - val_acc: 0.6915\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6916 - val_loss: 0.1817 - val_acc: 0.6915\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1817 - acc: 0.6917 - val_loss: 0.1817 - val_acc: 0.6912\n",
      "testing theta = : 0.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1987 - acc: 0.6920 - val_loss: 0.1986 - val_acc: 0.6918\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1987 - acc: 0.6921 - val_loss: 0.1986 - val_acc: 0.6918\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1987 - acc: 0.6921 - val_loss: 0.1986 - val_acc: 0.6918\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1987 - acc: 0.6920 - val_loss: 0.1986 - val_acc: 0.6918\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1987 - acc: 0.6921 - val_loss: 0.1986 - val_acc: 0.6918\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1987 - acc: 0.6920 - val_loss: 0.1986 - val_acc: 0.6919\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1987 - acc: 0.6921 - val_loss: 0.1986 - val_acc: 0.6918\n",
      "testing theta = : 0.20000000000000018\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2145 - acc: 0.6908 - val_loss: 0.2142 - val_acc: 0.6907\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2145 - acc: 0.6907 - val_loss: 0.2143 - val_acc: 0.6912\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2144 - acc: 0.6908 - val_loss: 0.2143 - val_acc: 0.6899\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2144 - acc: 0.6909 - val_loss: 0.2143 - val_acc: 0.6905\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2145 - acc: 0.6908 - val_loss: 0.2143 - val_acc: 0.6904\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2144 - acc: 0.6909 - val_loss: 0.2143 - val_acc: 0.6899\n",
      "testing theta = : 0.40000000000000036\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2279 - acc: 0.6879 - val_loss: 0.2276 - val_acc: 0.6879\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6882 - val_loss: 0.2276 - val_acc: 0.6891\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6882 - val_loss: 0.2276 - val_acc: 0.6887\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6883 - val_loss: 0.2277 - val_acc: 0.6860\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6884 - val_loss: 0.2276 - val_acc: 0.6887\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6885 - val_loss: 0.2276 - val_acc: 0.6889\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6886 - val_loss: 0.2276 - val_acc: 0.6897\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6886 - val_loss: 0.2276 - val_acc: 0.6890\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6884 - val_loss: 0.2276 - val_acc: 0.6880\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6885 - val_loss: 0.2276 - val_acc: 0.6891\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2278 - acc: 0.6885 - val_loss: 0.2276 - val_acc: 0.6889\n",
      "testing theta = : 0.6000000000000001\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2386 - acc: 0.6849 - val_loss: 0.2382 - val_acc: 0.6871\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6858 - val_loss: 0.2382 - val_acc: 0.6884\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6861 - val_loss: 0.2382 - val_acc: 0.6866\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6859 - val_loss: 0.2382 - val_acc: 0.6865\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6863 - val_loss: 0.2383 - val_acc: 0.6839\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6865 - val_loss: 0.2382 - val_acc: 0.6838\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2386 - acc: 0.6863 - val_loss: 0.2382 - val_acc: 0.6869\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6864 - val_loss: 0.2382 - val_acc: 0.6856\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6865 - val_loss: 0.2382 - val_acc: 0.6879\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6861 - val_loss: 0.2382 - val_acc: 0.6874\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6863 - val_loss: 0.2382 - val_acc: 0.6851\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.6864 - val_loss: 0.2383 - val_acc: 0.6853\n",
      "testing theta = : 0.8000000000000003\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2458 - acc: 0.6831 - val_loss: 0.2455 - val_acc: 0.6788\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2458 - acc: 0.6860 - val_loss: 0.2454 - val_acc: 0.6874\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2458 - acc: 0.6858 - val_loss: 0.2454 - val_acc: 0.6898\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2458 - acc: 0.6849 - val_loss: 0.2454 - val_acc: 0.6883\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2458 - acc: 0.6855 - val_loss: 0.2454 - val_acc: 0.6825\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2458 - acc: 0.6851 - val_loss: 0.2454 - val_acc: 0.6808\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2458 - acc: 0.6843 - val_loss: 0.2454 - val_acc: 0.6864\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2492 - acc: 0.6308 - val_loss: 0.2486 - val_acc: 0.6686\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.6101 - val_loss: 0.2486 - val_acc: 0.5967\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.6023 - val_loss: 0.2486 - val_acc: 0.5880\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.5970 - val_loss: 0.2486 - val_acc: 0.6070\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.6019 - val_loss: 0.2486 - val_acc: 0.5983\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.5996 - val_loss: 0.2486 - val_acc: 0.5742\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.5952 - val_loss: 0.2486 - val_acc: 0.6542\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2491 - acc: 0.6018 - val_loss: 0.2486 - val_acc: 0.5708\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.5980 - val_loss: 0.2487 - val_acc: 0.5696\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.5964 - val_loss: 0.2486 - val_acc: 0.6008\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.5937 - val_loss: 0.2486 - val_acc: 0.6173\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.6000 - val_loss: 0.2487 - val_acc: 0.6483\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.5968 - val_loss: 0.2486 - val_acc: 0.6019\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.6005 - val_loss: 0.2486 - val_acc: 0.6021\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.5977 - val_loss: 0.2486 - val_acc: 0.5911\n",
      "testing theta = : 1.2000000000000002\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2473 - acc: 0.3590 - val_loss: 0.2467 - val_acc: 0.3545\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.3499 - val_loss: 0.2467 - val_acc: 0.3502\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2471 - acc: 0.3505 - val_loss: 0.2467 - val_acc: 0.3585\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.3499 - val_loss: 0.2467 - val_acc: 0.3730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.3517 - val_loss: 0.2467 - val_acc: 0.3469\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.3488 - val_loss: 0.2467 - val_acc: 0.3531\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2471 - acc: 0.3498 - val_loss: 0.2467 - val_acc: 0.3612\n",
      "testing theta = : 1.4000000000000004\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2403 - acc: 0.3482 - val_loss: 0.2398 - val_acc: 0.3454\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2403 - acc: 0.3472 - val_loss: 0.2400 - val_acc: 0.3428\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2402 - acc: 0.3479 - val_loss: 0.2398 - val_acc: 0.3529\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2403 - acc: 0.3479 - val_loss: 0.2398 - val_acc: 0.3473\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2403 - acc: 0.3480 - val_loss: 0.2399 - val_acc: 0.3424\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2403 - acc: 0.3476 - val_loss: 0.2399 - val_acc: 0.3569\n",
      "testing theta = : 1.6\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2295 - acc: 0.3556 - val_loss: 0.2290 - val_acc: 0.3577\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2294 - acc: 0.3565 - val_loss: 0.2290 - val_acc: 0.3606\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2294 - acc: 0.3564 - val_loss: 0.2291 - val_acc: 0.3500\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2294 - acc: 0.3567 - val_loss: 0.2289 - val_acc: 0.3580\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2294 - acc: 0.3566 - val_loss: 0.2290 - val_acc: 0.3620\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2294 - acc: 0.3575 - val_loss: 0.2289 - val_acc: 0.3531\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2294 - acc: 0.3566 - val_loss: 0.2289 - val_acc: 0.3582\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2294 - acc: 0.3570 - val_loss: 0.2291 - val_acc: 0.3485\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2294 - acc: 0.3567 - val_loss: 0.2290 - val_acc: 0.3527\n",
      "testing theta = : 1.8000000000000003\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2156 - acc: 0.3664 - val_loss: 0.2152 - val_acc: 0.3645\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2156 - acc: 0.3672 - val_loss: 0.2152 - val_acc: 0.3705\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2156 - acc: 0.3681 - val_loss: 0.2151 - val_acc: 0.3687\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3680 - val_loss: 0.2152 - val_acc: 0.3723\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3682 - val_loss: 0.2151 - val_acc: 0.3708\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2156 - acc: 0.3680 - val_loss: 0.2151 - val_acc: 0.3686\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3681 - val_loss: 0.2152 - val_acc: 0.3715\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3684 - val_loss: 0.2152 - val_acc: 0.3660\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2156 - acc: 0.3685 - val_loss: 0.2152 - val_acc: 0.3637\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3679 - val_loss: 0.2151 - val_acc: 0.3697\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3683 - val_loss: 0.2152 - val_acc: 0.3654\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3681 - val_loss: 0.2151 - val_acc: 0.3676\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3677 - val_loss: 0.2151 - val_acc: 0.3697\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3687 - val_loss: 0.2152 - val_acc: 0.3727\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2156 - acc: 0.3677 - val_loss: 0.2152 - val_acc: 0.3747\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2002 - acc: 0.3780 - val_loss: 0.1997 - val_acc: 0.3804\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2001 - acc: 0.3797 - val_loss: 0.1997 - val_acc: 0.3822\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2001 - acc: 0.3803 - val_loss: 0.1999 - val_acc: 0.3764\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2001 - acc: 0.3805 - val_loss: 0.2000 - val_acc: 0.3732\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2001 - acc: 0.3806 - val_loss: 0.1998 - val_acc: 0.3850\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2001 - acc: 0.3804 - val_loss: 0.1997 - val_acc: 0.3825\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2001 - acc: 0.3805 - val_loss: 0.1997 - val_acc: 0.3795\n",
      "[0.051684527443721894, 0.06136392753198743, 0.07241557202115655, 0.08467847231402993, 0.09814654180780052, 0.1131666258238256, 0.12909842636063695, 0.1462371379993856, 0.16417450762540103, 0.18169589114934206, 0.19867415139824152, 0.21444116377830505, 0.22782970818132162, 0.23853721331804992, 0.24579288480430841, 0.24903850362449884, 0.24714689079672097, 0.24024147909879684, 0.2293814164251089, 0.2155602402612567, 0.20011967405676842]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2, 2, 21)\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "earlystopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=my_loss_wrapper(myinputs, theta, MSE_loss=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train),\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              batch_size=500,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1,\n",
    "              callbacks=[earlystopping])\n",
    "    lvals += [np.min(model.history.history['loss'])]\n",
    "    vlvals += [np.min(model.history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T00:16:35.757880Z",
     "start_time": "2020-06-08T00:16:35.167100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Theta vs. Loss\")\n",
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(np.mean(theta1),\n",
    "           ymin=np.min(lvals),\n",
    "           ymax=np.max(lvals),\n",
    "           label='Truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training Fitting Model: Gradient Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T00:16:35.768315Z",
     "start_time": "2020-06-08T00:16:35.760954Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T00:16:35.871162Z",
     "start_time": "2020-06-08T00:16:35.771682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 1         \n",
      "=================================================================\n",
      "Total params: 16,898\n",
      "Trainable params: 16,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1, ))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers) - 1].add_weight(\n",
    "    name=\"thetaX\",\n",
    "    shape=list(),\n",
    "    initializer=keras.initializers.Constant(value=theta_fit_init),\n",
    "    trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = 2 * N  # for training theta only\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "\n",
    "def my_loss_wrapper_fit(\n",
    "        inputs,\n",
    "        mysign=1,  # -1 for training theta, +1 for training g\n",
    "        reweight_analytically=False,\n",
    "        MSE_loss=True):\n",
    "    \n",
    "    x = inputs\n",
    "\n",
    "    # Getting theta_prime and fixing batch size:\n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        #  when not training theta, fetch as np array\n",
    "        theta_prime = model_fit.layers[-1].get_weights()\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta_prime = model_fit.trainable_weights[-1]\n",
    "\n",
    "    if reweight_analytically:\n",
    "        # analytical reweight\n",
    "        weights = analytical_reweight(x, theta_prime)\n",
    "    else:\n",
    "        # NN reweight\n",
    "        weights = reweight(x, theta_prime)\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:07:00.795318Z",
     "start_time": "2020-06-08T00:16:35.874397Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.5828 - acc: 0.6910 - val_loss: 0.5812 - val_acc: 0.6918\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5817 - acc: 0.6919 - val_loss: 0.5814 - val_acc: 0.6918\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5817 - acc: 0.6920 - val_loss: 0.5816 - val_acc: 0.6916\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5817 - acc: 0.6920 - val_loss: 0.5813 - val_acc: 0.6915\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6919 - val_loss: 0.5811 - val_acc: 0.6918\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6920 - val_loss: 0.5812 - val_acc: 0.6918\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6920 - val_loss: 0.5812 - val_acc: 0.6918\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5817 - acc: 0.6919 - val_loss: 0.5815 - val_acc: 0.6918\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6921 - val_loss: 0.5816 - val_acc: 0.6918\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6919 - val_loss: 0.5812 - val_acc: 0.6918\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6920 - val_loss: 0.5812 - val_acc: 0.6918\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6920 - val_loss: 0.5813 - val_acc: 0.6917\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6920 - val_loss: 0.5812 - val_acc: 0.6917\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5816 - acc: 0.6921 - val_loss: 0.5812 - val_acc: 0.6917\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.5816 - acc: 0.6920 - val_loss: 0.5813 - val_acc: 0.6918\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 3us/step - loss: -0.5813 - acc: 0.6919\n",
      ". theta fit =  0.49999186\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6593 - acc: 0.6865 - val_loss: 0.6583 - val_acc: 0.6897\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6591 - acc: 0.6870 - val_loss: 0.6582 - val_acc: 0.6869\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6591 - acc: 0.6872 - val_loss: 0.6582 - val_acc: 0.6885\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6591 - acc: 0.6875 - val_loss: 0.6583 - val_acc: 0.6865\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6590 - acc: 0.6868 - val_loss: 0.6583 - val_acc: 0.6878\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6590 - acc: 0.6874 - val_loss: 0.6582 - val_acc: 0.6871\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6590 - acc: 0.6871 - val_loss: 0.6582 - val_acc: 0.6860\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6591 - acc: 0.6872 - val_loss: 0.6583 - val_acc: 0.6873\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6590 - acc: 0.6871 - val_loss: 0.6582 - val_acc: 0.6879\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6590 - acc: 0.6872 - val_loss: 0.6582 - val_acc: 0.6881\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6590 - acc: 0.6872 - val_loss: 0.6583 - val_acc: 0.6887\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6590 - acc: 0.6873 - val_loss: 0.6583 - val_acc: 0.6867\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6586 - acc: 0.6869\n",
      ". theta fit =  0.8720484\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6874 - acc: 0.6828 - val_loss: 0.6859 - val_acc: 0.6835\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6855 - val_loss: 0.6860 - val_acc: 0.6793\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6847 - val_loss: 0.6859 - val_acc: 0.6896\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6850 - val_loss: 0.6859 - val_acc: 0.6881\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6834 - val_loss: 0.6860 - val_acc: 0.6898\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6853 - val_loss: 0.6860 - val_acc: 0.6842\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6848 - val_loss: 0.6859 - val_acc: 0.6886\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6851 - val_loss: 0.6859 - val_acc: 0.6737\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6835 - val_loss: 0.6859 - val_acc: 0.6860\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6840 - val_loss: 0.6859 - val_acc: 0.6878\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6839 - val_loss: 0.6859 - val_acc: 0.6905\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6845 - val_loss: 0.6860 - val_acc: 0.6894\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6848 - val_loss: 0.6859 - val_acc: 0.6899\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6848 - val_loss: 0.6859 - val_acc: 0.6853\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6843 - val_loss: 0.6860 - val_acc: 0.6883\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6858 - val_loss: 0.6859 - val_acc: 0.6821\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6842 - val_loss: 0.6859 - val_acc: 0.6858\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6849 - val_loss: 0.6859 - val_acc: 0.6835\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6871 - acc: 0.6835 - val_loss: 0.6859 - val_acc: 0.6903\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6865 - acc: 0.6860\n",
      ". theta fit =  1.19143\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.3697 - val_loss: 0.6867 - val_acc: 0.3341\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3507 - val_loss: 0.6862 - val_acc: 0.3548\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3539 - val_loss: 0.6862 - val_acc: 0.3543\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6876 - acc: 0.3513 - val_loss: 0.6863 - val_acc: 0.3357\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3509 - val_loss: 0.6865 - val_acc: 0.3320\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3512 - val_loss: 0.6862 - val_acc: 0.3421\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3498 - val_loss: 0.6862 - val_acc: 0.3542\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3490 - val_loss: 0.6862 - val_acc: 0.3497\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3490 - val_loss: 0.6862 - val_acc: 0.3553\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3491 - val_loss: 0.6863 - val_acc: 0.3646\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3494 - val_loss: 0.6862 - val_acc: 0.3576\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3497 - val_loss: 0.6863 - val_acc: 0.3356\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3473 - val_loss: 0.6862 - val_acc: 0.3458\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3506 - val_loss: 0.6863 - val_acc: 0.3616\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3484 - val_loss: 0.6863 - val_acc: 0.3358\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3474 - val_loss: 0.6862 - val_acc: 0.3578\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3481 - val_loss: 0.6862 - val_acc: 0.3420\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3470 - val_loss: 0.6862 - val_acc: 0.3469\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3493 - val_loss: 0.6862 - val_acc: 0.3436\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6875 - acc: 0.3468 - val_loss: 0.6863 - val_acc: 0.3628\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6876 - acc: 0.3507 - val_loss: 0.6862 - val_acc: 0.3486\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6869 - acc: 0.3576\n",
      ". theta fit =  0.9008915\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6886 - acc: 0.6617 - val_loss: 0.6870 - val_acc: 0.6820\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6845 - val_loss: 0.6870 - val_acc: 0.6754\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6851 - val_loss: 0.6870 - val_acc: 0.6773\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6845 - val_loss: 0.6870 - val_acc: 0.6870\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6849 - val_loss: 0.6870 - val_acc: 0.6894\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6849 - val_loss: 0.6870 - val_acc: 0.6850\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6844 - val_loss: 0.6870 - val_acc: 0.6863\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6849 - val_loss: 0.6870 - val_acc: 0.6851\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6857 - val_loss: 0.6870 - val_acc: 0.6708\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6853 - val_loss: 0.6870 - val_acc: 0.6902\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6850 - val_loss: 0.6870 - val_acc: 0.6850\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6856 - val_loss: 0.6870 - val_acc: 0.6852\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6881 - acc: 0.6852 - val_loss: 0.6869 - val_acc: 0.6847\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6881 - acc: 0.6850 - val_loss: 0.6870 - val_acc: 0.6879\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6881 - acc: 0.6853 - val_loss: 0.6870 - val_acc: 0.6857\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6855 - val_loss: 0.6870 - val_acc: 0.6837\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6858 - val_loss: 0.6870 - val_acc: 0.6880\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6881 - acc: 0.6853 - val_loss: 0.6870 - val_acc: 0.6891\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6860 - val_loss: 0.6870 - val_acc: 0.6879\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6861 - val_loss: 0.6870 - val_acc: 0.6854\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6881 - acc: 0.6858 - val_loss: 0.6870 - val_acc: 0.6870\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.6856 - val_loss: 0.6871 - val_acc: 0.6724\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6881 - acc: 0.6853 - val_loss: 0.6870 - val_acc: 0.6850\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6875 - acc: 0.6849\n",
      ". theta fit =  1.1736124\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6893 - acc: 0.3824 - val_loss: 0.6870 - val_acc: 0.3406\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6883 - acc: 0.3513 - val_loss: 0.6869 - val_acc: 0.3502\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3560 - val_loss: 0.6869 - val_acc: 0.3697\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3599 - val_loss: 0.6869 - val_acc: 0.3631\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3604 - val_loss: 0.6869 - val_acc: 0.3622\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3563 - val_loss: 0.6868 - val_acc: 0.3598\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3539 - val_loss: 0.6870 - val_acc: 0.3501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3520 - val_loss: 0.6869 - val_acc: 0.3483\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3506 - val_loss: 0.6868 - val_acc: 0.3540\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3534 - val_loss: 0.6868 - val_acc: 0.3535\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3497 - val_loss: 0.6869 - val_acc: 0.3682\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3521 - val_loss: 0.6870 - val_acc: 0.3301\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3500 - val_loss: 0.6870 - val_acc: 0.3447\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3499 - val_loss: 0.6869 - val_acc: 0.3557\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3514 - val_loss: 0.6869 - val_acc: 0.3612\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3516 - val_loss: 0.6869 - val_acc: 0.3509\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3511 - val_loss: 0.6869 - val_acc: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3502 - val_loss: 0.6868 - val_acc: 0.3491\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3504 - val_loss: 0.6868 - val_acc: 0.3610\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6882 - acc: 0.3513 - val_loss: 0.6868 - val_acc: 0.3464\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6875 - acc: 0.3535\n",
      ". theta fit =  0.9125772\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6890 - acc: 0.6481 - val_loss: 0.6874 - val_acc: 0.6908\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6886 - acc: 0.6857 - val_loss: 0.6874 - val_acc: 0.6871\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6850 - val_loss: 0.6874 - val_acc: 0.6908\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6857 - val_loss: 0.6874 - val_acc: 0.6776\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6850 - val_loss: 0.6874 - val_acc: 0.6870\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6852 - val_loss: 0.6874 - val_acc: 0.6850\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6854 - val_loss: 0.6874 - val_acc: 0.6813\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6852 - val_loss: 0.6875 - val_acc: 0.6689\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6844 - val_loss: 0.6874 - val_acc: 0.6898\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6855 - val_loss: 0.6874 - val_acc: 0.6777\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6849 - val_loss: 0.6874 - val_acc: 0.6916\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6849 - val_loss: 0.6874 - val_acc: 0.6897\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6855 - val_loss: 0.6874 - val_acc: 0.6822\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6856 - val_loss: 0.6874 - val_acc: 0.6872\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.6844 - val_loss: 0.6874 - val_acc: 0.6845\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6879 - acc: 0.6870\n",
      ". theta fit =  1.1657424\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6891 - acc: 0.3833 - val_loss: 0.6871 - val_acc: 0.3591\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3540 - val_loss: 0.6871 - val_acc: 0.3486\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3524 - val_loss: 0.6872 - val_acc: 0.3428\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3516 - val_loss: 0.6871 - val_acc: 0.3573\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3515 - val_loss: 0.6871 - val_acc: 0.3602\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6885 - acc: 0.3506 - val_loss: 0.6871 - val_acc: 0.3555\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3506 - val_loss: 0.6871 - val_acc: 0.3505\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3497 - val_loss: 0.6872 - val_acc: 0.3487\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3517 - val_loss: 0.6871 - val_acc: 0.3619\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3498 - val_loss: 0.6871 - val_acc: 0.3474\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3508 - val_loss: 0.6871 - val_acc: 0.3587\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3504 - val_loss: 0.6873 - val_acc: 0.3324\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3492 - val_loss: 0.6871 - val_acc: 0.3503\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3505 - val_loss: 0.6871 - val_acc: 0.3506\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3506 - val_loss: 0.6871 - val_acc: 0.3531\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3503 - val_loss: 0.6871 - val_acc: 0.3574\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6877 - acc: 0.3555\n",
      ". theta fit =  0.9178889\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.6574 - val_loss: 0.6876 - val_acc: 0.6830\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6887 - acc: 0.6860 - val_loss: 0.6877 - val_acc: 0.6793\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6855 - val_loss: 0.6876 - val_acc: 0.6827\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6858 - val_loss: 0.6876 - val_acc: 0.6863\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6887 - acc: 0.6861 - val_loss: 0.6875 - val_acc: 0.6803\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6845 - val_loss: 0.6875 - val_acc: 0.6874\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6847 - val_loss: 0.6876 - val_acc: 0.6909\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6852 - val_loss: 0.6876 - val_acc: 0.6862\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6857 - val_loss: 0.6875 - val_acc: 0.6828\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6849 - val_loss: 0.6877 - val_acc: 0.6917\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6850 - val_loss: 0.6875 - val_acc: 0.6892\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6888 - acc: 0.6850 - val_loss: 0.6876 - val_acc: 0.6780\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6887 - acc: 0.6848 - val_loss: 0.6876 - val_acc: 0.6906\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6887 - acc: 0.6842 - val_loss: 0.6876 - val_acc: 0.6900\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6887 - acc: 0.6843 - val_loss: 0.6876 - val_acc: 0.6858\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6881 - acc: 0.6806\n",
      ". theta fit =  1.1622443\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.3947 - val_loss: 0.6873 - val_acc: 0.3440\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3530 - val_loss: 0.6873 - val_acc: 0.3557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3520 - val_loss: 0.6872 - val_acc: 0.3492\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3498 - val_loss: 0.6872 - val_acc: 0.3652\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3518 - val_loss: 0.6872 - val_acc: 0.3553\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3518 - val_loss: 0.6873 - val_acc: 0.3421\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3509 - val_loss: 0.6873 - val_acc: 0.3488\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3510 - val_loss: 0.6872 - val_acc: 0.3599\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3513 - val_loss: 0.6873 - val_acc: 0.3424\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3524 - val_loss: 0.6873 - val_acc: 0.3349\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3517 - val_loss: 0.6873 - val_acc: 0.3452\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3526 - val_loss: 0.6873 - val_acc: 0.3449\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3507 - val_loss: 0.6872 - val_acc: 0.3453\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3505 - val_loss: 0.6873 - val_acc: 0.3381\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3502 - val_loss: 0.6873 - val_acc: 0.3480\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3528 - val_loss: 0.6872 - val_acc: 0.3395\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3494 - val_loss: 0.6873 - val_acc: 0.3394\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3514 - val_loss: 0.6873 - val_acc: 0.3402\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3482 - val_loss: 0.6873 - val_acc: 0.3526\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6885 - acc: 0.3509 - val_loss: 0.6873 - val_acc: 0.3730\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3497 - val_loss: 0.6873 - val_acc: 0.3570\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3492 - val_loss: 0.6873 - val_acc: 0.3433\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6886 - acc: 0.3520 - val_loss: 0.6873 - val_acc: 0.3395\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6879 - acc: 0.3451\n",
      ". theta fit =  1.1380256\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.3538 - val_loss: 0.6879 - val_acc: 0.3506\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3566 - val_loss: 0.6879 - val_acc: 0.3721\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6893 - acc: 0.3571 - val_loss: 0.6879 - val_acc: 0.3732\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3561 - val_loss: 0.6879 - val_acc: 0.3509\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3553 - val_loss: 0.6880 - val_acc: 0.3528\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3594 - val_loss: 0.6879 - val_acc: 0.3843\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3592 - val_loss: 0.6880 - val_acc: 0.3632\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3580 - val_loss: 0.6879 - val_acc: 0.3676\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3578 - val_loss: 0.6879 - val_acc: 0.3691\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3606 - val_loss: 0.6879 - val_acc: 0.3483\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3580 - val_loss: 0.6879 - val_acc: 0.3549\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3583 - val_loss: 0.6879 - val_acc: 0.3606\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3586 - val_loss: 0.6879 - val_acc: 0.3590\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3594 - val_loss: 0.6880 - val_acc: 0.3744\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3607 - val_loss: 0.6880 - val_acc: 0.3551\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3578 - val_loss: 0.6882 - val_acc: 0.3294\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3572 - val_loss: 0.6879 - val_acc: 0.3553\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3580 - val_loss: 0.6880 - val_acc: 0.3805\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3591 - val_loss: 0.6879 - val_acc: 0.3496\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3567 - val_loss: 0.6880 - val_acc: 0.3870\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3576 - val_loss: 0.6880 - val_acc: 0.3523\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6892 - acc: 0.3607 - val_loss: 0.6879 - val_acc: 0.3697\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6886 - acc: 0.3606\n",
      ". theta fit =  1.1139222\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6898 - acc: 0.3633 - val_loss: 0.6885 - val_acc: 0.3627\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3689 - val_loss: 0.6885 - val_acc: 0.3540\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3660 - val_loss: 0.6885 - val_acc: 0.3442\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3689 - val_loss: 0.6885 - val_acc: 0.3702\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3700 - val_loss: 0.6885 - val_acc: 0.3657\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3695 - val_loss: 0.6885 - val_acc: 0.3680\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3710 - val_loss: 0.6885 - val_acc: 0.3957\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3713 - val_loss: 0.6885 - val_acc: 0.4047\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3721 - val_loss: 0.6884 - val_acc: 0.3781\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3709 - val_loss: 0.6885 - val_acc: 0.3946\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3736 - val_loss: 0.6885 - val_acc: 0.3831\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3728 - val_loss: 0.6885 - val_acc: 0.3806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3757 - val_loss: 0.6884 - val_acc: 0.3847\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3724 - val_loss: 0.6885 - val_acc: 0.3801\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6898 - acc: 0.3712 - val_loss: 0.6885 - val_acc: 0.3695\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6897 - acc: 0.3725 - val_loss: 0.6885 - val_acc: 0.3804\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6897 - acc: 0.3751 - val_loss: 0.6886 - val_acc: 0.3520\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3725 - val_loss: 0.6885 - val_acc: 0.3722\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3756 - val_loss: 0.6886 - val_acc: 0.3366\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6891 - acc: 0.3783\n",
      ". theta fit =  1.089855\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6903 - acc: 0.3806 - val_loss: 0.6889 - val_acc: 0.3992\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.3839 - val_loss: 0.6890 - val_acc: 0.4070\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.3863 - val_loss: 0.6890 - val_acc: 0.4176\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.3912 - val_loss: 0.6890 - val_acc: 0.3922\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.3899 - val_loss: 0.6890 - val_acc: 0.3831\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6902 - acc: 0.3901 - val_loss: 0.6890 - val_acc: 0.3798\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6902 - acc: 0.3915 - val_loss: 0.6890 - val_acc: 0.3798\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.3893 - val_loss: 0.6890 - val_acc: 0.4086\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.3917 - val_loss: 0.6889 - val_acc: 0.3833\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.3922 - val_loss: 0.6890 - val_acc: 0.3903\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6902 - acc: 0.3896 - val_loss: 0.6890 - val_acc: 0.4137\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6896 - acc: 0.3994\n",
      ". theta fit =  1.0657593\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6906 - acc: 0.3998 - val_loss: 0.6893 - val_acc: 0.3990\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6905 - acc: 0.4047 - val_loss: 0.6894 - val_acc: 0.3931\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4011 - val_loss: 0.6893 - val_acc: 0.4058\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4056 - val_loss: 0.6894 - val_acc: 0.3418\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4079 - val_loss: 0.6893 - val_acc: 0.4287\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4094 - val_loss: 0.6893 - val_acc: 0.4255\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4109 - val_loss: 0.6893 - val_acc: 0.4090\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4126 - val_loss: 0.6893 - val_acc: 0.4006\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4111 - val_loss: 0.6893 - val_acc: 0.4111\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4074 - val_loss: 0.6893 - val_acc: 0.4214\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4113 - val_loss: 0.6893 - val_acc: 0.4198\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4101 - val_loss: 0.6893 - val_acc: 0.4152\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4090 - val_loss: 0.6894 - val_acc: 0.3929\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4103 - val_loss: 0.6893 - val_acc: 0.4185\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4113 - val_loss: 0.6893 - val_acc: 0.4092\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4116 - val_loss: 0.6893 - val_acc: 0.3800\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4113 - val_loss: 0.6893 - val_acc: 0.4135\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6906 - acc: 0.4100 - val_loss: 0.6893 - val_acc: 0.4388\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4117 - val_loss: 0.6893 - val_acc: 0.4265\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4147 - val_loss: 0.6893 - val_acc: 0.4111\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.4060 - val_loss: 0.6893 - val_acc: 0.4038\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6899 - acc: 0.4200\n",
      ". theta fit =  1.0415883\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6907 - acc: 0.4267 - val_loss: 0.6894 - val_acc: 0.4861\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4375 - val_loss: 0.6894 - val_acc: 0.4412\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4411 - val_loss: 0.6894 - val_acc: 0.4343\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4391 - val_loss: 0.6894 - val_acc: 0.4400\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4460 - val_loss: 0.6894 - val_acc: 0.4420\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4398 - val_loss: 0.6894 - val_acc: 0.4324\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4450 - val_loss: 0.6894 - val_acc: 0.4465\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4445 - val_loss: 0.6895 - val_acc: 0.4089\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4375 - val_loss: 0.6894 - val_acc: 0.4243\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4388 - val_loss: 0.6894 - val_acc: 0.4668\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4489 - val_loss: 0.6894 - val_acc: 0.4171\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4376 - val_loss: 0.6894 - val_acc: 0.4357\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4444 - val_loss: 0.6894 - val_acc: 0.4344\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6900 - acc: 0.4343\n",
      ". theta fit =  1.0173137\n",
      "Iteration:  15\n",
      "Training g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6907 - acc: 0.5376 - val_loss: 0.6894 - val_acc: 0.5738\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5440 - val_loss: 0.6894 - val_acc: 0.5167\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5454 - val_loss: 0.6894 - val_acc: 0.4603\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5329 - val_loss: 0.6894 - val_acc: 0.5712\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5387 - val_loss: 0.6894 - val_acc: 0.5495\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5482 - val_loss: 0.6895 - val_acc: 0.5267\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5398 - val_loss: 0.6894 - val_acc: 0.5194\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5448 - val_loss: 0.6894 - val_acc: 0.5623\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5493 - val_loss: 0.6894 - val_acc: 0.5433\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5372 - val_loss: 0.6894 - val_acc: 0.5690\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5383 - val_loss: 0.6894 - val_acc: 0.5588\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6900 - acc: 0.5744\n",
      ". theta fit =  1.0419396\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6907 - acc: 0.4502 - val_loss: 0.6894 - val_acc: 0.4265\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4338 - val_loss: 0.6894 - val_acc: 0.4502\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4428 - val_loss: 0.6894 - val_acc: 0.4270\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4424 - val_loss: 0.6894 - val_acc: 0.4208\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4465 - val_loss: 0.6894 - val_acc: 0.4240\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4392 - val_loss: 0.6894 - val_acc: 0.4240\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4392 - val_loss: 0.6894 - val_acc: 0.4343\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4492 - val_loss: 0.6896 - val_acc: 0.4865\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4395 - val_loss: 0.6894 - val_acc: 0.4292\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4407 - val_loss: 0.6895 - val_acc: 0.4478\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4413 - val_loss: 0.6894 - val_acc: 0.4433\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4424 - val_loss: 0.6895 - val_acc: 0.4429\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4412 - val_loss: 0.6894 - val_acc: 0.4271\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4478 - val_loss: 0.6894 - val_acc: 0.4342\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4378 - val_loss: 0.6894 - val_acc: 0.4662\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4458 - val_loss: 0.6894 - val_acc: 0.4452\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4466 - val_loss: 0.6895 - val_acc: 0.4329\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4425 - val_loss: 0.6894 - val_acc: 0.4337\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4434 - val_loss: 0.6894 - val_acc: 0.4503\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.6900 - acc: 0.4292\n",
      ". theta fit =  1.0173205\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6907 - acc: 0.5285 - val_loss: 0.6894 - val_acc: 0.5539\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5447 - val_loss: 0.6894 - val_acc: 0.5304\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5385 - val_loss: 0.6895 - val_acc: 0.5807\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5438 - val_loss: 0.6895 - val_acc: 0.5218\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5380 - val_loss: 0.6895 - val_acc: 0.5852\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5445 - val_loss: 0.6894 - val_acc: 0.4888\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5372 - val_loss: 0.6894 - val_acc: 0.5594\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5427 - val_loss: 0.6894 - val_acc: 0.5583\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5440 - val_loss: 0.6894 - val_acc: 0.5470\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6906 - acc: 0.5520 - val_loss: 0.6895 - val_acc: 0.4924\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5336 - val_loss: 0.6895 - val_acc: 0.5179\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5471 - val_loss: 0.6894 - val_acc: 0.4927\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.6900 - acc: 0.5305\n",
      ". theta fit =  1.0421437\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6907 - acc: 0.4459 - val_loss: 0.6894 - val_acc: 0.4741\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4409 - val_loss: 0.6894 - val_acc: 0.4342\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4448 - val_loss: 0.6894 - val_acc: 0.4210\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4399 - val_loss: 0.6894 - val_acc: 0.4465\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4476 - val_loss: 0.6894 - val_acc: 0.4457\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4490 - val_loss: 0.6894 - val_acc: 0.4221\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4460 - val_loss: 0.6894 - val_acc: 0.4137\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4414 - val_loss: 0.6894 - val_acc: 0.4508\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4470 - val_loss: 0.6895 - val_acc: 0.4323\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4491 - val_loss: 0.6895 - val_acc: 0.4227\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4443 - val_loss: 0.6894 - val_acc: 0.4376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.6900 - acc: 0.4740\n",
      ". theta fit =  1.0173559\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6907 - acc: 0.5322 - val_loss: 0.6894 - val_acc: 0.5338\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5399 - val_loss: 0.6894 - val_acc: 0.5206\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5439 - val_loss: 0.6894 - val_acc: 0.4991\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5345 - val_loss: 0.6894 - val_acc: 0.5333\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5410 - val_loss: 0.6894 - val_acc: 0.5508\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5507 - val_loss: 0.6894 - val_acc: 0.5563\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5407 - val_loss: 0.6894 - val_acc: 0.5840\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5459 - val_loss: 0.6894 - val_acc: 0.5101\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5382 - val_loss: 0.6894 - val_acc: 0.5736\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5477 - val_loss: 0.6894 - val_acc: 0.5481\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5400 - val_loss: 0.6894 - val_acc: 0.5597\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5443 - val_loss: 0.6894 - val_acc: 0.5213\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.6900 - acc: 0.5206\n",
      ". theta fit =  1.0426656\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6907 - acc: 0.4563 - val_loss: 0.6894 - val_acc: 0.4548\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4414 - val_loss: 0.6894 - val_acc: 0.4525\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4466 - val_loss: 0.6895 - val_acc: 0.4103\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4361 - val_loss: 0.6894 - val_acc: 0.4409\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4455 - val_loss: 0.6894 - val_acc: 0.4223\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4403 - val_loss: 0.6894 - val_acc: 0.4633\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4401 - val_loss: 0.6894 - val_acc: 0.4293\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4414 - val_loss: 0.6894 - val_acc: 0.4518\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4511 - val_loss: 0.6894 - val_acc: 0.4250\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4407 - val_loss: 0.6894 - val_acc: 0.4664\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4424 - val_loss: 0.6894 - val_acc: 0.4281\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4375 - val_loss: 0.6894 - val_acc: 0.4440\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4459 - val_loss: 0.6895 - val_acc: 0.4171\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4404 - val_loss: 0.6894 - val_acc: 0.4448\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4404 - val_loss: 0.6894 - val_acc: 0.4316\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4412 - val_loss: 0.6895 - val_acc: 0.4175\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4386 - val_loss: 0.6895 - val_acc: 0.4174\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4433 - val_loss: 0.6894 - val_acc: 0.4549\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4505 - val_loss: 0.6894 - val_acc: 0.4277\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4365 - val_loss: 0.6894 - val_acc: 0.4816\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4470 - val_loss: 0.6894 - val_acc: 0.4422\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4436 - val_loss: 0.6894 - val_acc: 0.4242\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4382 - val_loss: 0.6894 - val_acc: 0.4353\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4360 - val_loss: 0.6895 - val_acc: 0.4491\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4348 - val_loss: 0.6894 - val_acc: 0.4735\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4478 - val_loss: 0.6895 - val_acc: 0.4151\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4374 - val_loss: 0.6895 - val_acc: 0.4218\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.4450 - val_loss: 0.6894 - val_acc: 0.4454\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.6900 - acc: 0.4549\n",
      ". theta fit =  1.0171402\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6907 - acc: 0.5332 - val_loss: 0.6895 - val_acc: 0.4984\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6906 - acc: 0.5470 - val_loss: 0.6894 - val_acc: 0.5496\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5507 - val_loss: 0.6894 - val_acc: 0.4642\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5350 - val_loss: 0.6894 - val_acc: 0.5852\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5523 - val_loss: 0.6894 - val_acc: 0.5523\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5376 - val_loss: 0.6894 - val_acc: 0.5185\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5426 - val_loss: 0.6895 - val_acc: 0.5814\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5495 - val_loss: 0.6894 - val_acc: 0.5219\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5403 - val_loss: 0.6894 - val_acc: 0.5681\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5437 - val_loss: 0.6895 - val_acc: 0.5427\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5335 - val_loss: 0.6894 - val_acc: 0.5989\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5471 - val_loss: 0.6894 - val_acc: 0.5564\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5423 - val_loss: 0.6894 - val_acc: 0.5503\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5378 - val_loss: 0.6894 - val_acc: 0.5459\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6900 - acc: 0.5854\n",
      ". theta fit =  1.0197263\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6907 - acc: 0.5390 - val_loss: 0.6895 - val_acc: 0.5243\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5242 - val_loss: 0.6895 - val_acc: 0.5529\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5253 - val_loss: 0.6894 - val_acc: 0.5321\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5331 - val_loss: 0.6894 - val_acc: 0.5332\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5265 - val_loss: 0.6894 - val_acc: 0.5253\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5331 - val_loss: 0.6894 - val_acc: 0.4779\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5247 - val_loss: 0.6895 - val_acc: 0.5451\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5332 - val_loss: 0.6894 - val_acc: 0.4939\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6906 - acc: 0.5259 - val_loss: 0.6896 - val_acc: 0.5393\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5387 - val_loss: 0.6894 - val_acc: 0.5373\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5315 - val_loss: 0.6894 - val_acc: 0.5290\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6907 - acc: 0.5280 - val_loss: 0.6895 - val_acc: 0.5091\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5278 - val_loss: 0.6894 - val_acc: 0.5193\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5248 - val_loss: 0.6894 - val_acc: 0.5767\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6900 - acc: 0.5333\n",
      ". theta fit =  1.0223398\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6907 - acc: 0.5144 - val_loss: 0.6895 - val_acc: 0.5121\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5189 - val_loss: 0.6894 - val_acc: 0.5071\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5186 - val_loss: 0.6894 - val_acc: 0.5175\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5121 - val_loss: 0.6894 - val_acc: 0.5234\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5171 - val_loss: 0.6894 - val_acc: 0.4658\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5143 - val_loss: 0.6895 - val_acc: 0.5272\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5244 - val_loss: 0.6895 - val_acc: 0.4892\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5127 - val_loss: 0.6894 - val_acc: 0.5153\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5075 - val_loss: 0.6894 - val_acc: 0.5796\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5204 - val_loss: 0.6894 - val_acc: 0.5647\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5200 - val_loss: 0.6895 - val_acc: 0.5300\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5184 - val_loss: 0.6894 - val_acc: 0.5003\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5137 - val_loss: 0.6894 - val_acc: 0.5588\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6900 - acc: 0.5175\n",
      ". theta fit =  1.0249815\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6907 - acc: 0.5041 - val_loss: 0.6895 - val_acc: 0.5323\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5041 - val_loss: 0.6894 - val_acc: 0.5216\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5144 - val_loss: 0.6894 - val_acc: 0.5003\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5021 - val_loss: 0.6894 - val_acc: 0.5392\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5053 - val_loss: 0.6894 - val_acc: 0.5476\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5074 - val_loss: 0.6894 - val_acc: 0.5054\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5054 - val_loss: 0.6895 - val_acc: 0.5458\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5089 - val_loss: 0.6894 - val_acc: 0.5213\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5043 - val_loss: 0.6894 - val_acc: 0.5142\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5143 - val_loss: 0.6895 - val_acc: 0.4577\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4990 - val_loss: 0.6894 - val_acc: 0.5301\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5067 - val_loss: 0.6894 - val_acc: 0.4994\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5108 - val_loss: 0.6894 - val_acc: 0.4975\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5094 - val_loss: 0.6894 - val_acc: 0.4942\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5007 - val_loss: 0.6894 - val_acc: 0.5184\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5067 - val_loss: 0.6895 - val_acc: 0.4680\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5103 - val_loss: 0.6894 - val_acc: 0.4986\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5031 - val_loss: 0.6895 - val_acc: 0.5113\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5084 - val_loss: 0.6894 - val_acc: 0.5203\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5049 - val_loss: 0.6894 - val_acc: 0.5043\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5053 - val_loss: 0.6894 - val_acc: 0.5083\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5073 - val_loss: 0.6895 - val_acc: 0.4765\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5045 - val_loss: 0.6894 - val_acc: 0.4993\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5043 - val_loss: 0.6894 - val_acc: 0.5059\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5026 - val_loss: 0.6895 - val_acc: 0.4777\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6900 - acc: 0.5182\n",
      ". theta fit =  1.0276548\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6907 - acc: 0.4922 - val_loss: 0.6895 - val_acc: 0.5576\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4967 - val_loss: 0.6895 - val_acc: 0.5285\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4963 - val_loss: 0.6895 - val_acc: 0.4898\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5005 - val_loss: 0.6895 - val_acc: 0.4762\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4916 - val_loss: 0.6895 - val_acc: 0.5250\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4967 - val_loss: 0.6894 - val_acc: 0.4689\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4917 - val_loss: 0.6894 - val_acc: 0.4748\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4909 - val_loss: 0.6894 - val_acc: 0.5202\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4970 - val_loss: 0.6895 - val_acc: 0.4569\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4864 - val_loss: 0.6894 - val_acc: 0.5076\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4988 - val_loss: 0.6894 - val_acc: 0.5213\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4997 - val_loss: 0.6894 - val_acc: 0.4842\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5024 - val_loss: 0.6895 - val_acc: 0.4677\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4915 - val_loss: 0.6894 - val_acc: 0.4846\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4962 - val_loss: 0.6894 - val_acc: 0.4885\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4907 - val_loss: 0.6894 - val_acc: 0.5262\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4977 - val_loss: 0.6895 - val_acc: 0.5214\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4914 - val_loss: 0.6894 - val_acc: 0.4814\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4987 - val_loss: 0.6894 - val_acc: 0.4831\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4934 - val_loss: 0.6894 - val_acc: 0.4919\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5005 - val_loss: 0.6894 - val_acc: 0.4855\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.5010 - val_loss: 0.6895 - val_acc: 0.4543\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4903 - val_loss: 0.6894 - val_acc: 0.5123\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4900 - val_loss: 0.6895 - val_acc: 0.5825\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6900 - acc: 0.4846\n",
      ". theta fit =  1.0303544\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6907 - acc: 0.4798 - val_loss: 0.6895 - val_acc: 0.5326\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4857 - val_loss: 0.6895 - val_acc: 0.4664\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4785 - val_loss: 0.6895 - val_acc: 0.4902\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4861 - val_loss: 0.6895 - val_acc: 0.5049\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4911 - val_loss: 0.6894 - val_acc: 0.4514\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4816 - val_loss: 0.6894 - val_acc: 0.4827\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4856 - val_loss: 0.6894 - val_acc: 0.5121\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4856 - val_loss: 0.6894 - val_acc: 0.5002\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4853 - val_loss: 0.6895 - val_acc: 0.4840\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4861 - val_loss: 0.6894 - val_acc: 0.5124\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4822 - val_loss: 0.6894 - val_acc: 0.4953\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4848 - val_loss: 0.6895 - val_acc: 0.5117\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4881 - val_loss: 0.6895 - val_acc: 0.5058\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4841 - val_loss: 0.6895 - val_acc: 0.4894\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4862 - val_loss: 0.6894 - val_acc: 0.4641\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4868 - val_loss: 0.6895 - val_acc: 0.5400\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4857 - val_loss: 0.6894 - val_acc: 0.5356\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4904 - val_loss: 0.6894 - val_acc: 0.4846\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4827 - val_loss: 0.6895 - val_acc: 0.5361\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4919 - val_loss: 0.6894 - val_acc: 0.4717\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4785 - val_loss: 0.6894 - val_acc: 0.4659\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6900 - acc: 0.4952\n",
      ". theta fit =  1.0330867\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6907 - acc: 0.4750 - val_loss: 0.6894 - val_acc: 0.4788\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4782 - val_loss: 0.6894 - val_acc: 0.4511\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4700 - val_loss: 0.6895 - val_acc: 0.4897\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4732 - val_loss: 0.6894 - val_acc: 0.4565\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4711 - val_loss: 0.6895 - val_acc: 0.4568\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4743 - val_loss: 0.6894 - val_acc: 0.4713\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4728 - val_loss: 0.6894 - val_acc: 0.5021\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4777 - val_loss: 0.6894 - val_acc: 0.4550\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4717 - val_loss: 0.6894 - val_acc: 0.4730\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4714 - val_loss: 0.6894 - val_acc: 0.5061\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4707 - val_loss: 0.6894 - val_acc: 0.4833\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4767 - val_loss: 0.6895 - val_acc: 0.4834\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6901 - acc: 0.4511\n",
      ". theta fit =  1.0303243\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6907 - acc: 0.4838 - val_loss: 0.6895 - val_acc: 0.4708\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4859 - val_loss: 0.6894 - val_acc: 0.4636\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4823 - val_loss: 0.6894 - val_acc: 0.4617\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4832 - val_loss: 0.6894 - val_acc: 0.4890\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4850 - val_loss: 0.6895 - val_acc: 0.5210\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4811 - val_loss: 0.6895 - val_acc: 0.4915\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4906 - val_loss: 0.6894 - val_acc: 0.4586\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4702 - val_loss: 0.6894 - val_acc: 0.5284\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4860 - val_loss: 0.6895 - val_acc: 0.4680\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4781 - val_loss: 0.6894 - val_acc: 0.5091\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4813 - val_loss: 0.6894 - val_acc: 0.4914\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4824 - val_loss: 0.6895 - val_acc: 0.4488\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4826 - val_loss: 0.6895 - val_acc: 0.4387\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4841 - val_loss: 0.6895 - val_acc: 0.4836\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6901 - acc: 0.4888\n",
      ". theta fit =  1.0331229\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6907 - acc: 0.4812 - val_loss: 0.6895 - val_acc: 0.4368\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4760 - val_loss: 0.6895 - val_acc: 0.4331\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4714 - val_loss: 0.6894 - val_acc: 0.4883\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4716 - val_loss: 0.6895 - val_acc: 0.5084\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4713 - val_loss: 0.6895 - val_acc: 0.4648\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4645 - val_loss: 0.6894 - val_acc: 0.4748\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4772 - val_loss: 0.6894 - val_acc: 0.4719\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4686 - val_loss: 0.6894 - val_acc: 0.4784\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4761 - val_loss: 0.6895 - val_acc: 0.4722\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4680 - val_loss: 0.6894 - val_acc: 0.4551\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4778 - val_loss: 0.6894 - val_acc: 0.4654\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4722 - val_loss: 0.6894 - val_acc: 0.4415\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4752 - val_loss: 0.6895 - val_acc: 0.4408\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4708 - val_loss: 0.6895 - val_acc: 0.5062\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4756 - val_loss: 0.6895 - val_acc: 0.5111\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4761 - val_loss: 0.6894 - val_acc: 0.4783\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6901 - acc: 0.4748\n",
      ". theta fit =  1.0359266\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6907 - acc: 0.4627 - val_loss: 0.6894 - val_acc: 0.4479\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4661 - val_loss: 0.6895 - val_acc: 0.4928\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4630 - val_loss: 0.6894 - val_acc: 0.4641\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4655 - val_loss: 0.6895 - val_acc: 0.4426\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4572 - val_loss: 0.6894 - val_acc: 0.4278\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4612 - val_loss: 0.6894 - val_acc: 0.4393\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4602 - val_loss: 0.6894 - val_acc: 0.4483\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4674 - val_loss: 0.6895 - val_acc: 0.4551\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4617 - val_loss: 0.6895 - val_acc: 0.4713\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4656 - val_loss: 0.6894 - val_acc: 0.5101\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4688 - val_loss: 0.6896 - val_acc: 0.4770\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6900 - acc: 0.4480\n",
      ". theta fit =  1.0330597\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6907 - acc: 0.4684 - val_loss: 0.6894 - val_acc: 0.4865\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4753 - val_loss: 0.6895 - val_acc: 0.4616\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4694 - val_loss: 0.6895 - val_acc: 0.4421\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4721 - val_loss: 0.6894 - val_acc: 0.4883\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4763 - val_loss: 0.6894 - val_acc: 0.4575\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4741 - val_loss: 0.6894 - val_acc: 0.4403\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4750 - val_loss: 0.6895 - val_acc: 0.4441\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4745 - val_loss: 0.6895 - val_acc: 0.4961\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4842 - val_loss: 0.6894 - val_acc: 0.4656\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4687 - val_loss: 0.6894 - val_acc: 0.4445\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4702 - val_loss: 0.6895 - val_acc: 0.4939\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4687 - val_loss: 0.6895 - val_acc: 0.4700\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4737 - val_loss: 0.6894 - val_acc: 0.5131\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4797 - val_loss: 0.6895 - val_acc: 0.4560\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4774 - val_loss: 0.6895 - val_acc: 0.4954\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6900 - acc: 0.4577\n",
      ". theta fit =  1.0301749\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6907 - acc: 0.4867 - val_loss: 0.6894 - val_acc: 0.4857\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4776 - val_loss: 0.6894 - val_acc: 0.4873\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4927 - val_loss: 0.6895 - val_acc: 0.4664\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4794 - val_loss: 0.6895 - val_acc: 0.4542\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4821 - val_loss: 0.6894 - val_acc: 0.5248\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4849 - val_loss: 0.6894 - val_acc: 0.4822\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4864 - val_loss: 0.6895 - val_acc: 0.5087\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4875 - val_loss: 0.6895 - val_acc: 0.5135\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4767 - val_loss: 0.6895 - val_acc: 0.5599\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4866 - val_loss: 0.6894 - val_acc: 0.5111\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4816 - val_loss: 0.6894 - val_acc: 0.4935\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4845 - val_loss: 0.6895 - val_acc: 0.5078\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4869 - val_loss: 0.6895 - val_acc: 0.4877\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4759 - val_loss: 0.6895 - val_acc: 0.5255\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4849 - val_loss: 0.6895 - val_acc: 0.4972\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4890 - val_loss: 0.6894 - val_acc: 0.4627\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4821\n",
      ". theta fit =  1.0331012\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6907 - acc: 0.4860 - val_loss: 0.6895 - val_acc: 0.4424\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4698 - val_loss: 0.6895 - val_acc: 0.4573\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4739 - val_loss: 0.6894 - val_acc: 0.4790\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4751 - val_loss: 0.6894 - val_acc: 0.4781\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4735 - val_loss: 0.6894 - val_acc: 0.4612\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4725 - val_loss: 0.6894 - val_acc: 0.4617\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4714 - val_loss: 0.6894 - val_acc: 0.5270\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4763 - val_loss: 0.6895 - val_acc: 0.5328\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4773 - val_loss: 0.6894 - val_acc: 0.4721\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4696 - val_loss: 0.6894 - val_acc: 0.4509\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4707 - val_loss: 0.6895 - val_acc: 0.4777\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4729 - val_loss: 0.6894 - val_acc: 0.4324\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4695 - val_loss: 0.6894 - val_acc: 0.4790\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4740 - val_loss: 0.6894 - val_acc: 0.4586\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4738 - val_loss: 0.6894 - val_acc: 0.4492\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4681 - val_loss: 0.6894 - val_acc: 0.4626\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4743 - val_loss: 0.6895 - val_acc: 0.4743\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4718 - val_loss: 0.6894 - val_acc: 0.4669\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4718 - val_loss: 0.6894 - val_acc: 0.4612\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4778 - val_loss: 0.6894 - val_acc: 0.4816\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4509\n",
      ". theta fit =  1.0301331\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6907 - acc: 0.4835 - val_loss: 0.6895 - val_acc: 0.4522\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4773 - val_loss: 0.6894 - val_acc: 0.4666\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4835 - val_loss: 0.6895 - val_acc: 0.4733\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4839 - val_loss: 0.6895 - val_acc: 0.4341\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4795 - val_loss: 0.6895 - val_acc: 0.5081\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4882 - val_loss: 0.6894 - val_acc: 0.4554\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4815 - val_loss: 0.6894 - val_acc: 0.4803\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4858 - val_loss: 0.6894 - val_acc: 0.4898\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4782 - val_loss: 0.6894 - val_acc: 0.5131\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4926 - val_loss: 0.6894 - val_acc: 0.4466\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4779 - val_loss: 0.6894 - val_acc: 0.4899\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4896 - val_loss: 0.6894 - val_acc: 0.4595\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4874 - val_loss: 0.6894 - val_acc: 0.5299\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4831 - val_loss: 0.6894 - val_acc: 0.5010\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4843 - val_loss: 0.6894 - val_acc: 0.4706\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4807 - val_loss: 0.6895 - val_acc: 0.5093\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4845 - val_loss: 0.6894 - val_acc: 0.4845\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4862 - val_loss: 0.6895 - val_acc: 0.4447\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4833 - val_loss: 0.6895 - val_acc: 0.5140\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4884 - val_loss: 0.6895 - val_acc: 0.5126\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4836 - val_loss: 0.6894 - val_acc: 0.5147\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4899\n",
      ". theta fit =  1.0331335\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6907 - acc: 0.4757 - val_loss: 0.6895 - val_acc: 0.4345\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4787 - val_loss: 0.6894 - val_acc: 0.4599\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4664 - val_loss: 0.6895 - val_acc: 0.5346\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4749 - val_loss: 0.6895 - val_acc: 0.4327\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4774 - val_loss: 0.6895 - val_acc: 0.4325\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4695 - val_loss: 0.6894 - val_acc: 0.4402\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4775 - val_loss: 0.6894 - val_acc: 0.4727\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4736 - val_loss: 0.6895 - val_acc: 0.4344\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4724 - val_loss: 0.6894 - val_acc: 0.5011\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4727 - val_loss: 0.6894 - val_acc: 0.4944\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4780 - val_loss: 0.6894 - val_acc: 0.4786\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4673 - val_loss: 0.6894 - val_acc: 0.4705\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4730 - val_loss: 0.6895 - val_acc: 0.4637\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4720 - val_loss: 0.6895 - val_acc: 0.4842\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4792 - val_loss: 0.6894 - val_acc: 0.4820\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4714 - val_loss: 0.6895 - val_acc: 0.4910\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4704 - val_loss: 0.6894 - val_acc: 0.4602\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4726\n",
      ". theta fit =  1.0334326\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6907 - acc: 0.4759 - val_loss: 0.6895 - val_acc: 0.4828\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4708 - val_loss: 0.6894 - val_acc: 0.4770\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4712 - val_loss: 0.6895 - val_acc: 0.4975\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4717 - val_loss: 0.6894 - val_acc: 0.4373\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4756 - val_loss: 0.6894 - val_acc: 0.5099\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4668 - val_loss: 0.6894 - val_acc: 0.4933\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4791 - val_loss: 0.6894 - val_acc: 0.4711\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4749 - val_loss: 0.6895 - val_acc: 0.4447\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4691 - val_loss: 0.6894 - val_acc: 0.5133\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4689 - val_loss: 0.6895 - val_acc: 0.5123\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4756 - val_loss: 0.6894 - val_acc: 0.4882\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4738 - val_loss: 0.6894 - val_acc: 0.4502\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6901 - acc: 0.4770\n",
      ". theta fit =  1.0331389\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6907 - acc: 0.4779 - val_loss: 0.6894 - val_acc: 0.4571\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4659 - val_loss: 0.6894 - val_acc: 0.5018\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4755 - val_loss: 0.6894 - val_acc: 0.4723\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4731 - val_loss: 0.6895 - val_acc: 0.4861\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4773 - val_loss: 0.6895 - val_acc: 0.5002\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4722 - val_loss: 0.6894 - val_acc: 0.4656\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4782 - val_loss: 0.6894 - val_acc: 0.4502\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4764 - val_loss: 0.6894 - val_acc: 0.4711\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4742 - val_loss: 0.6894 - val_acc: 0.4429\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4709 - val_loss: 0.6894 - val_acc: 0.4871\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4787 - val_loss: 0.6895 - val_acc: 0.5018\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4571\n",
      ". theta fit =  1.0328285\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6907 - acc: 0.4707 - val_loss: 0.6894 - val_acc: 0.4791\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4765 - val_loss: 0.6894 - val_acc: 0.5045\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4774 - val_loss: 0.6894 - val_acc: 0.4975\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4702 - val_loss: 0.6894 - val_acc: 0.5040\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4782 - val_loss: 0.6894 - val_acc: 0.4764\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4740 - val_loss: 0.6894 - val_acc: 0.4756\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4746 - val_loss: 0.6894 - val_acc: 0.4572\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4725 - val_loss: 0.6894 - val_acc: 0.5193\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4732 - val_loss: 0.6894 - val_acc: 0.4436\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4771 - val_loss: 0.6895 - val_acc: 0.4450\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4700 - val_loss: 0.6894 - val_acc: 0.4828\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4776 - val_loss: 0.6895 - val_acc: 0.4933\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4811 - val_loss: 0.6895 - val_acc: 0.4824\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4756 - val_loss: 0.6894 - val_acc: 0.4474\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4770 - val_loss: 0.6894 - val_acc: 0.4917\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4764\n",
      ". theta fit =  1.0325221\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6907 - acc: 0.4793 - val_loss: 0.6894 - val_acc: 0.5385\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4843 - val_loss: 0.6894 - val_acc: 0.4449\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4699 - val_loss: 0.6894 - val_acc: 0.4700\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4749 - val_loss: 0.6894 - val_acc: 0.4770\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4747 - val_loss: 0.6895 - val_acc: 0.5051\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4723 - val_loss: 0.6894 - val_acc: 0.4940\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4795 - val_loss: 0.6895 - val_acc: 0.4937\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4826 - val_loss: 0.6894 - val_acc: 0.4827\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4740 - val_loss: 0.6894 - val_acc: 0.4775\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4736 - val_loss: 0.6895 - val_acc: 0.4456\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4803 - val_loss: 0.6894 - val_acc: 0.4579\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4764 - val_loss: 0.6894 - val_acc: 0.4633\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4764 - val_loss: 0.6895 - val_acc: 0.4420\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4749 - val_loss: 0.6895 - val_acc: 0.4410\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4801 - val_loss: 0.6894 - val_acc: 0.4706\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4763 - val_loss: 0.6895 - val_acc: 0.4989\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4749 - val_loss: 0.6894 - val_acc: 0.4618\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4743 - val_loss: 0.6895 - val_acc: 0.5114\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4740 - val_loss: 0.6894 - val_acc: 0.4695\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6900 - acc: 0.4775\n",
      ". theta fit =  1.032211\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6907 - acc: 0.4848 - val_loss: 0.6894 - val_acc: 0.4617\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4788 - val_loss: 0.6894 - val_acc: 0.4506\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4725 - val_loss: 0.6894 - val_acc: 0.5093\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4833 - val_loss: 0.6895 - val_acc: 0.5001\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4766 - val_loss: 0.6895 - val_acc: 0.5149\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4766 - val_loss: 0.6895 - val_acc: 0.4861\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4824 - val_loss: 0.6894 - val_acc: 0.4539\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4774 - val_loss: 0.6895 - val_acc: 0.4650\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4800 - val_loss: 0.6895 - val_acc: 0.5021\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4805 - val_loss: 0.6895 - val_acc: 0.4411\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4779 - val_loss: 0.6894 - val_acc: 0.4637\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4618\n",
      ". theta fit =  1.0318906\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6907 - acc: 0.4781 - val_loss: 0.6895 - val_acc: 0.4746\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4846 - val_loss: 0.6894 - val_acc: 0.4785\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4846 - val_loss: 0.6894 - val_acc: 0.4452\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4780 - val_loss: 0.6895 - val_acc: 0.5074\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4782 - val_loss: 0.6894 - val_acc: 0.4897\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4805 - val_loss: 0.6894 - val_acc: 0.4844\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4831 - val_loss: 0.6894 - val_acc: 0.4814\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4773 - val_loss: 0.6894 - val_acc: 0.4832\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4782 - val_loss: 0.6895 - val_acc: 0.4844\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4800 - val_loss: 0.6895 - val_acc: 0.4738\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4760 - val_loss: 0.6894 - val_acc: 0.4949\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4806 - val_loss: 0.6894 - val_acc: 0.4885\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4823 - val_loss: 0.6895 - val_acc: 0.4834\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4764 - val_loss: 0.6894 - val_acc: 0.4819\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4794 - val_loss: 0.6895 - val_acc: 0.5144\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4797 - val_loss: 0.6894 - val_acc: 0.4927\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4802 - val_loss: 0.6894 - val_acc: 0.4701\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4796 - val_loss: 0.6894 - val_acc: 0.5117\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4778 - val_loss: 0.6895 - val_acc: 0.5274\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4788 - val_loss: 0.6895 - val_acc: 0.5120\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4832 - val_loss: 0.6895 - val_acc: 0.4831\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4948\n",
      ". theta fit =  1.0322124\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6907 - acc: 0.4838 - val_loss: 0.6894 - val_acc: 0.4837\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4754 - val_loss: 0.6895 - val_acc: 0.4783\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4807 - val_loss: 0.6894 - val_acc: 0.4573\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4755 - val_loss: 0.6894 - val_acc: 0.4709\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4718 - val_loss: 0.6895 - val_acc: 0.4965\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4775 - val_loss: 0.6895 - val_acc: 0.4576\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4776 - val_loss: 0.6895 - val_acc: 0.4564\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4849 - val_loss: 0.6894 - val_acc: 0.4495\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4814 - val_loss: 0.6895 - val_acc: 0.4374\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4753 - val_loss: 0.6894 - val_acc: 0.4941\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4722 - val_loss: 0.6894 - val_acc: 0.4906\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4814 - val_loss: 0.6894 - val_acc: 0.4453\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4766 - val_loss: 0.6894 - val_acc: 0.4957\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4757 - val_loss: 0.6894 - val_acc: 0.4891\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6900 - acc: 0.4708\n",
      ". theta fit =  1.0318851\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6907 - acc: 0.4743 - val_loss: 0.6894 - val_acc: 0.5156\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4785 - val_loss: 0.6895 - val_acc: 0.4947\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4835 - val_loss: 0.6895 - val_acc: 0.4690\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4747 - val_loss: 0.6895 - val_acc: 0.4366\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4758 - val_loss: 0.6894 - val_acc: 0.4716\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4812 - val_loss: 0.6895 - val_acc: 0.4892\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4820 - val_loss: 0.6894 - val_acc: 0.4413\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4711 - val_loss: 0.6894 - val_acc: 0.4693\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4805 - val_loss: 0.6895 - val_acc: 0.4535\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4782 - val_loss: 0.6895 - val_acc: 0.5263\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4805 - val_loss: 0.6894 - val_acc: 0.4553\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4752 - val_loss: 0.6894 - val_acc: 0.4762\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4772 - val_loss: 0.6894 - val_acc: 0.4842\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4760 - val_loss: 0.6894 - val_acc: 0.4859\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4805 - val_loss: 0.6895 - val_acc: 0.4840\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4762 - val_loss: 0.6894 - val_acc: 0.4631\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4813 - val_loss: 0.6894 - val_acc: 0.4740\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4802 - val_loss: 0.6895 - val_acc: 0.4670\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4760 - val_loss: 0.6895 - val_acc: 0.4582\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4778 - val_loss: 0.6895 - val_acc: 0.4382\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4779 - val_loss: 0.6894 - val_acc: 0.4872\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4796 - val_loss: 0.6894 - val_acc: 0.4875\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4761 - val_loss: 0.6894 - val_acc: 0.5152\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4807 - val_loss: 0.6894 - val_acc: 0.4825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4741 - val_loss: 0.6894 - val_acc: 0.4397\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4721 - val_loss: 0.6894 - val_acc: 0.4967\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4795 - val_loss: 0.6895 - val_acc: 0.4607\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4756 - val_loss: 0.6894 - val_acc: 0.4908\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4810 - val_loss: 0.6894 - val_acc: 0.4889\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4842 - val_loss: 0.6894 - val_acc: 0.4471\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4734 - val_loss: 0.6895 - val_acc: 0.4454\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4786 - val_loss: 0.6895 - val_acc: 0.4829\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4823 - val_loss: 0.6894 - val_acc: 0.4938\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4761 - val_loss: 0.6894 - val_acc: 0.4915\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4791 - val_loss: 0.6894 - val_acc: 0.4633\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4784 - val_loss: 0.6895 - val_acc: 0.5049\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6900 - acc: 0.4966\n",
      ". theta fit =  1.0322156\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6907 - acc: 0.4772 - val_loss: 0.6894 - val_acc: 0.4790\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4743 - val_loss: 0.6895 - val_acc: 0.4906\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4725 - val_loss: 0.6894 - val_acc: 0.4924\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4778 - val_loss: 0.6895 - val_acc: 0.5116\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4787 - val_loss: 0.6894 - val_acc: 0.4597\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4756 - val_loss: 0.6894 - val_acc: 0.4715\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4809 - val_loss: 0.6895 - val_acc: 0.5045\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4734 - val_loss: 0.6894 - val_acc: 0.4834\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4789 - val_loss: 0.6894 - val_acc: 0.4553\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4761 - val_loss: 0.6894 - val_acc: 0.5199\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4759 - val_loss: 0.6895 - val_acc: 0.4348\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4758 - val_loss: 0.6895 - val_acc: 0.4473\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4833 - val_loss: 0.6895 - val_acc: 0.4699\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4702 - val_loss: 0.6894 - val_acc: 0.4887\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4727 - val_loss: 0.6895 - val_acc: 0.5172\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4775 - val_loss: 0.6894 - val_acc: 0.4880\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4757 - val_loss: 0.6895 - val_acc: 0.4421\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4765 - val_loss: 0.6894 - val_acc: 0.4942\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4768 - val_loss: 0.6895 - val_acc: 0.4624\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4755 - val_loss: 0.6894 - val_acc: 0.4907\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4794 - val_loss: 0.6894 - val_acc: 0.4718\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4720 - val_loss: 0.6894 - val_acc: 0.4640\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4765 - val_loss: 0.6895 - val_acc: 0.4602\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4721 - val_loss: 0.6894 - val_acc: 0.4621\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4805 - val_loss: 0.6895 - val_acc: 0.4841\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4752 - val_loss: 0.6894 - val_acc: 0.4769\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4762 - val_loss: 0.6894 - val_acc: 0.4990\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4773 - val_loss: 0.6894 - val_acc: 0.4528\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6900 - acc: 0.4940\n",
      ". theta fit =  1.0325489\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6907 - acc: 0.4761 - val_loss: 0.6894 - val_acc: 0.4703\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4715 - val_loss: 0.6894 - val_acc: 0.4686\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4742 - val_loss: 0.6894 - val_acc: 0.4961\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4719 - val_loss: 0.6894 - val_acc: 0.4786\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4753 - val_loss: 0.6894 - val_acc: 0.4693\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4748 - val_loss: 0.6894 - val_acc: 0.4769\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4745 - val_loss: 0.6895 - val_acc: 0.4937\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4753 - val_loss: 0.6895 - val_acc: 0.4896\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4734 - val_loss: 0.6895 - val_acc: 0.4603\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4724 - val_loss: 0.6895 - val_acc: 0.4634\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4833 - val_loss: 0.6895 - val_acc: 0.4963\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4781 - val_loss: 0.6895 - val_acc: 0.4579\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4781 - val_loss: 0.6894 - val_acc: 0.4437\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4791 - val_loss: 0.6895 - val_acc: 0.4623\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6900 - acc: 0.4784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  1.0328821\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6907 - acc: 0.4811 - val_loss: 0.6894 - val_acc: 0.4563\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4734 - val_loss: 0.6895 - val_acc: 0.4568\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4771 - val_loss: 0.6894 - val_acc: 0.5072\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4801 - val_loss: 0.6895 - val_acc: 0.4445\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4728 - val_loss: 0.6894 - val_acc: 0.4771\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4735 - val_loss: 0.6894 - val_acc: 0.5098\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4736 - val_loss: 0.6894 - val_acc: 0.4930\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4722 - val_loss: 0.6894 - val_acc: 0.4890\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4748 - val_loss: 0.6895 - val_acc: 0.4630\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4783 - val_loss: 0.6894 - val_acc: 0.4809\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4743 - val_loss: 0.6895 - val_acc: 0.4806\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4673 - val_loss: 0.6894 - val_acc: 0.4961\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4777 - val_loss: 0.6894 - val_acc: 0.4675\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4717 - val_loss: 0.6894 - val_acc: 0.4867\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4795 - val_loss: 0.6894 - val_acc: 0.4392\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6900 - acc: 0.4769\n",
      ". theta fit =  1.032543\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6907 - acc: 0.4788 - val_loss: 0.6895 - val_acc: 0.4649\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4777 - val_loss: 0.6894 - val_acc: 0.4681\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4702 - val_loss: 0.6895 - val_acc: 0.4532\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4738 - val_loss: 0.6895 - val_acc: 0.4553\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4768 - val_loss: 0.6894 - val_acc: 0.4621\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4713 - val_loss: 0.6895 - val_acc: 0.5082\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4759 - val_loss: 0.6894 - val_acc: 0.4587\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4741 - val_loss: 0.6894 - val_acc: 0.4489\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4720 - val_loss: 0.6894 - val_acc: 0.4692\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4785 - val_loss: 0.6894 - val_acc: 0.4495\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4771 - val_loss: 0.6894 - val_acc: 0.4561\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4757 - val_loss: 0.6895 - val_acc: 0.4357\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4693 - val_loss: 0.6895 - val_acc: 0.4928\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4759 - val_loss: 0.6894 - val_acc: 0.4925\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4752 - val_loss: 0.6894 - val_acc: 0.4872\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4725 - val_loss: 0.6895 - val_acc: 0.4752\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4742 - val_loss: 0.6894 - val_acc: 0.4703\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4819 - val_loss: 0.6895 - val_acc: 0.4628\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4765 - val_loss: 0.6894 - val_acc: 0.4773\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4754 - val_loss: 0.6894 - val_acc: 0.4786\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4744 - val_loss: 0.6894 - val_acc: 0.4843\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6900 - acc: 0.4561\n",
      ". theta fit =  1.0321999\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6907 - acc: 0.4726 - val_loss: 0.6894 - val_acc: 0.4860\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4759 - val_loss: 0.6895 - val_acc: 0.4798\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4725 - val_loss: 0.6894 - val_acc: 0.4991\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4836 - val_loss: 0.6895 - val_acc: 0.4668\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4766 - val_loss: 0.6895 - val_acc: 0.4903\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4743 - val_loss: 0.6894 - val_acc: 0.4757\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4740 - val_loss: 0.6894 - val_acc: 0.4893\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4776 - val_loss: 0.6894 - val_acc: 0.4671\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4679 - val_loss: 0.6894 - val_acc: 0.4811\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4848 - val_loss: 0.6894 - val_acc: 0.4657\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4707 - val_loss: 0.6894 - val_acc: 0.4823\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4753 - val_loss: 0.6895 - val_acc: 0.4600\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4722 - val_loss: 0.6894 - val_acc: 0.4754\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4776 - val_loss: 0.6894 - val_acc: 0.4705\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6907 - acc: 0.4726 - val_loss: 0.6894 - val_acc: 0.4994\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4856 - val_loss: 0.6895 - val_acc: 0.4741\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6900 - acc: 0.4755\n",
      ". theta fit =  1.0318574\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6907 - acc: 0.4798 - val_loss: 0.6894 - val_acc: 0.4877\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4802 - val_loss: 0.6895 - val_acc: 0.4847\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4787 - val_loss: 0.6895 - val_acc: 0.5070\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4794 - val_loss: 0.6894 - val_acc: 0.4595\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4753 - val_loss: 0.6894 - val_acc: 0.4634\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4795 - val_loss: 0.6894 - val_acc: 0.4767\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4743 - val_loss: 0.6895 - val_acc: 0.4796\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4812 - val_loss: 0.6894 - val_acc: 0.4820\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4790 - val_loss: 0.6895 - val_acc: 0.4609\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4789 - val_loss: 0.6894 - val_acc: 0.4985\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4777 - val_loss: 0.6894 - val_acc: 0.4680\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4762 - val_loss: 0.6895 - val_acc: 0.5035\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4768 - val_loss: 0.6895 - val_acc: 0.5109\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6907 - acc: 0.4776 - val_loss: 0.6894 - val_acc: 0.4439\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6900 - acc: 0.4595\n",
      ". theta fit =  1.0315078\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration)\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()\n",
    "\n",
    "    # regular optimizer and batch size\n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(),\n",
    "                      loss=my_loss_wrapper_fit(myinputs_fit, 1,\n",
    "                                               MSE_loss=False),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train),\n",
    "                  y_train,\n",
    "                  epochs=100,\n",
    "                  batch_size=1000,\n",
    "                  validation_data=(np.array(X_test), y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = True\n",
    "\n",
    "    # special optimizer and batch size = 2*N\n",
    "    model_fit.compile(optimizer=optimizer,\n",
    "                      loss=my_loss_wrapper_fit(myinputs_fit,\n",
    "                                               -1,\n",
    "                                               MSE_loss=False),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta),\n",
    "                  Y_train_theta,\n",
    "                  epochs=1,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:07:01.082270Z",
     "start_time": "2020-06-08T02:07:00.797393Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(theta1_param, 0, len(fit_vals), label='Truth')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"GaussianAltFit-1D (DCTR Reweight)\\nN = {:.0e}, Iterations = {:.0f}\".format(\n",
    "    N, iterations))\n",
    "# plt.savefig(\"GaussianAltFit-1D (DCTR Reweight)\\nN = {:.0e}, Iterations = {:.0f}.png\".format(\n",
    "#     N, iterations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:07:01.418640Z",
     "start_time": "2020-06-08T02:07:01.085470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "fit_vals = np.array(fit_vals)\n",
    "plt.title(\n",
    "    \"GaussianAltFit-1D (DCTR Reweight) Zoomed:\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\".\n",
    "    format(N, index_refine[1], iterations))\n",
    "plt.plot(np.arange(index_refine[1], len(fit_vals)),\n",
    "         fit_vals[index_refine[1]:],\n",
    "         label='Model $\\mu$ Fit')\n",
    "plt.hlines(theta1_param, index_refine[1], len(fit_vals), label='$\\mu_{Truth}$')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-1D (DCTR Reweight) Zoomed:\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\".\n",
    "#     format(N, index_refine[1], iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Fitting between DCTR Reweighting and Analytical Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:07:01.445314Z",
     "start_time": "2020-06-08T02:07:01.421834Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_vals = [theta_fit_init]\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T04:44:39.076519Z",
     "start_time": "2020-06-08T02:07:01.448054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6937 - acc: 0.4119 - val_loss: 0.6924 - val_acc: 0.3217\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3636 - val_loss: 0.6923 - val_acc: 0.3279\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3502 - val_loss: 0.6923 - val_acc: 0.3401\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6936 - acc: 0.3433 - val_loss: 0.6923 - val_acc: 0.3427\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3491 - val_loss: 0.6923 - val_acc: 0.3928\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3487 - val_loss: 0.6923 - val_acc: 0.3397\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3433 - val_loss: 0.6923 - val_acc: 0.3602\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3518 - val_loss: 0.6923 - val_acc: 0.3231\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3517 - val_loss: 0.6923 - val_acc: 0.3359\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3461 - val_loss: 0.6923 - val_acc: 0.3570\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3472 - val_loss: 0.6923 - val_acc: 0.3227\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.3506 - val_loss: 0.6923 - val_acc: 0.3533\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3490 - val_loss: 0.6923 - val_acc: 0.3202\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3558 - val_loss: 0.6923 - val_acc: 0.3088\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3549 - val_loss: 0.6923 - val_acc: 0.3886\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6935 - acc: 0.3589 - val_loss: 0.6923 - val_acc: 0.3664\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.3581 - val_loss: 0.6923 - val_acc: 0.3082\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6929 - acc: 0.3602\n",
      ". theta fit =  0.5316991\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6727 - acc: 0.6594 - val_loss: 0.6661 - val_acc: 0.6829\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6852 - val_loss: 0.6660 - val_acc: 0.6879\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6857 - val_loss: 0.6660 - val_acc: 0.6875\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6860 - val_loss: 0.6660 - val_acc: 0.6833\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6669 - acc: 0.6856 - val_loss: 0.6660 - val_acc: 0.6833\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6669 - acc: 0.6857 - val_loss: 0.6661 - val_acc: 0.6875\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6669 - acc: 0.6858 - val_loss: 0.6660 - val_acc: 0.6858\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6861 - val_loss: 0.6660 - val_acc: 0.6840\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6857 - val_loss: 0.6661 - val_acc: 0.6863\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6857 - val_loss: 0.6661 - val_acc: 0.6859\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6669 - acc: 0.6859 - val_loss: 0.6660 - val_acc: 0.6867\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6860 - val_loss: 0.6662 - val_acc: 0.6813\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6859 - val_loss: 0.6660 - val_acc: 0.6836\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6857 - val_loss: 0.6660 - val_acc: 0.6863\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6859 - val_loss: 0.6660 - val_acc: 0.6859\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6863 - val_loss: 0.6660 - val_acc: 0.6857\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6669 - acc: 0.6860 - val_loss: 0.6660 - val_acc: 0.6870\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6664 - acc: 0.6858\n",
      ". theta fit =  0.90375566\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6931 - acc: 0.6681 - val_loss: 0.6913 - val_acc: 0.6886\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.6780 - val_loss: 0.6913 - val_acc: 0.6832\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.6788 - val_loss: 0.6913 - val_acc: 0.6782\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.6759 - val_loss: 0.6913 - val_acc: 0.6876\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6925 - acc: 0.6788 - val_loss: 0.6913 - val_acc: 0.6678\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.6777 - val_loss: 0.6913 - val_acc: 0.6628\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6925 - acc: 0.6744 - val_loss: 0.6913 - val_acc: 0.6846\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6925 - acc: 0.6777 - val_loss: 0.6913 - val_acc: 0.6712\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6925 - acc: 0.6774 - val_loss: 0.6913 - val_acc: 0.6739\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6925 - acc: 0.6761 - val_loss: 0.6913 - val_acc: 0.6756\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6925 - acc: 0.6781 - val_loss: 0.6913 - val_acc: 0.6879\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.6773 - val_loss: 0.6913 - val_acc: 0.6910\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.6765 - val_loss: 0.6914 - val_acc: 0.6609\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6919 - acc: 0.6785\n",
      ". theta fit =  1.2231238\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6903 - acc: 0.3598 - val_loss: 0.6865 - val_acc: 0.3316\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6877 - acc: 0.3297 - val_loss: 0.6861 - val_acc: 0.3337\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6876 - acc: 0.3343 - val_loss: 0.6862 - val_acc: 0.3504\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.3389 - val_loss: 0.6861 - val_acc: 0.3439\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6875 - acc: 0.3412 - val_loss: 0.6862 - val_acc: 0.3320\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6875 - acc: 0.3407 - val_loss: 0.6862 - val_acc: 0.3321\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.3415 - val_loss: 0.6861 - val_acc: 0.3445\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.3405 - val_loss: 0.6861 - val_acc: 0.3433\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6875 - acc: 0.3415 - val_loss: 0.6861 - val_acc: 0.3412\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6875 - acc: 0.3387 - val_loss: 0.6862 - val_acc: 0.3640\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.3428 - val_loss: 0.6861 - val_acc: 0.3418\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6875 - acc: 0.3428 - val_loss: 0.6862 - val_acc: 0.3592\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6875 - acc: 0.3441 - val_loss: 0.6862 - val_acc: 0.3342\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6876 - acc: 0.3427 - val_loss: 0.6861 - val_acc: 0.3424\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6868 - acc: 0.3437\n",
      ". theta fit =  0.9325772\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6936 - acc: 0.6038 - val_loss: 0.6919 - val_acc: 0.6595\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6931 - acc: 0.6659 - val_loss: 0.6919 - val_acc: 0.6779\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6931 - acc: 0.6741 - val_loss: 0.6919 - val_acc: 0.6752\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6931 - acc: 0.6774 - val_loss: 0.6919 - val_acc: 0.6751\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.6774 - val_loss: 0.6919 - val_acc: 0.6880\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6931 - acc: 0.6784 - val_loss: 0.6919 - val_acc: 0.6755\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.6789 - val_loss: 0.6919 - val_acc: 0.6674\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.6766 - val_loss: 0.6919 - val_acc: 0.6748\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6931 - acc: 0.6783 - val_loss: 0.6919 - val_acc: 0.6750\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6931 - acc: 0.6768 - val_loss: 0.6919 - val_acc: 0.6721\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.6777 - val_loss: 0.6919 - val_acc: 0.6863\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6925 - acc: 0.6597\n",
      ". theta fit =  1.2052608\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6895 - acc: 0.3700 - val_loss: 0.6871 - val_acc: 0.3405\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3446 - val_loss: 0.6871 - val_acc: 0.3521\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3481 - val_loss: 0.6871 - val_acc: 0.3534\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3504 - val_loss: 0.6871 - val_acc: 0.3472\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3484 - val_loss: 0.6871 - val_acc: 0.3492\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3486 - val_loss: 0.6871 - val_acc: 0.3461\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6885 - acc: 0.3464 - val_loss: 0.6871 - val_acc: 0.3417\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3479 - val_loss: 0.6871 - val_acc: 0.3416\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3458 - val_loss: 0.6871 - val_acc: 0.3489\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3447 - val_loss: 0.6871 - val_acc: 0.3516\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3447 - val_loss: 0.6871 - val_acc: 0.3360\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3454 - val_loss: 0.6871 - val_acc: 0.3348\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3449 - val_loss: 0.6871 - val_acc: 0.3338\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3450 - val_loss: 0.6871 - val_acc: 0.3494\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3450 - val_loss: 0.6871 - val_acc: 0.3466\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3448 - val_loss: 0.6871 - val_acc: 0.3371\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3445 - val_loss: 0.6871 - val_acc: 0.3465\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3442 - val_loss: 0.6871 - val_acc: 0.3534\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3451 - val_loss: 0.6871 - val_acc: 0.3361\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3428 - val_loss: 0.6873 - val_acc: 0.3317\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3432 - val_loss: 0.6871 - val_acc: 0.3480\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3431 - val_loss: 0.6872 - val_acc: 0.3397\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3440 - val_loss: 0.6871 - val_acc: 0.3418\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3451 - val_loss: 0.6871 - val_acc: 0.3465\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6885 - acc: 0.3445 - val_loss: 0.6871 - val_acc: 0.3482\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6877 - acc: 0.3464\n",
      ". theta fit =  0.944218\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6938 - acc: 0.6004 - val_loss: 0.6921 - val_acc: 0.6654\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6676 - val_loss: 0.6920 - val_acc: 0.6707\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6671 - val_loss: 0.6921 - val_acc: 0.6907\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6717 - val_loss: 0.6920 - val_acc: 0.6780\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6698 - val_loss: 0.6920 - val_acc: 0.6828\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6762 - val_loss: 0.6920 - val_acc: 0.6709\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6696 - val_loss: 0.6921 - val_acc: 0.6915\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6732 - val_loss: 0.6921 - val_acc: 0.6525\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6740 - val_loss: 0.6920 - val_acc: 0.6916\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6731 - val_loss: 0.6921 - val_acc: 0.6913\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6708 - val_loss: 0.6921 - val_acc: 0.6446\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6761 - val_loss: 0.6920 - val_acc: 0.6810\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6926 - acc: 0.6710\n",
      ". theta fit =  1.1973441\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6897 - acc: 0.3439 - val_loss: 0.6875 - val_acc: 0.3489\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3404 - val_loss: 0.6876 - val_acc: 0.3309\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3395 - val_loss: 0.6875 - val_acc: 0.3439\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3407 - val_loss: 0.6876 - val_acc: 0.3289\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3399 - val_loss: 0.6875 - val_acc: 0.3494\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3404 - val_loss: 0.6875 - val_acc: 0.3370\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3394 - val_loss: 0.6875 - val_acc: 0.3411\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3403 - val_loss: 0.6875 - val_acc: 0.3384\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3399 - val_loss: 0.6875 - val_acc: 0.3473\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3397 - val_loss: 0.6875 - val_acc: 0.3384\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3398 - val_loss: 0.6875 - val_acc: 0.3322\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3405 - val_loss: 0.6875 - val_acc: 0.3488\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3400 - val_loss: 0.6876 - val_acc: 0.3561\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3411 - val_loss: 0.6875 - val_acc: 0.3451\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3410 - val_loss: 0.6875 - val_acc: 0.3345\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3402 - val_loss: 0.6875 - val_acc: 0.3474\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3420 - val_loss: 0.6875 - val_acc: 0.3385\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3413 - val_loss: 0.6875 - val_acc: 0.3357\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3412 - val_loss: 0.6876 - val_acc: 0.3569\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.3419 - val_loss: 0.6875 - val_acc: 0.3454\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6882 - acc: 0.3383\n",
      ". theta fit =  0.9494828\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6938 - acc: 0.5972 - val_loss: 0.6923 - val_acc: 0.5937\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6632 - val_loss: 0.6922 - val_acc: 0.6041\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.6658 - val_loss: 0.6922 - val_acc: 0.6919\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.6702 - val_loss: 0.6921 - val_acc: 0.6855\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6709 - val_loss: 0.6921 - val_acc: 0.6657\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6701 - val_loss: 0.6921 - val_acc: 0.6850\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6686 - val_loss: 0.6921 - val_acc: 0.6848\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.6684 - val_loss: 0.6921 - val_acc: 0.6862\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.6677 - val_loss: 0.6922 - val_acc: 0.6569\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.6701 - val_loss: 0.6921 - val_acc: 0.6873\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6724 - val_loss: 0.6921 - val_acc: 0.6792\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6738 - val_loss: 0.6921 - val_acc: 0.6859\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6672 - val_loss: 0.6921 - val_acc: 0.6914\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.6726 - val_loss: 0.6921 - val_acc: 0.6915\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6737 - val_loss: 0.6921 - val_acc: 0.6824\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6729 - val_loss: 0.6921 - val_acc: 0.6702\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6742 - val_loss: 0.6921 - val_acc: 0.6892\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6682 - val_loss: 0.6921 - val_acc: 0.6849\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6742 - val_loss: 0.6922 - val_acc: 0.6917\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6765 - val_loss: 0.6921 - val_acc: 0.6620\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6737 - val_loss: 0.6921 - val_acc: 0.6673\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6731 - val_loss: 0.6921 - val_acc: 0.6818\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6727 - val_loss: 0.6922 - val_acc: 0.6419\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6732 - val_loss: 0.6921 - val_acc: 0.6749\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6933 - acc: 0.6749 - val_loss: 0.6921 - val_acc: 0.6857\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6927 - acc: 0.6827\n",
      ". theta fit =  1.1937994\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6900 - acc: 0.3618 - val_loss: 0.6878 - val_acc: 0.3421\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3367 - val_loss: 0.6878 - val_acc: 0.3281\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3394 - val_loss: 0.6877 - val_acc: 0.3482\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6890 - acc: 0.3418 - val_loss: 0.6877 - val_acc: 0.3424\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3397 - val_loss: 0.6877 - val_acc: 0.3339\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3398 - val_loss: 0.6878 - val_acc: 0.3564\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3426 - val_loss: 0.6878 - val_acc: 0.3284\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3417 - val_loss: 0.6877 - val_acc: 0.3367\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3408 - val_loss: 0.6877 - val_acc: 0.3425\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3406 - val_loss: 0.6878 - val_acc: 0.3312\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3406 - val_loss: 0.6877 - val_acc: 0.3474\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3408 - val_loss: 0.6877 - val_acc: 0.3416\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3412 - val_loss: 0.6877 - val_acc: 0.3388\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3401 - val_loss: 0.6877 - val_acc: 0.3349\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3415 - val_loss: 0.6877 - val_acc: 0.3389\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6890 - acc: 0.3409 - val_loss: 0.6877 - val_acc: 0.3536\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6891 - acc: 0.3416 - val_loss: 0.6877 - val_acc: 0.3380\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6890 - acc: 0.3416 - val_loss: 0.6877 - val_acc: 0.3375\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6890 - acc: 0.3419 - val_loss: 0.6877 - val_acc: 0.3366\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6883 - acc: 0.3424\n",
      ". theta fit =  1.1695797\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6901 - acc: 0.3392 - val_loss: 0.6888 - val_acc: 0.3347\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3391 - val_loss: 0.6888 - val_acc: 0.3320\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3404 - val_loss: 0.6888 - val_acc: 0.3299\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3389 - val_loss: 0.6888 - val_acc: 0.3492\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3409 - val_loss: 0.6889 - val_acc: 0.3253\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3391 - val_loss: 0.6888 - val_acc: 0.3514\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3396 - val_loss: 0.6888 - val_acc: 0.3373\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3388 - val_loss: 0.6888 - val_acc: 0.3409\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3394 - val_loss: 0.6888 - val_acc: 0.3380\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3402 - val_loss: 0.6888 - val_acc: 0.3335\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3400 - val_loss: 0.6888 - val_acc: 0.3392\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3410 - val_loss: 0.6890 - val_acc: 0.3196\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3389 - val_loss: 0.6888 - val_acc: 0.3426\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3398 - val_loss: 0.6888 - val_acc: 0.3382\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3404 - val_loss: 0.6888 - val_acc: 0.3425\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3399 - val_loss: 0.6889 - val_acc: 0.3282\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3390 - val_loss: 0.6888 - val_acc: 0.3402\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3399 - val_loss: 0.6888 - val_acc: 0.3374\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3403 - val_loss: 0.6888 - val_acc: 0.3467\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3401 - val_loss: 0.6888 - val_acc: 0.3334\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3402 - val_loss: 0.6888 - val_acc: 0.3435\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3397 - val_loss: 0.6888 - val_acc: 0.3424\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3408 - val_loss: 0.6888 - val_acc: 0.3455\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3398 - val_loss: 0.6888 - val_acc: 0.3443\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6901 - acc: 0.3397 - val_loss: 0.6888 - val_acc: 0.3413\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6894 - acc: 0.3424\n",
      ". theta fit =  1.1454749\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6911 - acc: 0.3396 - val_loss: 0.6897 - val_acc: 0.3467\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3379 - val_loss: 0.6898 - val_acc: 0.3294\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6910 - acc: 0.3392 - val_loss: 0.6897 - val_acc: 0.3318\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3382 - val_loss: 0.6897 - val_acc: 0.3367\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3392 - val_loss: 0.6897 - val_acc: 0.3417\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3387 - val_loss: 0.6897 - val_acc: 0.3518\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3391 - val_loss: 0.6897 - val_acc: 0.3492\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3378 - val_loss: 0.6897 - val_acc: 0.3374\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3389 - val_loss: 0.6897 - val_acc: 0.3394\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3398 - val_loss: 0.6897 - val_acc: 0.3349\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6911 - acc: 0.3384 - val_loss: 0.6897 - val_acc: 0.3401\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6904 - acc: 0.3465\n",
      ". theta fit =  1.1214056\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6919 - acc: 0.3371 - val_loss: 0.6905 - val_acc: 0.3340\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3370 - val_loss: 0.6905 - val_acc: 0.3319\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3353 - val_loss: 0.6905 - val_acc: 0.3540\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3370 - val_loss: 0.6907 - val_acc: 0.3197\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3368 - val_loss: 0.6905 - val_acc: 0.3431\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3381 - val_loss: 0.6905 - val_acc: 0.3299\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3361 - val_loss: 0.6905 - val_acc: 0.3456\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3380 - val_loss: 0.6906 - val_acc: 0.3654\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3387 - val_loss: 0.6905 - val_acc: 0.3497\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3377 - val_loss: 0.6905 - val_acc: 0.3333\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3384 - val_loss: 0.6905 - val_acc: 0.3538\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3377 - val_loss: 0.6905 - val_acc: 0.3424\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3393 - val_loss: 0.6905 - val_acc: 0.3289\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3371 - val_loss: 0.6905 - val_acc: 0.3340\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3382 - val_loss: 0.6905 - val_acc: 0.3358\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.3374 - val_loss: 0.6905 - val_acc: 0.3464\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3386 - val_loss: 0.6905 - val_acc: 0.3465\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3395 - val_loss: 0.6906 - val_acc: 0.3266\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3390 - val_loss: 0.6906 - val_acc: 0.3285\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3381 - val_loss: 0.6905 - val_acc: 0.3522\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3385 - val_loss: 0.6906 - val_acc: 0.3270\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6918 - acc: 0.3370 - val_loss: 0.6905 - val_acc: 0.3338\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6912 - acc: 0.3422\n",
      ". theta fit =  1.0973085\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6925 - acc: 0.3369 - val_loss: 0.6912 - val_acc: 0.3406\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3365 - val_loss: 0.6912 - val_acc: 0.3454\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3370 - val_loss: 0.6912 - val_acc: 0.3487\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3371 - val_loss: 0.6912 - val_acc: 0.3520\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3367 - val_loss: 0.6912 - val_acc: 0.3426\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3376 - val_loss: 0.6912 - val_acc: 0.3328\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3367 - val_loss: 0.6912 - val_acc: 0.3285\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3350 - val_loss: 0.6912 - val_acc: 0.3491\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3385 - val_loss: 0.6912 - val_acc: 0.3441\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3374 - val_loss: 0.6912 - val_acc: 0.3286\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3368 - val_loss: 0.6912 - val_acc: 0.3377\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3367 - val_loss: 0.6912 - val_acc: 0.3251\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6918 - acc: 0.3452\n",
      ". theta fit =  1.0731326\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6930 - acc: 0.3375 - val_loss: 0.6917 - val_acc: 0.3371\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3359 - val_loss: 0.6917 - val_acc: 0.3571\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3374 - val_loss: 0.6917 - val_acc: 0.3473\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3374 - val_loss: 0.6917 - val_acc: 0.3332\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3363 - val_loss: 0.6917 - val_acc: 0.3239\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3365 - val_loss: 0.6917 - val_acc: 0.3534\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3379 - val_loss: 0.6917 - val_acc: 0.3537\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3394 - val_loss: 0.6917 - val_acc: 0.3305\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3399 - val_loss: 0.6917 - val_acc: 0.3465\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3413 - val_loss: 0.6917 - val_acc: 0.3208\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3369 - val_loss: 0.6917 - val_acc: 0.3632\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3395 - val_loss: 0.6917 - val_acc: 0.3303\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3397 - val_loss: 0.6917 - val_acc: 0.3579\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6923 - acc: 0.3471\n",
      ". theta fit =  1.048836\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6934 - acc: 0.3388 - val_loss: 0.6921 - val_acc: 0.3339\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3395 - val_loss: 0.6921 - val_acc: 0.3558\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3417 - val_loss: 0.6921 - val_acc: 0.3458\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3364 - val_loss: 0.6921 - val_acc: 0.3560\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3401 - val_loss: 0.6921 - val_acc: 0.3474\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3401 - val_loss: 0.6921 - val_acc: 0.3658\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3410 - val_loss: 0.6921 - val_acc: 0.3581\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3438 - val_loss: 0.6922 - val_acc: 0.3086\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3413 - val_loss: 0.6921 - val_acc: 0.3643\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3446 - val_loss: 0.6921 - val_acc: 0.3616\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3427 - val_loss: 0.6921 - val_acc: 0.3718\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3440 - val_loss: 0.6921 - val_acc: 0.3589\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6934 - acc: 0.3437 - val_loss: 0.6922 - val_acc: 0.3106\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6927 - acc: 0.3457\n",
      ". theta fit =  1.0243847\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6936 - acc: 0.3577 - val_loss: 0.6923 - val_acc: 0.3085\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3599 - val_loss: 0.6924 - val_acc: 0.3127\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3566 - val_loss: 0.6923 - val_acc: 0.3927\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3612 - val_loss: 0.6924 - val_acc: 0.4193\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3567 - val_loss: 0.6923 - val_acc: 0.4159\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3555 - val_loss: 0.6923 - val_acc: 0.3721\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3552 - val_loss: 0.6923 - val_acc: 0.3552\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3619 - val_loss: 0.6923 - val_acc: 0.3355\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3464 - val_loss: 0.6923 - val_acc: 0.3768\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3639 - val_loss: 0.6923 - val_acc: 0.3187\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3458 - val_loss: 0.6923 - val_acc: 0.3966\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3561 - val_loss: 0.6923 - val_acc: 0.3800\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3497 - val_loss: 0.6923 - val_acc: 0.3861\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3519 - val_loss: 0.6923 - val_acc: 0.3590\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3536 - val_loss: 0.6923 - val_acc: 0.3913\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3524 - val_loss: 0.6923 - val_acc: 0.3518\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3494 - val_loss: 0.6923 - val_acc: 0.4178\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3548 - val_loss: 0.6923 - val_acc: 0.3181\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3500 - val_loss: 0.6923 - val_acc: 0.3654\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3509 - val_loss: 0.6923 - val_acc: 0.3417\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3497 - val_loss: 0.6923 - val_acc: 0.3702\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3536 - val_loss: 0.6923 - val_acc: 0.3160\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3533 - val_loss: 0.6924 - val_acc: 0.3217\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3529 - val_loss: 0.6923 - val_acc: 0.3213\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3532 - val_loss: 0.6923 - val_acc: 0.3573\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3512 - val_loss: 0.6923 - val_acc: 0.3579\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3562 - val_loss: 0.6924 - val_acc: 0.3391\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3540 - val_loss: 0.6924 - val_acc: 0.3266\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3487 - val_loss: 0.6923 - val_acc: 0.3870\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6929 - acc: 0.3654\n",
      ". theta fit =  0.999754\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6937 - acc: 0.4561 - val_loss: 0.6924 - val_acc: 0.4740\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5139 - val_loss: 0.6925 - val_acc: 0.4999\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5119 - val_loss: 0.6924 - val_acc: 0.4739\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4995 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4943 - val_loss: 0.6924 - val_acc: 0.5154\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4966 - val_loss: 0.6924 - val_acc: 0.4813\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4991 - val_loss: 0.6924 - val_acc: 0.5691\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5069 - val_loss: 0.6924 - val_acc: 0.4656\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4981 - val_loss: 0.6924 - val_acc: 0.4760\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4860 - val_loss: 0.6924 - val_acc: 0.5101\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5061 - val_loss: 0.6925 - val_acc: 0.4999\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4708 - val_loss: 0.6924 - val_acc: 0.5888\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4928 - val_loss: 0.6924 - val_acc: 0.5410\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4859 - val_loss: 0.6924 - val_acc: 0.5431\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5070 - val_loss: 0.6924 - val_acc: 0.4798\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4925 - val_loss: 0.6924 - val_acc: 0.4900\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4855 - val_loss: 0.6924 - val_acc: 0.5003\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5064 - val_loss: 0.6924 - val_acc: 0.4440\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4796 - val_loss: 0.6924 - val_acc: 0.4488\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4904 - val_loss: 0.6925 - val_acc: 0.4999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4864 - val_loss: 0.6924 - val_acc: 0.4564\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4967 - val_loss: 0.6924 - val_acc: 0.4830\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4755 - val_loss: 0.6924 - val_acc: 0.4800\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4983 - val_loss: 0.6924 - val_acc: 0.4713\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4913 - val_loss: 0.6924 - val_acc: 0.4764\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4945 - val_loss: 0.6924 - val_acc: 0.4886\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4912 - val_loss: 0.6924 - val_acc: 0.4609\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4808 - val_loss: 0.6924 - val_acc: 0.4977\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4886 - val_loss: 0.6924 - val_acc: 0.4841\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4927 - val_loss: 0.6924 - val_acc: 0.5005\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4854 - val_loss: 0.6924 - val_acc: 0.5078\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4985 - val_loss: 0.6924 - val_acc: 0.4789\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4928 - val_loss: 0.6924 - val_acc: 0.4915\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4673 - val_loss: 0.6924 - val_acc: 0.5007\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5169 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4850 - val_loss: 0.6924 - val_acc: 0.5485\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4940 - val_loss: 0.6924 - val_acc: 0.5087\n",
      "Epoch 38/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5201 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 39/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4749 - val_loss: 0.6924 - val_acc: 0.4785\n",
      "Epoch 40/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4725 - val_loss: 0.6924 - val_acc: 0.5827\n",
      "Epoch 41/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5162 - val_loss: 0.6924 - val_acc: 0.4864\n",
      "Epoch 42/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4869 - val_loss: 0.6924 - val_acc: 0.4945\n",
      "Epoch 43/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4670 - val_loss: 0.6924 - val_acc: 0.5476\n",
      "Epoch 44/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5225 - val_loss: 0.6924 - val_acc: 0.4393\n",
      "Epoch 45/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4850 - val_loss: 0.6924 - val_acc: 0.4491\n",
      "Epoch 46/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4880 - val_loss: 0.6924 - val_acc: 0.4836\n",
      "Epoch 47/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4942 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 48/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4979 - val_loss: 0.6924 - val_acc: 0.4886\n",
      "Epoch 49/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4901 - val_loss: 0.6924 - val_acc: 0.4965\n",
      "Epoch 50/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4810 - val_loss: 0.6924 - val_acc: 0.5092\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6930 - acc: 0.5829\n",
      ". theta fit =  0.9753383\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6936 - acc: 0.6404 - val_loss: 0.6924 - val_acc: 0.6209\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6722 - val_loss: 0.6923 - val_acc: 0.6881\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6674 - val_loss: 0.6924 - val_acc: 0.6778\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6767 - val_loss: 0.6924 - val_acc: 0.6671\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6607 - val_loss: 0.6924 - val_acc: 0.6917\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6688 - val_loss: 0.6924 - val_acc: 0.6421\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6748 - val_loss: 0.6924 - val_acc: 0.6487\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6736 - val_loss: 0.6923 - val_acc: 0.6852\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6692 - val_loss: 0.6923 - val_acc: 0.6811\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6764 - val_loss: 0.6923 - val_acc: 0.6783\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6673 - val_loss: 0.6923 - val_acc: 0.6898\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6635 - val_loss: 0.6923 - val_acc: 0.6856\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6748 - val_loss: 0.6923 - val_acc: 0.6864\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6742 - val_loss: 0.6924 - val_acc: 0.6629\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6668 - val_loss: 0.6924 - val_acc: 0.6918\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6749 - val_loss: 0.6924 - val_acc: 0.6699\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6715 - val_loss: 0.6924 - val_acc: 0.6695\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6667 - val_loss: 0.6923 - val_acc: 0.6761\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6653 - val_loss: 0.6923 - val_acc: 0.6879\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6689 - val_loss: 0.6924 - val_acc: 0.6693\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6742 - val_loss: 0.6923 - val_acc: 0.6804\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6737 - val_loss: 0.6923 - val_acc: 0.6887\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6929 - acc: 0.6856\n",
      ". theta fit =  1.0004095\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6937 - acc: 0.5892 - val_loss: 0.6924 - val_acc: 0.4427\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4729 - val_loss: 0.6924 - val_acc: 0.5189\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4933 - val_loss: 0.6924 - val_acc: 0.4038\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4534 - val_loss: 0.6924 - val_acc: 0.5631\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4968 - val_loss: 0.6925 - val_acc: 0.4999\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4614 - val_loss: 0.6924 - val_acc: 0.4792\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4728 - val_loss: 0.6924 - val_acc: 0.4580\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4769 - val_loss: 0.6924 - val_acc: 0.4211\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4629 - val_loss: 0.6924 - val_acc: 0.4955\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4940 - val_loss: 0.6924 - val_acc: 0.4006\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4716 - val_loss: 0.6924 - val_acc: 0.4735\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4836 - val_loss: 0.6924 - val_acc: 0.4581\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4601 - val_loss: 0.6924 - val_acc: 0.4882\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4652 - val_loss: 0.6924 - val_acc: 0.4965\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.6930 - acc: 0.5631\n",
      ". theta fit =  1.024307\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6936 - acc: 0.3717 - val_loss: 0.6923 - val_acc: 0.3539\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3377 - val_loss: 0.6923 - val_acc: 0.3481\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3353 - val_loss: 0.6923 - val_acc: 0.3981\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3403 - val_loss: 0.6924 - val_acc: 0.3284\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3430 - val_loss: 0.6923 - val_acc: 0.3935\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.3428 - val_loss: 0.6923 - val_acc: 0.3151\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3337 - val_loss: 0.6923 - val_acc: 0.3747\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3407 - val_loss: 0.6923 - val_acc: 0.3158\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3367 - val_loss: 0.6923 - val_acc: 0.3991\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3405 - val_loss: 0.6923 - val_acc: 0.3760\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3465 - val_loss: 0.6923 - val_acc: 0.3414\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3443 - val_loss: 0.6923 - val_acc: 0.4038\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.6929 - acc: 0.3478\n",
      ". theta fit =  0.9987231\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6937 - acc: 0.4392 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5005 - val_loss: 0.6925 - val_acc: 0.4999\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4968 - val_loss: 0.6924 - val_acc: 0.4924\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5049 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5135 - val_loss: 0.6924 - val_acc: 0.4432\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4971 - val_loss: 0.6924 - val_acc: 0.4607\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4940 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4970 - val_loss: 0.6924 - val_acc: 0.6002\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5027 - val_loss: 0.6924 - val_acc: 0.5011\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5085 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4925 - val_loss: 0.6924 - val_acc: 0.5057\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5037 - val_loss: 0.6924 - val_acc: 0.5157\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5238 - val_loss: 0.6924 - val_acc: 0.4919\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5176 - val_loss: 0.6925 - val_acc: 0.4999\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4997 - val_loss: 0.6924 - val_acc: 0.4624\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4902 - val_loss: 0.6924 - val_acc: 0.4416\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4901 - val_loss: 0.6924 - val_acc: 0.4946\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5112 - val_loss: 0.6924 - val_acc: 0.4729\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4965 - val_loss: 0.6924 - val_acc: 0.5003\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5046 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4857 - val_loss: 0.6924 - val_acc: 0.5048\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5160 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.6930 - acc: 0.5157\n",
      ". theta fit =  0.97311115\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6936 - acc: 0.6388 - val_loss: 0.6923 - val_acc: 0.6917\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6721 - val_loss: 0.6923 - val_acc: 0.6830\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6795 - val_loss: 0.6923 - val_acc: 0.6861\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6808 - val_loss: 0.6923 - val_acc: 0.6766\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6672 - val_loss: 0.6923 - val_acc: 0.6873\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6628 - val_loss: 0.6923 - val_acc: 0.6891\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6772 - val_loss: 0.6923 - val_acc: 0.6827\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6631 - val_loss: 0.6923 - val_acc: 0.6801\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6701 - val_loss: 0.6924 - val_acc: 0.6913\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6797 - val_loss: 0.6924 - val_acc: 0.6656\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6772 - val_loss: 0.6923 - val_acc: 0.6767\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6698 - val_loss: 0.6923 - val_acc: 0.6867\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6732 - val_loss: 0.6923 - val_acc: 0.6741\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6682 - val_loss: 0.6923 - val_acc: 0.6799\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6748 - val_loss: 0.6924 - val_acc: 0.6314\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6712 - val_loss: 0.6923 - val_acc: 0.6646\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6731 - val_loss: 0.6924 - val_acc: 0.6601\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6707 - val_loss: 0.6923 - val_acc: 0.6882\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6736 - val_loss: 0.6923 - val_acc: 0.6680\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6755 - val_loss: 0.6923 - val_acc: 0.6879\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6751 - val_loss: 0.6923 - val_acc: 0.6812\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6765 - val_loss: 0.6923 - val_acc: 0.6789\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6670 - val_loss: 0.6924 - val_acc: 0.6642\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6776 - val_loss: 0.6923 - val_acc: 0.6848\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6773 - val_loss: 0.6923 - val_acc: 0.6805\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6722 - val_loss: 0.6923 - val_acc: 0.6796\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6707 - val_loss: 0.6923 - val_acc: 0.6679\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6751 - val_loss: 0.6923 - val_acc: 0.6878\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6689 - val_loss: 0.6923 - val_acc: 0.6808\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6741 - val_loss: 0.6923 - val_acc: 0.6671\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6724 - val_loss: 0.6924 - val_acc: 0.5366\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6731 - val_loss: 0.6923 - val_acc: 0.6889\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6734 - val_loss: 0.6923 - val_acc: 0.6784\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6776 - val_loss: 0.6923 - val_acc: 0.6784\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.6929 - acc: 0.6850\n",
      ". theta fit =  0.9992645\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6937 - acc: 0.6170 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5150 - val_loss: 0.6924 - val_acc: 0.5406\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4987 - val_loss: 0.6924 - val_acc: 0.5077\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5263 - val_loss: 0.6924 - val_acc: 0.4116\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4788 - val_loss: 0.6924 - val_acc: 0.4747\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4892 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4790 - val_loss: 0.6924 - val_acc: 0.5843\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5289 - val_loss: 0.6924 - val_acc: 0.5008\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4842 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5094 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5314 - val_loss: 0.6924 - val_acc: 0.4432\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4565 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4827 - val_loss: 0.6924 - val_acc: 0.5373\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4816 - val_loss: 0.6925 - val_acc: 0.4999\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5053 - val_loss: 0.6924 - val_acc: 0.4350\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4883 - val_loss: 0.6924 - val_acc: 0.4489\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4728 - val_loss: 0.6924 - val_acc: 0.5569\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6930 - acc: 0.5842\n",
      ". theta fit =  1.0246952\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6936 - acc: 0.3694 - val_loss: 0.6923 - val_acc: 0.3844\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3335 - val_loss: 0.6923 - val_acc: 0.3155\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3253 - val_loss: 0.6923 - val_acc: 0.3288\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3292 - val_loss: 0.6923 - val_acc: 0.3117\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3305 - val_loss: 0.6923 - val_acc: 0.3138\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3281 - val_loss: 0.6923 - val_acc: 0.3325\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3394 - val_loss: 0.6923 - val_acc: 0.3206\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3376 - val_loss: 0.6923 - val_acc: 0.3260\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3331 - val_loss: 0.6923 - val_acc: 0.3268\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3404 - val_loss: 0.6923 - val_acc: 0.3218\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3411 - val_loss: 0.6924 - val_acc: 0.3289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3364 - val_loss: 0.6923 - val_acc: 0.3304\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3406 - val_loss: 0.6923 - val_acc: 0.3441\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.3285\n",
      ". theta fit =  0.9979373\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6937 - acc: 0.4155 - val_loss: 0.6924 - val_acc: 0.4363\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4915 - val_loss: 0.6924 - val_acc: 0.5388\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5255 - val_loss: 0.6924 - val_acc: 0.5439\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5068 - val_loss: 0.6924 - val_acc: 0.5334\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5180 - val_loss: 0.6924 - val_acc: 0.5028\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5363 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5237 - val_loss: 0.6924 - val_acc: 0.4913\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5118 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5126 - val_loss: 0.6924 - val_acc: 0.5524\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.5437 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5031 - val_loss: 0.6924 - val_acc: 0.5483\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5245 - val_loss: 0.6924 - val_acc: 0.4672\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5054 - val_loss: 0.6924 - val_acc: 0.4441\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5212 - val_loss: 0.6924 - val_acc: 0.4600\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6937 - acc: 0.4897 - val_loss: 0.6924 - val_acc: 0.5614\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5427 - val_loss: 0.6924 - val_acc: 0.4100\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4997 - val_loss: 0.6924 - val_acc: 0.5090\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5068 - val_loss: 0.6924 - val_acc: 0.5824\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5357 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5109 - val_loss: 0.6924 - val_acc: 0.5571\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5323 - val_loss: 0.6924 - val_acc: 0.4875\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5062 - val_loss: 0.6924 - val_acc: 0.4625\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5211 - val_loss: 0.6924 - val_acc: 0.4829\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4921 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5460 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6930 - acc: 0.5619\n",
      ". theta fit =  0.97129625\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6936 - acc: 0.6433 - val_loss: 0.6924 - val_acc: 0.6884\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.6845 - val_loss: 0.6923 - val_acc: 0.6808\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6831 - val_loss: 0.6923 - val_acc: 0.6874\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6935 - acc: 0.6803 - val_loss: 0.6923 - val_acc: 0.6828\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6797 - val_loss: 0.6923 - val_acc: 0.6704\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6810 - val_loss: 0.6923 - val_acc: 0.6786\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6780 - val_loss: 0.6923 - val_acc: 0.6782\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6762 - val_loss: 0.6923 - val_acc: 0.6735\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6737 - val_loss: 0.6923 - val_acc: 0.6864\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6782 - val_loss: 0.6923 - val_acc: 0.6813\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6744 - val_loss: 0.6923 - val_acc: 0.6783\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6785 - val_loss: 0.6923 - val_acc: 0.6664\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6778 - val_loss: 0.6924 - val_acc: 0.6619\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6779 - val_loss: 0.6923 - val_acc: 0.6831\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6764 - val_loss: 0.6923 - val_acc: 0.6710\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6781 - val_loss: 0.6924 - val_acc: 0.6585\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6767 - val_loss: 0.6923 - val_acc: 0.6748\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6795 - val_loss: 0.6923 - val_acc: 0.6744\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6774 - val_loss: 0.6923 - val_acc: 0.6878\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6767 - val_loss: 0.6923 - val_acc: 0.6836\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6777 - val_loss: 0.6923 - val_acc: 0.6792\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6779 - val_loss: 0.6923 - val_acc: 0.6832\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6769 - val_loss: 0.6923 - val_acc: 0.6856\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6770 - val_loss: 0.6923 - val_acc: 0.6871\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6683 - val_loss: 0.6923 - val_acc: 0.6727\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6755 - val_loss: 0.6923 - val_acc: 0.6901\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6782 - val_loss: 0.6923 - val_acc: 0.6824\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6935 - acc: 0.6770 - val_loss: 0.6923 - val_acc: 0.6831\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6774 - val_loss: 0.6923 - val_acc: 0.6686\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6766 - val_loss: 0.6923 - val_acc: 0.6685\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6692 - val_loss: 0.6923 - val_acc: 0.6875\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6759 - val_loss: 0.6923 - val_acc: 0.6707\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6754 - val_loss: 0.6924 - val_acc: 0.6564\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6768 - val_loss: 0.6923 - val_acc: 0.6664\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6798 - val_loss: 0.6924 - val_acc: 0.6511\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6704 - val_loss: 0.6923 - val_acc: 0.6758\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6750 - val_loss: 0.6923 - val_acc: 0.6761\n",
      "Epoch 38/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6935 - acc: 0.6765 - val_loss: 0.6923 - val_acc: 0.6751\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.6834\n",
      ". theta fit =  0.9986893\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6937 - acc: 0.6276 - val_loss: 0.6924 - val_acc: 0.5343\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5401 - val_loss: 0.6924 - val_acc: 0.5744\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5273 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4996 - val_loss: 0.6924 - val_acc: 0.5474\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5207 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5151 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5352 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4938 - val_loss: 0.6924 - val_acc: 0.6023\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5237 - val_loss: 0.6924 - val_acc: 0.4249\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5060 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5151 - val_loss: 0.6924 - val_acc: 0.5722\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5056 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5352 - val_loss: 0.6924 - val_acc: 0.6155\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5157 - val_loss: 0.6924 - val_acc: 0.5552\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5276 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4984 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5676 - val_loss: 0.6924 - val_acc: 0.5035\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5392 - val_loss: 0.6924 - val_acc: 0.4453\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5207 - val_loss: 0.6924 - val_acc: 0.4214\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5094 - val_loss: 0.6924 - val_acc: 0.4666\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5181 - val_loss: 0.6924 - val_acc: 0.4508\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6930 - acc: 0.5722\n",
      ". theta fit =  1.0252863\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6936 - acc: 0.3923 - val_loss: 0.6923 - val_acc: 0.3234\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3204 - val_loss: 0.6923 - val_acc: 0.3133\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.3189 - val_loss: 0.6923 - val_acc: 0.3321\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3190 - val_loss: 0.6923 - val_acc: 0.3177\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3195 - val_loss: 0.6923 - val_acc: 0.3165\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3225 - val_loss: 0.6923 - val_acc: 0.3140\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3286 - val_loss: 0.6923 - val_acc: 0.3127\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3280 - val_loss: 0.6923 - val_acc: 0.3308\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3320 - val_loss: 0.6923 - val_acc: 0.3430\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3326 - val_loss: 0.6923 - val_acc: 0.3203\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3351 - val_loss: 0.6923 - val_acc: 0.3570\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3398 - val_loss: 0.6923 - val_acc: 0.3188\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3384 - val_loss: 0.6923 - val_acc: 0.3610\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.3366 - val_loss: 0.6923 - val_acc: 0.3678\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3395 - val_loss: 0.6923 - val_acc: 0.3545\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3405 - val_loss: 0.6923 - val_acc: 0.3773\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3432 - val_loss: 0.6923 - val_acc: 0.3641\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.3451 - val_loss: 0.6923 - val_acc: 0.3650\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.3304\n",
      ". theta fit =  0.99724007\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6937 - acc: 0.4449 - val_loss: 0.6924 - val_acc: 0.4098\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4950 - val_loss: 0.6924 - val_acc: 0.5425\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5077 - val_loss: 0.6924 - val_acc: 0.5332\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5087 - val_loss: 0.6924 - val_acc: 0.5710\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5214 - val_loss: 0.6924 - val_acc: 0.5101\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5146 - val_loss: 0.6924 - val_acc: 0.5228\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5105 - val_loss: 0.6924 - val_acc: 0.5118\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5409 - val_loss: 0.6924 - val_acc: 0.5178\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5221 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5135 - val_loss: 0.6924 - val_acc: 0.5461\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5181 - val_loss: 0.6924 - val_acc: 0.5635\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5442 - val_loss: 0.6924 - val_acc: 0.5276\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5423 - val_loss: 0.6924 - val_acc: 0.5075\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5186 - val_loss: 0.6924 - val_acc: 0.5244\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5542 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5313 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5225 - val_loss: 0.6924 - val_acc: 0.5266\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5258 - val_loss: 0.6924 - val_acc: 0.5573\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5385 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5124 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5372 - val_loss: 0.6924 - val_acc: 0.5421\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5673 - val_loss: 0.6924 - val_acc: 0.5076\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5247 - val_loss: 0.6924 - val_acc: 0.5796\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5437 - val_loss: 0.6924 - val_acc: 0.5471\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5372 - val_loss: 0.6924 - val_acc: 0.5009\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5254 - val_loss: 0.6924 - val_acc: 0.5692\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5324 - val_loss: 0.6925 - val_acc: 0.4999\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5601 - val_loss: 0.6924 - val_acc: 0.5084\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5205 - val_loss: 0.6924 - val_acc: 0.5312\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5403 - val_loss: 0.6924 - val_acc: 0.4860\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5060 - val_loss: 0.6924 - val_acc: 0.5030\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5169 - val_loss: 0.6924 - val_acc: 0.5249\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5268 - val_loss: 0.6924 - val_acc: 0.5083\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6930 - acc: 0.5796\n",
      ". theta fit =  0.9999665\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6936 - acc: 0.5018 - val_loss: 0.6924 - val_acc: 0.5133\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4899 - val_loss: 0.6924 - val_acc: 0.4770\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4668 - val_loss: 0.6924 - val_acc: 0.5355\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5249 - val_loss: 0.6924 - val_acc: 0.4288\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4574 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4872 - val_loss: 0.6924 - val_acc: 0.4674\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4719 - val_loss: 0.6924 - val_acc: 0.4908\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6924 - val_acc: 0.4573\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4794 - val_loss: 0.6924 - val_acc: 0.4790\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5119 - val_loss: 0.6924 - val_acc: 0.4497\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4641 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.4814 - val_loss: 0.6924 - val_acc: 0.4423\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6930 - acc: 0.4774\n",
      ". theta fit =  0.99711645\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6936 - acc: 0.5038 - val_loss: 0.6924 - val_acc: 0.5569\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5461 - val_loss: 0.6924 - val_acc: 0.5189\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5313 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5588 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5057 - val_loss: 0.6924 - val_acc: 0.5998\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5579 - val_loss: 0.6924 - val_acc: 0.5351\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5421 - val_loss: 0.6924 - val_acc: 0.5324\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5500 - val_loss: 0.6924 - val_acc: 0.4910\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5027 - val_loss: 0.6924 - val_acc: 0.5752\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5398 - val_loss: 0.6924 - val_acc: 0.5573\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5481 - val_loss: 0.6924 - val_acc: 0.5242\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5249 - val_loss: 0.6924 - val_acc: 0.5929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5817 - val_loss: 0.6924 - val_acc: 0.4955\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5398 - val_loss: 0.6924 - val_acc: 0.5404\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5179 - val_loss: 0.6924 - val_acc: 0.5470\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5440 - val_loss: 0.6924 - val_acc: 0.5842\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5474 - val_loss: 0.6924 - val_acc: 0.5237\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6930 - acc: 0.5324\n",
      ". theta fit =  0.9999463\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6936 - acc: 0.5183 - val_loss: 0.6924 - val_acc: 0.5074\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4590 - val_loss: 0.6924 - val_acc: 0.4747\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4667 - val_loss: 0.6924 - val_acc: 0.4944\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4947 - val_loss: 0.6924 - val_acc: 0.4798\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4690 - val_loss: 0.6924 - val_acc: 0.4685\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4887 - val_loss: 0.6924 - val_acc: 0.4577\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4854 - val_loss: 0.6924 - val_acc: 0.4465\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4604 - val_loss: 0.6924 - val_acc: 0.5107\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4808 - val_loss: 0.6924 - val_acc: 0.5107\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4955 - val_loss: 0.6924 - val_acc: 0.4511\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4940 - val_loss: 0.6924 - val_acc: 0.4338\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4505 - val_loss: 0.6924 - val_acc: 0.5233\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4870 - val_loss: 0.6924 - val_acc: 0.5158\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4804 - val_loss: 0.6924 - val_acc: 0.5140\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5086 - val_loss: 0.6924 - val_acc: 0.4499\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4865 - val_loss: 0.6924 - val_acc: 0.4771\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4569 - val_loss: 0.6924 - val_acc: 0.4542\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6930 - acc: 0.4470\n",
      ". theta fit =  0.9970171\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6936 - acc: 0.4842 - val_loss: 0.6924 - val_acc: 0.5854\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5718 - val_loss: 0.6924 - val_acc: 0.5402\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5166 - val_loss: 0.6924 - val_acc: 0.5718\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5389 - val_loss: 0.6924 - val_acc: 0.5192\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5435 - val_loss: 0.6924 - val_acc: 0.5215\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5415 - val_loss: 0.6924 - val_acc: 0.5342\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5461 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5460 - val_loss: 0.6924 - val_acc: 0.5154\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5171 - val_loss: 0.6924 - val_acc: 0.5748\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5383 - val_loss: 0.6924 - val_acc: 0.5284\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5468 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5710 - val_loss: 0.6924 - val_acc: 0.5578\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6930 - acc: 0.5404\n",
      ". theta fit =  0.99407107\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6936 - acc: 0.5356 - val_loss: 0.6924 - val_acc: 0.5867\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6018 - val_loss: 0.6924 - val_acc: 0.5196\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6239 - val_loss: 0.6924 - val_acc: 0.5646\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5986 - val_loss: 0.6924 - val_acc: 0.5862\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5944 - val_loss: 0.6924 - val_acc: 0.5834\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5999 - val_loss: 0.6924 - val_acc: 0.6913\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6072 - val_loss: 0.6924 - val_acc: 0.5554\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5972 - val_loss: 0.6924 - val_acc: 0.6899\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6269 - val_loss: 0.6924 - val_acc: 0.5025\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5944 - val_loss: 0.6924 - val_acc: 0.5472\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5837 - val_loss: 0.6924 - val_acc: 0.5867\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5951 - val_loss: 0.6924 - val_acc: 0.6891\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6146 - val_loss: 0.6924 - val_acc: 0.5911\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5999 - val_loss: 0.6924 - val_acc: 0.5385\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6114 - val_loss: 0.6924 - val_acc: 0.5858\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6162 - val_loss: 0.6924 - val_acc: 0.5515\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5968 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5931 - val_loss: 0.6924 - val_acc: 0.6088\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6930 - acc: 0.6900\n",
      ". theta fit =  0.9970676\n",
      "Iteration:  35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6936 - acc: 0.6056 - val_loss: 0.6924 - val_acc: 0.6900\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6012 - val_loss: 0.6924 - val_acc: 0.6917\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.6221 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5909 - val_loss: 0.6924 - val_acc: 0.5636\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5799 - val_loss: 0.6924 - val_acc: 0.6154\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5689 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5787 - val_loss: 0.6924 - val_acc: 0.5373\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5845 - val_loss: 0.6924 - val_acc: 0.5265\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5818 - val_loss: 0.6924 - val_acc: 0.5160\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5701 - val_loss: 0.6924 - val_acc: 0.6050\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5644 - val_loss: 0.6924 - val_acc: 0.6161\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6930 - acc: 0.6902\n",
      ". theta fit =  1.000096\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6936 - acc: 0.6117 - val_loss: 0.6924 - val_acc: 0.5021\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5885 - val_loss: 0.6924 - val_acc: 0.5267\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5703 - val_loss: 0.6924 - val_acc: 0.6009\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5771 - val_loss: 0.6924 - val_acc: 0.5234\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5637 - val_loss: 0.6924 - val_acc: 0.5522\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5706 - val_loss: 0.6924 - val_acc: 0.6066\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5581 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5405 - val_loss: 0.6924 - val_acc: 0.5817\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5569 - val_loss: 0.6924 - val_acc: 0.5407\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6936 - acc: 0.5518 - val_loss: 0.6924 - val_acc: 0.5821\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5367 - val_loss: 0.6924 - val_acc: 0.5878\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5536 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5300 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5646 - val_loss: 0.6924 - val_acc: 0.5731\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5596 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5558 - val_loss: 0.6924 - val_acc: 0.5020\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5150 - val_loss: 0.6924 - val_acc: 0.5494\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6930 - acc: 0.5000\n",
      ". theta fit =  0.9970835\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6936 - acc: 0.5529 - val_loss: 0.6924 - val_acc: 0.5263\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5630 - val_loss: 0.6924 - val_acc: 0.5996\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5771 - val_loss: 0.6924 - val_acc: 0.4679\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5664 - val_loss: 0.6924 - val_acc: 0.6903\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5475 - val_loss: 0.6924 - val_acc: 0.5791\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5430 - val_loss: 0.6924 - val_acc: 0.5404\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5849 - val_loss: 0.6924 - val_acc: 0.5660\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5566 - val_loss: 0.6924 - val_acc: 0.5890\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5489 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5338 - val_loss: 0.6924 - val_acc: 0.5897\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5472 - val_loss: 0.6924 - val_acc: 0.6026\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5701 - val_loss: 0.6924 - val_acc: 0.5700\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6930 - acc: 0.6000\n",
      ". theta fit =  1.000148\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6936 - acc: 0.5750 - val_loss: 0.6924 - val_acc: 0.6006\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5741 - val_loss: 0.6924 - val_acc: 0.6062\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5493 - val_loss: 0.6924 - val_acc: 0.5543\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5341 - val_loss: 0.6924 - val_acc: 0.6140\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5604 - val_loss: 0.6924 - val_acc: 0.6062\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5684 - val_loss: 0.6924 - val_acc: 0.5182\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5308 - val_loss: 0.6924 - val_acc: 0.5610\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5571 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5051 - val_loss: 0.6924 - val_acc: 0.4749\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5174 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5110 - val_loss: 0.6924 - val_acc: 0.5405\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.6930 - acc: 0.6010\n",
      ". theta fit =  1.0004433\n",
      "Iteration:  39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6936 - acc: 0.5587 - val_loss: 0.6924 - val_acc: 0.5807\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5416 - val_loss: 0.6924 - val_acc: 0.5912\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5540 - val_loss: 0.6924 - val_acc: 0.5446\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5709 - val_loss: 0.6924 - val_acc: 0.5537\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5548 - val_loss: 0.6924 - val_acc: 0.5516\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5573 - val_loss: 0.6924 - val_acc: 0.5648\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5676 - val_loss: 0.6924 - val_acc: 0.5517\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5593 - val_loss: 0.6924 - val_acc: 0.5324\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5582 - val_loss: 0.6924 - val_acc: 0.6084\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5584 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5597 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6930 - acc: 0.5814\n",
      ". theta fit =  1.00014\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6936 - acc: 0.5640 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5494 - val_loss: 0.6924 - val_acc: 0.5966\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5727 - val_loss: 0.6924 - val_acc: 0.5253\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5458 - val_loss: 0.6924 - val_acc: 0.5371\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5653 - val_loss: 0.6924 - val_acc: 0.5295\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5451 - val_loss: 0.6924 - val_acc: 0.5972\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5729 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5600 - val_loss: 0.6924 - val_acc: 0.5636\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5457 - val_loss: 0.6924 - val_acc: 0.5120\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5302 - val_loss: 0.6924 - val_acc: 0.4544\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5241 - val_loss: 0.6924 - val_acc: 0.5441\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5357 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4624 - val_loss: 0.6924 - val_acc: 0.5481\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5159 - val_loss: 0.6924 - val_acc: 0.5126\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4604 - val_loss: 0.6924 - val_acc: 0.4296\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5098 - val_loss: 0.6924 - val_acc: 0.4192\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4869 - val_loss: 0.6924 - val_acc: 0.4715\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5040 - val_loss: 0.6924 - val_acc: 0.4053\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4944 - val_loss: 0.6924 - val_acc: 0.4856\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6930 - acc: 0.5121\n",
      ". theta fit =  0.9998278\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6936 - acc: 0.5386 - val_loss: 0.6924 - val_acc: 0.5045\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4777 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4833 - val_loss: 0.6924 - val_acc: 0.5608\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5482 - val_loss: 0.6924 - val_acc: 0.5446\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5000 - val_loss: 0.6924 - val_acc: 0.6039\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5074 - val_loss: 0.6924 - val_acc: 0.5472\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4846 - val_loss: 0.6924 - val_acc: 0.4981\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4975 - val_loss: 0.6924 - val_acc: 0.5948\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5396 - val_loss: 0.6924 - val_acc: 0.5407\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4585 - val_loss: 0.6924 - val_acc: 0.5993\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5307 - val_loss: 0.6924 - val_acc: 0.5618\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4931 - val_loss: 0.6924 - val_acc: 0.4287\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5022 - val_loss: 0.6924 - val_acc: 0.5828\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5046 - val_loss: 0.6924 - val_acc: 0.4374\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4955 - val_loss: 0.6924 - val_acc: 0.4615\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5243 - val_loss: 0.6924 - val_acc: 0.5482\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.6930 - acc: 0.5476\n",
      ". theta fit =  0.9995068\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6936 - acc: 0.5077 - val_loss: 0.6924 - val_acc: 0.4785\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4916 - val_loss: 0.6924 - val_acc: 0.4888\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5154 - val_loss: 0.6924 - val_acc: 0.4861\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5017 - val_loss: 0.6924 - val_acc: 0.4379\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5072 - val_loss: 0.6924 - val_acc: 0.5351\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4978 - val_loss: 0.6924 - val_acc: 0.5557\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5115 - val_loss: 0.6924 - val_acc: 0.4293\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5002 - val_loss: 0.6924 - val_acc: 0.5671\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5255 - val_loss: 0.6924 - val_acc: 0.5763\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5218 - val_loss: 0.6924 - val_acc: 0.5603\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5156 - val_loss: 0.6924 - val_acc: 0.4386\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4815 - val_loss: 0.6924 - val_acc: 0.5415\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5256 - val_loss: 0.6924 - val_acc: 0.5258\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5084 - val_loss: 0.6924 - val_acc: 0.4616\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5255 - val_loss: 0.6924 - val_acc: 0.4613\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5344 - val_loss: 0.6924 - val_acc: 0.4215\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5008 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6930 - acc: 0.4297\n",
      ". theta fit =  0.9991806\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.6936 - acc: 0.5016 - val_loss: 0.6924 - val_acc: 0.4298\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5308 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4754 - val_loss: 0.6924 - val_acc: 0.4129\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5148 - val_loss: 0.6924 - val_acc: 0.5519\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5039 - val_loss: 0.6924 - val_acc: 0.5514\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5325 - val_loss: 0.6924 - val_acc: 0.5833\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5094 - val_loss: 0.6924 - val_acc: 0.4375\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5349 - val_loss: 0.6924 - val_acc: 0.5619\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5093 - val_loss: 0.6924 - val_acc: 0.5626\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5367 - val_loss: 0.6924 - val_acc: 0.4765\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5148 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5318 - val_loss: 0.6924 - val_acc: 0.5369\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5132 - val_loss: 0.6924 - val_acc: 0.5332\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6930 - acc: 0.4135\n",
      ". theta fit =  0.9988517\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.6936 - acc: 0.5397 - val_loss: 0.6924 - val_acc: 0.4281\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5099 - val_loss: 0.6924 - val_acc: 0.5246\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5226 - val_loss: 0.6924 - val_acc: 0.5627\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5360 - val_loss: 0.6924 - val_acc: 0.5354\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5201 - val_loss: 0.6924 - val_acc: 0.5787\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5422 - val_loss: 0.6924 - val_acc: 0.4689\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.4812 - val_loss: 0.6924 - val_acc: 0.5846\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5767 - val_loss: 0.6924 - val_acc: 0.5362\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5371 - val_loss: 0.6924 - val_acc: 0.4018\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5076 - val_loss: 0.6924 - val_acc: 0.5552\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5196 - val_loss: 0.6924 - val_acc: 0.5395\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5394 - val_loss: 0.6924 - val_acc: 0.5207\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5153 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5132 - val_loss: 0.6924 - val_acc: 0.5125\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5462 - val_loss: 0.6924 - val_acc: 0.4979\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5378\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.6930 - acc: 0.4691\n",
      ". theta fit =  0.9985198\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.6936 - acc: 0.5527 - val_loss: 0.6924 - val_acc: 0.5410\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5206 - val_loss: 0.6924 - val_acc: 0.5190\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5274 - val_loss: 0.6924 - val_acc: 0.5563\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5611 - val_loss: 0.6924 - val_acc: 0.5864\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5379 - val_loss: 0.6924 - val_acc: 0.5440\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5272 - val_loss: 0.6924 - val_acc: 0.5728\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5434 - val_loss: 0.6924 - val_acc: 0.5238\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5274 - val_loss: 0.6924 - val_acc: 0.5823\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5244 - val_loss: 0.6924 - val_acc: 0.5492\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5113 - val_loss: 0.6924 - val_acc: 0.6023\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5204 - val_loss: 0.6924 - val_acc: 0.5269\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6936 - acc: 0.5375 - val_loss: 0.6924 - val_acc: 0.5654\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5208 - val_loss: 0.6924 - val_acc: 0.5500\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5429 - val_loss: 0.6924 - val_acc: 0.4636\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5367 - val_loss: 0.6924 - val_acc: 0.5709\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5296 - val_loss: 0.6924 - val_acc: 0.5372\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5365 - val_loss: 0.6924 - val_acc: 0.5016\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5102 - val_loss: 0.6924 - val_acc: 0.4347\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5181 - val_loss: 0.6924 - val_acc: 0.5529\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5160 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5090 - val_loss: 0.6924 - val_acc: 0.5174\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5217 - val_loss: 0.6924 - val_acc: 0.5356\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5112 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5324 - val_loss: 0.6924 - val_acc: 0.4805\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6930 - acc: 0.4640\n",
      ". theta fit =  0.9981859\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.6936 - acc: 0.5362 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5181 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5401 - val_loss: 0.6924 - val_acc: 0.5346\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5147 - val_loss: 0.6924 - val_acc: 0.5404\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5463 - val_loss: 0.6924 - val_acc: 0.5922\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5503 - val_loss: 0.6924 - val_acc: 0.5086\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5443 - val_loss: 0.6924 - val_acc: 0.5255\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5467 - val_loss: 0.6924 - val_acc: 0.5584\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5238 - val_loss: 0.6924 - val_acc: 0.5944\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5528 - val_loss: 0.6924 - val_acc: 0.5574\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5362 - val_loss: 0.6924 - val_acc: 0.4911\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5090 - val_loss: 0.6924 - val_acc: 0.5346\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5305 - val_loss: 0.6924 - val_acc: 0.5156\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5447 - val_loss: 0.6924 - val_acc: 0.5174\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5225 - val_loss: 0.6924 - val_acc: 0.5835\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5422 - val_loss: 0.6924 - val_acc: 0.5636\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5246 - val_loss: 0.6924 - val_acc: 0.5089\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.6930 - acc: 0.5257\n",
      ". theta fit =  0.99785304\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.6936 - acc: 0.5265 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5368 - val_loss: 0.6924 - val_acc: 0.5382\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5202 - val_loss: 0.6924 - val_acc: 0.5685\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5414 - val_loss: 0.6924 - val_acc: 0.4473\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5559 - val_loss: 0.6924 - val_acc: 0.5995\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5587 - val_loss: 0.6924 - val_acc: 0.5319\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5230 - val_loss: 0.6924 - val_acc: 0.5177\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5709\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5557 - val_loss: 0.6924 - val_acc: 0.5068\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5327 - val_loss: 0.6924 - val_acc: 0.5856\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5374 - val_loss: 0.6924 - val_acc: 0.5718\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5581 - val_loss: 0.6924 - val_acc: 0.5465\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6930 - acc: 0.5388\n",
      ". theta fit =  0.9975125\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.6936 - acc: 0.5328 - val_loss: 0.6924 - val_acc: 0.5836\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5553 - val_loss: 0.6924 - val_acc: 0.5600\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 0.5369\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5249 - val_loss: 0.6924 - val_acc: 0.5139\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5341 - val_loss: 0.6924 - val_acc: 0.5943\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5326 - val_loss: 0.6924 - val_acc: 0.5143\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5195 - val_loss: 0.6924 - val_acc: 0.5396\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5319 - val_loss: 0.6924 - val_acc: 0.5311\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5676 - val_loss: 0.6924 - val_acc: 0.5476\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5288 - val_loss: 0.6924 - val_acc: 0.5762\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5485 - val_loss: 0.6924 - val_acc: 0.5627\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5575 - val_loss: 0.6924 - val_acc: 0.5699\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5225 - val_loss: 0.6924 - val_acc: 0.6067\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5776 - val_loss: 0.6924 - val_acc: 0.5983\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5791 - val_loss: 0.6924 - val_acc: 0.6004\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5764 - val_loss: 0.6924 - val_acc: 0.5519\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5596 - val_loss: 0.6924 - val_acc: 0.5991\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5467 - val_loss: 0.6924 - val_acc: 0.4999\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6930 - acc: 0.5316\n",
      ". theta fit =  0.9971774\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.6936 - acc: 0.5170 - val_loss: 0.6924 - val_acc: 0.5921\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5409 - val_loss: 0.6924 - val_acc: 0.5778\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5288 - val_loss: 0.6924 - val_acc: 0.5652\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5428 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5445 - val_loss: 0.6924 - val_acc: 0.5940\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5515 - val_loss: 0.6924 - val_acc: 0.5812\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5541 - val_loss: 0.6924 - val_acc: 0.5630\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5579 - val_loss: 0.6924 - val_acc: 0.5493\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5611 - val_loss: 0.6924 - val_acc: 0.5790\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5399 - val_loss: 0.6924 - val_acc: 0.5482\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5598 - val_loss: 0.6924 - val_acc: 0.5844\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5419 - val_loss: 0.6924 - val_acc: 0.5394\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5351 - val_loss: 0.6924 - val_acc: 0.5785\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5342 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5558 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5254 - val_loss: 0.6924 - val_acc: 0.5688\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5480 - val_loss: 0.6924 - val_acc: 0.5752\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5601 - val_loss: 0.6924 - val_acc: 0.5771\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5370 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5524 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5312 - val_loss: 0.6924 - val_acc: 0.5040\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5435 - val_loss: 0.6924 - val_acc: 0.5671\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5514 - val_loss: 0.6924 - val_acc: 0.5001\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5381 - val_loss: 0.6924 - val_acc: 0.5736\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5500 - val_loss: 0.6924 - val_acc: 0.5657\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5541 - val_loss: 0.6924 - val_acc: 0.5767\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5406 - val_loss: 0.6924 - val_acc: 0.5666\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5526 - val_loss: 0.6924 - val_acc: 0.5307\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5475 - val_loss: 0.6924 - val_acc: 0.5081\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5477 - val_loss: 0.6924 - val_acc: 0.5227\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5278 - val_loss: 0.6924 - val_acc: 0.5753\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5506 - val_loss: 0.6924 - val_acc: 0.5303\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6936 - acc: 0.5485 - val_loss: 0.6924 - val_acc: 0.5605\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.6930 - acc: 0.5000\n",
      ". theta fit =  0.99683917\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration)\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()\n",
    "\n",
    "    # regular optimizer and batch size\n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(),\n",
    "                      loss=my_loss_wrapper_fit(myinputs_fit,\n",
    "                                               1,\n",
    "                                               reweight_analytically=True,\n",
    "                                               MSE_loss=False),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train),\n",
    "                  y_train,\n",
    "                  epochs=100,\n",
    "                  batch_size=1000,\n",
    "                  validation_data=(np.array(X_test), y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = True\n",
    "\n",
    "    # special optimizer and batch size = 2*N\n",
    "    model_fit.compile(optimizer=optimizer,\n",
    "                      loss=my_loss_wrapper_fit(myinputs_fit,\n",
    "                                               -1,\n",
    "                                               reweight_analytically=True,\n",
    "                                               MSE_loss=False),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta),\n",
    "                  Y_train_theta,\n",
    "                  epochs=1,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    #     print(\"index_refine\", index_refine)\n",
    "    #     print(\"extrema\", extrema)\n",
    "\n",
    "    #     if (len(extrema) == 0\n",
    "    #         ):  # If none are found, keep fitting (catching index error)\n",
    "    #         pass\n",
    "    if (len(extrema) >= 6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T04:44:39.359024Z",
     "start_time": "2020-06-08T04:44:39.080414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(theta1_param, 0, len(fit_vals), label='Truth')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"GaussianAltFit-1D (Analytical Reweight)\\nN = {:.0e}, Iterations = {:.0f}\".format(\n",
    "    N, len(fit_vals)))\n",
    "# plt.savefig(\"GaussianAltFit-1D (Analytical Reweight)\\nN = {:.0e}, Iterations = {:.0f}.png\".format(\n",
    "#     N, iterations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T04:44:39.670687Z",
     "start_time": "2020-06-08T04:44:39.361569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "fit_vals = np.array(fit_vals)\n",
    "plt.title(\n",
    "    \"GaussianAltFit-1D (Analytical Reweight) Zoomed:\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\".\n",
    "    format(N, index_refine[1], len(fit_vals)))\n",
    "plt.plot(np.arange(index_refine[1], len(fit_vals)),\n",
    "         fit_vals[index_refine[1]:],\n",
    "         label='Model $\\mu$ Fit')\n",
    "plt.hlines(theta1_param, index_refine[1], len(fit_vals), label='$\\mu_{Truth}$')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-1D (Analytical Reweight) Zoomed:\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\".\n",
    "#     format(N, index_refine[1], iterations))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
