{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__) #1.15.0\n",
    "print(keras.__version__) #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{(1-f(x_{T,i},\\theta'))}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it a $\\mu$ value in addition to $X_i\\sim\\mathcal{N}(\\mu, 1)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = 10**6\n",
    "mu_min = -2\n",
    "mu_max = 2\n",
    "mu_values = np.random.uniform(mu_min, mu_max, n_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with this $\\mu$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = [(np.random.normal(0, 1), mu) for mu in mu_values] # Note the zero in normal(0, 1) \n",
    "X1 = [(np.random.normal(mu, 1), mu) for mu in mu_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu$ values uniformly sampled in the specified range [mu_min, mu_max].\n",
    "\n",
    "For every value of $\\mu$ in mu_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,1)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,1)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "X = np.concatenate((X0, X1))\n",
    "Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((2,))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs = inputs, outputs = outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1600000 samples, validate on 400000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1600000/1600000 [==============================] - 4s 2us/step - loss: 0.5782 - val_loss: 0.5668\n",
      "Epoch 2/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5629 - val_loss: 0.5648\n",
      "Epoch 3/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5623 - val_loss: 0.5646\n",
      "Epoch 4/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5622 - val_loss: 0.5645\n",
      "Epoch 5/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5621 - val_loss: 0.5645\n",
      "Epoch 6/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5621 - val_loss: 0.5643\n",
      "Epoch 7/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5621 - val_loss: 0.5645\n",
      "Epoch 8/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5621 - val_loss: 0.5648\n",
      "Epoch 9/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5621 - val_loss: 0.5645\n",
      "Epoch 10/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5621 - val_loss: 0.5645\n",
      "Epoch 11/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5646\n",
      "Epoch 12/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5644\n",
      "Epoch 13/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5643\n",
      "Epoch 14/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5621 - val_loss: 0.5645\n",
      "Epoch 15/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5644\n",
      "Epoch 16/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5644\n",
      "Epoch 17/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5644\n",
      "Epoch 18/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5621 - val_loss: 0.5643\n",
      "Epoch 19/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5619 - val_loss: 0.5643\n",
      "Epoch 20/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5645\n",
      "Epoch 21/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5643\n",
      "Epoch 22/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5643\n",
      "Epoch 23/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5645\n",
      "Epoch 24/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5643\n",
      "Epoch 25/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5643\n",
      "Epoch 26/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5645\n",
      "Epoch 27/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5643\n",
      "Epoch 28/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5644\n",
      "Epoch 29/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5619 - val_loss: 0.5644\n",
      "Epoch 30/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5644\n",
      "Epoch 31/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5647\n",
      "Epoch 32/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5645\n",
      "Epoch 33/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5645\n",
      "Epoch 34/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5620 - val_loss: 0.5644\n",
      "Epoch 35/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5619 - val_loss: 0.5643\n",
      "Epoch 36/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5619 - val_loss: 0.5643\n",
      "Epoch 37/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5619 - val_loss: 0.5644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f53307cdb90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience = 10,\n",
    "                              restore_best_weights=True)\n",
    "dctr_model.fit(X_train, Y_train, \n",
    "          epochs=200, \n",
    "          batch_size = 10000,\n",
    "          validation_data = (X_test, Y_test),\n",
    "          callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"1d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"1d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply detector effects, each event $x_{T,i}$in the sample is smeared by shifting by $Z_{i}$ from $Z = \\mathcal{N}(0,\\epsilon)$,where $\\epsilon$ represents the smearing. Thus: $x_{D,i} = x_{T,i} + Z_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "theta0_param = 0 #this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = 1 #this is the data (the target)\n",
    "sigma = 1. #Gaussian width\n",
    "epsilon =sigma/2 #Smearing width\n",
    "\n",
    "\n",
    "theta0_T = np.random.normal(theta0_param,sigma,N) #Truth Level Data\n",
    "theta0_D = np.array([(x + np.random.normal(0, epsilon)) for x in theta0_T]) #Detector smearing\n",
    "theta0 = np.stack([theta0_T, theta0_D], axis = 1)\n",
    "\n",
    "\n",
    "theta1_T = np.random.normal(theta1_param,sigma,N)\n",
    "theta1_D = np.array([(x + np.random.normal(0, epsilon)) for x in theta1_T]) #Detector smearing\n",
    "theta1 = np.stack([theta1_T, theta0_D], axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-5,5,31)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"1D Gaussian: Simulation\\n($\\mu$ = {:.2f}) w detector effects\".format(theta0_param))\n",
    "plt.hist(theta0_T, bins = bins, alpha = 0.5, label = 'Truth')\n",
    "plt.hist(theta0_D, bins = bins, alpha = 0.5, label = 'Detector')\n",
    "plt.legend()\n",
    "#plt.savefig(\"1D Gaussian: Simulation ($\\mu$ = {:.2f}) w detector effects.png\".format(theta0_param))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"1D Gaussian: Data\\n($\\mu$ = {:.2f}) w detector effects\".format(theta1_param))\n",
    "plt.hist(theta1_T, bins = bins, alpha = 0.5, label = 'Truth')\n",
    "plt.hist(theta1_D, bins = bins, alpha = 0.5, label = 'Detector')\n",
    "plt.legend()\n",
    "#plt.savefig(\"1D Gaussian: Data ($\\mu$ = {:.2f}) w detector effects.png\".format(theta1_param))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Erasing' Truth level for data, we can't actually observe this\n",
    "theta1 = np.stack([np.zeros_like(theta0_D), theta0_D], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0_D,theta1_D])\n",
    "y_true = np.concatenate([labels0,labels1]) \n",
    "# 'hiding' truth level for simulation in model output (used in reweighting)\n",
    "truth_level = np.concatenate([theta0_T, theta1_T])\n",
    "yvals = np.stack([y_true, truth_level], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals, yvals, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Model\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njson_file = open(\\'dctr_model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\ndctr_model = keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\ndctr_model.load_weights(\"dctr_model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr_model(d)\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights\n",
    "\n",
    "def analytical_reweight(d): #from analytical formula for normal distributions\n",
    "    events = d[:,0]\n",
    "    param = d[:,1]\n",
    "    weights = K.exp(-(0.5*(events-param)**2)+(0.5*(events-0.0)**2))\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 500\n",
    "\n",
    "def my_loss_wrapper(val=0):\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        y_true = tf.gather(y_true, np.arange(batch_size)) \n",
    "        y_labels = tf.gather(y_true, [0], axis = 1) #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis = 1) # sim truth for reweighting\n",
    "\n",
    "        theta = 0. #starting value\n",
    "    \n",
    "        #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "        concat_input_and_params = K.ones(shape = x_T.shape, dtype=tf.float32)*val\n",
    "        #combining and reshaping into correct format:\n",
    "        data = K.concatenate((x_T, concat_input_and_params), axis=-1)\n",
    "        \n",
    "        w = reweight(data) # NN reweight\n",
    "        \n",
    "        # w = analytical_reweight(data) #functional analytical reweight\n",
    "    \n",
    "        # w = K.exp(-(0.5*(x-val)**2)+(0.5*(x-theta)**2)) #direct analytical reweight\n",
    "        \n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_labels*(y_labels - y_pred)**2+(w)*(1.-y_labels)*(y_labels - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('testing theta = :', 0.0)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2087 - acc: 0.3362 - val_loss: 0.2085 - val_acc: 0.3362\n",
      "\n",
      "('testing theta = :', 0.05)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2121 - acc: 0.3362 - val_loss: 0.2119 - val_acc: 0.3362\n",
      "\n",
      "('testing theta = :', 0.1)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2155 - acc: 0.3362 - val_loss: 0.2154 - val_acc: 0.3362\n",
      "\n",
      "('testing theta = :', 0.15000000000000002)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2189 - acc: 0.3361 - val_loss: 0.2190 - val_acc: 0.3359\n",
      "\n",
      "('testing theta = :', 0.2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2222 - acc: 0.3361 - val_loss: 0.2221 - val_acc: 0.3359\n",
      "\n",
      "('testing theta = :', 0.25)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2254 - acc: 0.3359 - val_loss: 0.2253 - val_acc: 0.3357\n",
      "\n",
      "('testing theta = :', 0.30000000000000004)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2284 - acc: 0.3357 - val_loss: 0.2283 - val_acc: 0.3356\n",
      "\n",
      "('testing theta = :', 0.35000000000000003)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2313 - acc: 0.3355 - val_loss: 0.2312 - val_acc: 0.3355\n",
      "\n",
      "('testing theta = :', 0.4)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2340 - acc: 0.3351 - val_loss: 0.2339 - val_acc: 0.3341\n",
      "\n",
      "('testing theta = :', 0.45)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2365 - acc: 0.3346 - val_loss: 0.2368 - val_acc: 0.3321\n",
      "\n",
      "('testing theta = :', 0.5)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2388 - acc: 0.3342 - val_loss: 0.2387 - val_acc: 0.3343\n",
      "\n",
      "('testing theta = :', 0.55)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2408 - acc: 0.3338 - val_loss: 0.2408 - val_acc: 0.3355\n",
      "\n",
      "('testing theta = :', 0.6000000000000001)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2426 - acc: 0.3336 - val_loss: 0.2426 - val_acc: 0.3345\n",
      "\n",
      "('testing theta = :', 0.65)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2441 - acc: 0.3330 - val_loss: 0.2441 - val_acc: 0.3329\n",
      "\n",
      "('testing theta = :', 0.7000000000000001)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2455 - acc: 0.3328 - val_loss: 0.2454 - val_acc: 0.3339\n",
      "\n",
      "('testing theta = :', 0.75)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2467 - acc: 0.3325 - val_loss: 0.2466 - val_acc: 0.3340\n",
      "\n",
      "('testing theta = :', 0.8)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2476 - acc: 0.3317 - val_loss: 0.2476 - val_acc: 0.3332\n",
      "\n",
      "('testing theta = :', 0.8500000000000001)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2484 - acc: 0.3326 - val_loss: 0.2483 - val_acc: 0.3311\n",
      "\n",
      "('testing theta = :', 0.9)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2488 - acc: 0.3313 - val_loss: 0.2488 - val_acc: 0.3348\n",
      "\n",
      "('testing theta = :', 0.9500000000000001)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2491 - acc: 0.3319 - val_loss: 0.2490 - val_acc: 0.3282\n",
      "\n",
      "('testing theta = :', 1.0)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2491 - acc: 0.2875 - val_loss: 0.2491 - val_acc: 0.2499\n",
      "\n",
      "('testing theta = :', 1.05)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2491 - acc: 0.1863 - val_loss: 0.2490 - val_acc: 0.1765\n",
      "\n",
      "('testing theta = :', 1.1)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2488 - acc: 0.1785 - val_loss: 0.2487 - val_acc: 0.1801\n",
      "\n",
      "('testing theta = :', 1.1500000000000001)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2483 - acc: 0.1763 - val_loss: 0.2482 - val_acc: 0.1733\n",
      "\n",
      "('testing theta = :', 1.2000000000000002)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2475 - acc: 0.1766 - val_loss: 0.2475 - val_acc: 0.1818\n",
      "\n",
      "('testing theta = :', 1.25)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2465 - acc: 0.1773 - val_loss: 0.2464 - val_acc: 0.1771\n",
      "\n",
      "('testing theta = :', 1.3)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2451 - acc: 0.1773 - val_loss: 0.2450 - val_acc: 0.1805\n",
      "\n",
      "('testing theta = :', 1.35)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2434 - acc: 0.1797 - val_loss: 0.2434 - val_acc: 0.1815\n",
      "\n",
      "('testing theta = :', 1.4000000000000001)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2414 - acc: 0.1796 - val_loss: 0.2414 - val_acc: 0.1819\n",
      "\n",
      "('testing theta = :', 1.4500000000000002)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2392 - acc: 0.1805 - val_loss: 0.2392 - val_acc: 0.1805\n",
      "\n",
      "('testing theta = :', 1.5)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2369 - acc: 0.1813 - val_loss: 0.2368 - val_acc: 0.1812\n",
      "\n",
      "('testing theta = :', 1.55)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2344 - acc: 0.1826 - val_loss: 0.2343 - val_acc: 0.1800\n",
      "\n",
      "('testing theta = :', 1.6)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2318 - acc: 0.1828 - val_loss: 0.2319 - val_acc: 0.1787\n",
      "\n",
      "('testing theta = :', 1.6500000000000001)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2290 - acc: 0.1837 - val_loss: 0.2290 - val_acc: 0.1836\n",
      "\n",
      "('testing theta = :', 1.7000000000000002)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2261 - acc: 0.1846 - val_loss: 0.2261 - val_acc: 0.1863\n",
      "\n",
      "('testing theta = :', 1.75)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2231 - acc: 0.1853 - val_loss: 0.2231 - val_acc: 0.1854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('testing theta = :', 1.8)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2199 - acc: 0.1867 - val_loss: 0.2199 - val_acc: 0.1872\n",
      "\n",
      "('testing theta = :', 1.85)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2165 - acc: 0.1875 - val_loss: 0.2165 - val_acc: 0.1887\n",
      "\n",
      "('testing theta = :', 1.9000000000000001)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2131 - acc: 0.1881 - val_loss: 0.2130 - val_acc: 0.1892\n",
      "\n",
      "('testing theta = :', 1.9500000000000002)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2095 - acc: 0.1894 - val_loss: 0.2096 - val_acc: 0.1930\n",
      "\n",
      "('testing theta = :', 2.0)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2058 - acc: 0.1905 - val_loss: 0.2059 - val_acc: 0.1927\n",
      "\n",
      "[[0.20845115753263235], [0.21192500563710928], [0.2154078968539834], [0.2190395554676652], [0.22214548850804566], [0.2252750390842557], [0.22834071619063614], [0.2312350336611271], [0.23393823555856944], [0.23677461247891188], [0.23871915173530578], [0.2408439398780465], [0.24261226512491704], [0.24408992701023818], [0.2454149765074253], [0.24662789689004422], [0.2475657290443778], [0.2482753820642829], [0.24875305069983006], [0.2490300398170948], [0.24910632210969924], [0.2489994126111269], [0.24871083649992942], [0.24821088822185994], [0.24749010414630174], [0.2464161105081439], [0.2450306745916605], [0.24335620617866516], [0.24140022841095923], [0.23919271678477524], [0.23683392734825612], [0.2343441880196333], [0.23185530015081168], [0.2289782502800226], [0.2261401254683733], [0.22312068473547697], [0.21991537665575742], [0.21652919197827578], [0.213034638941288], [0.20958495600521565], [0.20585335312783717]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2, 2, 41)\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(1, ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.title(\"Detector Effects: Theta vs. Loss\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"Detector Effects: Theta vs. Loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 1)                 1         \n",
      "=================================================================\n",
      "Total params: 16,898\n",
      "Trainable params: 16,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2411 - acc: 0.2865 - val_loss: 0.2212 - val_acc: 0.3303\n",
      ". theta fit =  0.0\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2216 - acc: 0.3303 - val_loss: -0.2218 - val_acc: 0.3303\n",
      ". theta fit =  0.017014783\n",
      "Epoch:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2116 - acc: 0.3331 - val_loss: 0.2101 - val_acc: 0.3349\n",
      ". theta fit =  0.017014783\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2110 - acc: 0.3351 - val_loss: -0.2115 - val_acc: 0.3349\n",
      ". theta fit =  0.036557227\n",
      "Epoch:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2111 - acc: 0.3361 - val_loss: 0.2110 - val_acc: 0.3362\n",
      ". theta fit =  0.036557227\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2119 - acc: 0.3363 - val_loss: -0.2124 - val_acc: 0.3362\n",
      ". theta fit =  0.056354754\n",
      "Epoch:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2124 - acc: 0.3362 - val_loss: 0.2124 - val_acc: 0.3362\n",
      ". theta fit =  0.056354754\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2133 - acc: 0.3362 - val_loss: -0.2138 - val_acc: 0.3362\n",
      ". theta fit =  0.076166935\n",
      "Epoch:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2138 - acc: 0.3362 - val_loss: 0.2137 - val_acc: 0.3362\n",
      ". theta fit =  0.076166935\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2146 - acc: 0.3362 - val_loss: -0.2151 - val_acc: 0.3362\n",
      ". theta fit =  0.09612038\n",
      "Epoch:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2152 - acc: 0.3363 - val_loss: 0.2151 - val_acc: 0.3362\n",
      ". theta fit =  0.09612038\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2160 - acc: 0.3363 - val_loss: -0.2165 - val_acc: 0.3362\n",
      ". theta fit =  0.115868494\n",
      "Epoch:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2165 - acc: 0.3363 - val_loss: 0.2165 - val_acc: 0.3362\n",
      ". theta fit =  0.115868494\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2174 - acc: 0.3363 - val_loss: -0.2179 - val_acc: 0.3362\n",
      ". theta fit =  0.1355728\n",
      "Epoch:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2179 - acc: 0.3363 - val_loss: 0.2178 - val_acc: 0.3360\n",
      ". theta fit =  0.1355728\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2187 - acc: 0.3363 - val_loss: -0.2192 - val_acc: 0.3360\n",
      ". theta fit =  0.15537684\n",
      "Epoch:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2192 - acc: 0.3362 - val_loss: 0.2192 - val_acc: 0.3360\n",
      ". theta fit =  0.15537684\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2200 - acc: 0.3362 - val_loss: -0.2205 - val_acc: 0.3360\n",
      ". theta fit =  0.17523167\n",
      "Epoch:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2205 - acc: 0.3361 - val_loss: 0.2205 - val_acc: 0.3360\n",
      ". theta fit =  0.17523167\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2213 - acc: 0.3362 - val_loss: -0.2218 - val_acc: 0.3360\n",
      ". theta fit =  0.19502574\n",
      "Epoch:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2218 - acc: 0.3361 - val_loss: 0.2218 - val_acc: 0.3360\n",
      ". theta fit =  0.19502574\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2226 - acc: 0.3362 - val_loss: -0.2231 - val_acc: 0.3360\n",
      ". theta fit =  0.21497297\n",
      "Epoch:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2231 - acc: 0.3360 - val_loss: 0.2231 - val_acc: 0.3360\n",
      ". theta fit =  0.21497297\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2239 - acc: 0.3361 - val_loss: -0.2243 - val_acc: 0.3360\n",
      ". theta fit =  0.23469834\n",
      "Epoch:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2243 - acc: 0.3360 - val_loss: 0.2243 - val_acc: 0.3358\n",
      ". theta fit =  0.23469834\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2251 - acc: 0.3359 - val_loss: -0.2256 - val_acc: 0.3358\n",
      ". theta fit =  0.25457776\n",
      "Epoch:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2256 - acc: 0.3359 - val_loss: 0.2255 - val_acc: 0.3358\n",
      ". theta fit =  0.25457776\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2263 - acc: 0.3359 - val_loss: -0.2268 - val_acc: 0.3358\n",
      ". theta fit =  0.2744051\n",
      "Epoch:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2268 - acc: 0.3358 - val_loss: 0.2268 - val_acc: 0.3357\n",
      ". theta fit =  0.2744051\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2275 - acc: 0.3359 - val_loss: -0.2280 - val_acc: 0.3357\n",
      ". theta fit =  0.29413602\n",
      "Epoch:  15\n",
      "Training g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2280 - acc: 0.3358 - val_loss: 0.2279 - val_acc: 0.3353\n",
      ". theta fit =  0.29413602\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2287 - acc: 0.3355 - val_loss: -0.2291 - val_acc: 0.3353\n",
      ". theta fit =  0.31373894\n",
      "Epoch:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2291 - acc: 0.3356 - val_loss: 0.2291 - val_acc: 0.3354\n",
      ". theta fit =  0.31373894\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2298 - acc: 0.3357 - val_loss: -0.2303 - val_acc: 0.3354\n",
      ". theta fit =  0.33353418\n",
      "Epoch:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2303 - acc: 0.3355 - val_loss: 0.2302 - val_acc: 0.3352\n",
      ". theta fit =  0.33353418\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2310 - acc: 0.3354 - val_loss: -0.2314 - val_acc: 0.3352\n",
      ". theta fit =  0.35335398\n",
      "Epoch:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2314 - acc: 0.3353 - val_loss: 0.2314 - val_acc: 0.3351\n",
      ". theta fit =  0.35335398\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2321 - acc: 0.3352 - val_loss: -0.2325 - val_acc: 0.3351\n",
      ". theta fit =  0.37304416\n",
      "Epoch:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2325 - acc: 0.3352 - val_loss: 0.2325 - val_acc: 0.3350\n",
      ". theta fit =  0.37304416\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2332 - acc: 0.3351 - val_loss: -0.2335 - val_acc: 0.3350\n",
      ". theta fit =  0.39259246\n",
      "Epoch:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2336 - acc: 0.3349 - val_loss: 0.2335 - val_acc: 0.3349\n",
      ". theta fit =  0.39259246\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2342 - acc: 0.3351 - val_loss: -0.2346 - val_acc: 0.3349\n",
      ". theta fit =  0.41209584\n",
      "Epoch:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2346 - acc: 0.3347 - val_loss: 0.2346 - val_acc: 0.3346\n",
      ". theta fit =  0.41209584\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2352 - acc: 0.3348 - val_loss: -0.2356 - val_acc: 0.3346\n",
      ". theta fit =  0.43169478\n",
      "Epoch:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2356 - acc: 0.3345 - val_loss: 0.2356 - val_acc: 0.3344\n",
      ". theta fit =  0.43169478\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2362 - acc: 0.3345 - val_loss: -0.2365 - val_acc: 0.3344\n",
      ". theta fit =  0.45120445\n",
      "Epoch:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2366 - acc: 0.3344 - val_loss: 0.2365 - val_acc: 0.3342\n",
      ". theta fit =  0.45120445\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2371 - acc: 0.3342 - val_loss: -0.2374 - val_acc: 0.3342\n",
      ". theta fit =  0.47061577\n",
      "Epoch:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2375 - acc: 0.3343 - val_loss: 0.2374 - val_acc: 0.3342\n",
      ". theta fit =  0.47061577\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: -0.2380 - acc: 0.3343 - val_loss: -0.2383 - val_acc: 0.3342\n",
      ". theta fit =  0.4901643\n",
      "Epoch:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2383 - acc: 0.3340 - val_loss: 0.2383 - val_acc: 0.3335\n",
      ". theta fit =  0.4901643\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: -0.2388 - acc: 0.3335 - val_loss: -0.2391 - val_acc: 0.3335\n",
      ". theta fit =  0.50948846\n",
      "Epoch:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2392 - acc: 0.3337 - val_loss: 0.2391 - val_acc: 0.3334\n",
      ". theta fit =  0.50948846\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: -0.2397 - acc: 0.3334 - val_loss: -0.2399 - val_acc: 0.3334\n",
      ". theta fit =  0.52884066\n",
      "Epoch:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2400 - acc: 0.3336 - val_loss: 0.2399 - val_acc: 0.3336\n",
      ". theta fit =  0.52884066\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: -0.2404 - acc: 0.3336 - val_loss: -0.2407 - val_acc: 0.3336\n",
      ". theta fit =  0.5479191\n",
      "Epoch:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2407 - acc: 0.3335 - val_loss: 0.2407 - val_acc: 0.3332\n",
      ". theta fit =  0.5479191\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: -0.2411 - acc: 0.3333 - val_loss: -0.2414 - val_acc: 0.3332\n",
      ". theta fit =  0.5671991\n",
      "Epoch:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2414 - acc: 0.3334 - val_loss: 0.2414 - val_acc: 0.3337\n",
      ". theta fit =  0.5671991\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: -0.2418 - acc: 0.3337 - val_loss: -0.2421 - val_acc: 0.3337\n",
      ". theta fit =  0.58614784\n",
      "Epoch:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2421 - acc: 0.3333 - val_loss: 0.2420 - val_acc: 0.3333\n",
      ". theta fit =  0.58614784\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2425 - acc: 0.3334 - val_loss: -0.2427 - val_acc: 0.3333\n",
      ". theta fit =  0.6049056\n",
      "Epoch:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2427 - acc: 0.3331 - val_loss: 0.2427 - val_acc: 0.3335\n",
      ". theta fit =  0.6049056\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: -0.2431 - acc: 0.3335 - val_loss: -0.2433 - val_acc: 0.3335\n",
      ". theta fit =  0.62389493\n",
      "Epoch:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2433 - acc: 0.3330 - val_loss: 0.2433 - val_acc: 0.3328\n",
      ". theta fit =  0.62389493\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2437 - acc: 0.3330 - val_loss: -0.2438 - val_acc: 0.3328\n",
      ". theta fit =  0.6426625\n",
      "Epoch:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2439 - acc: 0.3328 - val_loss: 0.2438 - val_acc: 0.3331\n",
      ". theta fit =  0.6426625\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2442 - acc: 0.3332 - val_loss: -0.2444 - val_acc: 0.3331\n",
      ". theta fit =  0.661228\n",
      "Epoch:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2444 - acc: 0.3325 - val_loss: 0.2444 - val_acc: 0.3327\n",
      ". theta fit =  0.661228\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2447 - acc: 0.3328 - val_loss: -0.2449 - val_acc: 0.3327\n",
      ". theta fit =  0.6796705\n",
      "Epoch:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2449 - acc: 0.3327 - val_loss: 0.2449 - val_acc: 0.3326\n",
      ". theta fit =  0.6796705\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2453 - acc: 0.3327 - val_loss: -0.2454 - val_acc: 0.3326\n",
      ". theta fit =  0.6985091\n",
      "Epoch:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2454 - acc: 0.3324 - val_loss: 0.2454 - val_acc: 0.3322\n",
      ". theta fit =  0.6985091\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2457 - acc: 0.3321 - val_loss: -0.2458 - val_acc: 0.3322\n",
      ". theta fit =  0.7166503\n",
      "Epoch:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2459 - acc: 0.3322 - val_loss: 0.2458 - val_acc: 0.3320\n",
      ". theta fit =  0.7166503\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2461 - acc: 0.3320 - val_loss: -0.2463 - val_acc: 0.3320\n",
      ". theta fit =  0.73463285\n",
      "Epoch:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2463 - acc: 0.3319 - val_loss: 0.2463 - val_acc: 0.3327\n",
      ". theta fit =  0.73463285\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2466 - acc: 0.3328 - val_loss: -0.2467 - val_acc: 0.3327\n",
      ". theta fit =  0.75251967\n",
      "Epoch:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2467 - acc: 0.3317 - val_loss: 0.2466 - val_acc: 0.3318\n",
      ". theta fit =  0.75251967\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2469 - acc: 0.3317 - val_loss: -0.2470 - val_acc: 0.3318\n",
      ". theta fit =  0.769514\n",
      "Epoch:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2471 - acc: 0.3317 - val_loss: 0.2470 - val_acc: 0.3313\n",
      ". theta fit =  0.769514\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2472 - acc: 0.3312 - val_loss: -0.2473 - val_acc: 0.3313\n",
      ". theta fit =  0.786296\n",
      "Epoch:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2474 - acc: 0.3316 - val_loss: 0.2473 - val_acc: 0.3324\n",
      ". theta fit =  0.786296\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2476 - acc: 0.3324 - val_loss: -0.2476 - val_acc: 0.3324\n",
      ". theta fit =  0.8029959\n",
      "Epoch:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2477 - acc: 0.3314 - val_loss: 0.2476 - val_acc: 0.3318\n",
      ". theta fit =  0.8029959\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2478 - acc: 0.3318 - val_loss: -0.2479 - val_acc: 0.3318\n",
      ". theta fit =  0.8195695\n",
      "Epoch:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2479 - acc: 0.3316 - val_loss: 0.2479 - val_acc: 0.3302\n",
      ". theta fit =  0.8195695\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2481 - acc: 0.3301 - val_loss: -0.2481 - val_acc: 0.3302\n",
      ". theta fit =  0.835032\n",
      "Epoch:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2482 - acc: 0.3316 - val_loss: 0.2481 - val_acc: 0.3322\n",
      ". theta fit =  0.835032\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2483 - acc: 0.3322 - val_loss: -0.2483 - val_acc: 0.3322\n",
      ". theta fit =  0.8499468\n",
      "Epoch:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2483 - acc: 0.3312 - val_loss: 0.2483 - val_acc: 0.3312\n",
      ". theta fit =  0.8499468\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2484 - acc: 0.3311 - val_loss: -0.2484 - val_acc: 0.3312\n",
      ". theta fit =  0.86347884\n",
      "Epoch:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2485 - acc: 0.3316 - val_loss: 0.2484 - val_acc: 0.3313\n",
      ". theta fit =  0.86347884\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2486 - acc: 0.3313 - val_loss: -0.2486 - val_acc: 0.3313\n",
      ". theta fit =  0.87645954\n",
      "Epoch:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2486 - acc: 0.3316 - val_loss: 0.2485 - val_acc: 0.3327\n",
      ". theta fit =  0.87645954\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2487 - acc: 0.3328 - val_loss: -0.2487 - val_acc: 0.3327\n",
      ". theta fit =  0.88881326\n",
      "Epoch:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2487 - acc: 0.3310 - val_loss: 0.2487 - val_acc: 0.3328\n",
      ". theta fit =  0.88881326\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2488 - acc: 0.3330 - val_loss: -0.2488 - val_acc: 0.3328\n",
      ". theta fit =  0.90038455\n",
      "Epoch:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2488 - acc: 0.3318 - val_loss: 0.2488 - val_acc: 0.3289\n",
      ". theta fit =  0.90038455\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2489 - acc: 0.3289 - val_loss: -0.2488 - val_acc: 0.3289\n",
      ". theta fit =  0.91058403\n",
      "Epoch:  50\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2489 - acc: 0.3323 - val_loss: 0.2488 - val_acc: 0.3323\n",
      ". theta fit =  0.91058403\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2489 - acc: 0.3323 - val_loss: -0.2489 - val_acc: 0.3323\n",
      ". theta fit =  0.9192278\n",
      "Epoch:  51\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2489 - acc: 0.3315 - val_loss: 0.2489 - val_acc: 0.3341\n",
      ". theta fit =  0.9192278\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2490 - acc: 0.3341 - val_loss: -0.2489 - val_acc: 0.3341\n",
      ". theta fit =  0.92753035\n",
      "Epoch:  52\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2490 - acc: 0.3320 - val_loss: 0.2489 - val_acc: 0.3342\n",
      ". theta fit =  0.92753035\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2490 - acc: 0.3342 - val_loss: -0.2490 - val_acc: 0.3342\n",
      ". theta fit =  0.9351691\n",
      "Epoch:  53\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2490 - acc: 0.3318 - val_loss: 0.2490 - val_acc: 0.3323\n",
      ". theta fit =  0.9351691\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2490 - acc: 0.3323 - val_loss: -0.2490 - val_acc: 0.3323\n",
      ". theta fit =  0.9411443\n",
      "Epoch:  54\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2491 - acc: 0.3329 - val_loss: 0.2490 - val_acc: 0.3356\n",
      ". theta fit =  0.9411443\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2491 - acc: 0.3358 - val_loss: -0.2490 - val_acc: 0.3356\n",
      ". theta fit =  0.94788176\n",
      "Epoch:  55\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2491 - acc: 0.3325 - val_loss: 0.2490 - val_acc: 0.3286\n",
      ". theta fit =  0.94788176\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2491 - acc: 0.3285 - val_loss: -0.2490 - val_acc: 0.3286\n",
      ". theta fit =  0.9528095\n",
      "Epoch:  56\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2491 - acc: 0.3318 - val_loss: 0.2490 - val_acc: 0.3359\n",
      ". theta fit =  0.9528095\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2491 - acc: 0.3361 - val_loss: -0.2491 - val_acc: 0.3359\n",
      ". theta fit =  0.9583796\n",
      "Epoch:  57\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2491 - acc: 0.3295 - val_loss: 0.2490 - val_acc: 0.3361\n",
      ". theta fit =  0.9583796\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2491 - acc: 0.3363 - val_loss: -0.2491 - val_acc: 0.3361\n",
      ". theta fit =  0.9640628\n",
      "Epoch:  58\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2491 - acc: 0.3343 - val_loss: 0.2491 - val_acc: 0.3362\n",
      ". theta fit =  0.9640628\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2491 - acc: 0.3363 - val_loss: -0.2491 - val_acc: 0.3362\n",
      ". theta fit =  0.96768963\n",
      "Epoch:  59\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2491 - acc: 0.3347 - val_loss: 0.2491 - val_acc: 0.3320\n",
      ". theta fit =  0.96768963\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2491 - acc: 0.3320 - val_loss: -0.2491 - val_acc: 0.3320\n",
      ". theta fit =  0.9687791\n",
      "Epoch:  60\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2491 - acc: 0.3338 - val_loss: 0.2491 - val_acc: 0.3362\n",
      ". theta fit =  0.9687791\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2491 - acc: 0.3363 - val_loss: -0.2491 - val_acc: 0.3362\n",
      ". theta fit =  0.9711752\n",
      "Epoch:  61\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.3318 - val_loss: 0.2491 - val_acc: 0.3196\n",
      ". theta fit =  0.9711752\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2491 - acc: 0.3197 - val_loss: -0.2491 - val_acc: 0.3196\n",
      ". theta fit =  0.972263\n",
      "Epoch:  62\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2491 - acc: 0.3345 - val_loss: 0.2491 - val_acc: 0.3362\n",
      ". theta fit =  0.972263\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2491 - acc: 0.3363 - val_loss: -0.2491 - val_acc: 0.3362\n",
      ". theta fit =  0.97459775\n",
      "Epoch:  63\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2491 - acc: 0.3313 - val_loss: 0.2491 - val_acc: 0.3358\n",
      ". theta fit =  0.97459775\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2491 - acc: 0.3360 - val_loss: -0.2491 - val_acc: 0.3358\n",
      ". theta fit =  0.97729236\n",
      "Epoch:  64\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2491 - acc: 0.3298 - val_loss: 0.2491 - val_acc: 0.3351\n",
      ". theta fit =  0.97729236\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2491 - acc: 0.3353 - val_loss: -0.2491 - val_acc: 0.3351\n",
      ". theta fit =  0.97706175\n",
      "Epoch:  65\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.3318 - val_loss: 0.2491 - val_acc: 0.3349\n",
      ". theta fit =  0.97706175\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2491 - acc: 0.3351 - val_loss: -0.2491 - val_acc: 0.3349\n",
      ". theta fit =  0.9787681\n",
      "Epoch:  66\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.3319 - val_loss: 0.2491 - val_acc: 0.3356\n",
      ". theta fit =  0.9787681\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2491 - acc: 0.3358 - val_loss: -0.2491 - val_acc: 0.3356\n",
      ". theta fit =  0.97840685\n",
      "Epoch:  67\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.3306 - val_loss: 0.2491 - val_acc: 0.3350\n",
      ". theta fit =  0.97840685\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2491 - acc: 0.3353 - val_loss: -0.2491 - val_acc: 0.3350\n",
      ". theta fit =  0.9793692\n",
      "Epoch:  68\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2491 - acc: 0.3347 - val_loss: 0.2491 - val_acc: 0.3350\n",
      ". theta fit =  0.9793692\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2491 - acc: 0.3353 - val_loss: -0.2491 - val_acc: 0.3350\n",
      ". theta fit =  0.9805852\n",
      "Epoch:  69\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2491 - acc: 0.3254 - val_loss: 0.2491 - val_acc: 0.3337\n",
      ". theta fit =  0.9805852\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2491 - acc: 0.3338 - val_loss: -0.2491 - val_acc: 0.3337\n",
      ". theta fit =  0.9842572\n",
      "Epoch:  70\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.3255 - val_loss: 0.2491 - val_acc: 0.3337\n",
      ". theta fit =  0.9842572\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2491 - acc: 0.3338 - val_loss: -0.2491 - val_acc: 0.3337\n",
      ". theta fit =  0.98633045\n",
      "Epoch:  71\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.3277 - val_loss: 0.2491 - val_acc: 0.3337\n",
      ". theta fit =  0.98633045\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2491 - acc: 0.3338 - val_loss: -0.2491 - val_acc: 0.3337\n",
      ". theta fit =  0.9876737\n",
      "Epoch:  72\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.3268 - val_loss: 0.2491 - val_acc: 0.3231\n",
      ". theta fit =  0.9876737\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2491 - acc: 0.3235 - val_loss: -0.2491 - val_acc: 0.3231\n",
      ". theta fit =  0.98770624\n",
      "Epoch:  73\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2491 - acc: 0.3213 - val_loss: 0.2491 - val_acc: 0.3324\n",
      ". theta fit =  0.98770624\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2491 - acc: 0.3324 - val_loss: -0.2491 - val_acc: 0.3324\n",
      ". theta fit =  0.9899977\n",
      "Epoch:  74\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2491 - acc: 0.3262 - val_loss: 0.2491 - val_acc: 0.2849\n",
      ". theta fit =  0.9899977\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.2851 - val_loss: -0.2491 - val_acc: 0.2849\n",
      ". theta fit =  0.9899184\n",
      "Epoch:  75\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2491 - acc: 0.3233 - val_loss: 0.2491 - val_acc: 0.3321\n",
      ". theta fit =  0.9899184\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.3322 - val_loss: -0.2491 - val_acc: 0.3321\n",
      ". theta fit =  0.98997843\n",
      "Epoch:  76\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2491 - acc: 0.3195 - val_loss: 0.2491 - val_acc: 0.3303\n",
      ". theta fit =  0.98997843\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.3303 - val_loss: -0.2491 - val_acc: 0.3303\n",
      ". theta fit =  0.98949903\n",
      "Epoch:  77\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2491 - acc: 0.3215 - val_loss: 0.2491 - val_acc: 0.3066\n",
      ". theta fit =  0.98949903\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.3068 - val_loss: -0.2491 - val_acc: 0.3066\n",
      ". theta fit =  0.9885846\n",
      "Epoch:  78\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2491 - acc: 0.3176 - val_loss: 0.2491 - val_acc: 0.3325\n",
      ". theta fit =  0.9885846\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.3326 - val_loss: -0.2491 - val_acc: 0.3325\n",
      ". theta fit =  0.99011207\n",
      "Epoch:  79\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2491 - acc: 0.3237 - val_loss: 0.2491 - val_acc: 0.3309\n",
      ". theta fit =  0.99011207\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.3310 - val_loss: -0.2491 - val_acc: 0.3309\n",
      ". theta fit =  0.9914175\n",
      "Epoch:  80\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2491 - acc: 0.3178 - val_loss: 0.2491 - val_acc: 0.3316\n",
      ". theta fit =  0.9914175\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.3317 - val_loss: -0.2491 - val_acc: 0.3316\n",
      ". theta fit =  0.9924404\n",
      "Epoch:  81\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2491 - acc: 0.3178 - val_loss: 0.2491 - val_acc: 0.3308\n",
      ". theta fit =  0.9924404\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.3308 - val_loss: -0.2491 - val_acc: 0.3308\n",
      ". theta fit =  0.99245787\n",
      "Epoch:  82\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2491 - acc: 0.3183 - val_loss: 0.2491 - val_acc: 0.2943\n",
      ". theta fit =  0.99245787\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2491 - acc: 0.2944 - val_loss: -0.2491 - val_acc: 0.2943\n",
      ". theta fit =  0.99204326\n",
      "Epoch:  83\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2491 - acc: 0.3245 - val_loss: 0.2491 - val_acc: 0.3308\n",
      ". theta fit =  0.99204326\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2491 - acc: 0.3309 - val_loss: -0.2491 - val_acc: 0.3308\n",
      ". theta fit =  0.992494\n",
      "Epoch:  84\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2491 - acc: 0.3129 - val_loss: 0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.992494\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2491 - acc: 0.3307 - val_loss: -0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9938408\n",
      "Epoch:  85\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2491 - acc: 0.3172 - val_loss: 0.2491 - val_acc: 0.3102\n",
      ". theta fit =  0.9938408\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2491 - acc: 0.3105 - val_loss: -0.2491 - val_acc: 0.3102\n",
      ". theta fit =  0.9935413\n",
      "Epoch:  86\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2491 - acc: 0.3167 - val_loss: 0.2491 - val_acc: 0.3308\n",
      ". theta fit =  0.9935413\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2491 - acc: 0.3309 - val_loss: -0.2491 - val_acc: 0.3308\n",
      ". theta fit =  0.99363005\n",
      "Epoch:  87\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2491 - acc: 0.3162 - val_loss: 0.2491 - val_acc: 0.3018\n",
      ". theta fit =  0.99363005\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2491 - acc: 0.3020 - val_loss: -0.2491 - val_acc: 0.3018\n",
      ". theta fit =  0.99248767\n",
      "Epoch:  88\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2491 - acc: 0.3222 - val_loss: 0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.99248767\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2491 - acc: 0.3307 - val_loss: -0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9921893\n",
      "Epoch:  89\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2491 - acc: 0.3233 - val_loss: 0.2491 - val_acc: 0.3310\n",
      ". theta fit =  0.9921893\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2491 - acc: 0.3311 - val_loss: -0.2491 - val_acc: 0.3310\n",
      ". theta fit =  0.9920961\n",
      "Epoch:  90\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2491 - acc: 0.3189 - val_loss: 0.2491 - val_acc: 0.3311\n",
      ". theta fit =  0.9920961\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2491 - acc: 0.3313 - val_loss: -0.2491 - val_acc: 0.3311\n",
      ". theta fit =  0.991895\n",
      "Epoch:  91\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2491 - acc: 0.3175 - val_loss: 0.2491 - val_acc: 0.3323\n",
      ". theta fit =  0.991895\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2491 - acc: 0.3323 - val_loss: -0.2491 - val_acc: 0.3323\n",
      ". theta fit =  0.99240047\n",
      "Epoch:  92\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2491 - acc: 0.3166 - val_loss: 0.2491 - val_acc: 0.3305\n",
      ". theta fit =  0.99240047\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2491 - acc: 0.3305 - val_loss: -0.2491 - val_acc: 0.3305\n",
      ". theta fit =  0.9928568\n",
      "Epoch:  93\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2492 - acc: 0.3192 - val_loss: 0.2491 - val_acc: 0.3308\n",
      ". theta fit =  0.9928568\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2491 - acc: 0.3309 - val_loss: -0.2491 - val_acc: 0.3308\n",
      ". theta fit =  0.99213815\n",
      "Epoch:  94\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2491 - acc: 0.3106 - val_loss: 0.2491 - val_acc: 0.3305\n",
      ". theta fit =  0.99213815\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2491 - acc: 0.3305 - val_loss: -0.2491 - val_acc: 0.3305\n",
      ". theta fit =  0.9933613\n",
      "Epoch:  95\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2491 - acc: 0.3146 - val_loss: 0.2491 - val_acc: 0.3233\n",
      ". theta fit =  0.9933613\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2491 - acc: 0.3234 - val_loss: -0.2491 - val_acc: 0.3233\n",
      ". theta fit =  0.9936787\n",
      "Epoch:  96\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2491 - acc: 0.3183 - val_loss: 0.2491 - val_acc: 0.3327\n",
      ". theta fit =  0.9936787\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2491 - acc: 0.3327 - val_loss: -0.2491 - val_acc: 0.3327\n",
      ". theta fit =  0.9939365\n",
      "Epoch:  97\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2491 - acc: 0.3182 - val_loss: 0.2491 - val_acc: 0.3307\n",
      ". theta fit =  0.9939365\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2491 - acc: 0.3307 - val_loss: -0.2491 - val_acc: 0.3307\n",
      ". theta fit =  0.9941351\n",
      "Epoch:  98\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2491 - acc: 0.3099 - val_loss: 0.2491 - val_acc: 0.3307\n",
      ". theta fit =  0.9941351\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2491 - acc: 0.3308 - val_loss: -0.2491 - val_acc: 0.3307\n",
      ". theta fit =  0.99439037\n",
      "Epoch:  99\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2491 - acc: 0.3151 - val_loss: 0.2491 - val_acc: 0.3305\n",
      ". theta fit =  0.99439037\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 31s 31us/step - loss: -0.2491 - acc: 0.3305 - val_loss: -0.2491 - val_acc: 0.3305\n",
      ". theta fit =  0.9959604\n",
      "Epoch:  100\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 31s 31us/step - loss: 0.2491 - acc: 0.2976 - val_loss: 0.2491 - val_acc: 0.2749\n",
      ". theta fit =  0.9959604\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: -0.2491 - acc: 0.2748 - val_loss: -0.2491 - val_acc: 0.2749\n",
      ". theta fit =  0.9956881\n",
      "Epoch:  101\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: 0.2492 - acc: 0.3177 - val_loss: 0.2491 - val_acc: 0.2828\n",
      ". theta fit =  0.9956881\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: -0.2491 - acc: 0.2830 - val_loss: -0.2491 - val_acc: 0.2828\n",
      ". theta fit =  0.9941571\n",
      "Epoch:  102\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 31s 31us/step - loss: 0.2491 - acc: 0.3173 - val_loss: 0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9941571\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: -0.2491 - acc: 0.3307 - val_loss: -0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9937027\n",
      "Epoch:  103\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2491 - acc: 0.3111 - val_loss: 0.2491 - val_acc: 0.3232\n",
      ". theta fit =  0.9937027\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 31s 31us/step - loss: -0.2491 - acc: 0.3233 - val_loss: -0.2491 - val_acc: 0.3232\n",
      ". theta fit =  0.9948988\n",
      "Epoch:  104\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 32s 32us/step - loss: 0.2491 - acc: 0.3177 - val_loss: 0.2491 - val_acc: 0.3199\n",
      ". theta fit =  0.9948988\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 32s 32us/step - loss: -0.2491 - acc: 0.3202 - val_loss: -0.2491 - val_acc: 0.3199\n",
      ". theta fit =  0.9954834\n",
      "Epoch:  105\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 32s 32us/step - loss: 0.2491 - acc: 0.3190 - val_loss: 0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9954834\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2491 - acc: 0.3306 - val_loss: -0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9960323\n",
      "Epoch:  106\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2492 - acc: 0.3130 - val_loss: 0.2491 - val_acc: 0.3148\n",
      ". theta fit =  0.9960323\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2491 - acc: 0.3151 - val_loss: -0.2491 - val_acc: 0.3148\n",
      ". theta fit =  0.9953918\n",
      "Epoch:  107\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2491 - acc: 0.3172 - val_loss: 0.2491 - val_acc: 0.2965\n",
      ". theta fit =  0.9953918\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2491 - acc: 0.2967 - val_loss: -0.2491 - val_acc: 0.2965\n",
      ". theta fit =  0.9942999\n",
      "Epoch:  108\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2491 - acc: 0.3131 - val_loss: 0.2491 - val_acc: 0.3131\n",
      ". theta fit =  0.9942999\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2491 - acc: 0.3135 - val_loss: -0.2491 - val_acc: 0.3131\n",
      ". theta fit =  0.9937878\n",
      "Epoch:  109\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2491 - acc: 0.3141 - val_loss: 0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9937878\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: -0.2491 - acc: 0.3306 - val_loss: -0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9957834\n",
      "Epoch:  110\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2491 - acc: 0.3060 - val_loss: 0.2491 - val_acc: 0.3309\n",
      ". theta fit =  0.9957834\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: -0.2491 - acc: 0.3309 - val_loss: -0.2491 - val_acc: 0.3309\n",
      ". theta fit =  0.9966655\n",
      "Epoch:  111\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2491 - acc: 0.3062 - val_loss: 0.2491 - val_acc: 0.3302\n",
      ". theta fit =  0.9966655\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: -0.2491 - acc: 0.3303 - val_loss: -0.2491 - val_acc: 0.3302\n",
      ". theta fit =  0.9969299\n",
      "Epoch:  112\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2491 - acc: 0.3135 - val_loss: 0.2491 - val_acc: 0.2572\n",
      ". theta fit =  0.9969299\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 28s 28us/step - loss: -0.2491 - acc: 0.2573 - val_loss: -0.2491 - val_acc: 0.2572\n",
      ". theta fit =  0.99506766\n",
      "Epoch:  113\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 28s 28us/step - loss: 0.2491 - acc: 0.3128 - val_loss: 0.2491 - val_acc: 0.3305\n",
      ". theta fit =  0.99506766\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: -0.2491 - acc: 0.3305 - val_loss: -0.2491 - val_acc: 0.3305\n",
      ". theta fit =  0.99547756\n",
      "Epoch:  114\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2491 - acc: 0.3048 - val_loss: 0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.99547756\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2491 - acc: 0.3307 - val_loss: -0.2491 - val_acc: 0.3306\n",
      ". theta fit =  0.9955743\n",
      "Epoch:  115\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2491 - acc: 0.3091 - val_loss: 0.2491 - val_acc: 0.3307\n",
      ". theta fit =  0.9955743\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2491 - acc: 0.3308 - val_loss: -0.2491 - val_acc: 0.3307\n",
      ". theta fit =  0.9968019\n",
      "Epoch:  116\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2491 - acc: 0.3105 - val_loss: 0.2491 - val_acc: 0.2880\n",
      ". theta fit =  0.9968019\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: -0.2491 - acc: 0.2882 - val_loss: -0.2491 - val_acc: 0.2880\n",
      ". theta fit =  0.99511325\n",
      "Epoch:  117\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2491 - acc: 0.3189 - val_loss: 0.2491 - val_acc: 0.3253\n",
      ". theta fit =  0.99511325\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: -0.2491 - acc: 0.3255 - val_loss: -0.2491 - val_acc: 0.3253\n",
      ". theta fit =  0.9949056\n",
      "Epoch:  118\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2491 - acc: 0.3137 - val_loss: 0.2491 - val_acc: 0.3318\n",
      ". theta fit =  0.9949056\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: -0.2491 - acc: 0.3319 - val_loss: -0.2491 - val_acc: 0.3318\n",
      ". theta fit =  0.9958442\n",
      "Epoch:  119\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2491 - acc: 0.3155 - val_loss: 0.2491 - val_acc: 0.3110\n",
      ". theta fit =  0.9958442\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2491 - acc: 0.3112 - val_loss: -0.2491 - val_acc: 0.3110\n",
      ". theta fit =  0.99434406\n",
      "Epoch:  120\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2491 - acc: 0.3169 - val_loss: 0.2491 - val_acc: 0.3324\n",
      ". theta fit =  0.99434406\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2491 - acc: 0.3325 - val_loss: -0.2491 - val_acc: 0.3324\n",
      ". theta fit =  0.99468136\n",
      "Epoch:  121\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2491 - acc: 0.3223 - val_loss: 0.2491 - val_acc: 0.3150\n",
      ". theta fit =  0.99468136\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3d6ee2ec36c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m#model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training theta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),initializer = keras.initializers.Constant(value = theta_fit_init),trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 1e-5 #smaller learning rate yields better precision\n",
    "epochs = 200 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(mysign = 1):\n",
    "\n",
    "    theta = 0. #starting value\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "    \n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        y_true = tf.gather(y_true, np.arange(batch_size)) \n",
    "        y_labels = tf.gather(y_true, [0], axis = 1) #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis = 1) # sim truth for reweighting\n",
    "\n",
    "        theta = 0. #starting value\n",
    "        #theta0 = tf.constant(val, dtype= tf.float32)#target value\n",
    "    \n",
    "        #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "        concat_input_and_params = K.ones(shape = x_T.shape, dtype=tf.float32)*theta0 \n",
    "        #combining and reshaping into correct format:\n",
    "        data = K.concatenate((x_T, concat_input_and_params), axis=-1)\n",
    "        \n",
    "        w = reweight(data) # NN reweight\n",
    "        \n",
    "        #w = analytical_reweight(data) #functional analytical reweight\n",
    "        #w = K.exp(-(0.5*(x_T-theta0)**2)+(0.5*(x_T-theta)**2)) #direct analytical reweight\n",
    "        \n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_labels*(y_labels - y_pred)**2+(w)*(1.-y_labels)*(y_labels - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(theta1_param, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title(\"Gaussian fit with detector effects: N = {:.0e}, learning rate = {:.0e}, Epochs = {:.0f}\".format(N, lr, len(fit_vals)))\n",
    "#plt.savefig(\"Gaussian fit with detector effects: N = {:.0e}, learning rate = {:.0e}, Epochs = {:.0f}.png\".format(N, lr, len(fit_vals)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
