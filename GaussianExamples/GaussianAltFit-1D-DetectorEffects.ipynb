{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:14.571984Z",
     "start_time": "2020-06-07T23:44:12.456819Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import argrelmin, argrelmax\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:14.577545Z",
     "start_time": "2020-06-07T23:44:14.574280Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__)  #1.15.0\n",
    "print(keras.__version__)  #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{(1-f(x_{T,i},\\theta'))}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it a $\\mu$ value in addition to $X_i\\sim\\mathcal{N}(\\mu, 1)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:14.600138Z",
     "start_time": "2020-06-07T23:44:14.579961Z"
    }
   },
   "outputs": [],
   "source": [
    "n_data_points = 10**6\n",
    "mu_min = -2\n",
    "mu_max = 2\n",
    "mu_values = np.random.uniform(mu_min, mu_max, n_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with this $\\mu$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:22.356839Z",
     "start_time": "2020-06-07T23:44:14.602125Z"
    }
   },
   "outputs": [],
   "source": [
    "X0 = [(np.random.normal(0, 1), mu)\n",
    "      for mu in mu_values]  # Note the zero in normal(0, 1)\n",
    "X1 = [(np.random.normal(mu, 1), mu) for mu in mu_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu$ values uniformly sampled in the specified range [mu_min, mu_max].\n",
    "\n",
    "For every value of $\\mu$ in mu_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,1)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,1)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:23.473244Z",
     "start_time": "2020-06-07T23:44:22.358959Z"
    }
   },
   "outputs": [],
   "source": [
    "Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "X = np.concatenate((X0, X1))\n",
    "Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:44:23.565767Z",
     "start_time": "2020-06-07T23:44:23.475066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((2, ))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs=inputs, outputs=outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:54.581279Z",
     "start_time": "2020-06-07T23:44:23.567441Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1600000 samples, validate on 400000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1600000/1600000 [==============================] - 4s 2us/step - loss: 0.5805 - val_loss: 0.5647\n",
      "Epoch 2/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5631 - val_loss: 0.5621\n",
      "Epoch 3/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5622 - val_loss: 0.5617\n",
      "Epoch 4/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5621 - val_loss: 0.5620\n",
      "Epoch 5/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5621 - val_loss: 0.5618\n",
      "Epoch 6/200\n",
      "1600000/1600000 [==============================] - 2s 2us/step - loss: 0.5620 - val_loss: 0.5619\n",
      "Epoch 7/200\n",
      "1600000/1600000 [==============================] - 2s 2us/step - loss: 0.5621 - val_loss: 0.5616\n",
      "Epoch 8/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5617\n",
      "Epoch 9/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5618\n",
      "Epoch 10/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 11/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 12/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5619 - val_loss: 0.5615\n",
      "Epoch 13/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 14/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 15/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 16/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5615\n",
      "Epoch 17/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5615\n",
      "Epoch 18/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5618\n",
      "Epoch 19/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 20/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 21/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 22/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5615\n",
      "Epoch 23/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 24/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5617\n",
      "Epoch 25/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5615\n",
      "Epoch 26/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5619 - val_loss: 0.5620\n",
      "Epoch 27/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5618\n",
      "Epoch 28/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5615\n",
      "Epoch 29/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5618\n",
      "Epoch 30/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5619 - val_loss: 0.5616\n",
      "Epoch 31/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5616\n",
      "Epoch 32/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5620 - val_loss: 0.5617\n",
      "Epoch 33/200\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5619 - val_loss: 0.5617\n",
      "Epoch 34/200\n",
      "1600000/1600000 [==============================] - 2s 2us/step - loss: 0.5619 - val_loss: 0.5616\n",
      "Epoch 35/200\n",
      "1600000/1600000 [==============================] - 2s 2us/step - loss: 0.5620 - val_loss: 0.5617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e1733d550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "dctr_model.fit(X_train,\n",
    "               Y_train,\n",
    "               epochs=200,\n",
    "               batch_size=10000,\n",
    "               validation_data=(X_test, Y_test),\n",
    "               callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:54.593501Z",
     "start_time": "2020-06-07T23:45:54.587218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"1d_gaussian_dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"1d_gaussian_dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"1d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"1d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:45:54.608057Z",
     "start_time": "2020-06-07T23:45:54.597417Z"
    }
   },
   "outputs": [],
   "source": [
    "# from NN (DCTR)\n",
    "def reweight(events, param):\n",
    "\n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    concat_input_and_params = K.ones(shape=events.shape) * param\n",
    "    #combining and reshaping into correct format:\n",
    "    model_inputs = K.concatenate((events, concat_input_and_params), axis=-1)\n",
    "\n",
    "    f = dctr_model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "# from analytical formula for normal distributions\n",
    "def analytical_reweight(events, param):\n",
    "    mu0 = 0.0\n",
    "    weights = K.exp(-(0.5 * (events - param)**2) + (0.5 * (events - mu0)**2))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply detector effects, each event $x_{T,i}$in the sample is smeared by shifting by $Z_{i}$ from $Z = \\mathcal{N}(0,\\epsilon)$,where $\\epsilon$ represents the smearing. Thus: $x_{D,i} = x_{T,i} + Z_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:46:04.662080Z",
     "start_time": "2020-06-07T23:45:54.610697Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "theta0_param = 0  # this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = 1  # this is the data (the target)\n",
    "sigma = 1.  # Gaussian width\n",
    "epsilon = sigma / 2  # Smearing width\n",
    "\n",
    "theta0_T = np.random.normal(theta0_param, sigma, N)  #Truth Level Data\n",
    "theta0_D = np.array([(x + np.random.normal(0, epsilon))\n",
    "                     for x in theta0_T])  #Detector smearing\n",
    "theta0 = np.stack([theta0_T, theta0_D], axis=1)\n",
    "\n",
    "theta1_T = np.random.normal(theta1_param, sigma, N)\n",
    "theta1_D = np.array([(x + np.random.normal(0, epsilon))\n",
    "                     for x in theta1_T])  #Detector smearing\n",
    "theta1 = np.stack([theta1_T, theta0_D], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:46:06.479629Z",
     "start_time": "2020-06-07T23:46:04.664234Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-6, 6, 31)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, constrained_layout=True)\n",
    "fig.suptitle('1D Gaussian Fit (Detector Effects)')\n",
    "\n",
    "axs[0].set_title(\"Simulation\\n($\\mu$ = {:.2f})\".format(theta0_param))\n",
    "axs[0].hist(theta0_T, bins=bins, alpha=0.5, label='Truth')\n",
    "axs[0].hist(theta0_D, bins=bins, alpha=0.5, label='Detector')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title(\"Data\\n($\\mu$ = {:.2f})\".format(theta1_param))\n",
    "axs[1].hist(theta1_T, bins=bins, alpha=0.5, label='Truth')\n",
    "axs[1].hist(theta1_D, bins=bins, alpha=0.5, label='Detector')\n",
    "axs[1].legend()\n",
    "\n",
    "#fig.savefig(\"1D Gaussian: Data ($\\mu$ = {:.2f}) w detector effects.png\".format(theta1_param))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:46:06.495522Z",
     "start_time": "2020-06-07T23:46:06.482829Z"
    }
   },
   "outputs": [],
   "source": [
    "#'Erasing' Truth level for data, we can't actually observe this\n",
    "theta1 = np.stack([np.zeros_like(theta0_D), theta0_D], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:46:06.950649Z",
     "start_time": "2020-06-07T23:46:06.497808Z"
    }
   },
   "outputs": [],
   "source": [
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0_D, theta1_D])\n",
    "y_true = np.concatenate([labels0, labels1])\n",
    "# 'hiding' truth level for simulation in model output (used in reweighting)\n",
    "truth_level = np.concatenate([theta0_T, theta1_T])\n",
    "yvals = np.stack([y_true, truth_level], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals,\n",
    "                                                    yvals,\n",
    "                                                    test_size=0.5)\n",
    "\n",
    "X_train_theta, y_train_theta = shuffle(xvals, yvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model: Scan\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:46:06.958008Z",
     "start_time": "2020-06-07T23:46:06.952805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njson_file = open(\\'dctr_model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\ndctr_model = keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\ndctr_model.load_weights(\"dctr_model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-07T23:46:07.025444Z",
     "start_time": "2020-06-07T23:46:06.960985Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1, ), dtype=tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 500\n",
    "\n",
    "\n",
    "def my_loss_wrapper(val=0, reweight_analytically=False, MSE_loss=True):\n",
    "    def my_loss(y_true, y_pred):\n",
    "        y_true = tf.gather(y_true, np.arange(batch_size))\n",
    "        y_labels = tf.gather(y_true, [0], axis=1)  # actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis=1)  # sim event truth for reweighting\n",
    "\n",
    "        theta_prime = val\n",
    "\n",
    "        if reweight_analytically:\n",
    "            # analytical reweight\n",
    "            weights = analytical_reweight(x_T, theta_prime)\n",
    "        else:\n",
    "            #NN reweight\n",
    "            weights = analytical_reweight(x_T, theta_prime)\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_labels * (y_labels - y_pred)**2 + (weights) * (\n",
    "                1. - y_labels) * (y_labels - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_labels) * K.log(y_pred) + weights *\n",
    "                       (1 - y_labels) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T00:14:21.890572Z",
     "start_time": "2020-06-07T23:46:07.028097Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : -2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2217 - acc: 0.3098 - val_loss: 0.2185 - val_acc: 0.3107\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2180 - acc: 0.3093 - val_loss: 0.2182 - val_acc: 0.3114\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2179 - acc: 0.3093 - val_loss: 0.2181 - val_acc: 0.3098\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2180 - acc: 0.3093 - val_loss: 0.2180 - val_acc: 0.3100\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2179 - acc: 0.3093 - val_loss: 0.2181 - val_acc: 0.3099\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2179 - acc: 0.3093 - val_loss: 0.2181 - val_acc: 0.3087\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2179 - acc: 0.3097 - val_loss: 0.2185 - val_acc: 0.3071\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2179 - acc: 0.3092 - val_loss: 0.2182 - val_acc: 0.3091\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2179 - acc: 0.3095 - val_loss: 0.2181 - val_acc: 0.3103\n",
      "testing theta = : -1.8\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2509 - acc: 0.3142 - val_loss: 0.2511 - val_acc: 0.3131\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2509 - acc: 0.3142 - val_loss: 0.2510 - val_acc: 0.3141\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2509 - acc: 0.3143 - val_loss: 0.2512 - val_acc: 0.3144\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2508 - acc: 0.3142 - val_loss: 0.2511 - val_acc: 0.3159\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2508 - acc: 0.3144 - val_loss: 0.2510 - val_acc: 0.3158\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2508 - acc: 0.3145 - val_loss: 0.2511 - val_acc: 0.3146\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2508 - acc: 0.3143 - val_loss: 0.2510 - val_acc: 0.3139\n",
      "testing theta = : -1.6\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2864 - acc: 0.3177 - val_loss: 0.2865 - val_acc: 0.3192\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2864 - acc: 0.3180 - val_loss: 0.2868 - val_acc: 0.3178\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2864 - acc: 0.3181 - val_loss: 0.2866 - val_acc: 0.3167\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2864 - acc: 0.3177 - val_loss: 0.2866 - val_acc: 0.3192\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2864 - acc: 0.3179 - val_loss: 0.2864 - val_acc: 0.3183\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2863 - acc: 0.3179 - val_loss: 0.2865 - val_acc: 0.3187\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2864 - acc: 0.3181 - val_loss: 0.2864 - val_acc: 0.3177\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2863 - acc: 0.3177 - val_loss: 0.2866 - val_acc: 0.3188\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2863 - acc: 0.3179 - val_loss: 0.2868 - val_acc: 0.3188\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2863 - acc: 0.3179 - val_loss: 0.2865 - val_acc: 0.3176\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2863 - acc: 0.3179 - val_loss: 0.2866 - val_acc: 0.3175\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2863 - acc: 0.3180 - val_loss: 0.2865 - val_acc: 0.3178\n",
      "testing theta = : -1.4\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.3242 - acc: 0.3220 - val_loss: 0.3244 - val_acc: 0.3217\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3242 - acc: 0.3220 - val_loss: 0.3244 - val_acc: 0.3222\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3242 - acc: 0.3220 - val_loss: 0.3244 - val_acc: 0.3229\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3242 - acc: 0.3221 - val_loss: 0.3243 - val_acc: 0.3231\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3242 - acc: 0.3221 - val_loss: 0.3243 - val_acc: 0.3226\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3242 - acc: 0.3221 - val_loss: 0.3243 - val_acc: 0.3224\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3242 - acc: 0.3220 - val_loss: 0.3243 - val_acc: 0.3222\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3241 - acc: 0.3219 - val_loss: 0.3243 - val_acc: 0.3222\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3241 - acc: 0.3221 - val_loss: 0.3243 - val_acc: 0.3229\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3241 - acc: 0.3222 - val_loss: 0.3244 - val_acc: 0.3212\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3241 - acc: 0.3220 - val_loss: 0.3246 - val_acc: 0.3230\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3242 - acc: 0.3221 - val_loss: 0.3243 - val_acc: 0.3222\n",
      "testing theta = : -1.2\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.3639 - acc: 0.3251 - val_loss: 0.3640 - val_acc: 0.3253\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3639 - acc: 0.3251 - val_loss: 0.3639 - val_acc: 0.3255\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3639 - acc: 0.3254 - val_loss: 0.3639 - val_acc: 0.3256\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3639 - acc: 0.3251 - val_loss: 0.3639 - val_acc: 0.3256\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3639 - acc: 0.3252 - val_loss: 0.3643 - val_acc: 0.3255\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3639 - acc: 0.3252 - val_loss: 0.3646 - val_acc: 0.3264\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.3639 - acc: 0.3251 - val_loss: 0.3640 - val_acc: 0.3258\n",
      "testing theta = : -1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4049 - acc: 0.3280 - val_loss: 0.4050 - val_acc: 0.3286\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3281 - val_loss: 0.4051 - val_acc: 0.3276\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3281 - val_loss: 0.4053 - val_acc: 0.3284\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3281 - val_loss: 0.4050 - val_acc: 0.3283\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3281 - val_loss: 0.4050 - val_acc: 0.3281\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3280 - val_loss: 0.4050 - val_acc: 0.3296\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3280 - val_loss: 0.4049 - val_acc: 0.3286\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3280 - val_loss: 0.4050 - val_acc: 0.3290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3281 - val_loss: 0.4051 - val_acc: 0.3285\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3280 - val_loss: 0.4050 - val_acc: 0.3286\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3281 - val_loss: 0.4052 - val_acc: 0.3284\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4049 - acc: 0.3282 - val_loss: 0.4051 - val_acc: 0.3284\n",
      "testing theta = : -0.7999999999999998\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4467 - acc: 0.3302 - val_loss: 0.4468 - val_acc: 0.3319\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4467 - acc: 0.3307 - val_loss: 0.4467 - val_acc: 0.3308\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4467 - acc: 0.3305 - val_loss: 0.4469 - val_acc: 0.3301\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4467 - acc: 0.3307 - val_loss: 0.4466 - val_acc: 0.3312\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4467 - acc: 0.3306 - val_loss: 0.4467 - val_acc: 0.3315\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4467 - acc: 0.3307 - val_loss: 0.4468 - val_acc: 0.3318\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4467 - acc: 0.3307 - val_loss: 0.4470 - val_acc: 0.3305\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4467 - acc: 0.3306 - val_loss: 0.4466 - val_acc: 0.3314\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4466 - acc: 0.3307 - val_loss: 0.4467 - val_acc: 0.3315\n",
      "testing theta = : -0.5999999999999999\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4883 - acc: 0.3331 - val_loss: 0.4882 - val_acc: 0.3335\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4883 - acc: 0.3332 - val_loss: 0.4883 - val_acc: 0.3336\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4882 - acc: 0.3332 - val_loss: 0.4882 - val_acc: 0.3336\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.4882 - acc: 0.3332 - val_loss: 0.4882 - val_acc: 0.3335\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4882 - acc: 0.3333 - val_loss: 0.4883 - val_acc: 0.3339\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4882 - acc: 0.3333 - val_loss: 0.4882 - val_acc: 0.3337\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4882 - acc: 0.3332 - val_loss: 0.4882 - val_acc: 0.3338\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4883 - acc: 0.3332 - val_loss: 0.4883 - val_acc: 0.3341\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4882 - acc: 0.3332 - val_loss: 0.4882 - val_acc: 0.3336\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4882 - acc: 0.3332 - val_loss: 0.4882 - val_acc: 0.3340\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.4882 - acc: 0.3332 - val_loss: 0.4883 - val_acc: 0.3335\n",
      "testing theta = : -0.3999999999999999\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5288 - acc: 0.3348 - val_loss: 0.5288 - val_acc: 0.3354\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5288 - acc: 0.3347 - val_loss: 0.5289 - val_acc: 0.3355\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.5287 - acc: 0.3347 - val_loss: 0.5287 - val_acc: 0.3354\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5288 - acc: 0.3348 - val_loss: 0.5287 - val_acc: 0.3347\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5287 - acc: 0.3348 - val_loss: 0.5290 - val_acc: 0.3352\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.5288 - acc: 0.3347 - val_loss: 0.5287 - val_acc: 0.3352\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5288 - acc: 0.3348 - val_loss: 0.5288 - val_acc: 0.3350\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5287 - acc: 0.3347 - val_loss: 0.5289 - val_acc: 0.3348\n",
      "testing theta = : -0.19999999999999996\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5672 - acc: 0.3358 - val_loss: 0.5671 - val_acc: 0.3360\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5672 - acc: 0.3357 - val_loss: 0.5670 - val_acc: 0.3362\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5672 - acc: 0.3358 - val_loss: 0.5670 - val_acc: 0.3361\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5672 - acc: 0.3358 - val_loss: 0.5671 - val_acc: 0.3360\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5672 - acc: 0.3357 - val_loss: 0.5670 - val_acc: 0.3361\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5672 - acc: 0.3357 - val_loss: 0.5670 - val_acc: 0.3361\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5672 - acc: 0.3358 - val_loss: 0.5671 - val_acc: 0.3361\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.5672 - acc: 0.3357 - val_loss: 0.5671 - val_acc: 0.3361\n",
      "testing theta = : 0.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6024 - acc: 0.3362 - val_loss: 0.6022 - val_acc: 0.3364\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3361 - val_loss: 0.6027 - val_acc: 0.3361\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3362 - val_loss: 0.6022 - val_acc: 0.3364\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6024 - acc: 0.3362 - val_loss: 0.6024 - val_acc: 0.3364\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3362 - val_loss: 0.6026 - val_acc: 0.3362\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6024 - acc: 0.3362 - val_loss: 0.6023 - val_acc: 0.3363\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3362 - val_loss: 0.6022 - val_acc: 0.3364\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6024 - acc: 0.3361 - val_loss: 0.6023 - val_acc: 0.3363\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6023 - acc: 0.3361 - val_loss: 0.6025 - val_acc: 0.3363\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6024 - acc: 0.3362 - val_loss: 0.6022 - val_acc: 0.3364\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3362 - val_loss: 0.6022 - val_acc: 0.3364\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3361 - val_loss: 0.6023 - val_acc: 0.3364\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3361 - val_loss: 0.6023 - val_acc: 0.3364\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3361 - val_loss: 0.6022 - val_acc: 0.3364\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3362 - val_loss: 0.6022 - val_acc: 0.3364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6023 - acc: 0.3361 - val_loss: 0.6023 - val_acc: 0.3364\n",
      "testing theta = : 0.20000000000000018\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6332 - acc: 0.3357 - val_loss: 0.6330 - val_acc: 0.3359\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6332 - acc: 0.3357 - val_loss: 0.6331 - val_acc: 0.3362\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6332 - acc: 0.3357 - val_loss: 0.6330 - val_acc: 0.3357\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6332 - acc: 0.3357 - val_loss: 0.6330 - val_acc: 0.3359\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6332 - acc: 0.3357 - val_loss: 0.6331 - val_acc: 0.3359\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6332 - acc: 0.3357 - val_loss: 0.6331 - val_acc: 0.3362\n",
      "testing theta = : 0.40000000000000036\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6586 - acc: 0.3346 - val_loss: 0.6583 - val_acc: 0.3348\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.3347 - val_loss: 0.6585 - val_acc: 0.3348\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.3346 - val_loss: 0.6584 - val_acc: 0.3350\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.3347 - val_loss: 0.6583 - val_acc: 0.3348\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.3347 - val_loss: 0.6587 - val_acc: 0.3358\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.3347 - val_loss: 0.6584 - val_acc: 0.3351\n",
      "testing theta = : 0.6000000000000001\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6776 - acc: 0.3326 - val_loss: 0.6772 - val_acc: 0.3330\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6775 - acc: 0.3330 - val_loss: 0.6772 - val_acc: 0.3323\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6775 - acc: 0.3329 - val_loss: 0.6773 - val_acc: 0.3313\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6776 - acc: 0.3331 - val_loss: 0.6773 - val_acc: 0.3328\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6775 - acc: 0.3330 - val_loss: 0.6775 - val_acc: 0.3345\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6775 - acc: 0.3330 - val_loss: 0.6773 - val_acc: 0.3329\n",
      "testing theta = : 0.8000000000000003\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6893 - acc: 0.3292 - val_loss: 0.6890 - val_acc: 0.3232\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6893 - acc: 0.3301 - val_loss: 0.6888 - val_acc: 0.3305\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.3304 - val_loss: 0.6888 - val_acc: 0.3315\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6892 - acc: 0.3302 - val_loss: 0.6888 - val_acc: 0.3317\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6893 - acc: 0.3303 - val_loss: 0.6888 - val_acc: 0.3315\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6892 - acc: 0.3302 - val_loss: 0.6888 - val_acc: 0.3305\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6892 - acc: 0.3304 - val_loss: 0.6889 - val_acc: 0.3284\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6893 - acc: 0.3304 - val_loss: 0.6888 - val_acc: 0.3305\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6893 - acc: 0.3301 - val_loss: 0.6889 - val_acc: 0.3333\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6892 - acc: 0.3302 - val_loss: 0.6888 - val_acc: 0.3288\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6892 - acc: 0.3301 - val_loss: 0.6889 - val_acc: 0.3327\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6933 - acc: 0.2614 - val_loss: 0.6926 - val_acc: 0.2034\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2224 - val_loss: 0.6926 - val_acc: 0.2572\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2387 - val_loss: 0.6926 - val_acc: 0.2541\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2344 - val_loss: 0.6926 - val_acc: 0.2511\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2291 - val_loss: 0.6926 - val_acc: 0.2531\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2360 - val_loss: 0.6926 - val_acc: 0.3250\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2539 - val_loss: 0.6926 - val_acc: 0.1980\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2357 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "testing theta = : 1.2000000000000002\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6895 - acc: 0.1757 - val_loss: 0.6885 - val_acc: 0.1728\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6893 - acc: 0.1757 - val_loss: 0.6885 - val_acc: 0.1747\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6892 - acc: 0.1751 - val_loss: 0.6885 - val_acc: 0.1753\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.1755 - val_loss: 0.6886 - val_acc: 0.1781\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.1752 - val_loss: 0.6885 - val_acc: 0.1742\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.1741 - val_loss: 0.6885 - val_acc: 0.1741\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.1741 - val_loss: 0.6886 - val_acc: 0.1784\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6892 - acc: 0.1731 - val_loss: 0.6885 - val_acc: 0.1765\n",
      "testing theta = : 1.4000000000000004\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6777 - acc: 0.1781 - val_loss: 0.6766 - val_acc: 0.1756\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6776 - acc: 0.1790 - val_loss: 0.6767 - val_acc: 0.1749\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6776 - acc: 0.1790 - val_loss: 0.6766 - val_acc: 0.1773\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6775 - acc: 0.1794 - val_loss: 0.6768 - val_acc: 0.1844\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6775 - acc: 0.1792 - val_loss: 0.6771 - val_acc: 0.1739\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6776 - acc: 0.1788 - val_loss: 0.6766 - val_acc: 0.1769\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6775 - acc: 0.1793 - val_loss: 0.6767 - val_acc: 0.1829\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6776 - acc: 0.1790 - val_loss: 0.6766 - val_acc: 0.1805\n",
      "testing theta = : 1.6\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6588 - acc: 0.1820 - val_loss: 0.6574 - val_acc: 0.1792\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6587 - acc: 0.1820 - val_loss: 0.6574 - val_acc: 0.1814\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6587 - acc: 0.1824 - val_loss: 0.6575 - val_acc: 0.1860\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.1827 - val_loss: 0.6573 - val_acc: 0.1828\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.1828 - val_loss: 0.6573 - val_acc: 0.1839\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6587 - acc: 0.1830 - val_loss: 0.6574 - val_acc: 0.1828\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.1831 - val_loss: 0.6574 - val_acc: 0.1829\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.1831 - val_loss: 0.6574 - val_acc: 0.1856\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6586 - acc: 0.1828 - val_loss: 0.6574 - val_acc: 0.1805\n",
      "testing theta = : 1.8000000000000003\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6334 - acc: 0.1859 - val_loss: 0.6319 - val_acc: 0.1836\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6334 - acc: 0.1862 - val_loss: 0.6318 - val_acc: 0.1890\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6333 - acc: 0.1868 - val_loss: 0.6322 - val_acc: 0.1822\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6334 - acc: 0.1866 - val_loss: 0.6317 - val_acc: 0.1855\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6333 - acc: 0.1865 - val_loss: 0.6320 - val_acc: 0.1895\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6334 - acc: 0.1867 - val_loss: 0.6321 - val_acc: 0.1889\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6333 - acc: 0.1864 - val_loss: 0.6318 - val_acc: 0.1880\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6333 - acc: 0.1864 - val_loss: 0.6322 - val_acc: 0.1908\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6334 - acc: 0.1863 - val_loss: 0.6318 - val_acc: 0.1846\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6027 - acc: 0.1894 - val_loss: 0.6010 - val_acc: 0.1903\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6026 - acc: 0.1909 - val_loss: 0.6008 - val_acc: 0.1912\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6026 - acc: 0.1908 - val_loss: 0.6010 - val_acc: 0.1883\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6026 - acc: 0.1902 - val_loss: 0.6013 - val_acc: 0.1945\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6027 - acc: 0.1904 - val_loss: 0.6010 - val_acc: 0.1895\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6027 - acc: 0.1906 - val_loss: 0.6008 - val_acc: 0.1919\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6027 - acc: 0.1903 - val_loss: 0.6008 - val_acc: 0.1887\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6026 - acc: 0.1894 - val_loss: 0.6007 - val_acc: 0.1901\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6026 - acc: 0.1902 - val_loss: 0.6008 - val_acc: 0.1901\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6027 - acc: 0.1900 - val_loss: 0.6008 - val_acc: 0.1903\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6026 - acc: 0.1896 - val_loss: 0.6008 - val_acc: 0.1902\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6026 - acc: 0.1901 - val_loss: 0.6008 - val_acc: 0.1880\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6026 - acc: 0.1898 - val_loss: 0.6015 - val_acc: 0.1858\n",
      "[0.21787938244640828, 0.250811598919332, 0.2863102986738086, 0.32411490359902384, 0.3638629234731197, 0.40490137940645216, 0.4466454157978296, 0.4882319886535406, 0.5287371596843005, 0.5671546121239662, 0.6023253456354142, 0.6331807253360748, 0.658561692237854, 0.6775163673758506, 0.6892321901023388, 0.6931628462374211, 0.6892069027423858, 0.6775235853195191, 0.6586062822341919, 0.6332745454609394, 0.6026025854349136]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2, 2, 21)\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "earlystopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=my_loss_wrapper(theta, MSE_loss=False),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train),\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1,\n",
    "              callbacks=[earlystopping])\n",
    "    lvals += [np.min(model.history.history['loss'])]\n",
    "    vlvals += [np.min(model.history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T00:14:22.169620Z",
     "start_time": "2020-06-08T00:14:21.892848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param, ymin=np.min(lvals), ymax=np.max(lvals), label='Truth')\n",
    "plt.title(\"Detector Effects: Theta vs. Loss\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"Detector Effects: Theta vs. Loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training Fitting Model: Gradient Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T00:14:22.177840Z",
     "start_time": "2020-06-08T00:14:22.171792Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T00:14:22.275409Z",
     "start_time": "2020-06-08T00:14:22.179726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 1         \n",
      "=================================================================\n",
      "Total params: 16,898\n",
      "Trainable params: 16,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1, ))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers) - 1].add_weight(\n",
    "    name=\"thetaX\",\n",
    "    shape=list(),\n",
    "    initializer=keras.initializers.Constant(value=theta_fit_init),\n",
    "    trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = 2 * N\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "\n",
    "def my_loss_wrapper_fit(mysign=1, reweight_analytically=False, MSE_loss=True):\n",
    "    def my_loss(y_true, y_pred):\n",
    "        # Getting theta0:\n",
    "        if mysign == 1:\n",
    "            # when not training theta, fetch as np array\n",
    "            theta0 = model_fit.layers[-1].get_weights()\n",
    "            # regular batch size\n",
    "            y_true = tf.gather(y_true, np.arange(1000))\n",
    "        else:\n",
    "            # when trainingn theta, fetch as tf.Variable\n",
    "            theta0 = model_fit.trainable_weights[-1]\n",
    "            # special theta batch size\n",
    "            y_true = tf.gather(y_true, np.arange(batch_size))\n",
    "\n",
    "        y_labels = tf.gather(y_true, [0], axis=1)  # actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis=1)  # sim truth for reweighting\n",
    "\n",
    "        if reweight_analytically:\n",
    "            # analytical reweight\n",
    "            weights = analytical_reweight(x_T, theta0)\n",
    "        else:\n",
    "            # NN reweight\n",
    "            weights = reweight(x_T, theta0)\n",
    "\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_labels * (y_labels - y_pred)**2 + weights *\n",
    "                               (1. - y_labels) * (y_labels - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_labels) * K.log(y_pred) + weights *\n",
    "                                (1 - y_labels) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:09:24.697547Z",
     "start_time": "2020-06-08T00:14:22.277512Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.5990 - acc: 0.3359 - val_loss: 0.5985 - val_acc: 0.3364\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5985 - acc: 0.3362 - val_loss: 0.5987 - val_acc: 0.3363\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5985 - acc: 0.3362 - val_loss: 0.5983 - val_acc: 0.3363\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5985 - acc: 0.3360 - val_loss: 0.5984 - val_acc: 0.3363\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5984 - val_acc: 0.3363\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5983 - val_acc: 0.3364\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5982 - val_acc: 0.3363\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5983 - val_acc: 0.3363\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5983 - val_acc: 0.3364\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5985 - val_acc: 0.3363\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5982 - val_acc: 0.3363\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5987 - val_acc: 0.3363\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5983 - val_acc: 0.3364\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5982 - val_acc: 0.3363\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5983 - val_acc: 0.3363\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5983 - val_acc: 0.3364\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5984 - val_acc: 0.3364\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5982 - val_acc: 0.3363\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5984 - val_acc: 0.3364\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5983 - val_acc: 0.3364\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5984 - val_acc: 0.3364\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5984 - val_acc: 0.3364\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5983 - val_acc: 0.3363\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5983 - val_acc: 0.3364\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5982 - val_acc: 0.3363\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5983 - acc: 0.3361 - val_loss: 0.5982 - val_acc: 0.3363\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5987 - val_acc: 0.3364\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3362 - val_loss: 0.5982 - val_acc: 0.3364\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.5984 - acc: 0.3361 - val_loss: 0.5983 - val_acc: 0.3364\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 3us/step - loss: -0.5982 - acc: 0.3363\n",
      ". theta fit =  0.49999052\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6641 - acc: 0.3343 - val_loss: 0.6635 - val_acc: 0.3346\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6639 - acc: 0.3345 - val_loss: 0.6636 - val_acc: 0.3358\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6639 - acc: 0.3346 - val_loss: 0.6635 - val_acc: 0.3347\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6639 - acc: 0.3346 - val_loss: 0.6635 - val_acc: 0.3350\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6639 - acc: 0.3346 - val_loss: 0.6635 - val_acc: 0.3346\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3346 - val_loss: 0.6636 - val_acc: 0.3346\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6639 - acc: 0.3346 - val_loss: 0.6635 - val_acc: 0.3349\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3347 - val_loss: 0.6636 - val_acc: 0.3350\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6639 - acc: 0.3347 - val_loss: 0.6635 - val_acc: 0.3345\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6639 - acc: 0.3346 - val_loss: 0.6635 - val_acc: 0.3348\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3346 - val_loss: 0.6636 - val_acc: 0.3347\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6639 - acc: 0.3345 - val_loss: 0.6636 - val_acc: 0.3347\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3347 - val_loss: 0.6635 - val_acc: 0.3343\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3346 - val_loss: 0.6635 - val_acc: 0.3342\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3345 - val_loss: 0.6637 - val_acc: 0.3351\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3345 - val_loss: 0.6635 - val_acc: 0.3338\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3345 - val_loss: 0.6635 - val_acc: 0.3349\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3347 - val_loss: 0.6636 - val_acc: 0.3347\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3345 - val_loss: 0.6637 - val_acc: 0.3351\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6638 - acc: 0.3346 - val_loss: 0.6635 - val_acc: 0.3338\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6636 - acc: 0.3348\n",
      ". theta fit =  0.8720451\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6886 - acc: 0.3275 - val_loss: 0.6879 - val_acc: 0.3321\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3310 - val_loss: 0.6878 - val_acc: 0.3307\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3312 - val_loss: 0.6878 - val_acc: 0.3329\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3310 - val_loss: 0.6878 - val_acc: 0.3301\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3314 - val_loss: 0.6878 - val_acc: 0.3311\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3311 - val_loss: 0.6878 - val_acc: 0.3309\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3311 - val_loss: 0.6879 - val_acc: 0.3292\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3312 - val_loss: 0.6878 - val_acc: 0.3303\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3315 - val_loss: 0.6878 - val_acc: 0.3326\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3316 - val_loss: 0.6878 - val_acc: 0.3328\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3316 - val_loss: 0.6879 - val_acc: 0.3338\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3314 - val_loss: 0.6878 - val_acc: 0.3308\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6883 - acc: 0.3318 - val_loss: 0.6878 - val_acc: 0.3331\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3315 - val_loss: 0.6879 - val_acc: 0.3300\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3313 - val_loss: 0.6878 - val_acc: 0.3308\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6883 - acc: 0.3315 - val_loss: 0.6879 - val_acc: 0.3317\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6880 - acc: 0.3310\n",
      ". theta fit =  1.1914275\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6900 - acc: 0.1851 - val_loss: 0.6889 - val_acc: 0.1807\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1752 - val_loss: 0.6888 - val_acc: 0.1776\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1750 - val_loss: 0.6888 - val_acc: 0.1757\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1744 - val_loss: 0.6888 - val_acc: 0.1797\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1760 - val_loss: 0.6888 - val_acc: 0.1758\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1747 - val_loss: 0.6888 - val_acc: 0.1746\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1746 - val_loss: 0.6887 - val_acc: 0.1777\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1760 - val_loss: 0.6888 - val_acc: 0.1733\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1761 - val_loss: 0.6888 - val_acc: 0.1737\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1747 - val_loss: 0.6888 - val_acc: 0.1753\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1758 - val_loss: 0.6888 - val_acc: 0.1749\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1761 - val_loss: 0.6888 - val_acc: 0.1764\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1752 - val_loss: 0.6888 - val_acc: 0.1791\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1761 - val_loss: 0.6888 - val_acc: 0.1744\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1764 - val_loss: 0.6888 - val_acc: 0.1690\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1751 - val_loss: 0.6888 - val_acc: 0.1800\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.1754 - val_loss: 0.6888 - val_acc: 0.1702\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 3us/step - loss: -0.6891 - acc: 0.1776\n",
      ". theta fit =  0.9008874\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.3124 - val_loss: 0.6890 - val_acc: 0.3354\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3318 - val_loss: 0.6889 - val_acc: 0.3296\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3307 - val_loss: 0.6889 - val_acc: 0.3352\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3310 - val_loss: 0.6889 - val_acc: 0.3337\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3309 - val_loss: 0.6889 - val_acc: 0.3298\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3309 - val_loss: 0.6889 - val_acc: 0.3276\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3301 - val_loss: 0.6889 - val_acc: 0.3344\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3303 - val_loss: 0.6889 - val_acc: 0.3315\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3309 - val_loss: 0.6889 - val_acc: 0.3293\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3302 - val_loss: 0.6889 - val_acc: 0.3290\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3307 - val_loss: 0.6889 - val_acc: 0.3292\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3301 - val_loss: 0.6889 - val_acc: 0.3308\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3305 - val_loss: 0.6889 - val_acc: 0.3301\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3304 - val_loss: 0.6889 - val_acc: 0.3319\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3309 - val_loss: 0.6889 - val_acc: 0.3292\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3303 - val_loss: 0.6889 - val_acc: 0.3280\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3301 - val_loss: 0.6889 - val_acc: 0.3316\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6894 - acc: 0.3300 - val_loss: 0.6889 - val_acc: 0.3310\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3291 - val_loss: 0.6889 - val_acc: 0.3308\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3307 - val_loss: 0.6889 - val_acc: 0.3319\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3306 - val_loss: 0.6889 - val_acc: 0.3329\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3305 - val_loss: 0.6892 - val_acc: 0.3359\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6893 - acc: 0.3310 - val_loss: 0.6889 - val_acc: 0.3328\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6893 - acc: 0.3305 - val_loss: 0.6889 - val_acc: 0.3326\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6891 - acc: 0.3320\n",
      ". theta fit =  1.1736066\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6909 - acc: 0.1942 - val_loss: 0.6894 - val_acc: 0.1715\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6901 - acc: 0.1754 - val_loss: 0.6894 - val_acc: 0.1715\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6901 - acc: 0.1755 - val_loss: 0.6894 - val_acc: 0.1758\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6901 - acc: 0.1751 - val_loss: 0.6894 - val_acc: 0.1704\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1745 - val_loss: 0.6894 - val_acc: 0.1742\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6901 - acc: 0.1749 - val_loss: 0.6894 - val_acc: 0.1744\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6901 - acc: 0.1752 - val_loss: 0.6894 - val_acc: 0.1742\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1749 - val_loss: 0.6894 - val_acc: 0.1740\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1759 - val_loss: 0.6894 - val_acc: 0.1770\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6901 - acc: 0.1751 - val_loss: 0.6894 - val_acc: 0.1735\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1756 - val_loss: 0.6894 - val_acc: 0.1751\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1759 - val_loss: 0.6894 - val_acc: 0.1742\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6901 - acc: 0.1749 - val_loss: 0.6894 - val_acc: 0.1765\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1750 - val_loss: 0.6894 - val_acc: 0.1794\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1757 - val_loss: 0.6894 - val_acc: 0.1759\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1753 - val_loss: 0.6894 - val_acc: 0.1768\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1755 - val_loss: 0.6894 - val_acc: 0.1821\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1761 - val_loss: 0.6894 - val_acc: 0.1724\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6900 - acc: 0.1753 - val_loss: 0.6895 - val_acc: 0.1839\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6900 - acc: 0.1754 - val_loss: 0.6894 - val_acc: 0.1780\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6897 - acc: 0.1735\n",
      ". theta fit =  0.912572\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6902 - acc: 0.3073 - val_loss: 0.6893 - val_acc: 0.3251\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3298 - val_loss: 0.6893 - val_acc: 0.3278\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3305 - val_loss: 0.6893 - val_acc: 0.3346\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3314 - val_loss: 0.6893 - val_acc: 0.3246\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3301 - val_loss: 0.6893 - val_acc: 0.3323\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3318 - val_loss: 0.6893 - val_acc: 0.3288\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3310 - val_loss: 0.6893 - val_acc: 0.3318\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3310 - val_loss: 0.6893 - val_acc: 0.3307\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3307 - val_loss: 0.6893 - val_acc: 0.3312\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3307 - val_loss: 0.6893 - val_acc: 0.3292\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3310 - val_loss: 0.6893 - val_acc: 0.3251\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3312 - val_loss: 0.6893 - val_acc: 0.3306\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6898 - acc: 0.3310 - val_loss: 0.6893 - val_acc: 0.3346\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3308 - val_loss: 0.6893 - val_acc: 0.3344\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3309 - val_loss: 0.6893 - val_acc: 0.3353\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3321 - val_loss: 0.6894 - val_acc: 0.3221\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3308 - val_loss: 0.6893 - val_acc: 0.3338\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6898 - acc: 0.3313 - val_loss: 0.6893 - val_acc: 0.3333\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6895 - acc: 0.3307\n",
      ". theta fit =  1.1657362\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6910 - acc: 0.1834 - val_loss: 0.6898 - val_acc: 0.1784\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1730 - val_loss: 0.6898 - val_acc: 0.1800\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1737 - val_loss: 0.6897 - val_acc: 0.1730\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1740 - val_loss: 0.6897 - val_acc: 0.1718\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1743 - val_loss: 0.6897 - val_acc: 0.1756\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1745 - val_loss: 0.6897 - val_acc: 0.1773\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1744 - val_loss: 0.6897 - val_acc: 0.1782\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1739 - val_loss: 0.6897 - val_acc: 0.1701\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1741 - val_loss: 0.6897 - val_acc: 0.1689\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1743 - val_loss: 0.6899 - val_acc: 0.1667\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6903 - acc: 0.1740 - val_loss: 0.6898 - val_acc: 0.1800\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1747 - val_loss: 0.6897 - val_acc: 0.1727\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1748 - val_loss: 0.6897 - val_acc: 0.1731\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1748 - val_loss: 0.6897 - val_acc: 0.1741\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1747 - val_loss: 0.6897 - val_acc: 0.1771\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1744 - val_loss: 0.6897 - val_acc: 0.1758\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1747 - val_loss: 0.6898 - val_acc: 0.1807\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1737 - val_loss: 0.6897 - val_acc: 0.1748\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1741 - val_loss: 0.6897 - val_acc: 0.1764\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1742 - val_loss: 0.6897 - val_acc: 0.1727\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1748 - val_loss: 0.6897 - val_acc: 0.1770\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1749 - val_loss: 0.6897 - val_acc: 0.1707\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6903 - acc: 0.1740 - val_loss: 0.6897 - val_acc: 0.1706\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6900 - acc: 0.1731\n",
      ". theta fit =  0.9178831\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6903 - acc: 0.3097 - val_loss: 0.6895 - val_acc: 0.3242\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3304 - val_loss: 0.6896 - val_acc: 0.3355\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6899 - acc: 0.3300 - val_loss: 0.6895 - val_acc: 0.3353\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3305 - val_loss: 0.6895 - val_acc: 0.3284\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3298 - val_loss: 0.6895 - val_acc: 0.3290\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3300 - val_loss: 0.6895 - val_acc: 0.3321\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3303 - val_loss: 0.6895 - val_acc: 0.3302\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3301 - val_loss: 0.6895 - val_acc: 0.3337\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3307 - val_loss: 0.6895 - val_acc: 0.3319\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3304 - val_loss: 0.6895 - val_acc: 0.3329\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3300 - val_loss: 0.6895 - val_acc: 0.3296\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3312 - val_loss: 0.6895 - val_acc: 0.3282\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3308 - val_loss: 0.6895 - val_acc: 0.3270\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3298 - val_loss: 0.6895 - val_acc: 0.3313\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3311 - val_loss: 0.6895 - val_acc: 0.3325\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3300 - val_loss: 0.6895 - val_acc: 0.3351\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3300 - val_loss: 0.6895 - val_acc: 0.3292\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3299 - val_loss: 0.6895 - val_acc: 0.3284\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3309 - val_loss: 0.6895 - val_acc: 0.3309\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3309 - val_loss: 0.6895 - val_acc: 0.3298\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3291 - val_loss: 0.6895 - val_acc: 0.3301\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3303 - val_loss: 0.6895 - val_acc: 0.3328\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3297 - val_loss: 0.6895 - val_acc: 0.3318\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3310 - val_loss: 0.6895 - val_acc: 0.3314\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3290 - val_loss: 0.6895 - val_acc: 0.3350\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3303 - val_loss: 0.6895 - val_acc: 0.3279\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6899 - acc: 0.3300 - val_loss: 0.6895 - val_acc: 0.3305\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3304 - val_loss: 0.6895 - val_acc: 0.3308\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6899 - acc: 0.3302 - val_loss: 0.6895 - val_acc: 0.3346\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6897 - acc: 0.3309\n",
      ". theta fit =  1.1622362\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6914 - acc: 0.1992 - val_loss: 0.6899 - val_acc: 0.1777\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1742 - val_loss: 0.6898 - val_acc: 0.1753\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1735 - val_loss: 0.6898 - val_acc: 0.1808\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1747 - val_loss: 0.6898 - val_acc: 0.1722\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1742 - val_loss: 0.6898 - val_acc: 0.1768\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1747 - val_loss: 0.6898 - val_acc: 0.1708\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1739 - val_loss: 0.6898 - val_acc: 0.1736\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1736 - val_loss: 0.6898 - val_acc: 0.1732\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1735 - val_loss: 0.6898 - val_acc: 0.1785\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1737 - val_loss: 0.6898 - val_acc: 0.1688\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1740 - val_loss: 0.6898 - val_acc: 0.1757\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1736 - val_loss: 0.6898 - val_acc: 0.1758\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1736 - val_loss: 0.6899 - val_acc: 0.1669\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1737 - val_loss: 0.6898 - val_acc: 0.1679\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1733 - val_loss: 0.6899 - val_acc: 0.1678\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1736 - val_loss: 0.6899 - val_acc: 0.1675\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1733 - val_loss: 0.6898 - val_acc: 0.1738\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6904 - acc: 0.1734 - val_loss: 0.6898 - val_acc: 0.1695\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6901 - acc: 0.1732\n",
      ". theta fit =  1.1380173\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6911 - acc: 0.1735 - val_loss: 0.6905 - val_acc: 0.1702\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1731 - val_loss: 0.6905 - val_acc: 0.1732\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1738 - val_loss: 0.6905 - val_acc: 0.1698\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1732 - val_loss: 0.6905 - val_acc: 0.1727\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1730 - val_loss: 0.6905 - val_acc: 0.1741\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1731 - val_loss: 0.6905 - val_acc: 0.1769\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1736 - val_loss: 0.6905 - val_acc: 0.1719\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1729 - val_loss: 0.6905 - val_acc: 0.1689\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1730 - val_loss: 0.6905 - val_acc: 0.1790\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1732 - val_loss: 0.6905 - val_acc: 0.1730\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1732 - val_loss: 0.6905 - val_acc: 0.1710\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1730 - val_loss: 0.6905 - val_acc: 0.1715\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1728 - val_loss: 0.6905 - val_acc: 0.1739\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6911 - acc: 0.1730 - val_loss: 0.6905 - val_acc: 0.1746\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6908 - acc: 0.1726\n",
      ". theta fit =  1.1139135\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6916 - acc: 0.1734 - val_loss: 0.6910 - val_acc: 0.1746\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1739 - val_loss: 0.6911 - val_acc: 0.1685\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1727 - val_loss: 0.6911 - val_acc: 0.1656\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1722 - val_loss: 0.6910 - val_acc: 0.1742\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6916 - acc: 0.1728 - val_loss: 0.6910 - val_acc: 0.1736\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1733 - val_loss: 0.6910 - val_acc: 0.1761\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1735 - val_loss: 0.6910 - val_acc: 0.1774\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1730 - val_loss: 0.6910 - val_acc: 0.1742\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1736 - val_loss: 0.6910 - val_acc: 0.1698\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1727 - val_loss: 0.6911 - val_acc: 0.1792\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1737 - val_loss: 0.6910 - val_acc: 0.1694\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1730 - val_loss: 0.6910 - val_acc: 0.1787\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1737 - val_loss: 0.6910 - val_acc: 0.1721\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1729 - val_loss: 0.6910 - val_acc: 0.1706\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1737 - val_loss: 0.6910 - val_acc: 0.1689\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6916 - acc: 0.1732 - val_loss: 0.6910 - val_acc: 0.1707\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6916 - acc: 0.1731 - val_loss: 0.6910 - val_acc: 0.1741\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6916 - acc: 0.1733 - val_loss: 0.6910 - val_acc: 0.1776\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.6913 - acc: 0.1741\n",
      ". theta fit =  1.0898463\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6920 - acc: 0.1738 - val_loss: 0.6914 - val_acc: 0.1691\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1741 - val_loss: 0.6915 - val_acc: 0.1888\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1745 - val_loss: 0.6914 - val_acc: 0.1748\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1736 - val_loss: 0.6915 - val_acc: 0.1662\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1724 - val_loss: 0.6914 - val_acc: 0.1721\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1748 - val_loss: 0.6915 - val_acc: 0.1655\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1731 - val_loss: 0.6914 - val_acc: 0.1690\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1718 - val_loss: 0.6914 - val_acc: 0.1787\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1734 - val_loss: 0.6914 - val_acc: 0.1746\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1740 - val_loss: 0.6914 - val_acc: 0.1740\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1732 - val_loss: 0.6914 - val_acc: 0.1737\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1731 - val_loss: 0.6914 - val_acc: 0.1776\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1725 - val_loss: 0.6914 - val_acc: 0.1756\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1734 - val_loss: 0.6914 - val_acc: 0.1766\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6920 - acc: 0.1738 - val_loss: 0.6914 - val_acc: 0.1706\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6917 - acc: 0.1720\n",
      ". theta fit =  1.0657527\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6916 - val_acc: 0.1678\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6916 - val_acc: 0.1706\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6916 - val_acc: 0.1731\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6916 - val_acc: 0.1838\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6916 - val_acc: 0.1705\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6916 - val_acc: 0.1796\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6916 - val_acc: 0.1670\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1717 - val_loss: 0.6916 - val_acc: 0.1700\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6916 - val_acc: 0.1765\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6916 - val_acc: 0.1721\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6916 - val_acc: 0.1754\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6919 - acc: 0.1706\n",
      ". theta fit =  1.0415914\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6922 - acc: 0.1762 - val_loss: 0.6917 - val_acc: 0.1761\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1807 - val_loss: 0.6917 - val_acc: 0.1764\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1794 - val_loss: 0.6917 - val_acc: 0.1887\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1810 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1786 - val_loss: 0.6917 - val_acc: 0.1648\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1797 - val_loss: 0.6917 - val_acc: 0.1756\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1783 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1795 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1779 - val_loss: 0.6917 - val_acc: 0.1648\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1776 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1805 - val_loss: 0.6917 - val_acc: 0.1640\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1776 - val_loss: 0.6917 - val_acc: 0.1847\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6919 - acc: 0.1763\n",
      ". theta fit =  1.0658461\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6916 - val_acc: 0.1748\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6916 - val_acc: 0.1666\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6916 - val_acc: 0.1701\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6916 - val_acc: 0.1657\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6916 - val_acc: 0.1677\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6916 - val_acc: 0.1675\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6916 - val_acc: 0.1713\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6916 - val_acc: 0.1788\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6916 - val_acc: 0.1705\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6916 - val_acc: 0.1741\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6916 - val_acc: 0.1665\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6916 - val_acc: 0.1654\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6916 - val_acc: 0.1868\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6919 - acc: 0.1700\n",
      ". theta fit =  1.0414052\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6922 - acc: 0.1805 - val_loss: 0.6917 - val_acc: 0.1985\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1795 - val_loss: 0.6917 - val_acc: 0.1782\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1789 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1770 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1799 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1795 - val_loss: 0.6917 - val_acc: 0.1802\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1768 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1776 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1881\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1928\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1766 - val_loss: 0.6917 - val_acc: 0.1801\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1895\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.6919 - acc: 0.1781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  1.066007\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6922 - acc: 0.1729 - val_loss: 0.6916 - val_acc: 0.1718\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1725 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6916 - val_acc: 0.1811\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6916 - val_acc: 0.1665\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6916 - val_acc: 0.1727\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1728 - val_loss: 0.6916 - val_acc: 0.1727\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1929\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6916 - val_acc: 0.1717\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6916 - val_acc: 0.1708\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1729 - val_loss: 0.6916 - val_acc: 0.1825\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6916 - val_acc: 0.1731\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6916 - val_acc: 0.1703\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1729 - val_loss: 0.6917 - val_acc: 0.1867\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1755 - val_loss: 0.6916 - val_acc: 0.1715\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6916 - val_acc: 0.1848\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6916 - val_acc: 0.1825\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6916 - val_acc: 0.1728\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6916 - val_acc: 0.1820\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.6919 - acc: 0.1717\n",
      ". theta fit =  1.0411693\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6922 - acc: 0.1761 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1812 - val_loss: 0.6917 - val_acc: 0.1725\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1786 - val_loss: 0.6917 - val_acc: 0.1733\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1775 - val_loss: 0.6917 - val_acc: 0.1785\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1799 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1765 - val_loss: 0.6917 - val_acc: 0.1844\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1797 - val_loss: 0.6917 - val_acc: 0.1640\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1788 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.6922 - acc: 0.1764 - val_loss: 0.6917 - val_acc: 0.1846\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1769 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1759 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.6919 - acc: 0.1725\n",
      ". theta fit =  1.0661541\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6916 - val_acc: 0.1679\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6916 - val_acc: 0.1767\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1908\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6916 - val_acc: 0.1666\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6916 - val_acc: 0.1717\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6916 - val_acc: 0.1684\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1725 - val_loss: 0.6916 - val_acc: 0.1764\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6916 - val_acc: 0.1754\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6916 - val_acc: 0.1754\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6916 - val_acc: 0.1830\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6916 - val_acc: 0.1769\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6916 - val_acc: 0.1770\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6916 - val_acc: 0.1654\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1729 - val_loss: 0.6916 - val_acc: 0.1697\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6916 - val_acc: 0.1743\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1728 - val_loss: 0.6916 - val_acc: 0.1750\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6916 - val_acc: 0.1745\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1725 - val_loss: 0.6916 - val_acc: 0.1812\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1729 - val_loss: 0.6916 - val_acc: 0.1852\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6916 - val_acc: 0.1666\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6916 - val_acc: 0.1675\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1716 - val_loss: 0.6916 - val_acc: 0.1786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1717 - val_loss: 0.6916 - val_acc: 0.1781\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6916 - val_acc: 0.1702\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1717 - val_loss: 0.6916 - val_acc: 0.1858\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6916 - val_acc: 0.1662\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1716 - val_loss: 0.6916 - val_acc: 0.1873\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6916 - val_acc: 0.1797\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6916 - val_acc: 0.1818\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6916 - val_acc: 0.1784\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6916 - val_acc: 0.1808\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.6919 - acc: 0.1674\n",
      ". theta fit =  1.0408399\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1769 - val_loss: 0.6917 - val_acc: 0.1901\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1771 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1784 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1775 - val_loss: 0.6917 - val_acc: 0.2006\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1778 - val_loss: 0.6917 - val_acc: 0.1816\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1767 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1776 - val_loss: 0.6917 - val_acc: 0.1847\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1883\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1773 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6917 - val_acc: 0.1847\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1865\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6919 - acc: 0.1656\n",
      ". theta fit =  1.0433931\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1655\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1767 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1725 - val_loss: 0.6917 - val_acc: 0.1903\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1910\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1640\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1781 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1785 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1776 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.2235\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1797 - val_loss: 0.6917 - val_acc: 0.1675\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1723 - val_loss: 0.6917 - val_acc: 0.1802\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1862\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6917 - val_acc: 0.1836\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1755 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1815\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6919 - acc: 0.1656\n",
      ". theta fit =  1.0459727\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6922 - acc: 0.1755 - val_loss: 0.6917 - val_acc: 0.1665\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1764 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1767 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.2174\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1883\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1647\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1762 - val_loss: 0.6917 - val_acc: 0.1668\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1723 - val_loss: 0.6917 - val_acc: 0.1831\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1761 - val_loss: 0.6917 - val_acc: 0.1795\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1670\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6919 - acc: 0.1662\n",
      ". theta fit =  1.0485679\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1672\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1755 - val_loss: 0.6917 - val_acc: 0.1680\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1686\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1725 - val_loss: 0.6917 - val_acc: 0.1640\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1817\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1644\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1761 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1678\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1706 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6919 - acc: 0.1686\n",
      ". theta fit =  1.0459338\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1992\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6917 - val_acc: 0.1855\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1818\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1755 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1832\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.1640\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1804\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1854\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1764 - val_loss: 0.6917 - val_acc: 0.1742\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1807\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1829\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1766 - val_loss: 0.6917 - val_acc: 0.1644\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1821\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1755 - val_loss: 0.6917 - val_acc: 0.1869\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1790 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1726 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1889\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1898\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6919 - acc: 0.1741\n",
      ". theta fit =  1.0485882\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.1935\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1834\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1846\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1680\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1756 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1702 - val_loss: 0.6917 - val_acc: 0.1928\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1665\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1719 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1712 - val_loss: 0.6917 - val_acc: 0.1938\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1782 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1666\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1708 - val_loss: 0.6917 - val_acc: 0.1890\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1782\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1673\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1725 - val_loss: 0.6917 - val_acc: 0.1662\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1777 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6917 - val_acc: 0.1975\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1763 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.2009\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6917 - val_acc: 0.1660\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6919 - acc: 0.1666\n",
      ". theta fit =  1.0512863\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6922 - acc: 0.1756 - val_loss: 0.6917 - val_acc: 0.1662\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1709 - val_loss: 0.6917 - val_acc: 0.1939\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1718 - val_loss: 0.6917 - val_acc: 0.1830\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1704 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1710 - val_loss: 0.6917 - val_acc: 0.1659\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1718 - val_loss: 0.6917 - val_acc: 0.1766\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1710 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1655\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1723 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6919 - acc: 0.1662\n",
      ". theta fit =  1.0485563\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1680\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1699 - val_loss: 0.6917 - val_acc: 0.1680\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.1836\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1655\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1655\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1723 - val_loss: 0.6917 - val_acc: 0.1881\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1791\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1676\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1857\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1706 - val_loss: 0.6917 - val_acc: 0.1931\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1678\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.2088\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6917 - val_acc: 0.1655\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1655\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1714 - val_loss: 0.6917 - val_acc: 0.1773\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.2010\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1696 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1714 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1702 - val_loss: 0.6917 - val_acc: 0.1991\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1929\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6919 - acc: 0.1656\n",
      ". theta fit =  1.0512246\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1640\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1700 - val_loss: 0.6917 - val_acc: 0.1764\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1701 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1691 - val_loss: 0.6917 - val_acc: 0.1958\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1875\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1726 - val_loss: 0.6917 - val_acc: 0.1674\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1705 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1711 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1687 - val_loss: 0.6917 - val_acc: 0.1648\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1720 - val_loss: 0.6917 - val_acc: 0.1664\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1866\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1751\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1674\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1844\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1655\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1653\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1801\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1665\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.2018\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1766 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6919 - acc: 0.1750\n",
      ". theta fit =  1.048425\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1709 - val_loss: 0.6917 - val_acc: 0.1974\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1839\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1871\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1891\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1868\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1941\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1942\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6919 - acc: 0.1659\n",
      ". theta fit =  1.0455977\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1784\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1726 - val_loss: 0.6917 - val_acc: 0.1927\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1761 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1710 - val_loss: 0.6917 - val_acc: 0.1896\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1774 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1758 - val_loss: 0.6917 - val_acc: 0.1822\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6919 - acc: 0.1654\n",
      ". theta fit =  1.0484641\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6922 - acc: 0.1718 - val_loss: 0.6917 - val_acc: 0.1853\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1708 - val_loss: 0.6917 - val_acc: 0.1791\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1764 - val_loss: 0.6917 - val_acc: 0.1931\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1763 - val_loss: 0.6917 - val_acc: 0.1795\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1648\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1706 - val_loss: 0.6917 - val_acc: 0.1990\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1768 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.1903\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6919 - acc: 0.1663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  1.048176\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6922 - acc: 0.1712 - val_loss: 0.6917 - val_acc: 0.1900\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1758 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.2112\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1787\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1865\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1822\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1927\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1710 - val_loss: 0.6917 - val_acc: 0.1857\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6917 - val_acc: 0.1683\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6919 - acc: 0.1786\n",
      ". theta fit =  1.048444\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1955\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1763 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1824\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1747 - val_loss: 0.6917 - val_acc: 0.1662\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1667\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1812\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1725 - val_loss: 0.6917 - val_acc: 0.1760\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1912\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1751\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1726 - val_loss: 0.6917 - val_acc: 0.2023\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1764 - val_loss: 0.6917 - val_acc: 0.1653\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6919 - acc: 0.1661\n",
      ". theta fit =  1.0481478\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1717 - val_loss: 0.6917 - val_acc: 0.1740\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1644\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1707 - val_loss: 0.6917 - val_acc: 0.1841\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1794\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1696 - val_loss: 0.6917 - val_acc: 0.1846\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1950\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1666\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1667\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1761 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6919 - acc: 0.1794\n",
      ". theta fit =  1.048443\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1842\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1660\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1719 - val_loss: 0.6917 - val_acc: 0.1818\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1821\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1808\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1772 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1723 - val_loss: 0.6917 - val_acc: 0.1789\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1666\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1821\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1774 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1768 - val_loss: 0.6917 - val_acc: 0.1662\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1653\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1830\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1747 - val_loss: 0.6917 - val_acc: 0.1895\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1770 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1973\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6919 - acc: 0.1666\n",
      ". theta fit =  1.0481404\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1768\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1712 - val_loss: 0.6917 - val_acc: 0.1849\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1836\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1824\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1720 - val_loss: 0.6917 - val_acc: 0.1871\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1673\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6919 - acc: 0.1767\n",
      ". theta fit =  1.0478337\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1770\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6917 - val_acc: 0.1662\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1668\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1726 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1666\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1894\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1783\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1858\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1760 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1715 - val_loss: 0.6917 - val_acc: 0.1806\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1734 - val_loss: 0.6917 - val_acc: 0.1754\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1784 - val_loss: 0.6917 - val_acc: 0.1669\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1816\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1747 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6919 - acc: 0.1666\n",
      ". theta fit =  1.0475237\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1760 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1819\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6917 - val_acc: 0.1888\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1714 - val_loss: 0.6917 - val_acc: 0.1838\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1760 - val_loss: 0.6917 - val_acc: 0.1660\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1928\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1762 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1766 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1647\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1716 - val_loss: 0.6917 - val_acc: 0.1644\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.1805\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1729 - val_loss: 0.6917 - val_acc: 0.1882\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1772 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1647\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1866\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1665\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1868\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6919 - acc: 0.1651\n",
      ". theta fit =  1.0472125\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1747 - val_loss: 0.6917 - val_acc: 0.1664\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1659\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1850\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1759 - val_loss: 0.6917 - val_acc: 0.1676\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1669\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1796\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1832\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.2068\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1772 - val_loss: 0.6917 - val_acc: 0.1655\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1747 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1872\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6919 - acc: 0.1664\n",
      ". theta fit =  1.0469196\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1840\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1659\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1707 - val_loss: 0.6917 - val_acc: 0.2027\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1740\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1758 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1771 - val_loss: 0.6917 - val_acc: 0.1640\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1709 - val_loss: 0.6917 - val_acc: 0.1966\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1832\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1887\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1939\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1929\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.6919 - acc: 0.1659\n",
      ". theta fit =  1.0466015\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1997\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1855\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1946\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1671\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6917 - val_acc: 0.1774\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1719 - val_loss: 0.6917 - val_acc: 0.1669\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1769 - val_loss: 0.6917 - val_acc: 0.1665\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1762 - val_loss: 0.6917 - val_acc: 0.1662\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1873\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1736 - val_loss: 0.6917 - val_acc: 0.1678\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1756 - val_loss: 0.6917 - val_acc: 0.1830\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1770 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1750 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1672\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1768 - val_loss: 0.6917 - val_acc: 0.2082\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6919 - acc: 0.1664\n",
      ". theta fit =  1.0469238\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1981\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1771 - val_loss: 0.6917 - val_acc: 0.1844\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1770 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1718 - val_loss: 0.6917 - val_acc: 0.1664\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1716 - val_loss: 0.6917 - val_acc: 0.1679\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1671\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1704 - val_loss: 0.6917 - val_acc: 0.1799\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1745 - val_loss: 0.6917 - val_acc: 0.1653\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1807\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1706 - val_loss: 0.6917 - val_acc: 0.1938\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1824 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6919 - acc: 0.1664\n",
      ". theta fit =  1.0466216\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1666\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6917 - val_acc: 0.1860\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.1814\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1669\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1756 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1660\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6918 - val_acc: 0.1649\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1831\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1970\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1788 - val_loss: 0.6917 - val_acc: 0.1671\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1771 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1887\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1766 - val_loss: 0.6917 - val_acc: 0.1941\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1667\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1787 - val_loss: 0.6917 - val_acc: 0.1647\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1747 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1737 - val_loss: 0.6917 - val_acc: 0.2067\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1807 - val_loss: 0.6917 - val_acc: 0.1668\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6917 - val_acc: 0.1638\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1723 - val_loss: 0.6917 - val_acc: 0.1831\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1779 - val_loss: 0.6917 - val_acc: 0.1647\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.2014\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1747 - val_loss: 0.6917 - val_acc: 0.1668\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1717 - val_loss: 0.6917 - val_acc: 0.1678\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6919 - acc: 0.1667\n",
      ". theta fit =  1.0466546\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6922 - acc: 0.1792 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1721 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1678\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1769 - val_loss: 0.6917 - val_acc: 0.1669\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1746\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1796 - val_loss: 0.6917 - val_acc: 0.1786\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1854\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1887\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.2038\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1758 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6919 - acc: 0.1663\n",
      ". theta fit =  1.046688\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6922 - acc: 0.1806 - val_loss: 0.6917 - val_acc: 0.1636\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1662 - val_loss: 0.6917 - val_acc: 0.1711\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1765 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1685\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1660\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1717 - val_loss: 0.6917 - val_acc: 0.1980\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1895\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1680\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1715 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1702 - val_loss: 0.6917 - val_acc: 0.1818\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1720 - val_loss: 0.6917 - val_acc: 0.1670\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1776 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1822\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6919 - acc: 0.1660\n",
      ". theta fit =  1.0467216\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1654\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1717 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1762 - val_loss: 0.6917 - val_acc: 0.1694\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1704 - val_loss: 0.6917 - val_acc: 0.2156\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.2104\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1699 - val_loss: 0.6917 - val_acc: 0.1692\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1718 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1857\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1719 - val_loss: 0.6917 - val_acc: 0.1709\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1771 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1718 - val_loss: 0.6917 - val_acc: 0.1668\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1716 - val_loss: 0.6917 - val_acc: 0.2111\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1676\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1744 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.2037\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1770 - val_loss: 0.6917 - val_acc: 0.1639\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1752\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1766 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1926\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1714 - val_loss: 0.6917 - val_acc: 0.1815\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1741 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1888\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1778 - val_loss: 0.6917 - val_acc: 0.1640\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1755 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6919 - acc: 0.1751\n",
      ". theta fit =  1.0467552\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6922 - acc: 0.1742 - val_loss: 0.6917 - val_acc: 0.1937\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1887\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1765 - val_loss: 0.6917 - val_acc: 0.1646\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1776 - val_loss: 0.6917 - val_acc: 0.1647\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1733 - val_loss: 0.6917 - val_acc: 0.1656\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1730 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1727 - val_loss: 0.6917 - val_acc: 0.1786\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1729 - val_loss: 0.6917 - val_acc: 0.1834\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1788 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1767 - val_loss: 0.6917 - val_acc: 0.1728\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6922 - acc: 0.1768 - val_loss: 0.6917 - val_acc: 0.1845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1816 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1739 - val_loss: 0.6917 - val_acc: 0.1835\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1765 - val_loss: 0.6917 - val_acc: 0.1643\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1781 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1774 - val_loss: 0.6917 - val_acc: 0.1653\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1748 - val_loss: 0.6917 - val_acc: 0.1666\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1763 - val_loss: 0.6917 - val_acc: 0.1662\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1749\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1751 - val_loss: 0.6917 - val_acc: 0.1963\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6919 - acc: 0.1728\n",
      ". theta fit =  1.0467893\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6922 - acc: 0.1764 - val_loss: 0.6917 - val_acc: 0.1945\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1769 - val_loss: 0.6917 - val_acc: 0.2089\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1800 - val_loss: 0.6917 - val_acc: 0.1647\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1758 - val_loss: 0.6917 - val_acc: 0.1651\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1791\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1740 - val_loss: 0.6917 - val_acc: 0.1839\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1771 - val_loss: 0.6917 - val_acc: 0.1637\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1753 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1716 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1724 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1699 - val_loss: 0.6917 - val_acc: 0.1740\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1786 - val_loss: 0.6917 - val_acc: 0.1645\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1771 - val_loss: 0.6917 - val_acc: 0.1650\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1769\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1754 - val_loss: 0.6917 - val_acc: 0.1898\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1768 - val_loss: 0.6917 - val_acc: 0.1661\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1749 - val_loss: 0.6917 - val_acc: 0.1847\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1743 - val_loss: 0.6917 - val_acc: 0.1846\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1796 - val_loss: 0.6917 - val_acc: 0.1642\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1702 - val_loss: 0.6917 - val_acc: 0.1647\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1977\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1757 - val_loss: 0.6917 - val_acc: 0.1859\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1735 - val_loss: 0.6917 - val_acc: 0.1887\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1825\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1752 - val_loss: 0.6917 - val_acc: 0.1652\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1738 - val_loss: 0.6917 - val_acc: 0.1954\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6919 - acc: 0.1660\n",
      ". theta fit =  1.0467547\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.6922 - acc: 0.1760 - val_loss: 0.6917 - val_acc: 0.1657\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1732 - val_loss: 0.6917 - val_acc: 0.1952\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1767 - val_loss: 0.6917 - val_acc: 0.1649\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1762 - val_loss: 0.6917 - val_acc: 0.1663\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1713 - val_loss: 0.6917 - val_acc: 0.1686\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1711 - val_loss: 0.6917 - val_acc: 0.1915\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1760 - val_loss: 0.6917 - val_acc: 0.1641\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1731 - val_loss: 0.6917 - val_acc: 0.1803\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1722 - val_loss: 0.6917 - val_acc: 0.1658\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1756 - val_loss: 0.6917 - val_acc: 0.1934\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6922 - acc: 0.1746 - val_loss: 0.6917 - val_acc: 0.1763\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6919 - acc: 0.1657\n",
      ". theta fit =  1.0467889\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration)\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()\n",
    "\n",
    "    # regular optimizer and batch size\n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(),\n",
    "                      loss=my_loss_wrapper_fit(1, MSE_loss=False),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train),\n",
    "                  y_train,\n",
    "                  epochs=100,\n",
    "                  batch_size=1000,\n",
    "                  validation_data=(np.array(X_test), y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = True\n",
    "\n",
    "    # special optimizer and batch size = 2*N\n",
    "    model_fit.compile(optimizer=optimizer,\n",
    "                      loss=my_loss_wrapper_fit(-1, MSE_loss=False),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta),\n",
    "                  y_train_theta,\n",
    "                  epochs=1,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema\", extrema)\n",
    "    '''\n",
    "\n",
    "    if (len(extrema) == 0\n",
    "        ):  # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema) >= 6):  # If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:09:24.986159Z",
     "start_time": "2020-06-08T02:09:24.700173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(theta1_param, 0, len(fit_vals), label='Truth')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"GaussianAltFit-1D-DetectorEffects (DCTR Reweight)\\n\" +\n",
    "          \"N = {:.0e}, Iterations = {:.0f}\".format(N, iterations))\n",
    "# plt.savefig(\"GaussianAltFit-1D-DetectorEffects (DCTR Reweight)\" +\n",
    "#             \"N = {:.0e}, Iterations = {:.0f}.png\".format(N, iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:09:25.267933Z",
     "start_time": "2020-06-08T02:09:24.988246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "fit_vals = np.array(fit_vals)\n",
    "plt.title(\"GaussianAltFit-1D-DetectorEffects (DCTR Reweight) Zoomed:\\n\" +\n",
    "          \"N = {:.0e}, Iterations {:.0f} to {:.0f}\".format(\n",
    "              N, index_refine[1], iterations))\n",
    "plt.plot(np.arange(index_refine[1], len(fit_vals)),\n",
    "         fit_vals[index_refine[1]:],\n",
    "         label='Model $\\mu$ Fit')\n",
    "plt.hlines(theta1_param, index_refine[1], len(fit_vals), label='$\\mu_{Truth}$')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "# plt.savefig(\"GaussianAltFit-1D-DetectorEffects (DCTR Reweight) Zoomed:\" +\n",
    "#             \"N = {:.0e}, Iterations {:.0f} to {:.0f}.png\".format(\n",
    "#                 N, index_refine[1], iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with analytical reweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T02:09:25.293075Z",
     "start_time": "2020-06-08T02:09:25.272484Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_vals = [theta_fit_init]\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T04:46:59.751307Z",
     "start_time": "2020-06-08T02:09:25.295258Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6930 - acc: 0.1720 - val_loss: 0.6924 - val_acc: 0.1660\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1722 - val_loss: 0.6924 - val_acc: 0.1719\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1724 - val_loss: 0.6924 - val_acc: 0.1729\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1727 - val_loss: 0.6924 - val_acc: 0.1689\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1710 - val_loss: 0.6924 - val_acc: 0.1682\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1726 - val_loss: 0.6924 - val_acc: 0.1694\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1749 - val_loss: 0.6924 - val_acc: 0.1642\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1699 - val_loss: 0.6924 - val_acc: 0.1691\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1726 - val_loss: 0.6924 - val_acc: 0.1840\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1730 - val_loss: 0.6924 - val_acc: 0.1716\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1716 - val_loss: 0.6923 - val_acc: 0.1739\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1719 - val_loss: 0.6924 - val_acc: 0.1736\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1739 - val_loss: 0.6924 - val_acc: 0.1652\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1728 - val_loss: 0.6924 - val_acc: 0.1669\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1728 - val_loss: 0.6924 - val_acc: 0.1840\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1723 - val_loss: 0.6924 - val_acc: 0.1706\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1743 - val_loss: 0.6924 - val_acc: 0.1651\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1700 - val_loss: 0.6924 - val_acc: 0.1682\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1718 - val_loss: 0.6924 - val_acc: 0.1653\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1696 - val_loss: 0.6924 - val_acc: 0.1663\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.1728 - val_loss: 0.6924 - val_acc: 0.1670\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6926 - acc: 0.1739\n",
      ". theta fit =  0.54694784\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6772 - acc: 0.3171 - val_loss: 0.6729 - val_acc: 0.3336\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3334 - val_loss: 0.6729 - val_acc: 0.3327\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3332\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3319\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3333\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3333\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3321\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3331 - val_loss: 0.6729 - val_acc: 0.3335\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3332\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3337\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3329\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3331 - val_loss: 0.6730 - val_acc: 0.3333\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3335\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3330\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3332\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3337\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3333\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3329\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3332 - val_loss: 0.6730 - val_acc: 0.3322\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3331\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3325\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3334\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3333 - val_loss: 0.6730 - val_acc: 0.3332\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3338\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3334\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3342\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3340\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6732 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3325\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3324\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3324\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3324\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3341\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3332 - val_loss: 0.6729 - val_acc: 0.3330\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3333 - val_loss: 0.6729 - val_acc: 0.3340\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6731 - acc: 0.3335 - val_loss: 0.6729 - val_acc: 0.3322\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6730 - acc: 0.3334\n",
      ". theta fit =  0.9190014\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6931 - acc: 0.3335 - val_loss: 0.6920 - val_acc: 0.3343\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3311 - val_loss: 0.6920 - val_acc: 0.3294\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3294 - val_loss: 0.6920 - val_acc: 0.3318\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3291 - val_loss: 0.6920 - val_acc: 0.3266\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3292 - val_loss: 0.6920 - val_acc: 0.3281\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3283 - val_loss: 0.6920 - val_acc: 0.3322\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3282 - val_loss: 0.6921 - val_acc: 0.3361\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3290 - val_loss: 0.6921 - val_acc: 0.3190\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3289 - val_loss: 0.6921 - val_acc: 0.3341\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3288 - val_loss: 0.6920 - val_acc: 0.3281\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3296 - val_loss: 0.6920 - val_acc: 0.3296\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3288 - val_loss: 0.6920 - val_acc: 0.3278\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6925 - acc: 0.3281 - val_loss: 0.6921 - val_acc: 0.3248\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6923 - acc: 0.3318\n",
      ". theta fit =  1.238337\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6897 - acc: 0.1969 - val_loss: 0.6871 - val_acc: 0.1711\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6878 - acc: 0.1745 - val_loss: 0.6869 - val_acc: 0.1715\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1751 - val_loss: 0.6868 - val_acc: 0.1745\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1753 - val_loss: 0.6868 - val_acc: 0.1719\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1752 - val_loss: 0.6867 - val_acc: 0.1752\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1752 - val_loss: 0.6867 - val_acc: 0.1770\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1752 - val_loss: 0.6867 - val_acc: 0.1757\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1752 - val_loss: 0.6868 - val_acc: 0.1807\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1752 - val_loss: 0.6868 - val_acc: 0.1791\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1756 - val_loss: 0.6867 - val_acc: 0.1754\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1755 - val_loss: 0.6867 - val_acc: 0.1758\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1755 - val_loss: 0.6867 - val_acc: 0.1772\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1754 - val_loss: 0.6867 - val_acc: 0.1757\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1753 - val_loss: 0.6867 - val_acc: 0.1749\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1756 - val_loss: 0.6867 - val_acc: 0.1752\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1754 - val_loss: 0.6867 - val_acc: 0.1743\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1755 - val_loss: 0.6867 - val_acc: 0.1748\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1753 - val_loss: 0.6868 - val_acc: 0.1805\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1755 - val_loss: 0.6867 - val_acc: 0.1770\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1756 - val_loss: 0.6867 - val_acc: 0.1738\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1753 - val_loss: 0.6867 - val_acc: 0.1740\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1755 - val_loss: 0.6867 - val_acc: 0.1769\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6876 - acc: 0.1757 - val_loss: 0.6868 - val_acc: 0.1723\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.6871 - acc: 0.1756\n",
      ". theta fit =  0.94779325\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6939 - acc: 0.2397 - val_loss: 0.6925 - val_acc: 0.3222\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3250 - val_loss: 0.6924 - val_acc: 0.3230\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3271 - val_loss: 0.6924 - val_acc: 0.3358\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3262 - val_loss: 0.6924 - val_acc: 0.3291\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3271 - val_loss: 0.6924 - val_acc: 0.3271\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3267 - val_loss: 0.6927 - val_acc: 0.2737\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3261 - val_loss: 0.6924 - val_acc: 0.3361\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3265 - val_loss: 0.6924 - val_acc: 0.3284\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3273 - val_loss: 0.6925 - val_acc: 0.3070\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3253 - val_loss: 0.6924 - val_acc: 0.3149\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3263 - val_loss: 0.6924 - val_acc: 0.3142\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3254 - val_loss: 0.6924 - val_acc: 0.3346\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3263 - val_loss: 0.6925 - val_acc: 0.3351\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3274 - val_loss: 0.6924 - val_acc: 0.3299\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3247 - val_loss: 0.6924 - val_acc: 0.3330\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3275 - val_loss: 0.6924 - val_acc: 0.3302\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3255 - val_loss: 0.6924 - val_acc: 0.3364\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3278 - val_loss: 0.6924 - val_acc: 0.3360\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3262 - val_loss: 0.6924 - val_acc: 0.3355\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3270 - val_loss: 0.6924 - val_acc: 0.3127\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3263 - val_loss: 0.6924 - val_acc: 0.3231\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3272 - val_loss: 0.6924 - val_acc: 0.3175\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3266 - val_loss: 0.6924 - val_acc: 0.3171\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6929 - acc: 0.3265 - val_loss: 0.6924 - val_acc: 0.3352\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6926 - acc: 0.3299\n",
      ". theta fit =  1.2204499\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6901 - acc: 0.1959 - val_loss: 0.6876 - val_acc: 0.1755\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1752 - val_loss: 0.6876 - val_acc: 0.1719\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1749 - val_loss: 0.6876 - val_acc: 0.1781\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1751 - val_loss: 0.6876 - val_acc: 0.1761\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1752 - val_loss: 0.6876 - val_acc: 0.1732\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1750 - val_loss: 0.6877 - val_acc: 0.1835\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1754 - val_loss: 0.6876 - val_acc: 0.1746\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1752 - val_loss: 0.6876 - val_acc: 0.1729\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1750 - val_loss: 0.6876 - val_acc: 0.1763\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1753 - val_loss: 0.6876 - val_acc: 0.1732\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1750 - val_loss: 0.6876 - val_acc: 0.1735\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1751 - val_loss: 0.6876 - val_acc: 0.1772\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1753 - val_loss: 0.6876 - val_acc: 0.1761\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1753 - val_loss: 0.6876 - val_acc: 0.1788\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1751 - val_loss: 0.6876 - val_acc: 0.1769\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1752 - val_loss: 0.6876 - val_acc: 0.1784\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6884 - acc: 0.1754 - val_loss: 0.6876 - val_acc: 0.1760\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6879 - acc: 0.1746\n",
      ". theta fit =  0.9594095\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6939 - acc: 0.2038 - val_loss: 0.6926 - val_acc: 0.2905\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3202 - val_loss: 0.6925 - val_acc: 0.3045\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3239 - val_loss: 0.6926 - val_acc: 0.3344\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3263 - val_loss: 0.6925 - val_acc: 0.3294\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3222 - val_loss: 0.6925 - val_acc: 0.3204\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3221 - val_loss: 0.6926 - val_acc: 0.3349\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3250 - val_loss: 0.6926 - val_acc: 0.2976\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3243 - val_loss: 0.6925 - val_acc: 0.3281\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3241 - val_loss: 0.6925 - val_acc: 0.3229\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3235 - val_loss: 0.6925 - val_acc: 0.3235\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3257 - val_loss: 0.6925 - val_acc: 0.3124\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3256 - val_loss: 0.6925 - val_acc: 0.3257\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3254 - val_loss: 0.6925 - val_acc: 0.3326\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.3255 - val_loss: 0.6925 - val_acc: 0.3145\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6927 - acc: 0.3294\n",
      ". theta fit =  1.2124909\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.6905 - acc: 0.1988 - val_loss: 0.6879 - val_acc: 0.1761\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1756 - val_loss: 0.6879 - val_acc: 0.1746\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1750 - val_loss: 0.6879 - val_acc: 0.1759\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1751 - val_loss: 0.6881 - val_acc: 0.1693\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1748 - val_loss: 0.6879 - val_acc: 0.1737\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1750 - val_loss: 0.6879 - val_acc: 0.1758\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1751 - val_loss: 0.6879 - val_acc: 0.1728\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1751 - val_loss: 0.6880 - val_acc: 0.1716\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1752 - val_loss: 0.6879 - val_acc: 0.1771\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1750 - val_loss: 0.6879 - val_acc: 0.1733\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1750 - val_loss: 0.6879 - val_acc: 0.1732\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6887 - acc: 0.1751 - val_loss: 0.6880 - val_acc: 0.1817\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6883 - acc: 0.1746\n",
      ". theta fit =  0.9646326\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6938 - acc: 0.1876 - val_loss: 0.6927 - val_acc: 0.1769\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.2709 - val_loss: 0.6928 - val_acc: 0.2542\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3123 - val_loss: 0.6925 - val_acc: 0.3347\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3222 - val_loss: 0.6925 - val_acc: 0.3250\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3199 - val_loss: 0.6925 - val_acc: 0.3341\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3233 - val_loss: 0.6925 - val_acc: 0.3363\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3255 - val_loss: 0.6925 - val_acc: 0.3193\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3218 - val_loss: 0.6925 - val_acc: 0.3336\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3218 - val_loss: 0.6925 - val_acc: 0.3271\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3236 - val_loss: 0.6926 - val_acc: 0.3074\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3223 - val_loss: 0.6925 - val_acc: 0.3356\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3251 - val_loss: 0.6925 - val_acc: 0.3172\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3227 - val_loss: 0.6925 - val_acc: 0.3320\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3237 - val_loss: 0.6925 - val_acc: 0.3253\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3221 - val_loss: 0.6926 - val_acc: 0.2922\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3226 - val_loss: 0.6925 - val_acc: 0.3326\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3253 - val_loss: 0.6926 - val_acc: 0.2963\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3233 - val_loss: 0.6925 - val_acc: 0.3363\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.3213 - val_loss: 0.6925 - val_acc: 0.3345\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6928 - acc: 0.3272\n",
      ". theta fit =  1.2088852\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6909 - acc: 0.2041 - val_loss: 0.6881 - val_acc: 0.1744\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1748 - val_loss: 0.6881 - val_acc: 0.1768\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1753 - val_loss: 0.6881 - val_acc: 0.1735\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1746 - val_loss: 0.6881 - val_acc: 0.1788\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1752 - val_loss: 0.6881 - val_acc: 0.1748\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6889 - acc: 0.1749 - val_loss: 0.6881 - val_acc: 0.1735\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1748 - val_loss: 0.6881 - val_acc: 0.1745\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6888 - acc: 0.1750 - val_loss: 0.6881 - val_acc: 0.1737\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6888 - acc: 0.1750 - val_loss: 0.6881 - val_acc: 0.1761\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1750 - val_loss: 0.6881 - val_acc: 0.1750\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1750 - val_loss: 0.6881 - val_acc: 0.1743\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1746 - val_loss: 0.6881 - val_acc: 0.1738\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1751 - val_loss: 0.6881 - val_acc: 0.1726\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1751 - val_loss: 0.6881 - val_acc: 0.1734\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1747 - val_loss: 0.6881 - val_acc: 0.1783\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1750 - val_loss: 0.6881 - val_acc: 0.1752\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1751 - val_loss: 0.6881 - val_acc: 0.1761\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6888 - acc: 0.1750 - val_loss: 0.6881 - val_acc: 0.1717\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1749 - val_loss: 0.6881 - val_acc: 0.1750\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1748 - val_loss: 0.6881 - val_acc: 0.1740\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6889 - acc: 0.1750 - val_loss: 0.6881 - val_acc: 0.1747\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.6884 - acc: 0.1742\n",
      ". theta fit =  1.1846657\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6898 - acc: 0.1744 - val_loss: 0.6891 - val_acc: 0.1791\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1748 - val_loss: 0.6890 - val_acc: 0.1735\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1747 - val_loss: 0.6890 - val_acc: 0.1742\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1749 - val_loss: 0.6891 - val_acc: 0.1708\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1745 - val_loss: 0.6890 - val_acc: 0.1759\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1750 - val_loss: 0.6890 - val_acc: 0.1757\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1746 - val_loss: 0.6892 - val_acc: 0.1825\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1747 - val_loss: 0.6890 - val_acc: 0.1770\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1750 - val_loss: 0.6891 - val_acc: 0.1783\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1746 - val_loss: 0.6891 - val_acc: 0.1803\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1746 - val_loss: 0.6891 - val_acc: 0.1821\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1747 - val_loss: 0.6891 - val_acc: 0.1774\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6898 - acc: 0.1749 - val_loss: 0.6890 - val_acc: 0.1734\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6894 - acc: 0.1742\n",
      ". theta fit =  1.160561\n",
      "Iteration:  11\n",
      "Training g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.6906 - acc: 0.1745 - val_loss: 0.6899 - val_acc: 0.1706\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1744 - val_loss: 0.6899 - val_acc: 0.1728\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1745 - val_loss: 0.6899 - val_acc: 0.1778\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1743 - val_loss: 0.6899 - val_acc: 0.1719\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1746 - val_loss: 0.6899 - val_acc: 0.1787\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1751 - val_loss: 0.6899 - val_acc: 0.1724\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1743 - val_loss: 0.6900 - val_acc: 0.1845\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1746 - val_loss: 0.6899 - val_acc: 0.1786\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1747 - val_loss: 0.6899 - val_acc: 0.1718\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1742 - val_loss: 0.6899 - val_acc: 0.1792\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1747 - val_loss: 0.6899 - val_acc: 0.1729\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6906 - acc: 0.1747 - val_loss: 0.6899 - val_acc: 0.1724\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6902 - acc: 0.1728\n",
      ". theta fit =  1.1364919\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6907 - val_acc: 0.1771\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1744 - val_loss: 0.6907 - val_acc: 0.1688\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1741 - val_loss: 0.6907 - val_acc: 0.1683\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6906 - val_acc: 0.1768\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1746 - val_loss: 0.6907 - val_acc: 0.1688\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1744 - val_loss: 0.6907 - val_acc: 0.1692\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6907 - val_acc: 0.1667\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1741 - val_loss: 0.6907 - val_acc: 0.1704\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6906 - val_acc: 0.1746\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1743 - val_loss: 0.6907 - val_acc: 0.1694\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1746 - val_loss: 0.6907 - val_acc: 0.1678\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6906 - val_acc: 0.1720\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6907 - val_acc: 0.1681\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1739 - val_loss: 0.6906 - val_acc: 0.1759\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6907 - val_acc: 0.1779\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1741 - val_loss: 0.6907 - val_acc: 0.1809\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1743 - val_loss: 0.6906 - val_acc: 0.1751\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6906 - val_acc: 0.1731\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1740 - val_loss: 0.6906 - val_acc: 0.1749\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1743 - val_loss: 0.6907 - val_acc: 0.1696\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1739 - val_loss: 0.6906 - val_acc: 0.1710\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1743 - val_loss: 0.6906 - val_acc: 0.1731\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6906 - val_acc: 0.1745\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6907 - val_acc: 0.1695\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1738 - val_loss: 0.6906 - val_acc: 0.1737\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1737 - val_loss: 0.6907 - val_acc: 0.1779\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1741 - val_loss: 0.6906 - val_acc: 0.1732\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1741 - val_loss: 0.6906 - val_acc: 0.1762\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6906 - val_acc: 0.1721\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1737 - val_loss: 0.6907 - val_acc: 0.1780\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1741 - val_loss: 0.6906 - val_acc: 0.1769\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1744 - val_loss: 0.6906 - val_acc: 0.1743\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1739 - val_loss: 0.6907 - val_acc: 0.1768\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1742 - val_loss: 0.6906 - val_acc: 0.1738\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6913 - acc: 0.1739 - val_loss: 0.6906 - val_acc: 0.1767\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6910 - acc: 0.1737\n",
      ". theta fit =  1.112395\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6919 - acc: 0.1738 - val_loss: 0.6913 - val_acc: 0.1752\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1741 - val_loss: 0.6913 - val_acc: 0.1739\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1742 - val_loss: 0.6913 - val_acc: 0.1672\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1737 - val_loss: 0.6913 - val_acc: 0.1771\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1740 - val_loss: 0.6913 - val_acc: 0.1735\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1737 - val_loss: 0.6913 - val_acc: 0.1770\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1738 - val_loss: 0.6913 - val_acc: 0.1724\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1739 - val_loss: 0.6913 - val_acc: 0.1819\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1744 - val_loss: 0.6913 - val_acc: 0.1706\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1738 - val_loss: 0.6913 - val_acc: 0.1782\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1744 - val_loss: 0.6913 - val_acc: 0.1713\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1735 - val_loss: 0.6913 - val_acc: 0.1826\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1739 - val_loss: 0.6913 - val_acc: 0.1746\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1739 - val_loss: 0.6913 - val_acc: 0.1756\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6919 - acc: 0.1740 - val_loss: 0.6913 - val_acc: 0.1802\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.6916 - acc: 0.1734\n",
      ". theta fit =  1.0882194\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6924 - acc: 0.1739 - val_loss: 0.6918 - val_acc: 0.1754\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1739 - val_loss: 0.6918 - val_acc: 0.1700\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1737 - val_loss: 0.6918 - val_acc: 0.1829\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1742 - val_loss: 0.6918 - val_acc: 0.1751\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1742 - val_loss: 0.6918 - val_acc: 0.1802\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1738 - val_loss: 0.6918 - val_acc: 0.1744\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1738 - val_loss: 0.6918 - val_acc: 0.1790\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1739 - val_loss: 0.6918 - val_acc: 0.1742\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1744 - val_loss: 0.6918 - val_acc: 0.1693\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1738 - val_loss: 0.6918 - val_acc: 0.1754\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1737 - val_loss: 0.6918 - val_acc: 0.1776\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1740 - val_loss: 0.6918 - val_acc: 0.1822\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1741 - val_loss: 0.6918 - val_acc: 0.1732\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1742 - val_loss: 0.6918 - val_acc: 0.1761\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1738 - val_loss: 0.6918 - val_acc: 0.1790\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1743 - val_loss: 0.6918 - val_acc: 0.1662\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1732 - val_loss: 0.6918 - val_acc: 0.1730\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1740 - val_loss: 0.6918 - val_acc: 0.1713\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1739 - val_loss: 0.6918 - val_acc: 0.1757\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1740 - val_loss: 0.6918 - val_acc: 0.1699\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1742 - val_loss: 0.6918 - val_acc: 0.1689\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1735 - val_loss: 0.6918 - val_acc: 0.1753\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6924 - acc: 0.1738 - val_loss: 0.6918 - val_acc: 0.1743\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6921 - acc: 0.1732\n",
      ". theta fit =  1.0639231\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6928 - acc: 0.1743 - val_loss: 0.6922 - val_acc: 0.1788\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1742 - val_loss: 0.6922 - val_acc: 0.1773\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1748 - val_loss: 0.6922 - val_acc: 0.1637\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1740 - val_loss: 0.6922 - val_acc: 0.1828\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1748 - val_loss: 0.6922 - val_acc: 0.1667\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1739 - val_loss: 0.6922 - val_acc: 0.1850\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1747 - val_loss: 0.6922 - val_acc: 0.1667\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1754 - val_loss: 0.6922 - val_acc: 0.1694\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1739 - val_loss: 0.6922 - val_acc: 0.1703\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1746 - val_loss: 0.6922 - val_acc: 0.1859\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1746 - val_loss: 0.6922 - val_acc: 0.1652\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1740 - val_loss: 0.6922 - val_acc: 0.1736\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1743 - val_loss: 0.6922 - val_acc: 0.1909\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1745 - val_loss: 0.6922 - val_acc: 0.1723\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1741 - val_loss: 0.6922 - val_acc: 0.1749\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1743 - val_loss: 0.6922 - val_acc: 0.1668\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1734 - val_loss: 0.6922 - val_acc: 0.1675\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1738 - val_loss: 0.6922 - val_acc: 0.1949\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1758 - val_loss: 0.6923 - val_acc: 0.1637\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1741 - val_loss: 0.6922 - val_acc: 0.1810\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1748 - val_loss: 0.6922 - val_acc: 0.1827\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6928 - acc: 0.1745 - val_loss: 0.6922 - val_acc: 0.1780\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6924 - acc: 0.1736\n",
      ". theta fit =  1.0394717\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6930 - acc: 0.1756 - val_loss: 0.6925 - val_acc: 0.1642\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1749 - val_loss: 0.6924 - val_acc: 0.1772\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1782 - val_loss: 0.6924 - val_acc: 0.1717\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1753 - val_loss: 0.6924 - val_acc: 0.1801\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1766 - val_loss: 0.6925 - val_acc: 0.1638\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1757 - val_loss: 0.6924 - val_acc: 0.1737\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1750 - val_loss: 0.6924 - val_acc: 0.1679\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1754 - val_loss: 0.6924 - val_acc: 0.1680\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1749 - val_loss: 0.6924 - val_acc: 0.1720\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1754 - val_loss: 0.6925 - val_acc: 0.2015\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1773 - val_loss: 0.6924 - val_acc: 0.1765\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1767 - val_loss: 0.6924 - val_acc: 0.1783\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6930 - acc: 0.1758 - val_loss: 0.6924 - val_acc: 0.1795\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6927 - acc: 0.1717\n",
      ". theta fit =  1.0148377\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.6932 - acc: 0.1870 - val_loss: 0.6926 - val_acc: 0.1637\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1855 - val_loss: 0.6926 - val_acc: 0.2006\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1876 - val_loss: 0.6926 - val_acc: 0.1929\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1887 - val_loss: 0.6926 - val_acc: 0.1670\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1902 - val_loss: 0.6926 - val_acc: 0.1638\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1944 - val_loss: 0.6926 - val_acc: 0.2466\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1921 - val_loss: 0.6926 - val_acc: 0.1804\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1932 - val_loss: 0.6926 - val_acc: 0.1751\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1901 - val_loss: 0.6926 - val_acc: 0.1914\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1944 - val_loss: 0.6926 - val_acc: 0.1783\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1875 - val_loss: 0.6926 - val_acc: 0.1714\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1939 - val_loss: 0.6926 - val_acc: 0.2071\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1929 - val_loss: 0.6926 - val_acc: 0.2047\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1861 - val_loss: 0.6926 - val_acc: 0.1836\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1903 - val_loss: 0.6926 - val_acc: 0.1637\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1838 - val_loss: 0.6926 - val_acc: 0.1638\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1897 - val_loss: 0.6926 - val_acc: 0.2127\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1936 - val_loss: 0.6926 - val_acc: 0.1652\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6929 - acc: 0.1750\n",
      ". theta fit =  0.990008\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2577 - val_loss: 0.6926 - val_acc: 0.3130\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2970 - val_loss: 0.6926 - val_acc: 0.3231\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3001 - val_loss: 0.6926 - val_acc: 0.3275\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3057 - val_loss: 0.6926 - val_acc: 0.3108\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3051 - val_loss: 0.6926 - val_acc: 0.3351\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3076 - val_loss: 0.6927 - val_acc: 0.2790\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3084 - val_loss: 0.6926 - val_acc: 0.3108\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3099 - val_loss: 0.6926 - val_acc: 0.2787\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2894 - val_loss: 0.6927 - val_acc: 0.3019\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3084 - val_loss: 0.6926 - val_acc: 0.2829\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3085 - val_loss: 0.6926 - val_acc: 0.3338\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3039 - val_loss: 0.6926 - val_acc: 0.2902\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2950 - val_loss: 0.6926 - val_acc: 0.3309\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2961 - val_loss: 0.6927 - val_acc: 0.2863\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3066 - val_loss: 0.6926 - val_acc: 0.3303\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3040 - val_loss: 0.6926 - val_acc: 0.3239\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3057 - val_loss: 0.6926 - val_acc: 0.3362\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3077 - val_loss: 0.6927 - val_acc: 0.2516\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.3023 - val_loss: 0.6926 - val_acc: 0.3162\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3033 - val_loss: 0.6926 - val_acc: 0.2854\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2998 - val_loss: 0.6926 - val_acc: 0.2795\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2919 - val_loss: 0.6927 - val_acc: 0.2743\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3034 - val_loss: 0.6927 - val_acc: 0.2516\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3098 - val_loss: 0.6926 - val_acc: 0.3209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3127 - val_loss: 0.6926 - val_acc: 0.3362\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6929 - acc: 0.3304\n",
      ". theta fit =  1.0150506\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2364 - val_loss: 0.6926 - val_acc: 0.1819\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1915 - val_loss: 0.6926 - val_acc: 0.2296\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1857 - val_loss: 0.6926 - val_acc: 0.1648\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1904 - val_loss: 0.6926 - val_acc: 0.1637\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1854 - val_loss: 0.6926 - val_acc: 0.2322\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6931 - acc: 0.1902 - val_loss: 0.6926 - val_acc: 0.2405\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1870 - val_loss: 0.6926 - val_acc: 0.2318\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1947 - val_loss: 0.6926 - val_acc: 0.1719\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1873 - val_loss: 0.6926 - val_acc: 0.1665\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1901 - val_loss: 0.6926 - val_acc: 0.1878\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1864 - val_loss: 0.6926 - val_acc: 0.1871\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1870 - val_loss: 0.6926 - val_acc: 0.2062\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1894 - val_loss: 0.6926 - val_acc: 0.2018\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1896 - val_loss: 0.6926 - val_acc: 0.2470\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1967 - val_loss: 0.6926 - val_acc: 0.1699\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1862 - val_loss: 0.6927 - val_acc: 0.2497\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2007 - val_loss: 0.6926 - val_acc: 0.2067\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1857 - val_loss: 0.6926 - val_acc: 0.1801\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.6929 - acc: 0.1719\n",
      ". theta fit =  0.9897404\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2591 - val_loss: 0.6926 - val_acc: 0.2552\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3059 - val_loss: 0.6926 - val_acc: 0.2889\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2984 - val_loss: 0.6926 - val_acc: 0.2887\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2958 - val_loss: 0.6926 - val_acc: 0.3107\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3056 - val_loss: 0.6926 - val_acc: 0.3363\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3106 - val_loss: 0.6926 - val_acc: 0.2635\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3040 - val_loss: 0.6926 - val_acc: 0.2622\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3122 - val_loss: 0.6926 - val_acc: 0.2620\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3037 - val_loss: 0.6926 - val_acc: 0.2719\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.3036 - val_loss: 0.6926 - val_acc: 0.2530\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2968 - val_loss: 0.6926 - val_acc: 0.2975\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3124 - val_loss: 0.6926 - val_acc: 0.2891\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3000 - val_loss: 0.6926 - val_acc: 0.3331\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3089 - val_loss: 0.6926 - val_acc: 0.3349\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.3151 - val_loss: 0.6926 - val_acc: 0.3347\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3095 - val_loss: 0.6926 - val_acc: 0.3111\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2966 - val_loss: 0.6926 - val_acc: 0.3363\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3094 - val_loss: 0.6926 - val_acc: 0.3103\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3185 - val_loss: 0.6926 - val_acc: 0.2930\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2991 - val_loss: 0.6927 - val_acc: 0.2994\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3102 - val_loss: 0.6926 - val_acc: 0.3046\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3095 - val_loss: 0.6926 - val_acc: 0.3077\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3068 - val_loss: 0.6926 - val_acc: 0.2688\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3054 - val_loss: 0.6927 - val_acc: 0.3045\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3061 - val_loss: 0.6926 - val_acc: 0.3315\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3085 - val_loss: 0.6926 - val_acc: 0.3358\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3067 - val_loss: 0.6926 - val_acc: 0.3119\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3125 - val_loss: 0.6926 - val_acc: 0.3181\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3061 - val_loss: 0.6926 - val_acc: 0.3319\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3043 - val_loss: 0.6926 - val_acc: 0.3099\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3088 - val_loss: 0.6926 - val_acc: 0.3316\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3012 - val_loss: 0.6926 - val_acc: 0.3358\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2981 - val_loss: 0.6927 - val_acc: 0.2906\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3060 - val_loss: 0.6926 - val_acc: 0.3348\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3097 - val_loss: 0.6926 - val_acc: 0.2937\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3120 - val_loss: 0.6926 - val_acc: 0.3358\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3158 - val_loss: 0.6926 - val_acc: 0.3288\n",
      "Epoch 38/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3055 - val_loss: 0.6926 - val_acc: 0.3339\n",
      "Epoch 39/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3082 - val_loss: 0.6926 - val_acc: 0.3166\n",
      "Epoch 40/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3100 - val_loss: 0.6926 - val_acc: 0.2704\n",
      "Epoch 41/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3032 - val_loss: 0.6926 - val_acc: 0.3087\n",
      "Epoch 42/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3009 - val_loss: 0.6926 - val_acc: 0.3356\n",
      "Epoch 43/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3087 - val_loss: 0.6926 - val_acc: 0.3231\n",
      "Epoch 44/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3055 - val_loss: 0.6926 - val_acc: 0.3364\n",
      "Epoch 45/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.2977 - val_loss: 0.6926 - val_acc: 0.3113\n",
      "Epoch 46/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3155 - val_loss: 0.6926 - val_acc: 0.3362\n",
      "Epoch 47/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.3116 - val_loss: 0.6926 - val_acc: 0.3031\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6929 - acc: 0.3289\n",
      ". theta fit =  1.015292\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2388 - val_loss: 0.6926 - val_acc: 0.2498\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1950 - val_loss: 0.6926 - val_acc: 0.1707\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1856 - val_loss: 0.6926 - val_acc: 0.1830\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1838 - val_loss: 0.6926 - val_acc: 0.1870\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1864 - val_loss: 0.6926 - val_acc: 0.1783\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1909 - val_loss: 0.6926 - val_acc: 0.1754\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6931 - acc: 0.1879 - val_loss: 0.6926 - val_acc: 0.1637\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1858 - val_loss: 0.6926 - val_acc: 0.1800\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1925 - val_loss: 0.6926 - val_acc: 0.1897\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1848 - val_loss: 0.6926 - val_acc: 0.1981\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1886 - val_loss: 0.6926 - val_acc: 0.2017\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1837 - val_loss: 0.6927 - val_acc: 0.2478\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1950 - val_loss: 0.6926 - val_acc: 0.2148\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1861 - val_loss: 0.6926 - val_acc: 0.2103\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1881 - val_loss: 0.6926 - val_acc: 0.2101\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6929 - acc: 0.1782\n",
      ". theta fit =  0.9894358\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2357 - val_loss: 0.6926 - val_acc: 0.2827\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2942 - val_loss: 0.6928 - val_acc: 0.2534\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3083 - val_loss: 0.6926 - val_acc: 0.3294\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3111 - val_loss: 0.6926 - val_acc: 0.3186\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3130 - val_loss: 0.6926 - val_acc: 0.3225\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3112 - val_loss: 0.6926 - val_acc: 0.3350\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3096 - val_loss: 0.6926 - val_acc: 0.3350\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3087 - val_loss: 0.6926 - val_acc: 0.3239\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3090 - val_loss: 0.6926 - val_acc: 0.3246\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3097 - val_loss: 0.6926 - val_acc: 0.2888\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3140 - val_loss: 0.6926 - val_acc: 0.3357\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3194 - val_loss: 0.6926 - val_acc: 0.2851\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3051 - val_loss: 0.6926 - val_acc: 0.3172\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6929 - acc: 0.3295\n",
      ". theta fit =  1.0155534\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2356 - val_loss: 0.6926 - val_acc: 0.1747\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1833 - val_loss: 0.6926 - val_acc: 0.2117\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1860 - val_loss: 0.6926 - val_acc: 0.1693\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1942 - val_loss: 0.6926 - val_acc: 0.1645\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1849 - val_loss: 0.6926 - val_acc: 0.2063\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1875 - val_loss: 0.6926 - val_acc: 0.1713\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1818 - val_loss: 0.6926 - val_acc: 0.1640\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1860 - val_loss: 0.6926 - val_acc: 0.1652\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1795 - val_loss: 0.6926 - val_acc: 0.2038\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1980 - val_loss: 0.6926 - val_acc: 0.1636\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1812 - val_loss: 0.6926 - val_acc: 0.1919\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1840 - val_loss: 0.6926 - val_acc: 0.2053\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1874 - val_loss: 0.6926 - val_acc: 0.2027\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.1814 - val_loss: 0.6926 - val_acc: 0.1946\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1892 - val_loss: 0.6926 - val_acc: 0.1673\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6931 - acc: 0.1888 - val_loss: 0.6926 - val_acc: 0.1662\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.6929 - acc: 0.1712\n",
      ". theta fit =  0.9891108\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2318 - val_loss: 0.6926 - val_acc: 0.2951\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2965 - val_loss: 0.6926 - val_acc: 0.3262\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3077 - val_loss: 0.6926 - val_acc: 0.3363\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3031 - val_loss: 0.6927 - val_acc: 0.2839\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3079 - val_loss: 0.6926 - val_acc: 0.3059\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3055 - val_loss: 0.6926 - val_acc: 0.3361\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.3152 - val_loss: 0.6926 - val_acc: 0.3208\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.3175 - val_loss: 0.6926 - val_acc: 0.2655\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3090 - val_loss: 0.6926 - val_acc: 0.3361\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3099 - val_loss: 0.6927 - val_acc: 0.2881\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3001 - val_loss: 0.6926 - val_acc: 0.3046\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3077 - val_loss: 0.6926 - val_acc: 0.3153\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.3263\n",
      ". theta fit =  0.9917836\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.3019 - val_loss: 0.6926 - val_acc: 0.3169\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3061 - val_loss: 0.6926 - val_acc: 0.3228\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3041 - val_loss: 0.6927 - val_acc: 0.2502\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3024 - val_loss: 0.6926 - val_acc: 0.3014\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3024 - val_loss: 0.6926 - val_acc: 0.2973\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2973 - val_loss: 0.6926 - val_acc: 0.3349\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2868 - val_loss: 0.6926 - val_acc: 0.3363\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3090 - val_loss: 0.6927 - val_acc: 0.2502\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2931 - val_loss: 0.6926 - val_acc: 0.3363\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3025 - val_loss: 0.6926 - val_acc: 0.3328\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3020 - val_loss: 0.6926 - val_acc: 0.3359\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3004 - val_loss: 0.6927 - val_acc: 0.2502\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2985 - val_loss: 0.6927 - val_acc: 0.2615\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.3036 - val_loss: 0.6926 - val_acc: 0.3260\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2941 - val_loss: 0.6926 - val_acc: 0.3127\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3028 - val_loss: 0.6926 - val_acc: 0.3360\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2963 - val_loss: 0.6926 - val_acc: 0.3351\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2932 - val_loss: 0.6926 - val_acc: 0.3117\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2972 - val_loss: 0.6927 - val_acc: 0.2793\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3085 - val_loss: 0.6926 - val_acc: 0.3362\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.6929 - acc: 0.3329\n",
      ". theta fit =  0.9944855\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2980 - val_loss: 0.6927 - val_acc: 0.2518\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.3011 - val_loss: 0.6926 - val_acc: 0.3312\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2863 - val_loss: 0.6927 - val_acc: 0.2550\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2800 - val_loss: 0.6926 - val_acc: 0.2820\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2863 - val_loss: 0.6926 - val_acc: 0.3212\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2930 - val_loss: 0.6926 - val_acc: 0.2552\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2872 - val_loss: 0.6926 - val_acc: 0.3337\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2927 - val_loss: 0.6926 - val_acc: 0.2727\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2834 - val_loss: 0.6926 - val_acc: 0.2919\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2907 - val_loss: 0.6926 - val_acc: 0.2557\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2987 - val_loss: 0.6926 - val_acc: 0.2554\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.3024 - val_loss: 0.6926 - val_acc: 0.2521\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.3312\n",
      ". theta fit =  0.99721456\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2994 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2579 - val_loss: 0.6926 - val_acc: 0.2891\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2740 - val_loss: 0.6926 - val_acc: 0.3042\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2841 - val_loss: 0.6926 - val_acc: 0.2673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2910 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2651 - val_loss: 0.6926 - val_acc: 0.2515\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2788 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2753 - val_loss: 0.6926 - val_acc: 0.2872\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2831 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2561 - val_loss: 0.6926 - val_acc: 0.2503\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2614 - val_loss: 0.6926 - val_acc: 0.3264\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2717 - val_loss: 0.6926 - val_acc: 0.3353\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2778 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2703 - val_loss: 0.6926 - val_acc: 0.2614\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2645 - val_loss: 0.6927 - val_acc: 0.2501\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2632 - val_loss: 0.6926 - val_acc: 0.2801\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2691 - val_loss: 0.6926 - val_acc: 0.3244\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2847 - val_loss: 0.6926 - val_acc: 0.2502\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2806 - val_loss: 0.6926 - val_acc: 0.2559\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2714 - val_loss: 0.6926 - val_acc: 0.2511\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2674 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2708 - val_loss: 0.6926 - val_acc: 0.2941\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.3353\n",
      ". theta fit =  0.9999674\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2655 - val_loss: 0.6926 - val_acc: 0.2521\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2600 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2602 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2475 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2611 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2467 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2550 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2526 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2422 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2561 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2453 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2648 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.9971867\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.6932 - acc: 0.2592 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2810 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2742 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2775 - val_loss: 0.6926 - val_acc: 0.2506\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2820 - val_loss: 0.6926 - val_acc: 0.2572\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2627 - val_loss: 0.6926 - val_acc: 0.3020\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2640 - val_loss: 0.6926 - val_acc: 0.2908\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2769 - val_loss: 0.6926 - val_acc: 0.3360\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2758 - val_loss: 0.6926 - val_acc: 0.2609\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2764 - val_loss: 0.6926 - val_acc: 0.2506\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2688 - val_loss: 0.6926 - val_acc: 0.2502\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2706 - val_loss: 0.6926 - val_acc: 0.2583\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2642 - val_loss: 0.6926 - val_acc: 0.2616\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2660 - val_loss: 0.6926 - val_acc: 0.2510\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2870 - val_loss: 0.6926 - val_acc: 0.2556\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2701 - val_loss: 0.6926 - val_acc: 0.2517\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2832 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2726 - val_loss: 0.6927 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.3359\n",
      ". theta fit =  0.999982\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6932 - acc: 0.2649 - val_loss: 0.6926 - val_acc: 0.2581\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2668 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2469 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2435 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2667 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2451 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2480 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2483 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2455 - val_loss: 0.6926 - val_acc: 0.2504\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2533 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.99713296\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6932 - acc: 0.2609 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2720 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2690 - val_loss: 0.6926 - val_acc: 0.2881\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2728 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2717 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2788 - val_loss: 0.6926 - val_acc: 0.2503\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2779 - val_loss: 0.6926 - val_acc: 0.2758\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2654 - val_loss: 0.6926 - val_acc: 0.2609\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2840 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2661 - val_loss: 0.6926 - val_acc: 0.2513\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2781 - val_loss: 0.6926 - val_acc: 0.2502\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2755 - val_loss: 0.6926 - val_acc: 0.2863\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2771 - val_loss: 0.6926 - val_acc: 0.2502\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.2881\n",
      ". theta fit =  0.99980724\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6932 - acc: 0.2720 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2526 - val_loss: 0.6926 - val_acc: 0.2508\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2576 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2565 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2509 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2484 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2489 - val_loss: 0.6926 - val_acc: 0.2782\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2617 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2416 - val_loss: 0.6926 - val_acc: 0.2500\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2598 - val_loss: 0.6926 - val_acc: 0.2892\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2574 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2563 - val_loss: 0.6926 - val_acc: 0.2500\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2479 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2514 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2543 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2479 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2640 - val_loss: 0.6926 - val_acc: 0.2677\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2515 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2512 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2429 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2590 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2575 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2620 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2517 - val_loss: 0.6926 - val_acc: 0.2523\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2638 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2507 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2677\n",
      ". theta fit =  0.9968932\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6932 - acc: 0.2642 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2678 - val_loss: 0.6926 - val_acc: 0.3192\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2616 - val_loss: 0.6926 - val_acc: 0.2550\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2808 - val_loss: 0.6926 - val_acc: 0.2944\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2808 - val_loss: 0.6926 - val_acc: 0.2508\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2721 - val_loss: 0.6926 - val_acc: 0.3014\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2776 - val_loss: 0.6927 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2765 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2745 - val_loss: 0.6926 - val_acc: 0.3223\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2642 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2886 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2881 - val_loss: 0.6926 - val_acc: 0.3364\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.3193\n",
      ". theta fit =  0.99952614\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6932 - acc: 0.2567 - val_loss: 0.6926 - val_acc: 0.2697\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2658 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2707 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2487 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2550 - val_loss: 0.6926 - val_acc: 0.2711\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2576 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2686 - val_loss: 0.6926 - val_acc: 0.2454\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2509 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2577 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2613 - val_loss: 0.6927 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2412 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2628 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2525 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2441 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2711\n",
      ". theta fit =  0.99922836\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6932 - acc: 0.2526 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2580 - val_loss: 0.6926 - val_acc: 0.2502\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2620 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2550 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2607 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2513 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2670 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2541 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2590 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2567 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2528 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2545 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2512 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2569 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2693 - val_loss: 0.6926 - val_acc: 0.3362\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2530 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2541 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2652 - val_loss: 0.6926 - val_acc: 0.3354\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2545 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2521 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2620 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2484 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2624 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2585 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2617 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2567 - val_loss: 0.6926 - val_acc: 0.3117\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2564 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2570 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.3354\n",
      ". theta fit =  0.99892735\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.6932 - acc: 0.2613 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2621 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2544 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2658 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2521 - val_loss: 0.6926 - val_acc: 0.2529\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2712 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2658 - val_loss: 0.6927 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2543 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2587 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2528 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2591 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2574 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.99862283\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2593 - val_loss: 0.6927 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2563 - val_loss: 0.6926 - val_acc: 0.2678\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2591 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2571 - val_loss: 0.6926 - val_acc: 0.2952\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2557 - val_loss: 0.6926 - val_acc: 0.2510\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2582 - val_loss: 0.6926 - val_acc: 0.2781\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2711 - val_loss: 0.6926 - val_acc: 0.2980\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2592 - val_loss: 0.6926 - val_acc: 0.2803\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2587 - val_loss: 0.6926 - val_acc: 0.3343\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2686 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2556 - val_loss: 0.6926 - val_acc: 0.2505\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2689 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2597 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2615 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2696 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2600 - val_loss: 0.6926 - val_acc: 0.2510\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2618 - val_loss: 0.6926 - val_acc: 0.2504\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2735 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2598 - val_loss: 0.6926 - val_acc: 0.2583\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.3341\n",
      ". theta fit =  0.9983166\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2535 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2769 - val_loss: 0.6926 - val_acc: 0.2880\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2692 - val_loss: 0.6926 - val_acc: 0.2533\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2714 - val_loss: 0.6926 - val_acc: 0.3231\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2643 - val_loss: 0.6926 - val_acc: 0.3271\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2701 - val_loss: 0.6926 - val_acc: 0.2671\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2716 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2612 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2666 - val_loss: 0.6926 - val_acc: 0.2503\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2731 - val_loss: 0.6926 - val_acc: 0.3188\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2671 - val_loss: 0.6926 - val_acc: 0.2574\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2678 - val_loss: 0.6926 - val_acc: 0.2503\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2651 - val_loss: 0.6926 - val_acc: 0.2641\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2658 - val_loss: 0.6926 - val_acc: 0.2710\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2634 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2618 - val_loss: 0.6926 - val_acc: 0.2523\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2685 - val_loss: 0.6926 - val_acc: 0.2936\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2614 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2673 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2666 - val_loss: 0.6926 - val_acc: 0.2506\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.3187\n",
      ". theta fit =  0.9980123\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2593 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2641 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2803 - val_loss: 0.6926 - val_acc: 0.2502\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2616 - val_loss: 0.6926 - val_acc: 0.3361\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2658 - val_loss: 0.6926 - val_acc: 0.3354\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2639 - val_loss: 0.6926 - val_acc: 0.2502\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2743 - val_loss: 0.6926 - val_acc: 0.2507\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2692 - val_loss: 0.6926 - val_acc: 0.2516\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2674 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2576 - val_loss: 0.6926 - val_acc: 0.3039\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2652 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2678 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2714 - val_loss: 0.6926 - val_acc: 0.3352\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2751 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2651 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2541 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2729 - val_loss: 0.6926 - val_acc: 0.2762\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2692 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2638 - val_loss: 0.6926 - val_acc: 0.2539\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2577 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2574 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2557 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2606 - val_loss: 0.6926 - val_acc: 0.2934\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.6929 - acc: 0.3352\n",
      ". theta fit =  0.9977044\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2678 - val_loss: 0.6926 - val_acc: 0.2500\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2661 - val_loss: 0.6926 - val_acc: 0.2699\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2661 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2657 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2668 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2631 - val_loss: 0.6926 - val_acc: 0.2508\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2636 - val_loss: 0.6926 - val_acc: 0.2907\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2689 - val_loss: 0.6926 - val_acc: 0.2956\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2719 - val_loss: 0.6926 - val_acc: 0.2766\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2654 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2634 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2651 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2558 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2554 - val_loss: 0.6926 - val_acc: 0.3317\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2575 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2523 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2510 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2523\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2511 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2496 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.3314\n",
      ". theta fit =  0.997387\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2517 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2557 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2504 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2496 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2494 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2506 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.99706507\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2503 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.9967398\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2503 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2504 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2504 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2503 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2494 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2503 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.9964112\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2496 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2505 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2495 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Training theta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.9960793\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2504 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.99574417\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2504 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.99540246\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2505 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2495 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2495 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2503 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2503 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.99506074\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2504 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2496 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.9947158\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2504 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2503 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6927 - val_acc: 0.2501\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.6932 - acc: 0.2498 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2495 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6927 - val_acc: 0.2499\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2497 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2496 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2502 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2500 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2499 - val_loss: 0.6926 - val_acc: 0.2501\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.6932 - acc: 0.2501 - val_loss: 0.6926 - val_acc: 0.2499\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.6929 - acc: 0.2500\n",
      ". theta fit =  0.99436766\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration)\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()\n",
    "\n",
    "    # regular optimizer and batch size\n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(),\n",
    "                      loss=my_loss_wrapper_fit(1,\n",
    "                                               reweight_analytically=True,\n",
    "                                               MSE_loss=False),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train),\n",
    "                  y_train,\n",
    "                  epochs=100,\n",
    "                  batch_size=1000,\n",
    "                  validation_data=(np.array(X_test), y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = True\n",
    "\n",
    "    # special optimizer and batch size = 2*N\n",
    "    model_fit.compile(optimizer=optimizer,\n",
    "                      loss=my_loss_wrapper_fit(-1,\n",
    "                                               reweight_analytically=True,\n",
    "                                               MSE_loss=False),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta),\n",
    "                  y_train_theta,\n",
    "                  epochs=1,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "\n",
    "    extrema = np.concatenate(\n",
    "        (argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema\", extrema)\n",
    "    '''\n",
    "\n",
    "    if (len(extrema) == 0\n",
    "        ):  # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema) >= 6):  # If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T04:46:59.967898Z",
     "start_time": "2020-06-08T04:46:59.752951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(theta1_param, 0, len(fit_vals), label='Truth')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"GaussianAltFit-1D-DetectorEffects (Analytical Reweight)\\n\" +\n",
    "          \"N = {:.0e}, Iterations = {:.0f}\".format(N, iterations))\n",
    "# plt.savefig(\"GaussianAltFit-1D-DetectorEffects (Analytical Reweight)\" +\n",
    "#             \"N = {:.0e}, Iterations = {:.0f}.png\".format(N, iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T04:47:00.195986Z",
     "start_time": "2020-06-08T04:46:59.969513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAElCAYAAAC1aab7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VeWd+PHPN3sCAUISkkDYFGRLAAXBXawKBFBrdUZtrdrWKq0z+mtrW2emM1qrHWfqdNpOV+u4tHbU1largiIuiNYVlB1kE5QlC2ENIZDl+/vjeW64udyb3Kz3Jvm+X6+8cu9Zn3vOued7zvc893lEVTHGGGPiVUKsC2CMMcY0xwKVMcaYuGaByhhjTFyzQGWMMSauWaAyxhgT1yxQGWOMiWu9NlCJyAsicn0XrUtFZFQb5hsmIlUiktgZ5TLhiUi6iDwnIgdE5E9+2D0iskdESmNYrlQRWSciBV2wrhkisqMd81eJyEntLMMjInJPe5bRmUTkn0XkwSinvUtEHmvFsvNEZL2IpLa9hPGpLfu1ywKViFwtIu+KyGERKfevvy4i0lVlCKaqJar6aEctT0RGikiDiPyqhelO2Ekisk1Ejvgvd+BvsKp+oqp9VbXeT7dERG5sYfkXiMhr/iS7Lcx49fugSkQqReQVEbmqhWXeICL1QWX7WEQeFpFTmpsvZBktlj3K5YzwnyGpA5ZRFfIX2A5XAnlAtqr+nYgMA74FjFfV/Hast10nf+AmYKmq7g5Z7l3+80xvx7LbLNy+9cft1k5cZ/AxeVBEVorIvM5aXziq+kNVbfcxDY3ngIuCll0GvIbb5+GmHxbm+K0SkToRebUjyhRPuiRQici3gJ8CPwLycSeB+cDZQEpXlKELXAfsA65q41XQJf7LHfjb1cZyHAYeAr7dzDSTVLUvMAZ4BPi5iNzZwnLf9vP0By4CjgDLRaSojeWMiZAANyBkmz/phw8HNqpqnX8/DKhU1fIuLeyJ5gO/Dx7gL/SuA/b6/71J4JgcAPwSeEJEBsS4TB3pD8DN4UYEXcQ2/gFn4b6XP+zKQnYJVe3UP9yJ7TBwRQvTzQU+BA4CnwJ3BY2bAewImX4bcJF/PQ1Y5uctA37sh6cBjwGVwH7gfSDPj1sC3Ohfnwy86qfbgztABoSs63ZgFXAAeBJICxovwBbga379V4aUVYFRuKujWuAYUAU8F/pZQuYb4edNAu4F6oEaP+/PW9ieFwHbwgxXYFTIsCv9crMjLOsG4M0ww58Hngp6fwbwlt/WK4EZfnjYsgNjgcW4k+xHwN8HLSsd+C9gu9/mb/phn/jPUOX/zsRdcH3PT1sO/A7oH7INv+LnXRq8XcN8pu/7/VPrl38z7svf4N8/0txn9eMGAg8Du3AXL88AfUKWUwUMJsKxG6Zcw/z8SSHDz/PDv4A7flNC9xtwvy/Hx0BJ0PgvAeuBQ8BW4OZw3zncRc+fQ9b7M9zFZ6R923icRdqXftyfgFI/fCkwIWgdjwD3RHNMAhl+nadHcTxeAKwOmm4x8H7Q+zeAz/rXg4E/AxV++90aNN1dwGNB76/zn7ES+FeanqPuAv6IOzYPAWuBqX7c7/1xccRvw+/44UlANTA8ivNsP2Aj8L2gYanAT3DH4S7/OjVo/FeBzbjv37PA4JDzxNeBTb68P8CdJ9/CHat/pOmxNg9Y4bf1W8DEoHGnAh/45TwJPBFpv0b8fK2ZuC1/wGygjjAnhZDpZgDFuJPORNyX9rNB45oLVG8DX/Sv+wJn+Nc3A8/5gzgRmAL08+OWcDxQjQIu9js2F/eF+UnIut7zB+1A3Jd7ftD4c4GjQBbwP/gAFLLTA1/aR0J3ElEEqtAyR7HdWxOokv0+KomwrBsIH6i+DJT510NwX9A5fh9e7N/nhis77sT9Ke5kmeQP5j249BrAL/w8Q/y+O8vvnybbJKgcm4GT/P7/C/D7kG34O7/O9HDLCPlcd9H0BDSDoOMvis+6APeFzPLb9vxmjuOwx26YMs0F1oYZ/r+4k0ayL8MVQeNuwAXcr/pt+DXcCUuClnky7kLrfNxJ8bTQsgIFuIvNAf59Eu6CYEqk45Kmx3zYfRm07zI5flJdEbSMR4giUPll3oK7wBjU0j7yx0ANkOO3Wxmw05cjHRcwsv18y4F/w2V+TsIF9FmhxwkwHhdkzvHT3u+3fXCgqvHlSQT+HXgninPAKuDSKL7vf8YddxI07G7gHWCQ/9xvAT/w4z6D+76d5rf9/+DSysH776+4ADgBd357xW+D/sA64Ho/7an+eJjuP9v1/vOk+m2xHfiG39ZX+u1yT9C69gPnNPf5uiL1lwPs0eNpFETkLRHZ75/LnAegqktUdbWqNqjqKuBx3JcnGrXAKBHJUdUqVX0naHg27gtTr6rLVfVg6MyqullVF6vqUVWtAH4cZt0/U9VdqroXF/wmB427HnhBVfcB/wfMFpFBUZY94Bm/TfaLyDOtnLddVLUWd9AObOWsu4LmuRZYqKoL/T5cjLtTmBNh3nm4QPqwqtap6oe4L9vfiUgC7gR2m6ru9PvuLVU9GmFZX8DdiWxV1Srgn4CrQ9J8d6nqYVU9EjRsT9A23y8i46L83BE/q6/oUIK7kNmnqrWq+nozy4p07IYagLsibSQiGcDfAf/n9+FTnJj+266qv1X3nPNRXNDJA1DVBaq6RZ3XgZdwF11NqHsmttSvC9zF5x5VXd7M5wqUsdl9qaoPqeoh//4uYJKI9G9pud4ZIrIfFwDuB67V4+nZiPvIHwPv4+5Gp+Dutv6GexRxBrBJVSuB03EXH3er6jF1z9x+C1wdpixX4i5Q31TVY7jgpiHTvOnLU4+7i5oUxWc8hNv3EflHK1NwFzzB6/wCcLeqlvvz2veBLwaNe0hVP/Db/p+AM0VkRND8/6mqB1V1LbAGeMl/xw4AL+ACFLhM0W9U9V2/fx/FBbYz/F8y7sK/VlWfwm37Rqo6QFXfbO4zdkWgqgRygk8aqnqWqg7w4xIARGS6rwRQISIHcPn4nCjX8RXgFGCDiLwf9FD198AiXO56l4j8p4gkh87sa9g8ISI7ReQgLl0Yuu7g2l7VuKtfRCQd9wX+g/9sb+NSTJ+PsuwBn/U7bICqfjaaGcTVOgo8RP11K9cXvJxk3BXXXhE5N2iZa1uYdQgubQDuuc7fBZ/4cVeXkWqoDQemh0z/BdwzzBxc2nZLlB9hMO6qLWA77qo/L2jYp2Hmywna5gNUdX2U62vusw4F9vqLlmhEOnZD7cNd8Qe7HHcnvNC//wNQIiK5QdM0HreqWu1fBo7dEhF5R0T2+s8wh8jfuUdxJ3/8/99HmC5UxH0pIokicp+IbPHfu21B80TjHX8eycKlroKDbEvH4+u4u8bz/OsluIvT8/37wDIGhyzjn2l6XAUMJugY89u6MmSa0HNIWhSVgjJxdxxhicg5uAB0pb+IDi1T6PdicLhx/gKvEvedDigLen0kzPu+/vVw4Fsh22moX8dgYGdIAA0uU1S6IlC9jYuul7Uw3f/hDrahqtof+DUuJQEu7ZARmFBcde3GL6OqblLVa3C3uP8BPCUifXwE/76qjselG+YR/oHzD3FXP8Wq2g/3RYy2NuLluNvjX4pIqbjqy0Nwd1nhhF5ltUaTedXVOgo8TJ3fjuVehjvhvaeqbwQtc0IL812Oy+eD+5L+PuTE30dV7wtXdj/96yHT91XVr+Hu7mpwaalQ4bbfLtyXJWCY/zzBX6z2bPdQzX3WT4GBER7qn1CGSMdumHlXASNDTmzX404Wn/jj7k+4q9cWL5J8hZ8/4+5E8vwJfyGRj/tngIm+8sw8/IVZpM8VpLl9+XncsXcRLp00IlC8lsofzJ9kvwZ8UUQCV/ktHY+hgep1TgxUnwIfhywjU1XDZQl2A4WBN/4CNrs1HyN0gN/Xo3B3fCcQkTxcivl2VV0WZpJw34td4cb5Yy4blwJtrU+Be0O2U4aqPo7bLkN8pZ/gcrRKpwcqVd2Pi/i/FJErRSRTRBJEZDLumUFAJu5KtEZEptH0y7YRd/Ux11/9fw+X/wRARK4VkVxVbeD41UeDuKraxT6wHcSlWRrCFDMTl18+ICJDaL7GXKjrcbXsinHpwMm4FMIkESkOM30ZLs/bFi3O67dtGu6EJSKSJiJha1aKyEAR+QLuGcJ/+HRHs/xV8EgR+R/cF/37ftRjwCUiMstPkyauOnbgyxta9ueBU0TkiyKS7P9OF5Fxfj8+BPxYRAb75Z3pT64VuH0YvKzHgW/4cvXFXXg8qUHp5g4W8bP6NNkLuOM9y3+u84K2QbYEpbYiHbuhK1TVHbjncNP8fEOAC3FBI3DcTcIFu2hq/6XgvkMVQJ2IlAAzI02sqjW41OL/4S5oPgkaHfG4bGFfZuIuYitxF6Jtrq3m7yYexKXcoOXj8S1crddp/vOsxd/l49Kc4J5LHxKR74r7bV2iiBSJyOlhivCUX99Z/vt2F60LuOG24TRcevyEOxB/TnsCeFVVI2VTHge+JyK5IpKD2zaPBY37kohM9vvih8C7qrqtFWUO+C0wX1xWTESkjz9XZ+JuVOqAW/134XP+c7WORvFgviP+cGmd93C3vBXAu7jcZooffyXulvAQ7iT2c5o+0L4BF53LcTXwtnH8QeVjfngVrjZNoBLGNbjaZIdxB8LPCFMxAfewcLmffwXuNzPBD88b16VBD1Fxd051uDux0M+7ELhfT3ywPJrjtWOeCbf8oGWMoGllijNxQXsf7plZuO08w88T/LckaLz67VGFS9u9Bny+hX13A65mV5WfdzsuFTQuZLrpuKvRvX4fLwCGRSo77kSxwE9biat5OdmPS8c9XN/J8RphgZpid/t59uNy4Am4L+GnfvhjQFa4bRgyrCrk75vB+zdkm4ZWgmjusw7026fMf96/BM33EMdroQ4mwrEbYT/cAvzKv74DWB5mmsG4C7IiwlSCoemxeIsv435cKq+xNlaEz3yOn/9LIcPD7dvg9YTdl7i7wb/ivvPbcQE2eL5HiLLWnx9WiAt8E1vaR37828BrQe+fAtaH2Z6P49J2+3CVE4IrSISeoz7heK2/ncC5EaYdQdPv9mV+3v24OyRwF5C3Rvj85/n5qznxOF7rp0nDnfN2+7+f0bS28nxcSnYv7pxbGO448e/fBG4Ien8P8GDQ+9m4Z0/7/br+BGT6cVNxNboDtf6epGlliqrAdor0F6j9Y4yJc/7K90PgQg350W8XrX8YsAHI1zCVksxx/s5+PzBaVT9uw/yDcEH2VHV3s72aBSpjTIvE1d77Me7nHV+OdXnikYhcgqvCLbjfjU3HVfe3k2w7tbkZGmNM7+AftJfh0nOzY1yceHYZLoUquKrwV1uQ6hh2R2WMMSau9drW040xxnQPFqiM6eZE5Nci8q+xLocxncUClQlLXLcD5cE/PhWRG0VkSQevJ0VEnvLrUxGZ0ZHLD1rPhSKyQUSqxbWAMjxk/EUi8oG4LlB2iMjfR7ncJv0MSRv7HouWuO4tmjQ3o6rzVfUHnbXOCOW4RETWiGvB5C0RGd/MtO3qV8rPf0yadmeRGDS+2X1ruj8LVKY5icBtXbCeN3GtgbSrU0If7EaEGZ6Da6j2X3G/cVqG+y1HYPx43A9Z/wXXQsIk3O/qupS0o4+triQio3EtU8zHtUP3HPBsJ5f/P7VptxaBPtqa3bemZ7BAZZrzI+B26cQ+ftQ19vkTdY1S1oeOF9er7f0i8omIlPk0V3orV/M53I8g/+R/k3IXruWQsX7893CNar6groHcSlWNtp3B4LIGWjRYKUEdMYrIPBFZIa4dtLdEZGLQPNt8ywergMMikiQid4hr/+6QuB59L/fTjsM1LXamX/5+P7zJHYuIfFVENotrw+9ZERkcNE5FZL6IbPLl+YWIa95GREaJyOviOt3cIyKRTvizgDfUNcBah2sNYwhhGpEWkZtwP/b/ji/zc4HPIq7Dxf0islZELm3t9vZa2remB7BAZZqzDNeCx+3RTCxNWyIP/bujjWW4D9do62Rcu2dDON5MTrQmENRemqoexv0iP9CW4Rm+/KtFZLeIPCYirW1JHlUNNJU0yV/1Pymu7bmHcF3OZAO/wd19BHeueQ2uy40B/sS/BdfAan9cE1WPiUiBukZz5+M7DFTXPl8TIvIZXBcSf49rgHU7rsWJYPNwLYNP9NPN8sN/gGtBPQvXysP/NPNxJeS14FrDCN0mD+DuvgJ3RJeIawbtOb+uQcA/An8QkTHNrO/rPvAuF5Ergoa3tG9ND2CByrTk34B/lKYtcoelTRulDP27r6X5Q/kr/ZuAb6jqXlU9hGuTLFw3C83pi2u6J9gBjrdGXojr/uAKXBNX6TR/km6N5rpACPiZqn6qvgsSf3ewS133FE/iOq+Ltn20aLpvuE9V96trr+81jndZU4tvMVxVazRy1wsvA+eLazsvBdeieApBDUe34AzcPrnP31G/imvC55oI0/8Mt18G4VJ8j4jI2X5cS/vW9AAWqEyzVHUN7iTS1jui9sjFnfyWy/HuA170wxGRYdK0a4FhwKqgYYGGjatwLdwH68fx/p2OAA+r6kZ1LXH/kMj9aLVWc10gBDTpgkRErgtKFe7H3alE2/VFNN03hO2yBvgO7s7oPZ+OC9sChapuwDXG/HNcu245uI70drSijJ+qa7A2YHtIGYPX94FPx9ap6kLcHdrn/OiW9q3pAbrFw1sTc3fiupL+r+YmEpGqZkb/UFVb2zr2HlwQmaCqJ3Q/4O8IGtNfIrIN1934tpBJ1xLU7Yq4mown++HgutAI/uV7R3cJcq+q3tvMNI3r8zXWfotrGf1tVa0XkRUcT7W1VLY2d9+gqqW43oAD/Ry9LCJLVXVzmGmfwjXiin+G+RVCOsQL9/mCyjhURBKCgtUwXMO20VCOb4+W9q3pAeyOyrTIn6ieBG5tYbq+zfxFDFK+wkSaf5sirksG8Sex3wL/Lb7HZBEZIiKzIi0rgqeBIhG5wq/n34BV/s4A4GFclwcnies19w7cXWSgfNtE5IYo1xXaXUNzXSCE0wd3Iq7w6/4STZ/9lAGFEqHrFtrRfYOI/J0c7wZjny9HuG5xEJEp4rq9yAUeAJ4N2p6hQrfJu7g7ue+I6/phBnAJJz5LC6zrShHpK64Lm5m4GqLP+tEt7VvTA1igMtG6m6b9h3Wkj3B3TkNwPTIf4fhdwXdx/TC9I64X2Jdx3YNETV033FcA9+JOwNMJes6lqg8Bv8OdQLfjniHdCu53Xrg7kkhdxIe6C3jUp+3+Xl2Hdl/Fpcn2+c9yQzNlXYe7c30bd4IvxnWTHvAq7m6hVET2hJn/ZdxznD/j0nInE/0zvdOBd/2d8bO47uO3Rpj2p7jWwT/yn+urzSz3f4Hxfps8o66r9kuAEtxd8y+B65oJLrfh7gj342qiflVVl0DL+9b0DNbWnzHN8CmwW9T1wmuMiQELVMYYY+Kapf6MMcbENQtUxhhj4poFKmOMMXGtx/6OKicnR0eMGBHrYhhjTLeyfPnyParaYks0XanHBqoRI0awbNmyWBfDGGO6FRHZ3vJUXctSf8YYY+KaBSpjjDFxzQKVMcaYuNZjn1EZY7qX2tpaduzYQU1NTayL0iukpaVRWFhIcnJyrIvSIgtUxpi4sGPHDjIzMxkxYgS+02HTSVSVyspKduzYwciRI2NdnBZZ6s8YExdqamrIzs62INUFRITs7Oxuc/dqgcoYEzcsSHWd7rStLVCFKD9Yw48WbeDTvdWxLooxxhgsUJ3gWH0Dv3htC8+v2h3rohhjjMEC1QkKszKYNHQAC1bvinVRjDHGYIEqrHnFBazZeZBPKi39Z0xvIyJce+21je/r6urIzc1l3rx5US/jrrvu4v77729xur59+7apjACJiYlMnjy58W/btm0AnHXWWQDs37+fX/7yl21efjyxQBVGSXE+AAtWW/rPmN6mT58+rFmzhiNHjgCwePFihgwZEuNSnSg9PZ0VK1Y0/gUa4X7rrbcAC1Q9XiD9t9AClTG90pw5c1iwYAEAjz/+ONdcc03juB//+McUFRVRVFTET37yk8bh9957L6eccgrnnHMOH330UZPlPfbYY0ybNo3Jkydz8803U19f3+z6Z8yYwYYNGwCorKykqKgo6rIH7tLuuOMOtmzZwuTJk/n2t78d9fzxqMt+8CsiDwHzgHJVPWGri8gXgO8CAhwCvqaqK/242cBPgUTgQVW9r7PLO7c4nx8u3MAnldUMy87o7NUZY4J8/7m1rNt1sEOXOX5wP+68ZEJU01599dXcfffdzJs3j1WrVvHlL3+ZN954g+XLl/Pwww/z7rvvoqpMnz6d888/n4aGBp544glWrFhBXV0dp512GlOmTAFg/fr1PPnkk/ztb38jOTmZr3/96/zhD3/guuuui7j+zZs3c8oppwCwatUqiouLT5jmyJEjTJ48GYCRI0fy9NNPNxl/3333sWbNGlasWBHVZ45nXdkyxSPAz4HfRRj/MXC+qu4TkRLgAWC6iCQCvwAuBnYA74vIs6q6rjMLW1JUwA8XbmDB6t18bcbJnbkqY0ycmThxItu2bePxxx9nzpw5jcPffPNNLr/8cvr06QPA5z73Od544w0aGhq4/PLLychwF7WXXnpp4zyvvPIKy5cv5/TTTwdcgBk0aFDEdW/fvp0hQ4aQkOASXqtWrWLixIknTBdI/fUGXRaoVHWpiIxoZvxbQW/fAQr962nAZlXdCiAiTwCXAZ0aqIYOzGBSYX8WWqAypstFe+fTmS699FJuv/12lixZQmVlZZuXo6pcf/31/Pu//3tU069cubJJYFq+fDlXXXVVm9ffE8TrM6qvAC/410OAT4PG7fDDTiAiN4nIMhFZVlFR0e5CzJ1YwOqdB6z2nzG90Je//GXuvPPOJmm3c889l2eeeYbq6moOHz7M008/zbnnnst5553HM888w5EjRzh06BDPPfdc4zwXXnghTz31FOXl5QDs3buX7dsj9024YsWKxqaNNm3axF//+tewqb+WZGZmcujQoVbPF4/iLlCJyAW4QPXd1s6rqg+o6lRVnZqb2/6elEuKCgBYuMYqVRjT2xQWFnLrrbc2GXbaaadxww03MG3aNKZPn86NN97IqaeeymmnncZVV13FpEmTKCkpaUzzAYwfP5577rmHmTNnMnHiRC6++GJ27458Tlm5ciUNDQ1MmjSJu+++m/Hjx/Poo4+2uvzZ2dmcffbZFBUVdfvKFKKqXbcyl/p7PlxlCj9+IvA0UKKqG/2wM4G7VHWWf/9PAKra7H301KlTtSO6or/s52+iwLP/cE67l2WMiWz9+vWMGzcu1sWIudGjR/PBBx+QmZnZ6esKt81FZLmqTu30lbdC3NxRicgw4C/AFwNBynsfGC0iI0UkBbgaeLaryjWnuIBVOw5Y23/GmE536NAhRKRLglR30mWBSkQeB94GxojIDhH5iojMF5H5fpJ/A7KBX4rIChFZBqCqdcA/AIuA9cAfVXVtV5V7TrFL/9mPf40xnS0zM5ONGze2PGEv05W1/q5pYfyNwI0Rxi0EFnZGuVoSXPtv/vlW+88YY7pa3KT+4pml/4wxJnYsUEUhkP6zJpWMMabrWaCKwtCBGUz06T9jjDFdywJVlOYUF7DS0n/GGNPlLFBFaa6l/4wxJiYsUEXJ0n/GGBMbFqhawdJ/xhjT9SxQtYKl/4zp+drTaaHpHF3ZH1W3N3RgBsVDXPrvZvvxrzGdasaMGR26vCVLlkQ1XTSdFkZj3759ZGVltWle05TdUbXS3ImW/jOmp4rUaeHDDz/M/PnzGTlyJPPnz+c3v/lN4zyRGvb+xje+AcCNN4ZtcMe0gt1RtdLc4gLue2GD3VUZ08mivQPqSJE6LZw7dy6XXXYZtbW1/PrXv6a0tJQzzzyTz372s5x11lm8++673H777dxyyy386Ec/YunSpWzYsIHvf//7bN68mX/5l39h3bp1J3QXb6Jjd1StZLX/jOm5muu0cPny5UyZMqVxumuuuYbvfve7fPzxx0yaNAmAqqoqMjIyyMnJ4dprr+XCCy/kiiuu4N57723svt60ngWqNphrtf+M6ZGa67QwNFBdfPHFAKxevZqJEydy8OBBRARwKcNJkybx/vvvc+GFFwKQmJgYg0/UM1jqrw3mFBfw75b+M6bHWbVqVcROC1euXMltt90GuLutMWPGADB27Fjuv/9+kpKSGDt2LAA5OTk8+OCD7Nq1i9tuu409e/bQEb2O91Zd2sNvV+qoHn4jsZ5/jelYse7h99ChQ0yZMqVX9QdlPfz2cNb1hzE9i3VaGL8sULWR9fxrjDFdwwJVGwV6/l2wygKVMcZ0JgtU7TB3YgGrdx7gk0pL/xnTEXrqM/N41J22tQWqdrD0nzEdJy0tjcrKym51Au2uVJXKykrS0tJiXZSoWPX0dijMymDS0AEsXL2br82waurGtEdhYSE7duygoqIi1kXpFdLS0igsLIx1MaJigaqd5hUXcO/C9XxSWc2w7IxYF8eYbis5OZmRI0fGuhgmDlnqr51KivMBS/8ZY0xnsUDVToVZGUweOoAFq3fFuijGGNMjWaDqAHOLC1iz8yDbKw/HuijGGNPjWKDqAJb+M8aYzmOBqgM0pv/sx7/GGNPhLFB1kHkTC1i76yDb9lj6zxhjOpIFqg5SYj/+NcaYTmGBqoMMGZDOqcMs/WeMMR3NAlUHmltcwLrdB/nY0n/GGNNhLFB1oEDbfwst/WeMMR3GAlUHGjwgndMs/WeMMR3KAlUHmztxMOt2H2RrRVWsi2KMMT2CBaoONsf/+NfSf8YY0zEsUHWwgv7pTBmexYLVpbEuijHG9AhdFqhE5CERKReRNRHGjxWRt0XkqIjcHjJum4isFpEVIrKsa0rcdnOLC1i/+yBbLP1njDHt1pV3VI8As5sZvxe4Fbg/wvgLVHWyqk7t6IJ1tMbaf1apwhhj2q3LApWqLsUFo0jjy1X1faC2q8rUWfL7pzF1eJa1UmGMMR2guzyjUuAlEVkuIjdFmkhEbhKRZSKyLNbdWc+dWMCG0kNsLrf0nzHGtEd3CVTnqOppQAlwi4icF24iVX1AVaeq6tTc3NyuLWEUpQolAAAfEklEQVSIkqICRKz2nzHGtFe3CFSqutP/LweeBqbFtkQta0z/2XMqY4xpl7gPVCLSR0QyA6+BmUDYmoPxZm5xAR+VHWJz+aFYF8UYY7qtrqye/jjwNjBGRHaIyFdEZL6IzPfj80VkB/BN4Ht+mn5AHvCmiKwE3gMWqOqLXVXu9igpdum/BavsN1XGGNNWSV21IlW9poXxpUBhmFEHgUmdUqhOltcvjdOHD2TB6l3cdtHoWBfHGGO6pbhP/XV3cycWsLGsik1llv4zxpi2sEDVyUqK8l36z2r/GWNMm1ig6mSD+qUxbcRAq/1njDFtZIGqC8ydWMCm8io2WvrPGGNazQJVF5gdSP/ZXZUxxrSaBaouMCgzjekjB9pzKmOMaQMLVF1kbnEBm8ur+KjU0n/GGNMaFqi6yKyifBKs9p8xxrSaBaouMigzjWkjB7Jg1S5UNdbFMcaYbsMCVReaO3EwWyoOs7HMuv4wxphoWaDqQrMn+PTfql2xLooxxnQbFqi6UG5mKtNHZvP86t2W/jPGmChZoOpicycWsLXiMB/Zj3+NMSYqFqi62OxA7T/78a8xxkTFAlUXy+mbyhknZbPA0n/GGBMVC1QxMKfYpf822I9/jTGmRRaoYsDSf8YYEz0LVDGQ0zeVM0/OZqGl/4wxpkUWqGJkTnEBW/ccZv1uS/8ZY0xzLFDFSOOPf1fbj3+NMaY5FqhiJLsx/Vdq6T9jjGmGBaoYmls8mI/3HGbd7oOxLooxxsQtC1QxNGtCHokJwkLr+sMYYyKyQBVD2X1TOfOkbBasstp/xhgTiQWqGJs7sYBtldWs3WXpP2OMCccCVYzNmpBv6T9jjGmGBaoYG9gnhbNOtrb/jDEmEgtUcWBOcQHbLf1njDFhWaCKA4H03wJL/xljzAksUMWBQPrP2v4zxpgTWaCKE3Mt/WeMMWFZoIoTMy39Z4wxYVmgihOW/jPGmPAsUMURS/8ZY8yJLFDFEUv/GWPMiSxQxRFL/xljzInaFKhE5FtBr8dEOc9DIlIuImsijB8rIm+LyFERuT1k3GwR+UhENovIHW0pc3dh6T9jjGmqVYFKRAaIyMPAlSLydRE5B4g2cDwCzG5m/F7gVuD+kHUmAr8ASoDxwDUiMr415e5OLP1njDFNtSpQqep+Vf0ScBfwLjAa+EuU8y7FBaNI48tV9X2gNmTUNGCzqm5V1WPAE8BlrSl3d2LpP2OMaarVqT8ReRI4HzgZ+JuqPtfhpWpqCPBp0Psdfli4st0kIstEZFlFRUUnF6vzWPrPGGOOa8szqk+AKmA/cLmI/LZji9R2qvqAqk5V1am5ubmxLk6bWfrPGGOOa0ugqgSuAuYCFcCvOrREJ9oJDA16X+iH9ViW/jPGmONaHahU9T7gq8C/AVuAczq6UCHeB0aLyEgRSQGuBp7t5HXGnKX/jDHGSWppAhEZAdyCeya1F1gBPKeqB4DX/V+LRORxYAaQIyI7gDuBZABV/bWI5APLgH5Ag4j8P2C8qh4UkX8AFgGJwEOqurYVn7Fbmjkhn395Zg0LVu+maEj/WBfHGGNipsVABfwV+BnwIvAQoMC3ReR54JuqejSaFanqNS2ML8Wl9cKNWwgsjGY9PUVw+u87s8YgIrEukjHGxEQ0qb9EVf1fVX0F2KuqX8XdXW0DHujMwvV2lv4zxpjoAtXLPvUG7m4KVa1T1R8BZ3ZayYzV/jPGGKILVN8E+ovIMmCw/63StSLyC1wNQNNJrPafMcZEEahUtUFV7wXOA24C8oEpwBpcs0amE1n6zxjT20VdPV1Vq1X1WVW9W1W/oaq/UtX9nVk4Y+k/Y4yxbj7inKX/jDG9nQWqbmCOT/+t223pP2NM72OBqhuY5dN/Cy39Z4zphSxQdQMD+6Rw5knZLFxdauk/Y0yvY4Gqm5hTXMDHew6zfvehWBfFGGO6lAWqbmLWhDxL/xljeiULVN1Edt9UzjhpoNX+M8b0OhaoupE5xQVs3XOYDaWW/jPG9B4WqLqRWRPySRAs/WeM6VUsUHUjOX1TOeOkbBZY+s8Y04tYoOpmSooL2FpxmI1lVbEuijHGdAkLVN3MbJ/+s7b/jDG9hQWqbiY3M5VpIwfacypjTK9hgaobmltcwObyKjaWWe0/Y0zPZ4GqG5pVlI8ILFhld1XGmJ7PAlU3NCgzjWkjLP1njOkdLFB1U3MnFrCpvIpNlv4zxvRwFqi6qdmB9J/dVRljejgLVN3UoMw0Trf0nzGmF7BA1Y3NKcpnY1kVm8st/WeM6bksUHVjJcUFiMDC1aWxLooxxnQaC1TdWF6/NKYOz7L0nzGmR7NA1c3NKS5gQ+khtlRY23/GmJ7JAlU3N7soH4AX7K7KGNNDWaDq5gr6pzNleJY9pzLG9FgWqHqAkqJ81u0+yLY9h2NdFGOM6XAWqHqAkuICABausfSfMabnsUDVAwwZkM7koQN4wdJ/xpgeyAJVDzGnOJ/VOw/w6d7qWBfFGGM6lAWqHqKkyKf/rPafMaaHsUDVQwwdmMHEwv4sXGPpP2NMz9JlgUpEHhKRchFZE2G8iMjPRGSziKwSkdOCxtWLyAr/92xXlbm7KSkqYOWn+9mxz9J/xpieoyvvqB4BZjczvgQY7f9uAn4VNO6Iqk72f5d2XhG7tznF7se/L9pdlTGmB+myQKWqS4G9zUxyGfA7dd4BBohIQdeUrmcYnt2HCYP72XMqY0yPEk/PqIYAnwa93+GHAaSJyDIReUdEPhtpASJyk59uWUVFRWeWNW7NKS7gg0/2s/vAkVgXxRhjOkQ8BarmDFfVqcDngZ+IyMnhJlLVB1R1qqpOzc3N7doSxomSxrb/LP1njOkZ4ilQ7QSGBr0v9MNQ1cD/rcAS4NSuLlx3cVJuX8bmZ/KCtVJhjOkh4ilQPQtc52v/nQEcUNXdIpIlIqkAIpIDnA2si2VB492c4gKWbd9H2cGaWBfFGGParSurpz8OvA2MEZEdIvIVEZkvIvP9JAuBrcBm4LfA1/3wccAyEVkJvAbcp6oWqJoxp7gAVav9Z4zpGZK6akWqek0L4xW4Jczwt4DizipXTzRqUF9OyevLwtW7uf6sEbEujjHGtEs8pf5MByopKuC9bXupOHQ01kUxxph2sUDVQ82d6NN/ay39Z4zp3ixQ9VCjB/Xl5Nw+1kW9Mabbs0DVQ4kIc4oLeGdrJZVVlv4zxnRfFqh6sJKiAhoUXlpXFuuiGGNMm1mg6sHGFWQyIjvD2v4zxnRrFqh6MBGhpLiAt7dUsr/6WKyLY4wxbWKBqoebU1RAXYOy2NJ/xphuygJVD1c0pB+FWem8YK1UGGO6KQtUPZyIUFKUzxubKjhYUxvr4hhjTKtZoOoFSooLqK1XXl1fHuuiGGNMq1mg6gUmFw6goH+a1f4zxnRLFqh6gYQEYdaEfJZsrKDqaF2si2OMMa1igaqXmFNcwLG6Bl7bYOk/Y0z3YoGql5gyPIvczFTr+dcY0+1YoOolEhOEWRPyeG1DBUeO1ce6OMYYEzULVL3InKICjtTW8/pGS/8ZY7oPC1S9yLSRAxnYJ4WFq+3Hv8aY7sMCVS+SlJjArAl5vLK+jJpaS/8ZY7oHC1S9zOyiAg4fq+fNTXtiXRRjjImKBape5qyTs+mfnsxCq/1njOkmLFD1MsmJCVw8Po/F68o4VtcQ6+IYY0yLLFD1QnOK8zlUU8fftlj6zxgT/yxQ9UJnj8ohMzWJF632nzGmG7BA1QulJiVy4bhBLFpXSm1969N/NbX1VB9rfZuBqsrSjRXUN2ir5319YwULVtlzNWN6IwtUvVRJcQH7q2t5d+veVs/7zT+u4PO/fbfV8725eQ/XPfQef/lgR6vmU1Xu/Osa/vnp1dS1IbAaY7o3C1S91Pmn5JKRktjqtv8O1dTy8rpyVny6n+2Vh1s1b6CX4UVry1o13+byKrZVVnPgSC3Ltu9r1bzGmO7PAlUvlZacyAVjBrFobWmrUnGvfVTBMX9Xs2ht9M+4GhqUxetcgHpjU0WrUocv+fmSE4WX17UuyBljuj8LVL3Y7KJ89lQdY9m26NN/L67ZTU7fVMYX9OPFNdEHqg8/3U/FoaNcNXUoR+saeKMVPzhevK6MiYX9OXtUDovXl6Ha+mdcxpjuywJVL3bB2EGkJCU0puRaUlNbz2sbKpg1IY85xfl88Ml+yg/WRDXvS2tLSUoQvlsylv7pybwUZfqv/GANKz7dz8Xj8rhoXB7bK6vZUlEV1bzGmJ7BAlUv1jc1ifNG57JobSkNUaT/lm6s4EhtPbOL8pk1IR+ARVGk4lSVRWtLOfPkbAb2SeHCsYN4ZUNZVBUjXl7vWnq/eEIeF44bBMDiddb6uzG9iQWqXq6kKJ/dB2pYuWN/i9O+uKaU/unJnHFSNqMG9eWk3D68FMVzqk2+MkQguM2ckMf+6lre39ZyxYjF60oZOjCdMXmZFPRPp3hIf15eb8+pjOlNLFD1cheNyyMpQVp83nSsroGX15dx0bg8khMTEBFmTcjn7S2VHKiubXbeRX7ZM8fnAXDu6FxSkhIaK1dEcvhoHX/bUsnF4/IRkcbyfvDJPvZUHY32IxpjujkLVL1c/4xkzhqVwwtrSputpPDO1koO1tRRUpTfOGz2hHzqGpRXNjQfcBatK+XUYQMY1C8NgD6pSZw7KoeX1jW/zqUbKzhW18DFPsABXDR+EKrw6gZL/xnTW1igMpQU5fPJ3mrW7T4YcZoX1pSSkZLIOaNzGodNLOxPQf+0Zu/Gdu4/wpqdBxvTfgEzJ+SxY98R1u8+FHHexevKGJCRzOkjshqHjS/ox+D+aVZN3ZhexAKVYeb4PBKEiAGnvkFZvK6UC8YOIi05sXF4IP23tJnfRQWeYYUGqgvH5SECL60Lv866+gZe/aicz4wZRFLi8cNURLhofB5vbNpjnT8a00t0WaASkYdEpFxE1kQYLyLyMxHZLCKrROS0oHHXi8gm/3d9V5W5t8jum8r0kdkRq6kv376PPVXHmqT9AmZOyKOmtoGlGyvCzrtobSmjB/VlZE6fJsNz+qYyZVhWxGrqy7bvY391LRcFpf0CLhqXx5Haet6y1t+N6RW68o7qEWB2M+NLgNH+7ybgVwAiMhC4E5gOTAPuFJGsSAsxbVNSnM/m8io2l5+YinthzW5SkhKYMWbQCeOmjRhIVkZy2LuxfYeP8d7He0+4mwqYOSGPdbsPsmNf9QnjFq8rIyUxgfNOyT1h3PSTBtI3NcmqqRvTS0hX/spfREYAz6tqUZhxvwGWqOrj/v1HwIzAn6reHG66SKZOnarLli1rc1lnzJjR5nm7o7rkPuyY8nUGfPoGA3a+0zhcgR2n3kzK4TLyNj4Tdt49J83m8MDRDFv+C0SP/zbqUO4EKk+eQ8Hq35F6+MQ7p9q0Aeyc/FUGbnuFfqUfNFnnzslfJfnIXvI++nPYdZaPvoSjmYUUfvArpG0fuc3qE1OpHjiaurQs0vdtIbVqV1RlUOBo3wKOZI0i8ehB+uz9iMS66H4wrQhHM4dQm9afjH1bSaw70qoy16b2RxOSkYY6pKHW/69DtL7Lt5/pfEuWLGnzvCKyXFWndlxp2i8p1gUIMgT4NOj9Dj8s0vATiMhNuLsxhg0b1jml7KGSag+Temgn1QNPaRKojvXJoz61H30+fTPivBl7N1I1qJiafsNIP7CtcXh11mgSjx4kJUyQAkiu2U9ydQXVWaOaBKra9Bzq0gbQf9d7kde5bwvV2WM51ief1MOd369WQ0Iy1VmjOJw9liMDRkBCEqhyYMgZJB49QJ/KDfTZs4GU6vITTvzH0nM4nDOOw9ljqUsbANoAksDeEReSfuBj+uxZT8a+zSQ0NH3Opwg1/QqpHjiG6oGjqU/pC0BlQz3pBz6mb8U60vdtIUFPfD6ouO14OHsM1QNPoTYj54Rp3ITaNHA11JLQ+LoOqa8NGVbbdLr64/MmNNSGX1ZgGqzpK9M28RSo2k1VHwAeAHdH1Z5lteeKpLt68I2t3LNgPb/780KGZWcA8B8vbuC3S7fy8u/+mwEZKWHnq6mtZ8oPFnPhDd/ih5cXA1B9rI5T717MF6cN467/XhJxnT9atIFfv76VZxa+1Lj8n7+6iftf2sjCB/+DPF+lPdT+6mNMuedlrrzt+3xr5ph2fOrIamrreXVDOc+v2sUr68s5WtdAfr80vjCxgHmTBnNSbh8Wry3juVW7eHPTAA4Ons5JuX24ZOJgzh2dw9tbKnlu1S62lVWRmCCcPSqHSyYWMKson08qq3l25S6eXdGH0qxRZKQkMmtCPpdOHkxqUgILV+/mxTVl7Kk6SlpyAjPHDGJOcQEjsvvw/KpdPLMig7KsUWSmJlFSnM/lpxYyfeRA1u0+yMLVu3lhTSm79hwmQeD0EQOZXZRPXr80amrrOVJbT01tAzW19e79sXpq6tywI7X1HA2a5six+uPT1R6fpi2SE4W05ETSkxOP/09JJD05ofH98WH+L+X4tOkpCU3mzUhJIj0laN6URNKSEklIsHvEniaeAtVOYGjQ+0I/bCcu/Rc8fEmXlaoXmTUhn3sWrOeFNbu5+fyTUVVeXOOaPooUpMC1xD5jzCBeWlvGDy4rIjFBWLqxgqN1DcyccGJliGAzx+fzi9e28OqGcj53WiHgnk9NGjogYpACGJCRwtThWSxeV9ahgaqmtp6lGyt4ftVuXl5fRvWxenL6pnL16UOZN2kwU4ZlNTkRXjGlkCumFLL38DFeWLOb51bu4mevbuKnr2wC4PQRWfzgsgmUFBeQ0ze1cb6iIf0pGtKf784ey3sf7+XZlTtZsGo3T3+4E4C05AQ+M9YFpwvGDKJP6vGvanFhf74zeyzvbK3k6Q/dfH9ctoP05ESO1NaTmCCceVI2N547kpnj88nNPL7ejqCqHK1r8B1oBgJZgw9kLvAd8YEtEPSOHGugps6NO1p3fL4jPmDuPXyscb7gZbShj03SkhMaA1lacgLpKYlkJCc1BkU3PJGMlOMBLj3wPmhYhg+SGSlJTaZLSbLK0l0tngLVs8A/iMgTuIoTB1R1t4gsAn4YVIFiJvBPsSpkTzZ0YAbFQ/rzwppSbj7/ZDaVV/HxnsN85ZyRLc47qyifBat38+En+5g6YiCL1paRlZHMtBEDm52veEh/8vul8dLaMj53WiFlB2tYueMA357VcvC5eHwe9yxYz6d7qxk6MCPqzxnqWF0Db26u4PmVu1m8roxDR+vIykjmsslDmDexgDNOyiaxhav0gX1S+ML04Xxh+nDKDtbw3sd7mTI8i8ED0pudLzFBOPPkbM48OZu7Lp3AGxv3UNfQwHmn5JKREvnrGbhDO3tUDj+4rIjF68t4a/MeThuWxcXj88jqE/nCor1E3J1RWnIiA9q+2Vukqhyrb6DmWENj4HMBrK7xbq+6tp6aoMDYGACPBQdD1yP1gepjlNU2UF1b5wKnH97aYJiUIE2CWyDouTu8wOtE0pOTmgS/wOuMlKSg1y6IpgfeJ9sdYThdFqhE5HHcnVGOiOzA1eRLBlDVXwMLgTnAZqAa+JIft1dEfgC87xd1t6q2vltaE5XZRfn8aNFH7Np/hBdWlyJCi3dFABeMySUlMYEX15QyaegAXllfxswJ+U1+AxVOQoJw8fg8nlq+g5ra+sZmlS4OUy091IXjXKB6ZX0ZN5zdcjANVlvfwN8272HBqt0sWlvKwZo6+qW5NNrciYM56+RsklsoeyR5/dK4ZNLgVs+XmpQYtjp+S9JTErl00mAubcM645mIkJqUSGpSIv3dqaLDBQfD6to6qo8dv5s7/jpo+Anj/OvaOg4fq2NP1dEm49sSCFOTEsIGvsb3PujNnJDP2aMiPHvsYbosUKnqNS2MV+CWCOMeAh7qjHKZpkp8oHpxTSkvri1l6vAsBmVGTsEFZKYlc/aobBatK+X8MbkcrKlrbNuvJTMn5PH7d7bz5qY9LF5XxvDsDEYP6tvifCNz+jBqUF8WRxmoausbeHtLpQtO60rZX11LZmoSF4/PY96kAs4ZlWtpnV6ms4NhIE0aGuCqj9VRHXTnd+SYC4bVx47fAR6f1v0vP1TTZNiInD4WqEzvdFJuX8bkZfLo29vYXlnN9+aOi3reWRPyee0vq/npy5tIT04M+xuocKaPzCYzNYmnP9zJ21sque7M4Y2N0LbkonF5PPjGVg4cqaV/+oknmrr6Bt7eWukrJ5Syr7qWvj44zS0u4NxTckhNSgyzZGPaLzhNaj/+bDsLVOYEs4vyGysDzA7TGkUkF43PI+Hp1Szbvo/ZE/KbNLfUnJSkBC4YO4hnV+4Cokv7BVw8fhC/fn0Lr2+saEx9Be6cFq52ab191bX0SXFptbnFBZx3Sm7UZTPGxJ4FKnOCkmIXqIqH9KcwK/qn5Tl9U5k6YiDvfbw3qudawWZOyOPZlbvIykhmyvDorz0nD80iu08Ki3xfWQuD0nqB4FRSVMCMMRacjOmuLFCZE4zJy+SzkwdzwdgTm0xqyZWnFfJR6SEuHNu6QHX+Ke750IXj8lqsgBEsMUH4zNhB/Gn5Dhas3k3f1CQuGueqddudkzE9Q5c2odSV2tuEkmkbVaW2XttUKWH1jgMMyUpnYCurVm8sO8Tv397Oeafkcu7oHAtOxrRDPDahZIHKGGNMo3gMVFYX1xhjTFyzQGWMMSauWaAyxhgT1yxQGWOMiWsWqIwxxsQ1C1TGGGPimgUqY4wxcc0ClTHGmLjWY3/wKyIVwPZOWnwOsKeTlt0T2PZpmW2j5tn2aVlnbaPhqhpd1wddpMcGqs4kIsvi7Zfb8cS2T8tsGzXPtk/LetM2stSfMcaYuGaByhhjTFyzQNU2D8S6AHHOtk/LbBs1z7ZPy3rNNrJnVMYYY+Ka3VEZY4yJaxaojDHGxDULVC0QkYdEpFxE1gQNGygii0Vkk/+fFcsyxpKIDBWR10RknYisFZHb/HDbRoCIpInIeyKy0m+f7/vhI0XkXRHZLCJPikjrujXuYUQkUUQ+FJHn/XvbPkFEZJuIrBaRFSKyzA/rNd8xC1QtewSYHTLsDuAVVR0NvOLf91Z1wLdUdTxwBnCLiIzHtlHAUeAzqjoJmAzMFpEzgP8A/ltVRwH7gK/EsIzx4DZgfdB72z4nukBVJwf9dqrXfMcsULVAVZcCe0MGXwY86l8/Cny2SwsVR1R1t6p+4F8fwp1shmDbCAB1qvzbZP+nwGeAp/zwXrt9AESkEJgLPOjfC7Z9otFrvmMWqNomT1V3+9elQF4sCxMvRGQEcCrwLraNGvm01gqgHFgMbAH2q2qdn2QHLrj3Vj8BvgM0+PfZ2PYJpcBLIrJcRG7yw3rNdywp1gXo7lRVRaTX1/EXkb7An4H/p6oH3UWx09u3karWA5NFZADwNDA2xkWKGyIyDyhX1eUiMiPW5Ylj56jqThEZBCwWkQ3BI3v6d8zuqNqmTEQKAPz/8hiXJ6ZEJBkXpP6gqn/xg20bhVDV/cBrwJnAABEJXCgWAjtjVrDYOhu4VES2AU/gUn4/xbZPE6q60/8vx13sTKMXfccsULXNs8D1/vX1wF9jWJaY8s8T/hdYr6o/Dhpl2wgQkVx/J4WIpAMX457jvQZc6SfrtdtHVf9JVQtVdQRwNfCqqn4B2z6NRKSPiGQGXgMzgTX0ou+YtUzRAhF5HJiBa1K/DLgTeAb4IzAM15XI36tqaIWLXkFEzgHeAFZz/BnDP+OeU/X6bSQiE3EPuhNxF4Z/VNW7ReQk3B3EQOBD4FpVPRq7ksaeT/3drqrzbPsc57fF0/5tEvB/qnqviGTTS75jFqiMMcbENUv9GWOMiWsWqIwxxsQ1C1TGGGPimgUqY4wxcc0ClTHGmLhmgcqYCESkyv8fISKf7+Bl/3PI+7c6cvnG9CQWqIxp2QigVYEqqFWFSJoEKlU9q5VlMqbXsEBlTMvuA871fQF9wzcy+yMReV9EVonIzeB+sCoib4jIs8A6P+wZ35Do2kBjoiJyH5Dul/cHPyxw9yZ+2Wt8/0NXBS17iYg8JSIbROQPvlUQROQ+3x/YKhG5v8u3jjGdzBqlNaZld+BbTADwAeeAqp4uIqnA30TkJT/taUCRqn7s339ZVff65pPeF5E/q+odIvIPqjo5zLo+h+u3ahKuNZT3RWSpH3cqMAHYBfwNOFtE1gOXA2N9w6QDOvzTGxNjdkdlTOvNBK7zXXe8i+uWYrQf915QkAK4VURWAu8AQ4Omi+Qc4HFVrVfVMuB14PSgZe9Q1QZgBS4leQCoAf5XRD4HVLf70xkTZyxQGdN6Avyj7211sqqOVNXAHdXhxolc23UXAWf6Hn4/BNLasd7gtu7qgSTfZ9M0XCeD84AX27F8Y+KSBSpjWnYIyAx6vwj4mu/eBBE5xbdqHao/sE9Vq0VkLHBG0LjawPwh3gCu8s/BcoHzgPciFcz3A9ZfVRcC38ClDI3pUewZlTEtWwXU+xTeI7j+kkYAH/gKDRWE7wb8RWC+f470ES79F/AAsEpEPvDdWgQ8jeuvaiWuV9fvqGqpD3ThZAJ/FZE03J3eN9v2EY2JX9Z6ujHGmLhmqT9jjDFxzQKVMcaYuGaByhhjTFyzQGWMMSauWaAyxhgT1yxQGWOMiWsWqIwxxsS1/w+PTDA/Xe5vSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "fit_vals = np.array(fit_vals)\n",
    "plt.title(\"GaussianAltFit-1D-DetectorEffects (Analytical Reweight) Zoomed:\\n\" +\n",
    "          \"N = {:.0e}, Iterations {:.0f} to {:.0f}\".format(\n",
    "              N, index_refine[1], iterations))\n",
    "plt.plot(np.arange(index_refine[1], len(fit_vals)),\n",
    "         fit_vals[index_refine[1]:],\n",
    "         label='Model $\\mu$ Fit')\n",
    "plt.hlines(theta1_param, index_refine[1], len(fit_vals), label='$\\mu_{Truth}$')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "# plt.savefig(\"GaussianAltFit-1D-DetectorEffects (Analytical Reweight) Zoomed:\" +\n",
    "#             \"N = {:.0e}, Iterations {:.0f} to {:.0f}.png\".format(\n",
    "#                 N, index_refine[1], iterations))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
