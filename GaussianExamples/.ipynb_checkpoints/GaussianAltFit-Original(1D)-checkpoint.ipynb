{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__) #1.15.0\n",
    "print(keras.__version__) #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{(1-f(x_{T,i},\\theta'))}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it a $\\mu$ value in addition to $X_i\\sim\\mathcal{N}(\\mu, 1)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = 10**6\n",
    "mu_min = -2\n",
    "mu_max = 2\n",
    "mu_values = np.random.uniform(mu_min, mu_max, n_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with this $\\mu$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = [(np.random.normal(0, 1), mu) for mu in mu_values] # Note the zero in normal(0, 1) \n",
    "X1 = [(np.random.normal(mu, 1), mu) for mu in mu_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu$ values uniformly sampled in the specified range [mu_min, mu_max].\n",
    "\n",
    "For every value of $\\mu$ in mu_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,1)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,1)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "X = np.concatenate((X0, X1))\n",
    "Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((2,))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs = inputs, outputs = outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1600000 samples, validate on 400000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1600000/1600000 [==============================] - 3s 2us/step - loss: 0.5765 - val_loss: 0.5656\n",
      "Epoch 2/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5635 - val_loss: 0.5646\n",
      "Epoch 3/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5630 - val_loss: 0.5643\n",
      "Epoch 4/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5630 - val_loss: 0.5643\n",
      "Epoch 5/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5641\n",
      "Epoch 6/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5629 - val_loss: 0.5644\n",
      "Epoch 7/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5641\n",
      "Epoch 8/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5629 - val_loss: 0.5641\n",
      "Epoch 9/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5642\n",
      "Epoch 10/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 11/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5642\n",
      "Epoch 12/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5641\n",
      "Epoch 13/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5641\n",
      "Epoch 14/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5640\n",
      "Epoch 15/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 16/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5644\n",
      "Epoch 17/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5642\n",
      "Epoch 18/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5642\n",
      "Epoch 19/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 20/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5643\n",
      "Epoch 21/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5641\n",
      "Epoch 22/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 23/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 24/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5640\n",
      "Epoch 25/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5643\n",
      "Epoch 26/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 27/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 28/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 29/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 30/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 31/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 32/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5640\n",
      "Epoch 33/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 34/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5628 - val_loss: 0.5642\n",
      "Epoch 35/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5643\n",
      "Epoch 36/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5642\n",
      "Epoch 37/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 38/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5640\n",
      "Epoch 39/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5643\n",
      "Epoch 40/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5642\n",
      "Epoch 41/200\n",
      "1600000/1600000 [==============================] - 2s 2us/step - loss: 0.5627 - val_loss: 0.5641\n",
      "Epoch 42/200\n",
      "1600000/1600000 [==============================] - 2s 1us/step - loss: 0.5627 - val_loss: 0.5641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc843aaef90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience = 10,\n",
    "                              restore_best_weights=True)\n",
    "dctr_model.fit(X_train, Y_train, \n",
    "          epochs=200, \n",
    "          batch_size = 10000,\n",
    "          validation_data = (X_test, Y_test),\n",
    "          callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"1d_gaussian_dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"1d_gaussian_dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"1d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"1d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "theta0_param = 0 #this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = 1. #this is the data (the target)\n",
    "\n",
    "theta0 = np.random.normal(theta0_param,1,N)\n",
    "theta1 = np.random.normal(theta1_param,1,N)\n",
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0,theta1])\n",
    "yvals = np.concatenate([labels0,labels1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals, yvals, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Model\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njson_file = open(\\'1d_gaussian_dctr_model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\ndctr_model = keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\ndctr_model.load_weights(\"1d_gaussian_dctr_model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('1d_gaussian_dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"1d_gaussian_dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr_model(d)\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights\n",
    "\n",
    "def analytical_reweight(d): #from analytical formula for normal distributions\n",
    "    events = d[:,0]\n",
    "    param = d[:,1]\n",
    "    weights = K.exp(-(0.5*(events-param)**2)+(0.5*(events-0.0)**2))\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to True for analytical_reweight\n",
    "\n",
    "reweight_analytically = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0.):\n",
    "    x  = inputs\n",
    "    x = K.gather(x, np.arange(500))\n",
    "\n",
    "    theta = 0. #starting value\n",
    "    theta_prime = val\n",
    "    \n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    concat_input_and_params = K.ones(shape = x.shape)*theta_prime\n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "    \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(data)\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : -2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0505 - acc: 0.6220 - val_loss: 0.0494 - val_acc: 0.6253\n",
      "testing theta = : -1.9\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0542 - acc: 0.6266 - val_loss: 0.0541 - val_acc: 0.6273\n",
      "testing theta = : -1.8\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0593 - acc: 0.6322 - val_loss: 0.0593 - val_acc: 0.6294\n",
      "testing theta = : -1.7\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.0648 - acc: 0.6367 - val_loss: 0.0648 - val_acc: 0.6367\n",
      "testing theta = : -1.6\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0706 - acc: 0.6429 - val_loss: 0.0706 - val_acc: 0.6408\n",
      "testing theta = : -1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0768 - acc: 0.6478 - val_loss: 0.0767 - val_acc: 0.6494\n",
      "testing theta = : -1.4\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0833 - acc: 0.6521 - val_loss: 0.0831 - val_acc: 0.6514\n",
      "testing theta = : -1.2999999999999998\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0901 - acc: 0.6569 - val_loss: 0.0900 - val_acc: 0.6579\n",
      "testing theta = : -1.2\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.0972 - acc: 0.6617 - val_loss: 0.0971 - val_acc: 0.6643\n",
      "testing theta = : -1.1\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1047 - acc: 0.6668 - val_loss: 0.1046 - val_acc: 0.6674\n",
      "testing theta = : -1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1125 - acc: 0.6710 - val_loss: 0.1123 - val_acc: 0.6719\n",
      "testing theta = : -0.8999999999999999\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1205 - acc: 0.6742 - val_loss: 0.1204 - val_acc: 0.6742\n",
      "testing theta = : -0.7999999999999998\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1288 - acc: 0.6779 - val_loss: 0.1287 - val_acc: 0.6779\n",
      "testing theta = : -0.7\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1373 - acc: 0.6810 - val_loss: 0.1372 - val_acc: 0.6815\n",
      "testing theta = : -0.5999999999999999\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1461 - acc: 0.6839 - val_loss: 0.1458 - val_acc: 0.6847\n",
      "testing theta = : -0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1549 - acc: 0.6862 - val_loss: 0.1547 - val_acc: 0.6865\n",
      "testing theta = : -0.3999999999999999\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1638 - acc: 0.6882 - val_loss: 0.1636 - val_acc: 0.6893\n",
      "testing theta = : -0.2999999999999998\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1728 - acc: 0.6898 - val_loss: 0.1726 - val_acc: 0.6889\n",
      "testing theta = : -0.19999999999999996\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1817 - acc: 0.6905 - val_loss: 0.1814 - val_acc: 0.6913\n",
      "testing theta = : -0.09999999999999987\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1905 - acc: 0.6913 - val_loss: 0.1902 - val_acc: 0.6918\n",
      "testing theta = : 0.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.1991 - acc: 0.6915 - val_loss: 0.1987 - val_acc: 0.6923\n",
      "testing theta = : 0.10000000000000009\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2074 - acc: 0.6915 - val_loss: 0.2071 - val_acc: 0.6919\n",
      "testing theta = : 0.20000000000000018\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2152 - acc: 0.6907 - val_loss: 0.2149 - val_acc: 0.6911\n",
      "testing theta = : 0.30000000000000027\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2226 - acc: 0.6895 - val_loss: 0.2224 - val_acc: 0.6895\n",
      "testing theta = : 0.40000000000000036\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2294 - acc: 0.6875 - val_loss: 0.2290 - val_acc: 0.6885\n",
      "testing theta = : 0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2353 - acc: 0.6853 - val_loss: 0.2351 - val_acc: 0.6838\n",
      "testing theta = : 0.6000000000000001\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2404 - acc: 0.6827 - val_loss: 0.2402 - val_acc: 0.6802\n",
      "testing theta = : 0.7000000000000002\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2445 - acc: 0.6788 - val_loss: 0.2443 - val_acc: 0.6799\n",
      "testing theta = : 0.8000000000000003\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2476 - acc: 0.6752 - val_loss: 0.2472 - val_acc: 0.6786\n",
      "testing theta = : 0.9000000000000004\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2494 - acc: 0.6671 - val_loss: 0.2491 - val_acc: 0.6688\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2500 - acc: 0.5135 - val_loss: 0.2498 - val_acc: 0.4997\n",
      "testing theta = : 1.1\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2494 - acc: 0.3335 - val_loss: 0.2491 - val_acc: 0.3422\n",
      "testing theta = : 1.2000000000000002\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2476 - acc: 0.3358 - val_loss: 0.2473 - val_acc: 0.3328\n",
      "testing theta = : 1.3000000000000003\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2446 - acc: 0.3410 - val_loss: 0.2443 - val_acc: 0.3498\n",
      "testing theta = : 1.4000000000000004\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2405 - acc: 0.3457 - val_loss: 0.2402 - val_acc: 0.3472\n",
      "testing theta = : 1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2354 - acc: 0.3509 - val_loss: 0.2351 - val_acc: 0.3515\n",
      "testing theta = : 1.6\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2295 - acc: 0.3563 - val_loss: 0.2292 - val_acc: 0.3553\n",
      "testing theta = : 1.7000000000000002\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2227 - acc: 0.3620 - val_loss: 0.2225 - val_acc: 0.3647\n",
      "testing theta = : 1.8000000000000003\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2154 - acc: 0.3671 - val_loss: 0.2151 - val_acc: 0.3670\n",
      "testing theta = : 1.9000000000000004\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2075 - acc: 0.3734 - val_loss: 0.2072 - val_acc: 0.3742\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.1993 - acc: 0.3779 - val_loss: 0.1993 - val_acc: 0.3693\n",
      "[[0.04936455615982413], [0.05411945015564561], [0.059342874117195604], [0.0647751251682639], [0.07058181018196047], [0.07672680929675699], [0.08314817225188018], [0.0899926825389266], [0.09710825695842505], [0.10455189909413456], [0.11229971563071013], [0.12038869357854128], [0.1287090079896152], [0.1371924045830965], [0.14581360932812096], [0.1546887948513031], [0.16357558917254209], [0.1725536772981286], [0.18140169803798198], [0.19022297774255276], [0.19873539913445712], [0.20710200262069703], [0.21488664473593236], [0.22235170226544143], [0.22897766000777484], [0.23511493342369794], [0.24017843055725097], [0.244260773293674], [0.24722023224830628], [0.24909134732186794], [0.24975993552058934], [0.24912440658360718], [0.24732508231699465], [0.24433992996811868], [0.2402358279451728], [0.23511920883506537], [0.22915256945788862], [0.2224599862396717], [0.2150909122005105], [0.20718188859522343], [0.19930777253955603]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2,2,41)\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train, epochs=1, batch_size=500,validation_data=(np.array(X_test), y_test),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(np.mean(theta1), ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 1         \n",
      "=================================================================\n",
      "Total params: 16,898\n",
      "Trainable params: 16,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2264 - acc: 0.6556 - val_loss: 0.2099 - val_acc: 0.6764\n",
      ". theta fit =  0.0\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: -0.2106 - acc: 0.6757 - val_loss: -0.2108 - val_acc: 0.6764\n",
      ". theta fit =  0.01718467\n",
      "Epoch:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2024 - acc: 0.6856 - val_loss: 0.2007 - val_acc: 0.6902\n",
      ". theta fit =  0.01718467\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: -0.2020 - acc: 0.6899 - val_loss: -0.2023 - val_acc: 0.6902\n",
      ". theta fit =  0.036865205\n",
      "Epoch:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2022 - acc: 0.6913 - val_loss: 0.2018 - val_acc: 0.6923\n",
      ". theta fit =  0.036865205\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: -0.2032 - acc: 0.6915 - val_loss: -0.2035 - val_acc: 0.6923\n",
      ". theta fit =  0.056815997\n",
      "Epoch:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2038 - acc: 0.6916 - val_loss: 0.2035 - val_acc: 0.6922\n",
      ". theta fit =  0.056815997\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: -0.2048 - acc: 0.6916 - val_loss: -0.2051 - val_acc: 0.6922\n",
      ". theta fit =  0.07677491\n",
      "Epoch:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2054 - acc: 0.6916 - val_loss: 0.2051 - val_acc: 0.6922\n",
      ". theta fit =  0.07677491\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: -0.2064 - acc: 0.6916 - val_loss: -0.2067 - val_acc: 0.6922\n",
      ". theta fit =  0.09670325\n",
      "Epoch:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2070 - acc: 0.6915 - val_loss: 0.2067 - val_acc: 0.6922\n",
      ". theta fit =  0.09670325\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: -0.2080 - acc: 0.6916 - val_loss: -0.2084 - val_acc: 0.6922\n",
      ". theta fit =  0.11654626\n",
      "Epoch:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2086 - acc: 0.6913 - val_loss: 0.2083 - val_acc: 0.6920\n",
      ". theta fit =  0.11654626\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: -0.2096 - acc: 0.6915 - val_loss: -0.2100 - val_acc: 0.6920\n",
      ". theta fit =  0.13635328\n",
      "Epoch:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2102 - acc: 0.6911 - val_loss: 0.2099 - val_acc: 0.6918\n",
      ". theta fit =  0.13635328\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: -0.2112 - acc: 0.6912 - val_loss: -0.2115 - val_acc: 0.6918\n",
      ". theta fit =  0.1560034\n",
      "Epoch:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2118 - acc: 0.6911 - val_loss: 0.2115 - val_acc: 0.6917\n",
      ". theta fit =  0.1560034\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: -0.2127 - acc: 0.6911 - val_loss: -0.2130 - val_acc: 0.6917\n",
      ". theta fit =  0.17592299\n",
      "Epoch:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2133 - acc: 0.6910 - val_loss: 0.2130 - val_acc: 0.6915\n",
      ". theta fit =  0.17592299\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: -0.2143 - acc: 0.6909 - val_loss: -0.2145 - val_acc: 0.6915\n",
      ". theta fit =  0.19566269\n",
      "Epoch:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2149 - acc: 0.6908 - val_loss: 0.2145 - val_acc: 0.6915\n",
      ". theta fit =  0.19566269\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: -0.2158 - acc: 0.6909 - val_loss: -0.2161 - val_acc: 0.6915\n",
      ". theta fit =  0.21549676\n",
      "Epoch:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2164 - acc: 0.6906 - val_loss: 0.2160 - val_acc: 0.6914\n",
      ". theta fit =  0.21549676\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: -0.2173 - acc: 0.6907 - val_loss: -0.2176 - val_acc: 0.6914\n",
      ". theta fit =  0.23537762\n",
      "Epoch:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2179 - acc: 0.6904 - val_loss: 0.2175 - val_acc: 0.6909\n",
      ". theta fit =  0.23537762\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2188 - acc: 0.6903 - val_loss: -0.2190 - val_acc: 0.6909\n",
      ". theta fit =  0.25520608\n",
      "Epoch:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2193 - acc: 0.6901 - val_loss: 0.2190 - val_acc: 0.6906\n",
      ". theta fit =  0.25520608\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2202 - acc: 0.6900 - val_loss: -0.2205 - val_acc: 0.6906\n",
      ". theta fit =  0.27509534\n",
      "Epoch:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2208 - acc: 0.6899 - val_loss: 0.2205 - val_acc: 0.6903\n",
      ". theta fit =  0.27509534\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2217 - acc: 0.6898 - val_loss: -0.2219 - val_acc: 0.6903\n",
      ". theta fit =  0.29499024\n",
      "Epoch:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2222 - acc: 0.6897 - val_loss: 0.2219 - val_acc: 0.6899\n",
      ". theta fit =  0.29499024\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2231 - acc: 0.6893 - val_loss: -0.2233 - val_acc: 0.6899\n",
      ". theta fit =  0.31473437\n",
      "Epoch:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2236 - acc: 0.6891 - val_loss: 0.2233 - val_acc: 0.6896\n",
      ". theta fit =  0.31473437\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2244 - acc: 0.6890 - val_loss: -0.2246 - val_acc: 0.6896\n",
      ". theta fit =  0.3344934\n",
      "Epoch:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2250 - acc: 0.6888 - val_loss: 0.2246 - val_acc: 0.6896\n",
      ". theta fit =  0.3344934\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: -0.2258 - acc: 0.6890 - val_loss: -0.2260 - val_acc: 0.6896\n",
      ". theta fit =  0.35419723\n",
      "Epoch:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2263 - acc: 0.6885 - val_loss: 0.2260 - val_acc: 0.6892\n",
      ". theta fit =  0.35419723\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2271 - acc: 0.6885 - val_loss: -0.2273 - val_acc: 0.6892\n",
      ". theta fit =  0.37387505\n",
      "Epoch:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2276 - acc: 0.6882 - val_loss: 0.2273 - val_acc: 0.6892\n",
      ". theta fit =  0.37387505\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2284 - acc: 0.6885 - val_loss: -0.2286 - val_acc: 0.6892\n",
      ". theta fit =  0.3934858\n",
      "Epoch:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2289 - acc: 0.6880 - val_loss: 0.2286 - val_acc: 0.6884\n",
      ". theta fit =  0.3934858\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2296 - acc: 0.6876 - val_loss: -0.2298 - val_acc: 0.6884\n",
      ". theta fit =  0.4129421\n",
      "Epoch:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2301 - acc: 0.6875 - val_loss: 0.2298 - val_acc: 0.6885\n",
      ". theta fit =  0.4129421\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2309 - acc: 0.6877 - val_loss: -0.2310 - val_acc: 0.6885\n",
      ". theta fit =  0.43247294\n",
      "Epoch:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2313 - acc: 0.6869 - val_loss: 0.2310 - val_acc: 0.6883\n",
      ". theta fit =  0.43247294\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2321 - acc: 0.6874 - val_loss: -0.2322 - val_acc: 0.6883\n",
      ". theta fit =  0.45208305\n",
      "Epoch:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2325 - acc: 0.6866 - val_loss: 0.2322 - val_acc: 0.6876\n",
      ". theta fit =  0.45208305\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: -0.2332 - acc: 0.6868 - val_loss: -0.2333 - val_acc: 0.6876\n",
      ". theta fit =  0.47156835\n",
      "Epoch:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2337 - acc: 0.6863 - val_loss: 0.2333 - val_acc: 0.6874\n",
      ". theta fit =  0.47156835\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2344 - acc: 0.6864 - val_loss: -0.2345 - val_acc: 0.6874\n",
      ". theta fit =  0.49115628\n",
      "Epoch:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2348 - acc: 0.6857 - val_loss: 0.2344 - val_acc: 0.6872\n",
      ". theta fit =  0.49115628\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2355 - acc: 0.6863 - val_loss: -0.2356 - val_acc: 0.6872\n",
      ". theta fit =  0.51076114\n",
      "Epoch:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2359 - acc: 0.6854 - val_loss: 0.2355 - val_acc: 0.6865\n",
      ". theta fit =  0.51076114\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2365 - acc: 0.6856 - val_loss: -0.2366 - val_acc: 0.6865\n",
      ". theta fit =  0.5301042\n",
      "Epoch:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2369 - acc: 0.6850 - val_loss: 0.2366 - val_acc: 0.6863\n",
      ". theta fit =  0.5301042\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2375 - acc: 0.6854 - val_loss: -0.2376 - val_acc: 0.6863\n",
      ". theta fit =  0.54957974\n",
      "Epoch:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2379 - acc: 0.6844 - val_loss: 0.2376 - val_acc: 0.6852\n",
      ". theta fit =  0.54957974\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: -0.2385 - acc: 0.6842 - val_loss: -0.2386 - val_acc: 0.6852\n",
      ". theta fit =  0.5689041\n",
      "Epoch:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2389 - acc: 0.6840 - val_loss: 0.2386 - val_acc: 0.6852\n",
      ". theta fit =  0.5689041\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: -0.2395 - acc: 0.6841 - val_loss: -0.2395 - val_acc: 0.6852\n",
      ". theta fit =  0.5881359\n",
      "Epoch:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2398 - acc: 0.6834 - val_loss: 0.2395 - val_acc: 0.6847\n",
      ". theta fit =  0.5881359\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2404 - acc: 0.6837 - val_loss: -0.2404 - val_acc: 0.6847\n",
      ". theta fit =  0.60720664\n",
      "Epoch:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2407 - acc: 0.6828 - val_loss: 0.2404 - val_acc: 0.6847\n",
      ". theta fit =  0.60720664\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2412 - acc: 0.6837 - val_loss: -0.2413 - val_acc: 0.6847\n",
      ". theta fit =  0.6262587\n",
      "Epoch:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2416 - acc: 0.6823 - val_loss: 0.2412 - val_acc: 0.6832\n",
      ". theta fit =  0.6262587\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2421 - acc: 0.6823 - val_loss: -0.2421 - val_acc: 0.6832\n",
      ". theta fit =  0.64523184\n",
      "Epoch:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2424 - acc: 0.6812 - val_loss: 0.2421 - val_acc: 0.6812\n",
      ". theta fit =  0.64523184\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: -0.2429 - acc: 0.6803 - val_loss: -0.2428 - val_acc: 0.6812\n",
      ". theta fit =  0.6643696\n",
      "Epoch:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2432 - acc: 0.6811 - val_loss: 0.2428 - val_acc: 0.6820\n",
      ". theta fit =  0.6643696\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2436 - acc: 0.6812 - val_loss: -0.2436 - val_acc: 0.6820\n",
      ". theta fit =  0.6831633\n",
      "Epoch:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2439 - acc: 0.6798 - val_loss: 0.2436 - val_acc: 0.6807\n",
      ". theta fit =  0.6831633\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2443 - acc: 0.6799 - val_loss: -0.2443 - val_acc: 0.6807\n",
      ". theta fit =  0.7019755\n",
      "Epoch:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2446 - acc: 0.6792 - val_loss: 0.2443 - val_acc: 0.6804\n",
      ". theta fit =  0.7019755\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2450 - acc: 0.6796 - val_loss: -0.2449 - val_acc: 0.6804\n",
      ". theta fit =  0.7205663\n",
      "Epoch:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2452 - acc: 0.6791 - val_loss: 0.2449 - val_acc: 0.6783\n",
      ". theta fit =  0.7205663\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2456 - acc: 0.6774 - val_loss: -0.2455 - val_acc: 0.6783\n",
      ". theta fit =  0.7395601\n",
      "Epoch:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2458 - acc: 0.6780 - val_loss: 0.2455 - val_acc: 0.6783\n",
      ". theta fit =  0.7395601\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2462 - acc: 0.6773 - val_loss: -0.2461 - val_acc: 0.6783\n",
      ". theta fit =  0.75717723\n",
      "Epoch:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2464 - acc: 0.6770 - val_loss: 0.2461 - val_acc: 0.6785\n",
      ". theta fit =  0.75717723\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2467 - acc: 0.6776 - val_loss: -0.2466 - val_acc: 0.6785\n",
      ". theta fit =  0.77545446\n",
      "Epoch:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2469 - acc: 0.6764 - val_loss: 0.2466 - val_acc: 0.6770\n",
      ". theta fit =  0.77545446\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2472 - acc: 0.6762 - val_loss: -0.2471 - val_acc: 0.6770\n",
      ". theta fit =  0.79340106\n",
      "Epoch:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2474 - acc: 0.6759 - val_loss: 0.2471 - val_acc: 0.6777\n",
      ". theta fit =  0.79340106\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2476 - acc: 0.6769 - val_loss: -0.2475 - val_acc: 0.6777\n",
      ". theta fit =  0.81101924\n",
      "Epoch:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2478 - acc: 0.6742 - val_loss: 0.2475 - val_acc: 0.6752\n",
      ". theta fit =  0.81101924\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2480 - acc: 0.6745 - val_loss: -0.2479 - val_acc: 0.6752\n",
      ". theta fit =  0.8285614\n",
      "Epoch:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2482 - acc: 0.6734 - val_loss: 0.2479 - val_acc: 0.6776\n",
      ". theta fit =  0.8285614\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: -0.2484 - acc: 0.6769 - val_loss: -0.2482 - val_acc: 0.6776\n",
      ". theta fit =  0.8455931\n",
      "Epoch:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2485 - acc: 0.6726 - val_loss: 0.2483 - val_acc: 0.6635\n",
      ". theta fit =  0.8455931\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2487 - acc: 0.6629 - val_loss: -0.2485 - val_acc: 0.6635\n",
      ". theta fit =  0.8615167\n",
      "Epoch:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2488 - acc: 0.6706 - val_loss: 0.2485 - val_acc: 0.6719\n",
      ". theta fit =  0.8615167\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2490 - acc: 0.6712 - val_loss: -0.2488 - val_acc: 0.6719\n",
      ". theta fit =  0.8773413\n",
      "Epoch:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2491 - acc: 0.6701 - val_loss: 0.2488 - val_acc: 0.6655\n",
      ". theta fit =  0.8773413\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2492 - acc: 0.6650 - val_loss: -0.2490 - val_acc: 0.6655\n",
      ". theta fit =  0.8914941\n",
      "Epoch:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2493 - acc: 0.6693 - val_loss: 0.2490 - val_acc: 0.6612\n",
      ". theta fit =  0.8914941\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: -0.2494 - acc: 0.6609 - val_loss: -0.2492 - val_acc: 0.6612\n",
      ". theta fit =  0.9047915\n",
      "Epoch:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2494 - acc: 0.6691 - val_loss: 0.2491 - val_acc: 0.6717\n",
      ". theta fit =  0.9047915\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2495 - acc: 0.6709 - val_loss: -0.2493 - val_acc: 0.6717\n",
      ". theta fit =  0.91794795\n",
      "Epoch:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2496 - acc: 0.6658 - val_loss: 0.2493 - val_acc: 0.6740\n",
      ". theta fit =  0.91794795\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2497 - acc: 0.6733 - val_loss: -0.2494 - val_acc: 0.6740\n",
      ". theta fit =  0.9310839\n",
      "Epoch:  50\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2497 - acc: 0.6621 - val_loss: 0.2494 - val_acc: 0.6715\n",
      ". theta fit =  0.9310839\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2498 - acc: 0.6707 - val_loss: -0.2495 - val_acc: 0.6715\n",
      ". theta fit =  0.9422963\n",
      "Epoch:  51\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2498 - acc: 0.6648 - val_loss: 0.2495 - val_acc: 0.6654\n",
      ". theta fit =  0.9422963\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2498 - acc: 0.6649 - val_loss: -0.2496 - val_acc: 0.6654\n",
      ". theta fit =  0.95288837\n",
      "Epoch:  52\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2499 - acc: 0.6538 - val_loss: 0.2496 - val_acc: 0.6731\n",
      ". theta fit =  0.95288837\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2499 - acc: 0.6725 - val_loss: -0.2496 - val_acc: 0.6731\n",
      ". theta fit =  0.9612217\n",
      "Epoch:  53\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2499 - acc: 0.6538 - val_loss: 0.2496 - val_acc: 0.6684\n",
      ". theta fit =  0.9612217\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2499 - acc: 0.6678 - val_loss: -0.2497 - val_acc: 0.6684\n",
      ". theta fit =  0.96913433\n",
      "Epoch:  54\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2499 - acc: 0.6351 - val_loss: 0.2497 - val_acc: 0.6853\n",
      ". theta fit =  0.96913433\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2500 - acc: 0.6843 - val_loss: -0.2497 - val_acc: 0.6853\n",
      ". theta fit =  0.97596604\n",
      "Epoch:  55\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2500 - acc: 0.6470 - val_loss: 0.2497 - val_acc: 0.6741\n",
      ". theta fit =  0.97596604\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: -0.2500 - acc: 0.6735 - val_loss: -0.2497 - val_acc: 0.6741\n",
      ". theta fit =  0.98171973\n",
      "Epoch:  56\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2500 - acc: 0.6307 - val_loss: 0.2497 - val_acc: 0.6628\n",
      ". theta fit =  0.98171973\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2500 - acc: 0.6623 - val_loss: -0.2497 - val_acc: 0.6628\n",
      ". theta fit =  0.9855121\n",
      "Epoch:  57\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2500 - acc: 0.6191 - val_loss: 0.2497 - val_acc: 0.6881\n",
      ". theta fit =  0.9855121\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2500 - acc: 0.6871 - val_loss: -0.2497 - val_acc: 0.6881\n",
      ". theta fit =  0.98969144\n",
      "Epoch:  58\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2500 - acc: 0.6217 - val_loss: 0.2497 - val_acc: 0.5944\n",
      ". theta fit =  0.98969144\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2500 - acc: 0.5949 - val_loss: -0.2497 - val_acc: 0.5944\n",
      ". theta fit =  0.9921652\n",
      "Epoch:  59\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2500 - acc: 0.6087 - val_loss: 0.2497 - val_acc: 0.6834\n",
      ". theta fit =  0.9921652\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2500 - acc: 0.6825 - val_loss: -0.2497 - val_acc: 0.6834\n",
      ". theta fit =  0.9945862\n",
      "Epoch:  60\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2500 - acc: 0.5639 - val_loss: 0.2497 - val_acc: 0.5747\n",
      ". theta fit =  0.9945862\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: -0.2500 - acc: 0.5755 - val_loss: -0.2497 - val_acc: 0.5747\n",
      ". theta fit =  0.99564826\n",
      "Epoch:  61\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2500 - acc: 0.5997 - val_loss: 0.2497 - val_acc: 0.6068\n",
      ". theta fit =  0.99564826\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2500 - acc: 0.6071 - val_loss: -0.2497 - val_acc: 0.6068\n",
      ". theta fit =  0.99696445\n",
      "Epoch:  62\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2500 - acc: 0.5516 - val_loss: 0.2497 - val_acc: 0.6899\n",
      ". theta fit =  0.99696445\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2500 - acc: 0.6895 - val_loss: -0.2497 - val_acc: 0.6899\n",
      ". theta fit =  0.9994431\n",
      "Epoch:  63\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2500 - acc: 0.5198 - val_loss: 0.2497 - val_acc: 0.5181\n",
      ". theta fit =  0.9994431\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2500 - acc: 0.5189 - val_loss: -0.2497 - val_acc: 0.5181\n",
      ". theta fit =  1.0000823\n",
      "Epoch:  64\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2500 - acc: 0.5199 - val_loss: 0.2498 - val_acc: 0.4178\n",
      ". theta fit =  1.0000823\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2500 - acc: 0.4187 - val_loss: -0.2498 - val_acc: 0.4178\n",
      ". theta fit =  0.998032\n",
      "Epoch:  65\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2500 - acc: 0.5308 - val_loss: 0.2497 - val_acc: 0.6770\n",
      ". theta fit =  0.998032\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2500 - acc: 0.6766 - val_loss: -0.2497 - val_acc: 0.6770\n",
      ". theta fit =  0.99899745\n",
      "Epoch:  66\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2500 - acc: 0.5319 - val_loss: 0.2497 - val_acc: 0.4917\n",
      ". theta fit =  0.99899745\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2500 - acc: 0.4925 - val_loss: -0.2497 - val_acc: 0.4917\n",
      ". theta fit =  0.99918026\n",
      "Epoch:  67\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2500 - acc: 0.5073 - val_loss: 0.2497 - val_acc: 0.5647\n",
      ". theta fit =  0.99918026\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: -0.2500 - acc: 0.5661 - val_loss: -0.2497 - val_acc: 0.5647\n",
      ". theta fit =  0.9998846\n",
      "Epoch:  68\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2500 - acc: 0.5169 - val_loss: 0.2497 - val_acc: 0.5054\n",
      ". theta fit =  0.9998846\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2500 - acc: 0.5073 - val_loss: -0.2497 - val_acc: 0.5054\n",
      ". theta fit =  1.0002912\n",
      "Epoch:  69\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2500 - acc: 0.4733 - val_loss: 0.2497 - val_acc: 0.5615\n",
      ". theta fit =  1.0002912\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2500 - acc: 0.5628 - val_loss: -0.2497 - val_acc: 0.5615\n",
      ". theta fit =  1.0011989\n",
      "Epoch:  70\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2500 - acc: 0.4506 - val_loss: 0.2497 - val_acc: 0.5166\n",
      ". theta fit =  1.0011989\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: -0.2500 - acc: 0.5174 - val_loss: -0.2497 - val_acc: 0.5166\n",
      ". theta fit =  1.0008618\n",
      "Epoch:  71\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2500 - acc: 0.5246 - val_loss: 0.2497 - val_acc: 0.3569\n",
      ". theta fit =  1.0008618\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2500 - acc: 0.3575 - val_loss: -0.2497 - val_acc: 0.3569\n",
      ". theta fit =  0.99999887\n",
      "Epoch:  72\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2500 - acc: 0.5070 - val_loss: 0.2497 - val_acc: 0.4728\n",
      ". theta fit =  0.99999887\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2500 - acc: 0.4738 - val_loss: -0.2497 - val_acc: 0.4728\n",
      ". theta fit =  0.9999531\n",
      "Epoch:  73\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2500 - acc: 0.5116 - val_loss: 0.2497 - val_acc: 0.5247\n",
      ". theta fit =  0.9999531\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2500 - acc: 0.5261 - val_loss: -0.2497 - val_acc: 0.5247\n",
      ". theta fit =  1.0008901\n",
      "Epoch:  74\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2500 - acc: 0.5059 - val_loss: 0.2497 - val_acc: 0.4786\n",
      ". theta fit =  1.0008901\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2500 - acc: 0.4804 - val_loss: -0.2497 - val_acc: 0.4786\n",
      ". theta fit =  1.0013624\n",
      "Epoch:  75\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2500 - acc: 0.5196 - val_loss: 0.2497 - val_acc: 0.4262\n",
      ". theta fit =  1.0013624\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: -0.2500 - acc: 0.4275 - val_loss: -0.2497 - val_acc: 0.4262\n",
      ". theta fit =  1.0002778\n",
      "Epoch:  76\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2500 - acc: 0.4988 - val_loss: 0.2497 - val_acc: 0.4896\n",
      ". theta fit =  1.0002778\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2500 - acc: 0.4904 - val_loss: -0.2497 - val_acc: 0.4896\n",
      ". theta fit =  0.99912345\n",
      "Epoch:  77\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2500 - acc: 0.5433 - val_loss: 0.2497 - val_acc: 0.4724\n",
      ". theta fit =  0.99912345\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2500 - acc: 0.4739 - val_loss: -0.2497 - val_acc: 0.4724\n",
      ". theta fit =  0.998905\n",
      "Epoch:  78\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2500 - acc: 0.5078 - val_loss: 0.2497 - val_acc: 0.4679\n",
      ". theta fit =  0.998905\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2500 - acc: 0.4689 - val_loss: -0.2498 - val_acc: 0.4679\n",
      ". theta fit =  0.9980532\n",
      "Epoch:  79\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2500 - acc: 0.5332 - val_loss: 0.2497 - val_acc: 0.4722\n",
      ". theta fit =  0.9980532\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2500 - acc: 0.4732 - val_loss: -0.2497 - val_acc: 0.4722\n",
      ". theta fit =  0.99713445\n",
      "Epoch:  80\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2500 - acc: 0.5679 - val_loss: 0.2497 - val_acc: 0.4687\n",
      ". theta fit =  0.99713445\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2500 - acc: 0.4698 - val_loss: -0.2497 - val_acc: 0.4687\n",
      ". theta fit =  0.99621344\n",
      "Epoch:  81\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2500 - acc: 0.5518 - val_loss: 0.2497 - val_acc: 0.6825\n",
      ". theta fit =  0.99621344\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: -0.2500 - acc: 0.6821 - val_loss: -0.2497 - val_acc: 0.6825\n",
      ". theta fit =  0.99804765\n",
      "Epoch:  82\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2500 - acc: 0.5614 - val_loss: 0.2497 - val_acc: 0.5027\n",
      ". theta fit =  0.99804765\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2500 - acc: 0.5047 - val_loss: -0.2497 - val_acc: 0.5027\n",
      ". theta fit =  0.9987766\n",
      "Epoch:  83\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2500 - acc: 0.5188 - val_loss: 0.2497 - val_acc: 0.5848\n",
      ". theta fit =  0.9987766\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2500 - acc: 0.5853 - val_loss: -0.2497 - val_acc: 0.5848\n",
      ". theta fit =  1.0002344\n",
      "Epoch:  84\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2500 - acc: 0.5206 - val_loss: 0.2497 - val_acc: 0.6179\n",
      ". theta fit =  1.0002344\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2500 - acc: 0.6178 - val_loss: -0.2497 - val_acc: 0.6179\n",
      ". theta fit =  1.0004667\n",
      "Epoch:  85\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2500 - acc: 0.5163 - val_loss: 0.2497 - val_acc: 0.6496\n",
      ". theta fit =  1.0004667\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2500 - acc: 0.6496 - val_loss: -0.2497 - val_acc: 0.6496\n",
      ". theta fit =  1.0023173\n",
      "Epoch:  86\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2500 - acc: 0.4707 - val_loss: 0.2497 - val_acc: 0.5662\n",
      ". theta fit =  1.0023173\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2500 - acc: 0.5660 - val_loss: -0.2497 - val_acc: 0.5662\n",
      ". theta fit =  1.0028389\n",
      "Epoch:  87\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2500 - acc: 0.4344 - val_loss: 0.2497 - val_acc: 0.5043\n",
      ". theta fit =  1.0028389\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: -0.2500 - acc: 0.5059 - val_loss: -0.2497 - val_acc: 0.5043\n",
      ". theta fit =  1.0028573\n",
      "Epoch:  88\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2500 - acc: 0.4708 - val_loss: 0.2497 - val_acc: 0.5161\n",
      ". theta fit =  1.0028573\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2500 - acc: 0.5160 - val_loss: -0.2497 - val_acc: 0.5161\n",
      ". theta fit =  1.0039176\n",
      "Epoch:  89\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2500 - acc: 0.4443 - val_loss: 0.2497 - val_acc: 0.5259\n",
      ". theta fit =  1.0039176\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2500 - acc: 0.5265 - val_loss: -0.2497 - val_acc: 0.5259\n",
      ". theta fit =  1.0041804\n",
      "Epoch:  90\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2500 - acc: 0.4444 - val_loss: 0.2497 - val_acc: 0.4028\n",
      ". theta fit =  1.0041804\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2500 - acc: 0.4036 - val_loss: -0.2497 - val_acc: 0.4028\n",
      ". theta fit =  1.0035673\n",
      "Epoch:  91\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2500 - acc: 0.4413 - val_loss: 0.2497 - val_acc: 0.5150\n",
      ". theta fit =  1.0035673\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2500 - acc: 0.5148 - val_loss: -0.2497 - val_acc: 0.5150\n",
      ". theta fit =  1.0035362\n",
      "Epoch:  92\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2500 - acc: 0.4241 - val_loss: 0.2497 - val_acc: 0.4557\n",
      ". theta fit =  1.0035362\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2500 - acc: 0.4568 - val_loss: -0.2497 - val_acc: 0.4557\n",
      ". theta fit =  1.0034561\n",
      "Epoch:  93\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2500 - acc: 0.4579 - val_loss: 0.2497 - val_acc: 0.5157\n",
      ". theta fit =  1.0034561\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: -0.2500 - acc: 0.5154 - val_loss: -0.2497 - val_acc: 0.5157\n",
      ". theta fit =  1.0038168\n",
      "Epoch:  94\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2500 - acc: 0.4564 - val_loss: 0.2497 - val_acc: 0.3477\n",
      ". theta fit =  1.0038168\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2500 - acc: 0.3489 - val_loss: -0.2497 - val_acc: 0.3477\n",
      ". theta fit =  1.0019482\n",
      "Epoch:  95\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2500 - acc: 0.4665 - val_loss: 0.2497 - val_acc: 0.4594\n",
      ". theta fit =  1.0019482\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2500 - acc: 0.4613 - val_loss: -0.2497 - val_acc: 0.4594\n",
      ". theta fit =  1.0010438\n",
      "Epoch:  96\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2500 - acc: 0.4642 - val_loss: 0.2497 - val_acc: 0.4744\n",
      ". theta fit =  1.0010438\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2500 - acc: 0.4753 - val_loss: -0.2497 - val_acc: 0.4744\n",
      ". theta fit =  0.9999998\n",
      "Epoch:  97\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2500 - acc: 0.4952 - val_loss: 0.2497 - val_acc: 0.5821\n",
      ". theta fit =  0.9999998\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2500 - acc: 0.5827 - val_loss: -0.2497 - val_acc: 0.5821\n",
      ". theta fit =  1.0005366\n",
      "Epoch:  98\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2500 - acc: 0.4763 - val_loss: 0.2497 - val_acc: 0.6683\n",
      ". theta fit =  1.0005366\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2500 - acc: 0.6680 - val_loss: -0.2497 - val_acc: 0.6683\n",
      ". theta fit =  1.0027527\n",
      "Epoch:  99\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2500 - acc: 0.4859 - val_loss: 0.2497 - val_acc: 0.3701\n",
      ". theta fit =  1.0027527\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: -0.2500 - acc: 0.3713 - val_loss: -0.2497 - val_acc: 0.3701\n",
      ". theta fit =  1.0018835\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape = list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 1e-5 #smaller learning rate yields better precision\n",
    "epochs = 100 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs\n",
    "    x = K.gather(x, np.arange(batch_size))\n",
    "    theta = 0. #starting value\n",
    "    #Getting theta_prime:\n",
    "    if train_theta == False:\n",
    "        theta_prime = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta_prime = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "        \n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    theta0_stack = K.ones_like(x,dtype=tf.float32)*theta0 \n",
    "    concat_input_and_params = K.ones(shape = (x.shape), dtype=tf.float32)*theta_prime\n",
    "    \n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "   \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(data)\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(np.mean(theta1), 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"GaussianAltFit-1D\\nN = {:.0e}, learning rate = {:.0e}, Epochs = {:.0f}\".format(N, lr, len(fit_vals)))\n",
    "#plt.savefig(\"GaussianAltFit-1D-eg4: N = {:.0e}, learning rate = {:.0e}, Epochs = {:.0f}.png\".format(N, lr, len(fit_vals)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
