{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import argrelmin, argrelmax\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__) #1.15.0\n",
    "print(keras.__version__) #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{(1-f(x_{T,i},\\theta'))}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it a $\\mu$ value in addition to $X_i\\sim\\mathcal{N}(\\mu, 1)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = 5*10**6\n",
    "sigma_min = 0.5\n",
    "sigma_max = 4.5\n",
    "sigma_values = np.random.uniform(sigma_min, sigma_max, n_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with this $\\mu$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = [(np.random.normal(0, 1), sigma) for sigma in sigma_values] # Note the zero in normal(0, 1) \n",
    "X1 = [(np.random.normal(0, sigma), sigma) for sigma in sigma_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\sigma=0$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\sigma$ values uniformly sampled in the specified range [sigma_min, sigma_max].\n",
    "\n",
    "For every value of $\\sigma$ in mu_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(0, \\sigma)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\sigma$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(0, \\sigma)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "X = np.concatenate((X0, X1))\n",
    "Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((2,))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs = inputs, outputs = outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 8000000 samples, validate on 2000000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "4450000/8000000 [===============>..............] - ETA: 6s - loss: 0.5650 - acc: 0.6804"
     ]
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience = 10,\n",
    "                              restore_best_weights=True)\n",
    "dctr_model.fit(X_train, Y_train, \n",
    "          epochs=200, \n",
    "          batch_size = 10000,\n",
    "          validation_data = (X_test, Y_test),\n",
    "          callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"1d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"1d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_param = 1\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr_model(d)\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def analytical_reweight(d): #from analytical formula for normal distributions\n",
    "    events = d[:,0]\n",
    "    param = d[:,1]\n",
    "    weights = (1/param)*K.exp(-0.5*(((events)/param)**2-((events))**2))\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate DCTR for any $\\mu$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma1 = 2\n",
    "assert sigma1>=sigma_min and sigma1<=sigma_max # choose mu1 in valid range\n",
    "X0_val = np.random.normal(0, 1, n_data_points)\n",
    "X1_val = np.random.normal(0, sigma1, n_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = np.array([(x, sigma1) for x in X0_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = reweight(tf.convert_to_tensor(X_input, dtype = tf.float32))\n",
    "analytical_weights = analytical_reweight(tf.convert_to_tensor(X_input, dtype = tf.float32))\n",
    "weights = K.eval(weights)\n",
    "analytical_weights = K.eval(analytical_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-8,8,31)\n",
    "plt.hist(X0_val, bins = bins, alpha = 0.5, label = r'$\\sigma=1$')\n",
    "plt.hist(X0_val, bins = bins, label = r'$\\sigma=1$ NN wgt.',\n",
    "         weights=weights, histtype='step', color='k')\n",
    "plt.hist(X0_val, bins = bins, label = r'$\\sigma=1$ analytical wgt.',\n",
    "         weights=analytical_weights, histtype='step', linestyle = '--',color='k')\n",
    "plt.hist(X1_val, bins = bins, alpha = 0.5, label = r'$\\sigma={}$'.format(sigma1))\n",
    "plt.legend()\n",
    "plt.title(\"Truth Level: Reweighting\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "theta0_param = 1 #this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = 1.5 #this is the data (the target)\n",
    "\n",
    "theta0 = np.random.normal(0,theta0_param,N)\n",
    "theta1 = np.random.normal(0,theta1_param,N)\n",
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0,theta1])\n",
    "yvals = np.concatenate([labels0,labels1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals, yvals, test_size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Model\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('1d_gaussian_dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"1d_gaussian_dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to True for analytical_reweight\n",
    "\n",
    "reweight_analytically = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',\n",
    "                              patience = 3, \n",
    "                              min_delta = 0.00005,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0.):\n",
    "    x  = inputs\n",
    "    x = K.gather(x, np.arange(1000))\n",
    "\n",
    "    theta = 0. #starting value\n",
    "    theta_prime = val\n",
    "    \n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    concat_input_and_params = K.ones(shape = x.shape)*theta_prime\n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "    \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(events = x,  \n",
    "                                sigma1 = theta_prime)\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thetas = np.linspace(0.5,4.5,33)\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train,\n",
    "              epochs=100, batch_size=1000,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1, callbacks = [earlystopping])\n",
    "    lvals+=[np.min(model.history.history['loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thetas,lvals)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(np.std(theta1), ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 1.\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',\n",
    "                              patience = 3, \n",
    "                              #min_delta = 0.00005,\n",
    "                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape = list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "index_refine = np.array([0])\n",
    "batch_size = 2*N\n",
    "lr = 5e-1 #smaller learning rate yields better precision\n",
    "iterations = 100 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "index_switch = 0\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs\n",
    "    \n",
    "    theta = 0. #starting value\n",
    "    #Getting theta_prime:\n",
    "    if train_theta == False:\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        theta_prime = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        theta_prime = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "        \n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    theta0_stack = K.ones_like(x,dtype=tf.float32)*theta0 \n",
    "    concat_input_and_params = K.ones(shape = (x.shape), dtype=tf.float32)*theta_prime\n",
    "    \n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "   \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(data)\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        #t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        \n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4391\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7000 - acc: 0.4226\n",
      ". theta fit =  1.531681\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.7002 - acc: 0.4404\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4326\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4343\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4337\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4340\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4370\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4297\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4331\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4343\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4330\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7000 - acc: 0.4318\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7000 - acc: 0.4624\n",
      ". theta fit =  1.531647\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.7002 - acc: 0.4351\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4271\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4321\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4352\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4332\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7001 - acc: 0.4688\n",
      ". theta fit =  1.5316818\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.7004 - acc: 0.4359\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7001 - acc: 0.4259\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4356\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7000 - acc: 0.4320\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7001 - acc: 0.4369\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4336\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4366\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4357\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4362\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4321\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7001 - acc: 0.4320\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7000 - acc: 0.4343\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7000 - acc: 0.4646\n",
      ". theta fit =  1.5316467\n",
      "Iteration:  50\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.7001 - acc: 0.4306\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4360\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4346\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4374\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4352\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4304\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7000 - acc: 0.4374\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4354\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7000 - acc: 0.4288\n",
      ". theta fit =  1.5316113\n",
      "Iteration:  51\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: 0.7001 - acc: 0.4319\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4286\n",
      "Epoch 3/50\n",
      " 669000/2000000 [=========>....................] - ETA: 7s - loss: 0.7003 - acc: 0.4339"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4355\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6999 - acc: 0.4269\n",
      ". theta fit =  1.5316471\n",
      "Iteration:  52\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: 0.7002 - acc: 0.4315\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4359\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4337\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4350\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7000 - acc: 0.4363\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4308\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7000 - acc: 0.4335\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7001 - acc: 0.4355\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4342\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.7000 - acc: 0.4194\n",
      ". theta fit =  1.5316831\n",
      "Iteration:  53\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.7001 - acc: 0.4377\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4291\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7000 - acc: 0.4340\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4370\n",
      "Epoch 5/50\n",
      " 908000/2000000 [============>.................] - ETA: 5s - loss: 0.7003 - acc: 0.4359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6999 - acc: 0.4334\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4364\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4393\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4269\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.7001 - acc: 0.4157\n",
      ". theta fit =  1.5317562\n",
      "Iteration:  55\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: 0.7002 - acc: 0.4385\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4329\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4322\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4351\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4340\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4333\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4340\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6999 - acc: 0.4325\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4330\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7001 - acc: 0.4313\n",
      "Epoch 11/50\n",
      " 390000/2000000 [====>.........................] - ETA: 9s - loss: 0.7019 - acc: 0.4412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4380\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4348\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4349\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4330\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4300\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.7000 - acc: 0.4380\n",
      ". theta fit =  1.5318294\n",
      "Iteration:  57\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.7003 - acc: 0.4352\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4351\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7000 - acc: 0.4295\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4294\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4380\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4328\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4414\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4314\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4295\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6999 - acc: 0.4175\n",
      ". theta fit =  1.5318669\n",
      "Iteration:  58\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.7002 - acc: 0.4378\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4357\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4313\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4347\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4326\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4347\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.7000 - acc: 0.4278\n",
      ". theta fit =  1.5319048\n",
      "Iteration:  59\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.7001 - acc: 0.4314\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4298\n",
      "Epoch 3/50\n",
      "1132000/2000000 [===============>..............] - ETA: 5s - loss: 0.7004 - acc: 0.4282"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4309\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4355\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4248\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4355\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4360\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.7000 - acc: 0.4229\n",
      ". theta fit =  1.5319046\n",
      "Iteration:  61\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 23s 11us/step - loss: 0.7001 - acc: 0.4267\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4370\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4340\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4321\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4349\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4348\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6999 - acc: 0.4653\n",
      ". theta fit =  1.5319432\n",
      "Refining learning rate\n",
      "Iteration:  62\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 25s 12us/step - loss: 0.7002 - acc: 0.4338\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4287\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4322\n",
      "Epoch 4/50\n",
      " 245000/2000000 [==>...........................] - ETA: 10s - loss: 0.6996 - acc: 0.4337"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7004 - acc: 0.4297\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4390\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4284\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4365\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4338\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4339\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4283\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7000 - acc: 0.4314\n",
      ". theta fit =  1.531955\n",
      "Iteration:  65\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7003 - acc: 0.4355\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4315\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4314\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4340\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6999 - acc: 0.4348\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6999 - acc: 0.4348\n",
      "Epoch 7/50\n",
      "1756000/2000000 [=========================>....] - ETA: 1s - loss: 0.7005 - acc: 0.4328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4335\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7001 - acc: 0.4160\n",
      ". theta fit =  1.5319551\n",
      "Iteration:  67\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7004 - acc: 0.4371\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4340\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4302\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4318\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6999 - acc: 0.4325\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4317\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4335\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4324\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7001 - acc: 0.4213\n",
      ". theta fit =  1.5319592\n",
      "Iteration:  68\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7002 - acc: 0.4357\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4267\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6999 - acc: 0.4327\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4303\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4352\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4333\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7000 - acc: 0.4219\n",
      ". theta fit =  1.5319551\n",
      "Iteration:  69\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: 0.7003 - acc: 0.4304\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4327\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4294\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7002 - acc: 0.4350\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4336\n",
      "Epoch 6/50\n",
      "1893000/2000000 [===========================>..] - ETA: 0s - loss: 0.6998 - acc: 0.4279"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4325\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7000 - acc: 0.4347\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - ETA: 0s - loss: 0.7000 - acc: 0.434 - 12s 6us/step - loss: 0.7000 - acc: 0.4349\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6999 - acc: 0.4209\n",
      ". theta fit =  1.5319633\n",
      "Iteration:  71\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: 0.7002 - acc: 0.4360\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4353\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4279\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4317\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4369\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4332\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4278\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4343\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6999 - acc: 0.4287\n",
      ". theta fit =  1.5319592\n",
      "Iteration:  72\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7002 - acc: 0.4351\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4312\n",
      "Epoch 3/50\n",
      "1308000/2000000 [==================>...........] - ETA: 4s - loss: 0.6995 - acc: 0.4351"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4298\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6999 - acc: 0.4616\n",
      ". theta fit =  1.5319633\n",
      "Refining learning rate\n",
      "Iteration:  73\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7000 - acc: 0.4359\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4285\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4371\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4338\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4337\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4345\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4355\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.6999 - acc: 0.4154\n",
      ". theta fit =  1.5319637\n",
      "Iteration:  74\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7001 - acc: 0.4339\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4342\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4324\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4321\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4350\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.7000 - acc: 0.4749\n",
      ". theta fit =  1.5319632\n",
      "Iteration:  75\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "1727000/2000000 [========================>.....] - ETA: 3s - loss: 0.7004 - acc: 0.4404"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4328\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4352\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4363\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4343\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4328\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4392\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4368\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4335\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4370\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.7000 - acc: 0.4704\n",
      ". theta fit =  1.5319632\n",
      "Iteration:  77\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 25s 12us/step - loss: 0.7002 - acc: 0.4363\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4373\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4301\n",
      "Epoch 4/50\n",
      "1676000/2000000 [========================>.....] - ETA: 2s - loss: 0.7004 - acc: 0.4365"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4345\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4338\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4294\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6999 - acc: 0.4320\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4394\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4317\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4316\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.7000 - acc: 0.4307\n",
      ". theta fit =  1.5319642\n",
      "Iteration:  79\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 25s 12us/step - loss: 0.7003 - acc: 0.4385\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4337\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4334\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4335\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4364\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4319\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4339\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4339\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4335\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.6999 - acc: 0.4135\n",
      ". theta fit =  1.5319647\n",
      "Iteration:  80\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: 0.7001 - acc: 0.4367\n",
      "Epoch 2/50\n",
      "1894000/2000000 [===========================>..] - ETA: 0s - loss: 0.6997 - acc: 0.4371"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4345\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6999 - acc: 0.4187\n",
      ". theta fit =  1.5319651\n",
      "Iteration:  81\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: 0.7004 - acc: 0.4381\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4375\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4310\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4309\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4371\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4364\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4330\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6999 - acc: 0.4607\n",
      ". theta fit =  1.5319656\n",
      "Iteration:  82\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: 0.7001 - acc: 0.4418\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7000 - acc: 0.4314\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4315\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6999 - acc: 0.4320\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6999 - acc: 0.4311\n",
      "Epoch 6/50\n",
      " 108000/2000000 [>.............................] - ETA: 13s - loss: 0.7040 - acc: 0.4649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4350\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7001 - acc: 0.4354\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4295\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6999 - acc: 0.4612\n",
      ". theta fit =  1.5319666\n",
      "Iteration:  84\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: 0.7001 - acc: 0.4371\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6999 - acc: 0.4300\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4377\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4305\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4378\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6999 - acc: 0.4222\n",
      ". theta fit =  1.531967\n",
      "Iteration:  85\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: 0.7002 - acc: 0.4371\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4335\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4363\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7000 - acc: 0.4322\n",
      "Epoch 5/50\n",
      "1799000/2000000 [=========================>....] - ETA: 1s - loss: 0.6997 - acc: 0.4293"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7001 - acc: 0.4357\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6999 - acc: 0.4339\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7001 - acc: 0.4368\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4358\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4342\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.7002 - acc: 0.4166\n",
      ". theta fit =  1.5319675\n",
      "Iteration:  88\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: 0.7005 - acc: 0.4380\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4334\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7001 - acc: 0.4335\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6999 - acc: 0.4311\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7001 - acc: 0.4371\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4300\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4353\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: -0.7001 - acc: 0.4184\n",
      ". theta fit =  1.531968\n",
      "Iteration:  89\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      " 452000/2000000 [=====>........................] - ETA: 58s - loss: 0.7009 - acc: 0.4363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4346\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4308\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6999 - acc: 0.4254\n",
      ". theta fit =  1.5319675\n",
      "Iteration:  90\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 27s 14us/step - loss: 0.7001 - acc: 0.4374\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4322\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4322\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4334\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4339\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4321\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: -0.6999 - acc: 0.4208\n",
      ". theta fit =  1.531967\n",
      "Iteration:  91\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.7002 - acc: 0.4364\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4311\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4326\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6999 - acc: 0.4345\n",
      "Epoch 5/50\n",
      "1180000/2000000 [================>.............] - ETA: 5s - loss: 0.7002 - acc: 0.4427"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6999 - acc: 0.4358\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7001 - acc: 0.4395\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7001 - acc: 0.4321\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4351\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.7001 - acc: 0.4170\n",
      ". theta fit =  1.5319675\n",
      "Iteration:  93\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.7004 - acc: 0.4442\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4296\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4369\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7001 - acc: 0.4315\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4308\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6999 - acc: 0.4306\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4342\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4296\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4311\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.7000 - acc: 0.4634\n",
      ". theta fit =  1.5319675\n",
      "Iteration:  94\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.7002 - acc: 0.4377\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6999 - acc: 0.4299\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4330\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4360\n",
      "Epoch 5/50\n",
      " 788000/2000000 [==========>...................] - ETA: 8s - loss: 0.6997 - acc: 0.4356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7001 - acc: 0.4356\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4284\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4334\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.7000 - acc: 0.4494\n",
      ". theta fit =  1.5319675\n",
      "Iteration:  96\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: 0.7001 - acc: 0.4329\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4316\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7000 - acc: 0.4341\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6999 - acc: 0.4306\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4345\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4335\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4353\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.7003 - acc: 0.4203\n",
      ". theta fit =  1.5319675\n",
      "Iteration:  97\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: 0.7003 - acc: 0.4365\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4338\n",
      "Epoch 3/50\n",
      " 980000/2000000 [=============>................] - ETA: 7s - loss: 0.7014 - acc: 0.4392"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4314\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4328\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4334\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.7000 - acc: 0.4174\n",
      ". theta fit =  1.5319675\n",
      "Iteration:  98\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 29s 15us/step - loss: 0.7004 - acc: 0.4390\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4374\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4283\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4344\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4332\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4326\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4321\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6999 - acc: 0.4426\n",
      ". theta fit =  1.5319675\n",
      "Iteration:  99\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 29s 15us/step - loss: 0.7004 - acc: 0.4402\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7001 - acc: 0.4348\n",
      "Epoch 3/50\n",
      " 878000/2000000 [============>.................] - ETA: 7s - loss: 0.6995 - acc: 0.4345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4345\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7000 - acc: 0.4320\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6999 - acc: 0.4235\n",
      ". theta fit =  1.5319675\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):    \n",
    "    print(\"Iteration: \",iteration )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=50, batch_size=1000,validation_data=(np.array(X_test), y_test),verbose=1, callbacks = [earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "    \n",
    "        \n",
    "        # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "    \n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "    \n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "            \n",
    "    extrema = np.concatenate((argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema\", extrema)\n",
    "    '''\n",
    "    \n",
    "    if (len(extrema) == 0): # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema) >= 6): #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print(\"Refining learning rate\")\n",
    "        optimizer.lr = optimizer.lr/10.\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(np.std(theta1), 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"GaussianAltFit-1D-sigma (DCTR Reweight)\\nN = {:.0e}, Iterations = {:.0f}\".format(N, len(fit_vals)))\n",
    "plt.savefig(\"GaussianAltFit1D-sigma (DCTR Reweight)\\nN = {:.0e}, Iterations = {:.0f}.png\".format(N, len(fit_vals)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweight_analytically = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 1.\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',\n",
    "                              patience = 3, \n",
    "                              #min_delta = 0.00005,\n",
    "                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 1)                 1         \n",
      "=================================================================\n",
      "Total params: 16,898\n",
      "Trainable params: 16,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape = list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "index_refine = np.array([0])\n",
    "batch_size = 2*N\n",
    "lr = 5e-1 #smaller learning rate yields better precision\n",
    "iterations = 100 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "index_switch = 0\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs\n",
    "    \n",
    "    theta = 0. #starting value\n",
    "    #Getting theta_prime:\n",
    "    if train_theta == False:\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        theta_prime = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        theta_prime = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "        \n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    theta0_stack = K.ones_like(x,dtype=tf.float32)*theta0 \n",
    "    concat_input_and_params = K.ones(shape = (x.shape), dtype=tf.float32)*theta_prime\n",
    "    \n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "   \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(data)\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        #t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        \n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.6595 - acc: 0.5912\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6567 - acc: 0.5971\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6565 - acc: 0.5969\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6565 - acc: 0.5969\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6565 - acc: 0.5969\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6565 - acc: 0.5970\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6565 - acc: 0.5970\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6565 - acc: 0.5970\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6565 - acc: 0.5969\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6565 - acc: 0.5969\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6565 - acc: 0.5970\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6565 - acc: 0.5970\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6565 - acc: 0.5969\n",
      ". theta fit =  1.4999907\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: 0.6969 - acc: 0.4954\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6940 - acc: 0.4732\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6941 - acc: 0.5062\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6940 - acc: 0.4834\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6939 - acc: 0.4873\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6940 - acc: 0.5045\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6940 - acc: 0.4980\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6939 - acc: 0.5031\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6939 - acc: 0.4933\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6939 - acc: 0.5000\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6939 - acc: 0.4943\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6939 - acc: 0.5042\n",
      "Epoch 13/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6940 - acc: 0.4966\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: -0.6938 - acc: 0.4441\n",
      ". theta fit =  1.1287115\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: 0.6750 - acc: 0.5935\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6743 - acc: 0.5967\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6743 - acc: 0.5966\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6743 - acc: 0.5965\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6743 - acc: 0.5966\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6743 - acc: 0.5965\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6743 - acc: 0.5966\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6743 - acc: 0.5965\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6743 - acc: 0.5965\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6743 - acc: 0.5966\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6743 - acc: 0.5966\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6743 - acc: 0.5969\n",
      ". theta fit =  1.4481108\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 29s 15us/step - loss: 0.6946 - acc: 0.5722\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6936 - acc: 0.5793\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6935 - acc: 0.5787\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6936 - acc: 0.5803\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6935 - acc: 0.5845\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6935 - acc: 0.5862\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6935 - acc: 0.5835\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6935 - acc: 0.5832\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.6934 - acc: 0.5962\n",
      ". theta fit =  1.73862\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 30s 15us/step - loss: 0.6896 - acc: 0.4145\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6891 - acc: 0.4102\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6889 - acc: 0.4119\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6888 - acc: 0.4129\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6889 - acc: 0.4131\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6889 - acc: 0.4135\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6890 - acc: 0.4129\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6886 - acc: 0.4117\n",
      ". theta fit =  1.4658978\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 29s 15us/step - loss: 0.6941 - acc: 0.5448\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6937 - acc: 0.5809\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6937 - acc: 0.5810\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6938 - acc: 0.5673\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6937 - acc: 0.5783\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6937 - acc: 0.5838\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6937 - acc: 0.5772\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6937 - acc: 0.5814\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6935 - acc: 0.5950\n",
      ". theta fit =  1.7268858\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 30s 15us/step - loss: 0.6902 - acc: 0.4165\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6893 - acc: 0.4089\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6893 - acc: 0.4104\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6893 - acc: 0.4119\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6893 - acc: 0.4122\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6892 - acc: 0.4128\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6892 - acc: 0.4136\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6891 - acc: 0.4137\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6895 - acc: 0.4128\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6894 - acc: 0.4128\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6892 - acc: 0.4130\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6895 - acc: 0.4192\n",
      ". theta fit =  1.4737233\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 32s 16us/step - loss: 0.6942 - acc: 0.4977\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6938 - acc: 0.5670\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6938 - acc: 0.5699\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.5761\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.5609\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.5777\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6937 - acc: 0.5747\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6938 - acc: 0.5717\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6937 - acc: 0.5714\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6937 - acc: 0.5959\n",
      ". theta fit =  1.7180203\n",
      "Refining learning rate\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 32s 16us/step - loss: 0.6920 - acc: 0.4284\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6899 - acc: 0.4088\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6897 - acc: 0.4108\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6896 - acc: 0.4108\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6895 - acc: 0.4117\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6896 - acc: 0.4113\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6895 - acc: 0.4122\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6898 - acc: 0.4114\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6897 - acc: 0.4119\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6895 - acc: 0.4118\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.6893 - acc: 0.4105\n",
      ". theta fit =  1.6935849\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 32s 16us/step - loss: 0.6917 - acc: 0.4104\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6906 - acc: 0.4115\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6903 - acc: 0.4117\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6904 - acc: 0.4114\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6903 - acc: 0.4121\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6905 - acc: 0.4114\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6903 - acc: 0.4116\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6905 - acc: 0.4118\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: -0.6902 - acc: 0.4130\n",
      ". theta fit =  1.669366\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 32s 16us/step - loss: 0.6919 - acc: 0.4111\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6912 - acc: 0.4109\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6912 - acc: 0.4113\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6915 - acc: 0.4116\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6911 - acc: 0.4104\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6911 - acc: 0.4114\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6914 - acc: 0.4116\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6910 - acc: 0.4107\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6912 - acc: 0.4112\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6911 - acc: 0.4114\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6910 - acc: 0.4115\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6920 - acc: 0.4112\n",
      "Epoch 13/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6912 - acc: 0.4108\n",
      "Epoch 14/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6914 - acc: 0.4104\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6910 - acc: 0.4150\n",
      ". theta fit =  1.645262\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 33s 16us/step - loss: 0.6932 - acc: 0.4105\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6921 - acc: 0.4104\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6919 - acc: 0.4105\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6919 - acc: 0.4101\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6916 - acc: 0.4111\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6922 - acc: 0.4103\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6918 - acc: 0.4098\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6918 - acc: 0.4101\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.6918 - acc: 0.4095\n",
      ". theta fit =  1.6211939\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 33s 16us/step - loss: 0.6929 - acc: 0.4101\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6925 - acc: 0.4092\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6923 - acc: 0.4099\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6924 - acc: 0.4091\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4094\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4096\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 34s 17us/step - loss: -0.6922 - acc: 0.4082\n",
      ". theta fit =  1.5970978\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 33s 16us/step - loss: 0.6935 - acc: 0.4082\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6931 - acc: 0.4082\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6931 - acc: 0.4093\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6931 - acc: 0.4095\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6929 - acc: 0.4090\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6929 - acc: 0.4101\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6930 - acc: 0.4094\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6928 - acc: 0.4097\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6933 - acc: 0.4110\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6928 - acc: 0.4091\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6929 - acc: 0.4090\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6927 - acc: 0.4133\n",
      ". theta fit =  1.572923\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 33s 16us/step - loss: 0.6936 - acc: 0.4093\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6933 - acc: 0.4091\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6931 - acc: 0.4103\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6935 - acc: 0.4105\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6935 - acc: 0.4113\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6934 - acc: 0.4098\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6939 - acc: 0.4180\n",
      ". theta fit =  1.5486317\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 33s 17us/step - loss: 0.6936 - acc: 0.4110\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6942 - acc: 0.4123\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6938 - acc: 0.4123\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6941 - acc: 0.4127\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6938 - acc: 0.4203\n",
      ". theta fit =  1.5241889\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 32s 16us/step - loss: 0.6941 - acc: 0.4220\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: 0.6939 - acc: 0.4189\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6938 - acc: 0.4219\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4285\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4221\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6939 - acc: 0.4270\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4304\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4326\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6943 - acc: 0.4299\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4271\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4256\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4245\n",
      "Epoch 13/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4309\n",
      "Epoch 14/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4249\n",
      "Epoch 15/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4320\n",
      "Epoch 16/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6944 - acc: 0.4345\n",
      "Epoch 17/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6941 - acc: 0.4225\n",
      "Epoch 18/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6938 - acc: 0.4258\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6938 - acc: 0.4123\n",
      ". theta fit =  1.4995863\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 36s 18us/step - loss: 0.6948 - acc: 0.4363\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6942 - acc: 0.4612\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4810\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4874\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4844\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6936 - acc: 0.4900\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6942 - acc: 0.5066\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6937 - acc: 0.4954\n",
      ". theta fit =  1.5244107\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 33s 17us/step - loss: 0.6946 - acc: 0.4725\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4375\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6938 - acc: 0.4327\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4266\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4301\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6940 - acc: 0.4319\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6937 - acc: 0.4281\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: -0.6935 - acc: 0.4264\n",
      ". theta fit =  1.4993443\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 34s 17us/step - loss: 0.6947 - acc: 0.4602\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6942 - acc: 0.4646\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4806\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6939 - acc: 0.4939\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4983\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 23s 11us/step - loss: 0.6936 - acc: 0.4990\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6938 - acc: 0.4956\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4898\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4961\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 11us/step - loss: -0.6936 - acc: 0.5107\n",
      ". theta fit =  1.5245818\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 33s 17us/step - loss: 0.6960 - acc: 0.4700\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6940 - acc: 0.4419\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4389\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4407\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4331\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4341\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4341\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4321\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6935 - acc: 0.4219\n",
      ". theta fit =  1.4989985\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 33s 17us/step - loss: 0.6953 - acc: 0.4584\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6940 - acc: 0.4703\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4771\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4746\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4864\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4912\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4858\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6938 - acc: 0.4977\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4929\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6936 - acc: 0.4938\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6942 - acc: 0.4985\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4981\n",
      "Epoch 13/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6941 - acc: 0.5013\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6938 - acc: 0.5142\n",
      ". theta fit =  1.5248536\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 35s 17us/step - loss: 0.6960 - acc: 0.4892\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6942 - acc: 0.4750\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4683\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6936 - acc: 0.4603\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4523\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6948 - acc: 0.4433\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4463\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6939 - acc: 0.4337\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6935 - acc: 0.5003\n",
      ". theta fit =  1.4987241\n",
      "Refining learning rate\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 34s 17us/step - loss: 0.6952 - acc: 0.4554\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6942 - acc: 0.4714\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4692\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6936 - acc: 0.4729\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6941 - acc: 0.4800\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6938 - acc: 0.4883\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4774\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6952 - acc: 0.4840\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.6941 - acc: 0.4842\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6945 - acc: 0.5024\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6936 - acc: 0.4839\n",
      ". theta fit =  1.5013667\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 35s 18us/step - loss: 0.6970 - acc: 0.4840\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6944 - acc: 0.4841\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4848\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4799\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4778\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6943 - acc: 0.4894\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4882\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4849\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6936 - acc: 0.4839\n",
      ". theta fit =  1.5040364\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 35s 17us/step - loss: 0.6945 - acc: 0.4886\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4766\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6936 - acc: 0.4844\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6936 - acc: 0.4794\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6946 - acc: 0.4784\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6937 - acc: 0.4766\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6937 - acc: 0.5002\n",
      ". theta fit =  1.5067037\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 35s 18us/step - loss: 0.6951 - acc: 0.4803\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6943 - acc: 0.4806\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6939 - acc: 0.4763\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4702\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4694\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6939 - acc: 0.4774\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4677\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6938 - acc: 0.4705\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6936 - acc: 0.4325\n",
      ". theta fit =  1.5094329\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 45s 22us/step - loss: 0.6956 - acc: 0.4774\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6942 - acc: 0.4608\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6938 - acc: 0.4621\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6936 - acc: 0.4685\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6938 - acc: 0.4582\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6936 - acc: 0.4594\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6941 - acc: 0.4612\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6936 - acc: 0.4606\n",
      ". theta fit =  1.5066886\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 36s 18us/step - loss: 0.6946 - acc: 0.4672\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6936 - acc: 0.4544\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4557\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4638\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6948 - acc: 0.4655\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6938 - acc: 0.4335\n",
      ". theta fit =  1.5094908\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 36s 18us/step - loss: 0.6957 - acc: 0.4680\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6939 - acc: 0.4514\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6939 - acc: 0.4616\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4566\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6939 - acc: 0.4539\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4524\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6944 - acc: 0.4550\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6943 - acc: 0.4583\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: 0.6936 - acc: 0.4433\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4462\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4531\n",
      "Epoch 12/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4498\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6938 - acc: 0.4847\n",
      ". theta fit =  1.5123224\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 34s 17us/step - loss: 0.6964 - acc: 0.4565\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.6944 - acc: 0.4509\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6937 - acc: 0.4457\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4497\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6940 - acc: 0.4504\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6938 - acc: 0.4413\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6939 - acc: 0.4460\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6936 - acc: 0.5003\n",
      ". theta fit =  1.5094589\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 36s 18us/step - loss: 0.6999 - acc: 0.4564\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6948 - acc: 0.4432\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6941 - acc: 0.4393\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4414\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4359\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6958 - acc: 0.4549\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6949 - acc: 0.4467\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6937 - acc: 0.4306\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6935 - acc: 0.4174\n",
      ". theta fit =  1.5065575\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 41s 21us/step - loss: 0.6954 - acc: 0.4514\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6941 - acc: 0.4490\n",
      "Epoch 3/50\n",
      "1442000/2000000 [====================>.........] - ETA: 4s - loss: 0.6936 - acc: 0.4475Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 41s 21us/step - loss: 0.6961 - acc: 0.4543\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6944 - acc: 0.4465\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4488\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6940 - acc: 0.4466\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4490\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4440\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 12us/step - loss: -0.6943 - acc: 0.4523\n",
      ". theta fit =  1.5124416\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 36s 18us/step - loss: 0.6960 - acc: 0.4639\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4444\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6935 - acc: 0.4360\n",
      "Epoch 4/50\n",
      "1799000/2000000 [=========================>....] - ETA: 1s - loss: 0.6933 - acc: 0.4431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6941 - acc: 0.4415\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4457\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4397\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6938 - acc: 0.4354\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 30s 15us/step - loss: 0.6944 - acc: 0.4356\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 23s 11us/step - loss: 0.6940 - acc: 0.4380\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6935 - acc: 0.4091\n",
      ". theta fit =  1.5124031\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.6965 - acc: 0.4458\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6940 - acc: 0.4315\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4449\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4364\n",
      "Epoch 5/50\n",
      "1047000/2000000 [==============>...............] - ETA: 7s - loss: 0.6962 - acc: 0.4500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6943 - acc: 0.4345\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4398\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4374\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6937 - acc: 0.4368\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4363\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4456\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: -0.6935 - acc: 0.5003\n",
      ". theta fit =  1.5150479\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.6967 - acc: 0.4489\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4416\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4376\n",
      "Epoch 4/50\n",
      "1904000/2000000 [===========================>..] - ETA: 0s - loss: 0.6938 - acc: 0.4405"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.6987 - acc: 0.4490\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6945 - acc: 0.4389\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6948 - acc: 0.4353\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6935 - acc: 0.4369\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6949 - acc: 0.4431\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4399\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4378\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6936 - acc: 0.5003\n",
      ". theta fit =  1.5150448\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.6974 - acc: 0.4477\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6945 - acc: 0.4328\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6940 - acc: 0.4391\n",
      "Epoch 4/50\n",
      "1131000/2000000 [===============>..............] - ETA: 6s - loss: 0.6938 - acc: 0.4328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.6946 - acc: 0.4484\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4410\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4430\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6935 - acc: 0.4482\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6939 - acc: 0.4400\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6937 - acc: 0.4462\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6938 - acc: 0.4404\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6975 - acc: 0.4219\n",
      ". theta fit =  1.5150487\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 43s 21us/step - loss: 0.6999 - acc: 0.4558\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6945 - acc: 0.4492\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6938 - acc: 0.4510\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4459\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4492\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6937 - acc: 0.4463\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4429\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6939 - acc: 0.4396\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6934 - acc: 0.4111\n",
      ". theta fit =  1.514721\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      " 884000/2000000 [============>.................] - ETA: 36s - loss: 0.6951 - acc: 0.4424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6942 - acc: 0.4412\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6935 - acc: 0.4447\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6943 - acc: 0.4551\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6938 - acc: 0.4530\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4391\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6946 - acc: 0.4119\n",
      ". theta fit =  1.5150522\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 43s 22us/step - loss: 0.6982 - acc: 0.4590\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6946 - acc: 0.4431\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6938 - acc: 0.4440\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4393\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4414\n",
      "Epoch 6/50\n",
      "  91000/2000000 [>.............................] - ETA: 18s - loss: 0.6919 - acc: 0.4305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4411\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6944 - acc: 0.4473\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4418\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6934 - acc: 0.4356\n",
      ". theta fit =  1.5146842\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 39s 20us/step - loss: 0.6971 - acc: 0.4500\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6942 - acc: 0.4438\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6937 - acc: 0.4420\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4369\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4476\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6943 - acc: 0.4433\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: 0.6951 - acc: 0.4476\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6935 - acc: 0.4444\n",
      ". theta fit =  1.5146501\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "  94000/2000000 [>.............................] - ETA: 8:10 - loss: 0.6883 - acc: 0.4701"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6937 - acc: 0.4424\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6934 - acc: 0.4110\n",
      ". theta fit =  1.5146157\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 40s 20us/step - loss: 0.6964 - acc: 0.4508\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6947 - acc: 0.4457\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6942 - acc: 0.4469\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6935 - acc: 0.4423\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6939 - acc: 0.4433\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4434\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4449\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: -0.6935 - acc: 0.4559\n",
      ". theta fit =  1.514581\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 40s 20us/step - loss: 0.7001 - acc: 0.4594\n",
      "Epoch 2/50\n",
      "1131000/2000000 [===============>..............] - ETA: 7s - loss: 0.6971 - acc: 0.4525"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6942 - acc: 0.4482\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4470\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.6938 - acc: 0.4522\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 27s 14us/step - loss: -0.6934 - acc: 0.4194\n",
      ". theta fit =  1.5145459\n",
      "Iteration:  50\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 40s 20us/step - loss: 0.7001 - acc: 0.4593\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6945 - acc: 0.4510\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6938 - acc: 0.4487\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4522\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4427\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6949 - acc: 0.4592\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4587\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6946 - acc: 0.4583\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4504\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6937 - acc: 0.4538\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6938 - acc: 0.4547\n",
      "Epoch 12/50\n",
      "1989000/2000000 [============================>.] - ETA: 0s - loss: 0.6941 - acc: 0.4593"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6938 - acc: 0.4532\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 23s 11us/step - loss: 0.6940 - acc: 0.4559\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6939 - acc: 0.4573\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6944 - acc: 0.4891\n",
      ". theta fit =  1.5145463\n",
      "Iteration:  52\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.6998 - acc: 0.4703\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6941 - acc: 0.4668\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6934 - acc: 0.4595\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6943 - acc: 0.4640\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6935 - acc: 0.4561\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6940 - acc: 0.4632\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6962 - acc: 0.5008\n",
      ". theta fit =  1.5145823\n",
      "Iteration:  53\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.6994 - acc: 0.4816\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6951 - acc: 0.4716\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6940 - acc: 0.4635\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6936 - acc: 0.4528\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6938 - acc: 0.4592\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6940 - acc: 0.4590\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6935 - acc: 0.4514\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6939 - acc: 0.4657\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6936 - acc: 0.4551\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6961 - acc: 0.4579\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6939 - acc: 0.4323\n",
      ". theta fit =  1.5146185\n",
      "Iteration:  54\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 46s 23us/step - loss: 0.7013 - acc: 0.4732\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6951 - acc: 0.4599\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6946 - acc: 0.4606\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6937 - acc: 0.4530\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4506\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6943 - acc: 0.4650\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4637\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6937 - acc: 0.4568\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: -0.6935 - acc: 0.4320\n",
      ". theta fit =  1.5145819\n",
      "Iteration:  55\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 43s 21us/step - loss: 0.7014 - acc: 0.4767\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6958 - acc: 0.4603\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6945 - acc: 0.4569\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6940 - acc: 0.4600\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6936 - acc: 0.4494\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6937 - acc: 0.4619\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6960 - acc: 0.4642\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6935 - acc: 0.4585\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6943 - acc: 0.4609\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6937 - acc: 0.4520\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6954 - acc: 0.4628\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6941 - acc: 0.4662\n",
      ". theta fit =  1.5146189\n",
      "Iteration:  56\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 41s 21us/step - loss: 0.6999 - acc: 0.4684\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6951 - acc: 0.4563\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6940 - acc: 0.4670\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6937 - acc: 0.4578\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6937 - acc: 0.4543\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: 0.6937 - acc: 0.4582\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6940 - acc: 0.4550\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6936 - acc: 0.4466\n",
      ". theta fit =  1.5145816\n",
      "Iteration:  57\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.6999 - acc: 0.4662\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4543\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6936 - acc: 0.4523\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6937 - acc: 0.4544\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6984 - acc: 0.4589\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6957 - acc: 0.4586\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6934 - acc: 0.4238\n",
      ". theta fit =  1.514544\n",
      "Iteration:  58\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.6980 - acc: 0.4593\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6957 - acc: 0.4511\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6944 - acc: 0.4497\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6936 - acc: 0.4512\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6943 - acc: 0.4499\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6940 - acc: 0.4546\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6942 - acc: 0.4606\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6937 - acc: 0.4869\n",
      ". theta fit =  1.5145072\n",
      "Iteration:  59\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 43s 22us/step - loss: 0.7000 - acc: 0.4496\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6946 - acc: 0.4503\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6937 - acc: 0.4568\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6937 - acc: 0.4597\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4488\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6940 - acc: 0.4567\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 23s 11us/step - loss: 0.6963 - acc: 0.4571\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6947 - acc: 0.4511\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6940 - acc: 0.4326\n",
      ". theta fit =  1.5145453\n",
      "Iteration:  60\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 41s 21us/step - loss: 0.7015 - acc: 0.4571\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: 0.6956 - acc: 0.4484\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6943 - acc: 0.4466\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6938 - acc: 0.4555\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.6938 - acc: 0.4607\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6935 - acc: 0.4533\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6936 - acc: 0.4612\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4565\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6958 - acc: 0.4537\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: -0.6964 - acc: 0.4647\n",
      ". theta fit =  1.5145838\n",
      "Iteration:  61\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 44s 22us/step - loss: 0.6982 - acc: 0.4652\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: 0.6937 - acc: 0.4632\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4488\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6936 - acc: 0.4608\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6941 - acc: 0.4610\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6935 - acc: 0.4565\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6950 - acc: 0.4643\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6939 - acc: 0.4578\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6939 - acc: 0.4568\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 31s 16us/step - loss: -0.6934 - acc: 0.4121\n",
      ". theta fit =  1.5145451\n",
      "Refining learning rate\n",
      "Iteration:  62\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 43s 21us/step - loss: 0.6967 - acc: 0.4653\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6938 - acc: 0.4556\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6936 - acc: 0.4623\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6935 - acc: 0.4665\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6942 - acc: 0.4619\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6957 - acc: 0.4671\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6945 - acc: 0.4659\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6965 - acc: 0.4682\n",
      ". theta fit =  1.514549\n",
      "Iteration:  63\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.7006 - acc: 0.4562\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6945 - acc: 0.4549\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6936 - acc: 0.4583\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6939 - acc: 0.4568\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6935 - acc: 0.4647\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6940 - acc: 0.4593\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6936 - acc: 0.4535\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6937 - acc: 0.4563\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6935 - acc: 0.4342\n",
      ". theta fit =  1.5145451\n",
      "Iteration:  64\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 40s 20us/step - loss: 0.7008 - acc: 0.4625\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.6958 - acc: 0.4530\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6946 - acc: 0.4541\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6937 - acc: 0.4566\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4655\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6961 - acc: 0.4542\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6938 - acc: 0.4599\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6936 - acc: 0.4563\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6936 - acc: 0.4551\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6973 - acc: 0.4576\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6946 - acc: 0.4554\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: -0.6950 - acc: 0.5008\n",
      ". theta fit =  1.514549\n",
      "Iteration:  65\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.6982 - acc: 0.4590\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6937 - acc: 0.4564\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6937 - acc: 0.4574\n",
      "Epoch 4/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6941 - acc: 0.4625\n",
      "Epoch 5/50\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.6935 - acc: 0.4544\n",
      "Epoch 6/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6946 - acc: 0.4576\n",
      "Epoch 7/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6936 - acc: 0.4568\n",
      "Epoch 8/50\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6934 - acc: 0.4568\n",
      "Epoch 9/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6949 - acc: 0.4610\n",
      "Epoch 10/50\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6938 - acc: 0.4635\n",
      "Epoch 11/50\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.6942 - acc: 0.4564\n",
      "Training theta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: -0.6943 - acc: 0.4602\n",
      ". theta fit =  1.5145531\n",
      "Iteration:  66\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-45fcf26adc28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_loss_wrapper_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyinputs_fit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearlystopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-3004b70dd542>\u001b[0m in \u001b[0;36mmy_loss_wrapper_fit\u001b[0;34m(inputs, mysign)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_theta\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtheta_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#when not training theta, fetch as np array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36mget_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \"\"\"\n\u001b[1;32m   1067\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \"\"\"\n\u001b[1;32m   2419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 199\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):    \n",
    "    print(\"Iteration: \",iteration )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=50, batch_size=1000,validation_data=(np.array(X_test), y_test),verbose=1, callbacks = [earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "    \n",
    "        \n",
    "        # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "    \n",
    "    fit_vals_recent = np.array(fit_vals)[(index_refine[-1]):]\n",
    "    \n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "            \n",
    "    extrema = np.concatenate((argrelmin(fit_vals_recent)[0], argrelmax(fit_vals_recent)[0]))\n",
    "    extrema = extrema[extrema >= iteration - index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema\", extrema)\n",
    "    '''\n",
    "    \n",
    "    if (len(extrema) == 0): # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema) >= 6): #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print(\"Refining learning rate\")\n",
    "        optimizer.lr = optimizer.lr/10.\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(np.std(theta1), 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"GaussianAltFit-1D-sigma (Analytical Reweight)\\nN = {:.0e}, Iterations = {:.0f}\".format(N, len(fit_vals)))\n",
    "plt.savefig(\"GaussianAltFit-1D-sigma (Analytical Reweight)\\nN = {:.0e}, Iterations = {:.0f}.png\".format(N, len(fit_vals)))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
