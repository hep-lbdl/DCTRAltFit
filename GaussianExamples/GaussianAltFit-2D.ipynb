{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__) #1.15.0\n",
    "print(keras.__version__) #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{(1-f(x_{T,i},\\theta'))}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it a $\\mu$ value in addition to $X_i\\sim\\mathcal{N}(\\mu, 1)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = 10**7\n",
    "mu_min = -2\n",
    "mu_max = 2\n",
    "mu_values = np.random.uniform(mu_min, mu_max, n_data_points)\n",
    "\n",
    "sigma_min = 1/3\n",
    "sigma_max = 3\n",
    "sigma_values = np.random.uniform(sigma_min, sigma_max, n_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with this $\\mu$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = [(np.random.normal(0, 1), mu_values[i], sigma_values[i]) for i in range(n_data_points)] # Note the zero in normal(0, 1) \n",
    "X1 = [(np.random.normal(mu_values[i], sigma_values[i]), mu_values[i], sigma_values[i]) for i in range(n_data_points)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0, \\sigma = 1$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu, \\sigma$ values uniformly sampled in the specified range [mu_min, mu_max] and [sigma_min, sigma_max].\n",
    "\n",
    "For every value of $\\mu$ in mu_values and every value of $\\sigma$ in sigma_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ and $\\sigma$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "X = np.concatenate((X0, X1))\n",
    "Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((3,))\n",
    "hidden_layer_1 = Dense(128, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(128, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(128, activation='relu')(hidden_layer_2)\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs = inputs, outputs = outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 16000000 samples, validate on 4000000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "16000000/16000000 [==============================] - 34s 2us/step - loss: 0.5356 - val_loss: 0.5331\n",
      "Epoch 2/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5333 - val_loss: 0.5332\n",
      "Epoch 3/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5332 - val_loss: 0.5329\n",
      "Epoch 4/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5331 - val_loss: 0.5332\n",
      "Epoch 5/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5331 - val_loss: 0.5332\n",
      "Epoch 6/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5331 - val_loss: 0.5330\n",
      "Epoch 7/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5331 - val_loss: 0.5332\n",
      "Epoch 8/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5331 - val_loss: 0.5330\n",
      "Epoch 9/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5331\n",
      "Epoch 10/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5329\n",
      "Epoch 11/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5329\n",
      "Epoch 12/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5329\n",
      "Epoch 13/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5329\n",
      "Epoch 14/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5330\n",
      "Epoch 15/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5330\n",
      "Epoch 16/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5329\n",
      "Epoch 17/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5329\n",
      "Epoch 18/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5328\n",
      "Epoch 19/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5328\n",
      "Epoch 20/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5328\n",
      "Epoch 21/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5330\n",
      "Epoch 22/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5329\n",
      "Epoch 23/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5330 - val_loss: 0.5328\n",
      "Epoch 24/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5329 - val_loss: 0.5332\n",
      "Epoch 25/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5329 - val_loss: 0.5328\n",
      "Epoch 26/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5329 - val_loss: 0.5328\n",
      "Epoch 27/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5329 - val_loss: 0.5328\n",
      "Epoch 28/200\n",
      "16000000/16000000 [==============================] - 33s 2us/step - loss: 0.5329 - val_loss: 0.5328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f578c1c9510>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience = 10,\n",
    "                              restore_best_weights=True)\n",
    "dctr_model.fit(X_train, Y_train, \n",
    "          epochs=200, \n",
    "          batch_size = 10000,\n",
    "          validation_data = (X_test, Y_test),\n",
    "          callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate DCTR for any $\\mu$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = 1\n",
    "sigma1 = 1.5\n",
    "assert mu1>=mu_min and mu1<=mu_max # choose mu1 in valid range\n",
    "assert sigma1>=sigma_min and sigma1<=sigma_max # choose mu1 in valid range\n",
    "X0_val = np.random.normal(0, 1, n_data_points)\n",
    "X1_val = np.random.normal(mu1, sigma1, n_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = np.array([(x, mu1, 1.5) for x in X0_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dctr_model.predict(X_input)\n",
    "weights = preds[:,1]/preds[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-6,5,31)\n",
    "plt.hist(X0_val, bins = bins, alpha = 0.5, label = r'$\\mu=0$, $\\sigma=1$')\n",
    "plt.hist(X0_val, bins = bins, label = r'$\\mu=0$, $\\sigma=1$ wgt.', weights=weights, histtype='step', color='k')\n",
    "plt.hist(X1_val, bins = bins, alpha = 0.5, label = r'$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "theta0_param = (0,1) #this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = (1,1.5) #this is the data (the target)\n",
    "\n",
    "theta0 = np.random.normal(theta0_param[0],theta0_param[1],N)\n",
    "theta1 = np.random.normal(theta1_param[0],theta1_param[1],N)\n",
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0,theta1])\n",
    "yvals = np.concatenate([labels0,labels1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals, yvals, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Model\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njson_file = open(\\'dctr_model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\ndctr_model = keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\ndctr_model.load_weights(\"dctr_model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr_model(d)\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights\n",
    "\n",
    "def analytical_reweight(d): #from analytical formula for normal distributions\n",
    "    events = d[:,0]\n",
    "    param = d[:,1]\n",
    "    weights = K.exp(-(0.5*(events-param)**2)+(0.5*(events-0.0)**2))\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0):\n",
    "    x  = inputs\n",
    "    #x = K.squeeze(x, axis = 1)\n",
    "    x = K.gather(x, np.arange(500))\n",
    "\n",
    "    theta = 0. #starting value\n",
    "    #theta0 = tf.constant(val, dtype= tf.float32)#target value\n",
    "    theta0 = [val, 1.5]\n",
    "    theta0_stack = K.ones(shape = (x.shape[0], 2))*theta0\n",
    "    print(\"theta0\",np.shape(theta0))\n",
    "    print(theta0_stack.shape)\n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, theta0_stack), axis=-1)\n",
    "    \n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    #w = analytical_reweight(data) #functional analytical reweight\n",
    "    \n",
    "    #w = K.exp(-(0.5*(x-val)**2)+(0.5*(x-theta)**2)) #direct analytical reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.25    1.28125 1.3125  1.34375 1.375   1.40625 1.4375  1.46875 1.5\n",
      " 1.53125 1.5625  1.59375 1.625   1.65625 1.6875  1.71875 1.75   ]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(1.25,1.75,17)\n",
    "print(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : -2.0\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.1159 - acc: 0.5769 - val_loss: 0.1156 - val_acc: 0.5723\n",
      "testing theta = : -1.75\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.1292 - acc: 0.5876 - val_loss: 0.1294 - val_acc: 0.5850\n",
      "testing theta = : -1.5\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.1428 - acc: 0.6030 - val_loss: 0.1431 - val_acc: 0.6021\n",
      "testing theta = : -1.25\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.1567 - acc: 0.6179 - val_loss: 0.1570 - val_acc: 0.6153\n",
      "testing theta = : -1.0\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.1712 - acc: 0.6278 - val_loss: 0.1714 - val_acc: 0.6309\n",
      "testing theta = : -0.75\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.1851 - acc: 0.6369 - val_loss: 0.1853 - val_acc: 0.6391\n",
      "testing theta = : -0.5\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.1991 - acc: 0.6453 - val_loss: 0.1993 - val_acc: 0.6439\n",
      "testing theta = : -0.25\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2128 - acc: 0.6544 - val_loss: 0.2129 - val_acc: 0.6519\n",
      "testing theta = : 0.0\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2243 - acc: 0.6624 - val_loss: 0.2245 - val_acc: 0.6644\n",
      "testing theta = : 0.25\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2333 - acc: 0.6678 - val_loss: 0.2336 - val_acc: 0.6659\n",
      "testing theta = : 0.5\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2410 - acc: 0.6693 - val_loss: 0.2410 - val_acc: 0.6709\n",
      "testing theta = : 0.75\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2467 - acc: 0.6651 - val_loss: 0.2469 - val_acc: 0.6711\n",
      "testing theta = : 1.0\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2495 - acc: 0.5743 - val_loss: 0.2496 - val_acc: 0.6190\n",
      "testing theta = : 1.25\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2480 - acc: 0.3586 - val_loss: 0.2485 - val_acc: 0.3647\n",
      "testing theta = : 1.5\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2433 - acc: 0.3465 - val_loss: 0.2428 - val_acc: 0.3358\n",
      "testing theta = : 1.75\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2347 - acc: 0.3459 - val_loss: 0.2342 - val_acc: 0.3507\n",
      "testing theta = : 2.0\n",
      "theta0 (2,)\n",
      "(500, 2)\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2260 - acc: 0.3538 - val_loss: 0.2233 - val_acc: 0.3440\n",
      "[[0.1156094362065196], [0.1293540322110057], [0.14309215892478824], [0.15696287389844657], [0.1713690091446042], [0.18530277182906865], [0.19925048331171274], [0.21292543344199658], [0.22445801532268525], [0.23360613029450178], [0.24103532780706882], [0.24687917490303515], [0.24958239925652742], [0.2484615057334304], [0.24279867878556252], [0.2342180924192071], [0.22332461596280337]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2,2,17)\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train, epochs=1, batch_size=500,validation_data=(np.array(X_test), y_test),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(np.mean(theta1), ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = [0., 1.]\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 16,899\n",
      "Trainable params: 16,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch:  0\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 35s 35us/step - loss: 0.6095 - acc: 0.6625 - val_loss: 0.5977 - val_acc: 0.6732\n",
      ". theta fit =  [0. 1.]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 35s 35us/step - loss: -0.6255 - acc: 0.6730 - val_loss: -0.6462 - val_acc: 0.6732\n",
      ". theta fit =  [0.17359583 1.1586288 ]\n",
      "Epoch:  1\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: 0.6347 - acc: 0.6711 - val_loss: 0.6351 - val_acc: 0.6723\n",
      ". theta fit =  [0.17359583 1.1586288 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 35s 35us/step - loss: -0.6557 - acc: 0.6718 - val_loss: -0.6714 - val_acc: 0.6723\n",
      ". theta fit =  [0.35840794 1.2991561 ]\n",
      "Epoch:  2\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 35s 35us/step - loss: 0.6605 - acc: 0.6692 - val_loss: 0.6621 - val_acc: 0.6710\n",
      ". theta fit =  [0.35840794 1.2991561 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: -0.6696 - acc: 0.6704 - val_loss: -0.6789 - val_acc: 0.6710\n",
      ". theta fit =  [0.5145134 1.3621436]\n",
      "Epoch:  3\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 35s 35us/step - loss: 0.6728 - acc: 0.6671 - val_loss: 0.6755 - val_acc: 0.6674\n",
      ". theta fit =  [0.5145134 1.3621436]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: -0.6772 - acc: 0.6672 - val_loss: -0.6827 - val_acc: 0.6674\n",
      ". theta fit =  [0.63965875 1.3814411 ]\n",
      "Epoch:  4\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: 0.6796 - acc: 0.6622 - val_loss: 0.6826 - val_acc: 0.6704\n",
      ". theta fit =  [0.63965875 1.3814411 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: -0.6830 - acc: 0.6699 - val_loss: -0.6892 - val_acc: 0.6704\n",
      ". theta fit =  [0.75628513 1.3987554 ]\n",
      "Epoch:  5\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: 0.6839 - acc: 0.6566 - val_loss: 0.6876 - val_acc: 0.6713\n",
      ". theta fit =  [0.75628513 1.3987554 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: -0.6852 - acc: 0.6709 - val_loss: -0.6903 - val_acc: 0.6713\n",
      ". theta fit =  [0.8288872 1.3918817]\n",
      "Epoch:  6\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: 0.6855 - acc: 0.6409 - val_loss: 0.6887 - val_acc: 0.6525\n",
      ". theta fit =  [0.8288872 1.3918817]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: -0.6854 - acc: 0.6523 - val_loss: -0.6893 - val_acc: 0.6525\n",
      ". theta fit =  [0.85734767 1.3958681 ]\n",
      "Epoch:  7\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: 0.6860 - acc: 0.6346 - val_loss: 0.6894 - val_acc: 0.6082\n",
      ". theta fit =  [0.85734767 1.3958681 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: -0.6858 - acc: 0.6081 - val_loss: -0.6894 - val_acc: 0.6082\n",
      ". theta fit =  [0.86753535 1.380973  ]\n",
      "Epoch:  8\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: 0.6858 - acc: 0.6202 - val_loss: 0.6898 - val_acc: 0.6243\n",
      ". theta fit =  [0.86753535 1.380973  ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: -0.6857 - acc: 0.6249 - val_loss: -0.6905 - val_acc: 0.6243\n",
      ". theta fit =  [0.8914677 1.3899312]\n",
      "Epoch:  9\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 36s 36us/step - loss: 0.6863 - acc: 0.6081 - val_loss: 0.6901 - val_acc: 0.6279\n",
      ". theta fit =  [0.8914677 1.3899312]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: -0.6859 - acc: 0.6283 - val_loss: -0.6906 - val_acc: 0.6279\n",
      ". theta fit =  [0.912168  1.3982399]\n",
      "Epoch:  10\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: 0.6863 - acc: 0.6019 - val_loss: 0.6900 - val_acc: 0.5762\n",
      ". theta fit =  [0.912168  1.3982399]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: -0.6858 - acc: 0.5766 - val_loss: -0.6898 - val_acc: 0.5762\n",
      ". theta fit =  [0.9030869 1.3907478]\n",
      "Epoch:  11\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: 0.6863 - acc: 0.5969 - val_loss: 0.6898 - val_acc: 0.6117\n",
      ". theta fit =  [0.9030869 1.3907478]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: -0.6857 - acc: 0.6120 - val_loss: -0.6900 - val_acc: 0.6117\n",
      ". theta fit =  [0.9108708 1.393299 ]\n",
      "Epoch:  12\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 37s 37us/step - loss: 0.6859 - acc: 0.5922 - val_loss: 0.6898 - val_acc: 0.6049\n",
      ". theta fit =  [0.9108708 1.393299 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: -0.6858 - acc: 0.6052 - val_loss: -0.6895 - val_acc: 0.6049\n",
      ". theta fit =  [0.9054875 1.3724927]\n",
      "Epoch:  13\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: 0.6861 - acc: 0.5915 - val_loss: 0.6897 - val_acc: 0.5872\n",
      ". theta fit =  [0.9054875 1.3724927]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: -0.6854 - acc: 0.5880 - val_loss: -0.6896 - val_acc: 0.5872\n",
      ". theta fit =  [0.90691596 1.370882  ]\n",
      "Epoch:  14\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: 0.6860 - acc: 0.5837 - val_loss: 0.6894 - val_acc: 0.5960\n",
      ". theta fit =  [0.90691596 1.370882  ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: -0.6856 - acc: 0.5967 - val_loss: -0.6892 - val_acc: 0.5960\n",
      ". theta fit =  [0.9038369 1.3586947]\n",
      "Epoch:  15\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: 0.6858 - acc: 0.5807 - val_loss: 0.6892 - val_acc: 0.5803\n",
      ". theta fit =  [0.9038369 1.3586947]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: -0.6854 - acc: 0.5814 - val_loss: -0.6892 - val_acc: 0.5803\n",
      ". theta fit =  [0.89876926 1.3547091 ]\n",
      "Epoch:  16\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: 0.6857 - acc: 0.5918 - val_loss: 0.6898 - val_acc: 0.6088\n",
      ". theta fit =  [0.89876926 1.3547091 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: -0.6855 - acc: 0.6097 - val_loss: -0.6904 - val_acc: 0.6088\n",
      ". theta fit =  [0.9199501 1.3619348]\n",
      "Epoch:  17\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 37s 37us/step - loss: 0.6859 - acc: 0.5684 - val_loss: 0.6894 - val_acc: 0.5835\n",
      ". theta fit =  [0.9199501 1.3619348]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: -0.6854 - acc: 0.5843 - val_loss: -0.6894 - val_acc: 0.5835\n",
      ". theta fit =  [0.9211654 1.3594624]\n",
      "Epoch:  18\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: 0.6859 - acc: 0.5737 - val_loss: 0.6898 - val_acc: 0.6005\n",
      ". theta fit =  [0.9211654 1.3594624]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: -0.6853 - acc: 0.6015 - val_loss: -0.6898 - val_acc: 0.6005\n",
      ". theta fit =  [0.92920595 1.3582517 ]\n",
      "Epoch:  19\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: 0.6857 - acc: 0.5618 - val_loss: 0.6902 - val_acc: 0.5951\n",
      ". theta fit =  [0.92920595 1.3582517 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: -0.6854 - acc: 0.5964 - val_loss: -0.6903 - val_acc: 0.5951\n",
      ". theta fit =  [0.9390369 1.3589368]\n",
      "Epoch:  20\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: 0.6859 - acc: 0.5598 - val_loss: 0.6898 - val_acc: 0.5348\n",
      ". theta fit =  [0.9390369 1.3589368]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: -0.6853 - acc: 0.5362 - val_loss: -0.6898 - val_acc: 0.5348\n",
      ". theta fit =  [0.92757386 1.363285  ]\n",
      "Epoch:  21\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 38s 38us/step - loss: 0.6855 - acc: 0.5725 - val_loss: 0.6904 - val_acc: 0.5474\n",
      ". theta fit =  [0.92757386 1.363285  ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: -0.6854 - acc: 0.5486 - val_loss: -0.6902 - val_acc: 0.5474\n",
      ". theta fit =  [0.9165366 1.358712 ]\n",
      "Epoch:  22\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: 0.6858 - acc: 0.5787 - val_loss: 0.6902 - val_acc: 0.5990\n",
      ". theta fit =  [0.9165366 1.358712 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: -0.6851 - acc: 0.5998 - val_loss: -0.6899 - val_acc: 0.5990\n",
      ". theta fit =  [0.9175998 1.3478168]\n",
      "Epoch:  23\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: 0.6857 - acc: 0.5678 - val_loss: 0.6896 - val_acc: 0.5858\n",
      ". theta fit =  [0.9175998 1.3478168]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: -0.6851 - acc: 0.5866 - val_loss: -0.6898 - val_acc: 0.5858\n",
      ". theta fit =  [0.9269819 1.351717 ]\n",
      "Epoch:  24\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: 0.6857 - acc: 0.5644 - val_loss: 0.6912 - val_acc: 0.5844\n",
      ". theta fit =  [0.9269819 1.351717 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: -0.6858 - acc: 0.5855 - val_loss: -0.6917 - val_acc: 0.5844\n",
      ". theta fit =  [0.93944645 1.3574026 ]\n",
      "Epoch:  25\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: 0.6861 - acc: 0.5535 - val_loss: 0.6897 - val_acc: 0.5480\n",
      ". theta fit =  [0.93944645 1.3574026 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: -0.6851 - acc: 0.5491 - val_loss: -0.6897 - val_acc: 0.5480\n",
      ". theta fit =  [0.9262428 1.3602898]\n",
      "Epoch:  26\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: 0.6857 - acc: 0.5678 - val_loss: 0.6910 - val_acc: 0.5790\n",
      ". theta fit =  [0.9262428 1.3602898]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: -0.6856 - acc: 0.5799 - val_loss: -0.6912 - val_acc: 0.5790\n",
      ". theta fit =  [0.9276816 1.3643306]\n",
      "Epoch:  27\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: 0.6859 - acc: 0.5588 - val_loss: 0.6895 - val_acc: 0.5854\n",
      ". theta fit =  [0.9276816 1.3643306]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: -0.6852 - acc: 0.5862 - val_loss: -0.6892 - val_acc: 0.5854\n",
      ". theta fit =  [0.9219341 1.3512927]\n",
      "Epoch:  28\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: 0.6853 - acc: 0.5700 - val_loss: 0.6891 - val_acc: 0.5527\n",
      ". theta fit =  [0.9219341 1.3512927]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: -0.6854 - acc: 0.5539 - val_loss: -0.6890 - val_acc: 0.5527\n",
      ". theta fit =  [0.90869504 1.347128  ]\n",
      "Epoch:  29\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 39s 39us/step - loss: 0.6855 - acc: 0.5780 - val_loss: 0.6900 - val_acc: 0.5906\n",
      ". theta fit =  [0.90869504 1.347128  ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: -0.6852 - acc: 0.5912 - val_loss: -0.6905 - val_acc: 0.5906\n",
      ". theta fit =  [0.91939664 1.355902  ]\n",
      "Epoch:  30\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: 0.6854 - acc: 0.5694 - val_loss: 0.6893 - val_acc: 0.5969\n",
      ". theta fit =  [0.91939664 1.355902  ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: -0.6851 - acc: 0.5979 - val_loss: -0.6889 - val_acc: 0.5969\n",
      ". theta fit =  [0.9171614 1.3369756]\n",
      "Epoch:  31\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: 0.6856 - acc: 0.5649 - val_loss: 0.6890 - val_acc: 0.5626\n",
      ". theta fit =  [0.9171614 1.3369756]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: -0.6849 - acc: 0.5639 - val_loss: -0.6894 - val_acc: 0.5626\n",
      ". theta fit =  [0.9191211 1.3505362]\n",
      "Epoch:  32\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: 0.6859 - acc: 0.5698 - val_loss: 0.6893 - val_acc: 0.5437\n",
      ". theta fit =  [0.9191211 1.3505362]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: -0.6855 - acc: 0.5450 - val_loss: -0.6891 - val_acc: 0.5437\n",
      ". theta fit =  [0.88877904 1.3415082 ]\n",
      "Epoch:  33\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: 0.6853 - acc: 0.5956 - val_loss: 0.6893 - val_acc: 0.5992\n",
      ". theta fit =  [0.88877904 1.3415082 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: -0.6850 - acc: 0.6002 - val_loss: -0.6899 - val_acc: 0.5992\n",
      ". theta fit =  [0.9089455 1.3475521]\n",
      "Epoch:  34\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: 0.6880 - acc: 0.5726 - val_loss: 0.6906 - val_acc: 0.5964\n",
      ". theta fit =  [0.9089455 1.3475521]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: -0.6853 - acc: 0.5974 - val_loss: -0.6911 - val_acc: 0.5964\n",
      ". theta fit =  [0.921733  1.3572966]\n",
      "Epoch:  35\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: 0.6861 - acc: 0.5693 - val_loss: 0.6896 - val_acc: 0.5841\n",
      ". theta fit =  [0.921733  1.3572966]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: -0.6851 - acc: 0.5852 - val_loss: -0.6896 - val_acc: 0.5841\n",
      ". theta fit =  [0.9195117 1.3565638]\n",
      "Epoch:  36\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: 0.6860 - acc: 0.5753 - val_loss: 0.6903 - val_acc: 0.5725\n",
      ". theta fit =  [0.9195117 1.3565638]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: -0.6851 - acc: 0.5736 - val_loss: -0.6904 - val_acc: 0.5725\n",
      ". theta fit =  [0.918117  1.3601649]\n",
      "Epoch:  37\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 40s 40us/step - loss: 0.6863 - acc: 0.5722 - val_loss: 0.6892 - val_acc: 0.5778\n",
      ". theta fit =  [0.918117  1.3601649]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: -0.6851 - acc: 0.5788 - val_loss: -0.6888 - val_acc: 0.5778\n",
      ". theta fit =  [0.904151  1.3350464]\n",
      "Epoch:  38\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: 0.6855 - acc: 0.5786 - val_loss: 0.6892 - val_acc: 0.5764\n",
      ". theta fit =  [0.904151  1.3350464]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: -0.6847 - acc: 0.5773 - val_loss: -0.6894 - val_acc: 0.5764\n",
      ". theta fit =  [0.9075526 1.3416085]\n",
      "Epoch:  39\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: 0.6857 - acc: 0.5737 - val_loss: 0.6896 - val_acc: 0.5832\n",
      ". theta fit =  [0.9075526 1.3416085]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: -0.6846 - acc: 0.5841 - val_loss: -0.6893 - val_acc: 0.5832\n",
      ". theta fit =  [0.9049052 1.3326259]\n",
      "Epoch:  40\n",
      "x (1000, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: 0.6864 - acc: 0.5722 - val_loss: 0.6902 - val_acc: 0.5805\n",
      ". theta fit =  [0.9049052 1.3326259]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: -0.6852 - acc: 0.5814 - val_loss: -0.6913 - val_acc: 0.5805\n",
      ". theta fit =  [0.92112577 1.3509192 ]\n",
      "Epoch:  41\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: 0.6851 - acc: 0.5648 - val_loss: 0.6889 - val_acc: 0.5615\n",
      ". theta fit =  [0.92112577 1.3509192 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: -0.6855 - acc: 0.5627 - val_loss: -0.6889 - val_acc: 0.5615\n",
      ". theta fit =  [0.91302574 1.3495519 ]\n",
      "Epoch:  42\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: 0.6858 - acc: 0.5742 - val_loss: 0.6895 - val_acc: 0.5766\n",
      ". theta fit =  [0.91302574 1.3495519 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: -0.6850 - acc: 0.5775 - val_loss: -0.6898 - val_acc: 0.5766\n",
      ". theta fit =  [0.9166627 1.3595492]\n",
      "Epoch:  43\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 41s 41us/step - loss: 0.6863 - acc: 0.5717 - val_loss: 0.6891 - val_acc: 0.5942\n",
      ". theta fit =  [0.9166627 1.3595492]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: -0.6852 - acc: 0.5948 - val_loss: -0.6892 - val_acc: 0.5942\n",
      ". theta fit =  [0.92202634 1.3615338 ]\n",
      "Epoch:  44\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: 0.6856 - acc: 0.5685 - val_loss: 0.6893 - val_acc: 0.5759\n",
      ". theta fit =  [0.92202634 1.3615338 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: -0.6850 - acc: 0.5768 - val_loss: -0.6892 - val_acc: 0.5759\n",
      ". theta fit =  [0.9129465 1.3560861]\n",
      "Epoch:  45\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: 0.6857 - acc: 0.5799 - val_loss: 0.6903 - val_acc: 0.5872\n",
      ". theta fit =  [0.9129465 1.3560861]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: -0.6850 - acc: 0.5881 - val_loss: -0.6904 - val_acc: 0.5872\n",
      ". theta fit =  [0.9161515 1.3591248]\n",
      "Epoch:  46\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: 0.6858 - acc: 0.5759 - val_loss: 0.6903 - val_acc: 0.5782\n",
      ". theta fit =  [0.9161515 1.3591248]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: -0.6851 - acc: 0.5791 - val_loss: -0.6905 - val_acc: 0.5782\n",
      ". theta fit =  [0.92053896 1.3642168 ]\n",
      "Epoch:  47\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 43s 43us/step - loss: 0.6862 - acc: 0.5721 - val_loss: 0.6892 - val_acc: 0.5536\n",
      ". theta fit =  [0.92053896 1.3642168 ]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: -0.6854 - acc: 0.5550 - val_loss: -0.6893 - val_acc: 0.5536\n",
      ". theta fit =  [0.9122109 1.3682172]\n",
      "Epoch:  48\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: 0.6862 - acc: 0.5834 - val_loss: 0.6893 - val_acc: 0.5788\n",
      ". theta fit =  [0.9122109 1.3682172]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 43s 43us/step - loss: -0.6852 - acc: 0.5796 - val_loss: -0.6891 - val_acc: 0.5788\n",
      ". theta fit =  [0.9035103 1.3589363]\n",
      "Epoch:  49\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 43s 43us/step - loss: 0.6860 - acc: 0.5854 - val_loss: 0.6892 - val_acc: 0.5833\n",
      ". theta fit =  [0.9035103 1.3589363]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 42s 42us/step - loss: -0.6849 - acc: 0.5842 - val_loss: -0.6887 - val_acc: 0.5833\n",
      ". theta fit =  [0.8949906 1.3348776]\n",
      "Epoch:  50\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 43s 43us/step - loss: 0.6854 - acc: 0.5798 - val_loss: 0.6893 - val_acc: 0.5845\n",
      ". theta fit =  [0.8949906 1.3348776]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 43s 43us/step - loss: -0.6846 - acc: 0.5852 - val_loss: -0.6891 - val_acc: 0.5845\n",
      ". theta fit =  [0.8966451 1.3283026]\n",
      "Epoch:  51\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 43s 43us/step - loss: 0.6857 - acc: 0.5811 - val_loss: 0.6888 - val_acc: 0.5773\n",
      ". theta fit =  [0.8966451 1.3283026]\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training theta\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      "1000000/1000000 [==============================] - 43s 43us/step - loss: -0.6847 - acc: 0.5783 - val_loss: -0.6892 - val_acc: 0.5773\n",
      ". theta fit =  [0.9046202 1.3370868]\n",
      "Epoch:  52\n",
      "x (1000, 1)\n",
      "theta0 (2,)\n",
      "(1000, 2)\n",
      "(1000, 3)\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=(2,),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = 1000\n",
    "lr = 1e-4 #smaller learning rate yields better precision\n",
    "epochs = 400 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs\n",
    "    \n",
    "    #x = K.squeeze(x, axis = 1)\n",
    "    x = K.gather(x, np.arange(batch_size))\n",
    "    print(\"x\", x.shape)\n",
    "    theta = 0. #starting value\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "        \n",
    "    #creating tensor with same shape as inputs, with val in every entry \n",
    "    theta0_stack = K.ones(shape = (x.shape[0], 2))*theta0\n",
    "    print(\"theta0\",np.shape(theta0))\n",
    "    print(theta0_stack.shape)\n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, theta0_stack), axis=-1)\n",
    "    print(data.shape)\n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    #w = analytical_reweight(data) #functional analytical reweight\n",
    "    \n",
    "    #w = K.exp(-(0.5*(x-theta0)**2)+(0.5*(x-theta)**2)) #direct analytical reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \"\"\"epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\"\"\"\n",
    "        \n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "\n",
    "plt.plot(fit_vals[:,0], label='Model $\\mu$ Fit')\n",
    "plt.plot(fit_vals[:,1], label='Model $\\sigma$ Fit')\n",
    "plt.hlines(theta1_param[0], 0, len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "plt.hlines(theta1_param[1], 0, len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"2D Gaussian Fit v2: N = {:.0e}, learning rate = {:.0e}, Epochs = {:.0f}\".format(N, lr, len(fit_vals)))\n",
    "#plt.savefig(\"2D Gaussian Fit v3: N = {:.0e}, learning rate = {:.0f}, Epochs = {:.0f}\".format(N, lr, len(fit_vals)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(fit_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_valsnp = np.array(fit_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.convert_to_tensor(X_train[0:100], dtype =tf.float32)\n",
    "x = K.expand_dims(x)\n",
    "print(np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tensor with same shape as inputs, with val in every entry \n",
    "theta0= [1.1, 1.2]\n",
    "theta0_stack = K.ones(shape = (np.shape(x)[0], 2))*theta0\n",
    "print(theta0_stack.shape)\n",
    "#combining and reshaping into correct format:\n",
    "data = K.concatenate((x, theta0_stack), axis=-1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.eval(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
