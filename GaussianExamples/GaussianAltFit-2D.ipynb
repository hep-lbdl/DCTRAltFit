{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import argrelmin, argrelmax\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__) #1.15.0\n",
    "print(keras.__version__) #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{1-f(x_{T,i},\\theta')}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it $\\mu$ and $\\sigma$ values in addition to $X_i\\sim\\mathcal{N}(\\mu, \\sigma)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ and $\\sigma$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = 5*10**6\n",
    "mu_min = -2\n",
    "mu_max = 2\n",
    "mu_values = np.random.uniform(mu_min, mu_max, n_data_points)\n",
    "\n",
    "sigma_min = 0.5\n",
    "sigma_max = 4.5\n",
    "sigma_values = np.random.uniform(sigma_min, sigma_max, n_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with these $\\mu$ and $\\sigma$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = [(np.random.normal(0, 1), mu_values[i], sigma_values[i]) for i in range(n_data_points)] # Note the zero in normal(0, 1) \n",
    "X1 = [(np.random.normal(mu_values[i], sigma_values[i]), mu_values[i], sigma_values[i]) for i in range(n_data_points)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0, \\sigma = 1$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu, \\sigma$ values uniformly sampled in the specified range [$\\mu_{min}, \\mu_{max}$] and [$\\sigma_{min}, \\sigma_{max}$].\n",
    "\n",
    "For every value of $\\mu$ in mu_values and every value of $\\sigma$ in sigma_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ and $\\sigma$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "X = np.concatenate((X0, X1))\n",
    "Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((3,))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs = inputs, outputs = outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 8000000 samples, validate on 2000000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "8000000/8000000 [==============================] - 13s 2us/step - loss: 0.5176 - acc: 0.7299 - val_loss: 0.5064 - val_acc: 0.7382\n",
      "Epoch 2/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5053 - acc: 0.7387 - val_loss: 0.5053 - val_acc: 0.7386\n",
      "Epoch 3/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5051 - acc: 0.7388 - val_loss: 0.5053 - val_acc: 0.7387\n",
      "Epoch 4/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5051 - acc: 0.7387 - val_loss: 0.5053 - val_acc: 0.7386\n",
      "Epoch 5/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5051 - acc: 0.7388 - val_loss: 0.5051 - val_acc: 0.7388\n",
      "Epoch 6/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5050 - acc: 0.7388 - val_loss: 0.5052 - val_acc: 0.7387\n",
      "Epoch 7/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5051 - acc: 0.7387 - val_loss: 0.5053 - val_acc: 0.7387\n",
      "Epoch 8/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5050 - acc: 0.7388 - val_loss: 0.5051 - val_acc: 0.7386\n",
      "Epoch 9/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5050 - acc: 0.7388 - val_loss: 0.5052 - val_acc: 0.7387\n",
      "Epoch 10/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5050 - acc: 0.7388 - val_loss: 0.5051 - val_acc: 0.7387\n",
      "Epoch 11/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5050 - acc: 0.7388 - val_loss: 0.5053 - val_acc: 0.7385\n",
      "Epoch 12/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5050 - acc: 0.7388 - val_loss: 0.5050 - val_acc: 0.7388\n",
      "Epoch 13/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5050 - acc: 0.7388 - val_loss: 0.5050 - val_acc: 0.7388\n",
      "Epoch 14/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5049 - acc: 0.7388 - val_loss: 0.5051 - val_acc: 0.7387\n",
      "Epoch 15/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5049 - acc: 0.7388 - val_loss: 0.5053 - val_acc: 0.7386\n",
      "Epoch 16/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - acc: 0.7388 - val_loss: 0.5050 - val_acc: 0.7389\n",
      "Epoch 17/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5049 - acc: 0.7388 - val_loss: 0.5049 - val_acc: 0.7388\n",
      "Epoch 18/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - acc: 0.7389 - val_loss: 0.5051 - val_acc: 0.7388\n",
      "Epoch 19/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5049 - acc: 0.7389 - val_loss: 0.5051 - val_acc: 0.7389\n",
      "Epoch 20/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - acc: 0.7389 - val_loss: 0.5050 - val_acc: 0.7389\n",
      "Epoch 21/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - acc: 0.7389 - val_loss: 0.5050 - val_acc: 0.7388\n",
      "Epoch 22/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - acc: 0.7388 - val_loss: 0.5050 - val_acc: 0.7389\n",
      "Epoch 23/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - acc: 0.7388 - val_loss: 0.5050 - val_acc: 0.7388\n",
      "Epoch 24/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - acc: 0.7388 - val_loss: 0.5051 - val_acc: 0.7388\n",
      "Epoch 25/200\n",
      "8000000/8000000 [==============================] - 13s 2us/step - loss: 0.5049 - acc: 0.7388 - val_loss: 0.5050 - val_acc: 0.7388\n",
      "Epoch 26/200\n",
      "8000000/8000000 [==============================] - 14s 2us/step - loss: 0.5049 - acc: 0.7389 - val_loss: 0.5051 - val_acc: 0.7387\n",
      "Epoch 27/200\n",
      "8000000/8000000 [==============================] - 14s 2us/step - loss: 0.5049 - acc: 0.7389 - val_loss: 0.5050 - val_acc: 0.7388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5764061350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience = 10,\n",
    "                              restore_best_weights=True)\n",
    "dctr_model.fit(X_train, Y_train, \n",
    "               epochs=200, \n",
    "               batch_size = 10000,\n",
    "               validation_data = (X_test, Y_test),\n",
    "               callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_param = (0, 1)\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr_model(d)\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights\n",
    "\n",
    "# from analytical formula for normal distributions\n",
    "def analytical_reweight(events, mu1, sigma1, mu0 = theta0_param[0], sigma0 = theta0_param[1]): \n",
    "    weights = (sigma0/sigma1)*K.exp(-0.5*(((events-mu1)/sigma1)**2-((events-mu0)/sigma0)**2))\n",
    "    #weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate DCTR for any $\\mu$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = 1\n",
    "sigma1 = 1.5\n",
    "assert mu1>=mu_min and mu1<=mu_max # choose mu1 in valid range\n",
    "assert sigma1>=sigma_min and sigma1<=sigma_max # choose mu1 in valid range\n",
    "X0_val = np.random.normal(0, 1, n_data_points)\n",
    "X1_val = np.random.normal(mu1, sigma1, n_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = np.array([(x, mu1, sigma1) for x in X0_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = reweight(tf.convert_to_tensor(X_input, dtype = tf.float32))\n",
    "analytical_weights = analytical_reweight(X0_val, mu1, sigma1)\n",
    "weights = K.eval(weights)\n",
    "analytical_weights = K.eval(analytical_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-8,8,31)\n",
    "plt.hist(X0_val, bins = bins, alpha = 0.5, label = r'$\\mu=0$, $\\sigma=1$')\n",
    "plt.hist(X0_val, bins = bins, label = r'$\\mu=0$, $\\sigma=1$ NN wgt.',\n",
    "         weights=weights, histtype='step', color='k')\n",
    "plt.hist(X0_val, bins = bins, label = r'$\\mu=0$, $\\sigma=1$ analytical wgt.',\n",
    "         weights=analytical_weights, histtype='step', linestyle = '--',color='k')\n",
    "plt.hist(X1_val, bins = bins, alpha = 0.5, label = r'$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "plt.legend()\n",
    "plt.title(\"Truth Level: Reweighting\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"2d_gaussian_dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"2d_gaussian_dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"2d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"2d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "theta0_param = (0,1) #this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = (1,1.5) #this is the data (the target)\n",
    "\n",
    "theta0 = np.random.normal(theta0_param[0],theta0_param[1],N)\n",
    "theta1 = np.random.normal(theta1_param[0],theta1_param[1],N)\n",
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0,theta1])\n",
    "yvals = np.concatenate([labels0,labels1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals, yvals, test_size=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Model\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njson_file = open(\\'dctr_model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\ndctr_model = keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\ndctr_model.load_weights(\"dctr_model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to True for analytical_reweight\n",
    "\n",
    "reweight_analytically = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Section for $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',\n",
    "                              patience = 3, \n",
    "                              min_delta = 0.00005,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0):\n",
    "    x  = inputs\n",
    "    x = K.gather(x, np.arange(batch_size))\n",
    "\n",
    "    theta = 0. #starting value\n",
    "    theta_prime = [val, theta1_param[1]] #fixed theta_sigma = sigma_truth\n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    concat_input_and_params = K.ones(shape = (x.shape[0], 2))*theta_prime\n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "    \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(events = x, \n",
    "                                mu1 = theta_prime[0], \n",
    "                                sigma1 = theta_prime[1]) \n",
    "    \n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : -2.0\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1152 - acc: 0.5805\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1145 - acc: 0.5799\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1145 - acc: 0.5809\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1145 - acc: 0.5805\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1145 - acc: 0.5809\n",
      "[0.11448619325086475]\n",
      "testing theta = : -1.75\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.1274 - acc: 0.5909\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1274 - acc: 0.5907\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1274 - acc: 0.5906\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1274 - acc: 0.5907\n",
      "[0.11448619325086475, 0.12735259186849]\n",
      "testing theta = : -1.5\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1411 - acc: 0.6017\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1411 - acc: 0.6012\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1411 - acc: 0.6016\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1411 - acc: 0.6019\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982]\n",
      "testing theta = : -1.25\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.1555 - acc: 0.6123\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1555 - acc: 0.6123\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1555 - acc: 0.6122\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1555 - acc: 0.6127\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933]\n",
      "testing theta = : -1.0\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1706 - acc: 0.6227\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1706 - acc: 0.6223\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1706 - acc: 0.6228\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1706 - acc: 0.6227\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173]\n",
      "testing theta = : -0.75\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1849 - acc: 0.6349\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1849 - acc: 0.6342\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1849 - acc: 0.6352\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1849 - acc: 0.6346\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136]\n",
      "testing theta = : -0.5\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.1982 - acc: 0.6441\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: 0.1982 - acc: 0.6434\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: 0.1982 - acc: 0.6440\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: 0.1982 - acc: 0.6433\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702]\n",
      "testing theta = : -0.25\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.2115 - acc: 0.6532\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2115 - acc: 0.6530\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2115 - acc: 0.6523\n",
      "Epoch 4/100\n",
      " 832000/2000000 [===========>..................] - ETA: 4s - loss: 0.2112 - acc: 0.6535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2351 - acc: 0.6659\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2351 - acc: 0.6655\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702, 0.21152295786887407, 0.2240594956576824, 0.23509092042595148]\n",
      "testing theta = : 0.5\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.2448 - acc: 0.6697\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2449 - acc: 0.6695\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2448 - acc: 0.6687\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2448 - acc: 0.6691\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2449 - acc: 0.6693\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2448 - acc: 0.6696\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702, 0.21152295786887407, 0.2240594956576824, 0.23509092042595148, 0.24478134578466415]\n",
      "testing theta = : 0.75\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.2508 - acc: 0.6585\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2509 - acc: 0.6605\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2508 - acc: 0.6603\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2509 - acc: 0.6585\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702, 0.21152295786887407, 0.2240594956576824, 0.23509092042595148, 0.24478134578466415, 0.25082467117905616]\n",
      "testing theta = : 1.0\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.2532 - acc: 0.5330\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2531 - acc: 0.5058\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2530 - acc: 0.4885\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2533 - acc: 0.5142\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2530 - acc: 0.5039\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2531 - acc: 0.5159\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702, 0.21152295786887407, 0.2240594956576824, 0.23509092042595148, 0.24478134578466415, 0.25082467117905616, 0.2530105312243104]\n",
      "testing theta = : 1.25\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.2514 - acc: 0.3511\n",
      "Epoch 2/100\n",
      " 393000/2000000 [====>.........................] - ETA: 7s - loss: 0.2530 - acc: 0.3315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2448 - acc: 0.3369\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2448 - acc: 0.3375\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2449 - acc: 0.3355\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2451 - acc: 0.3353\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702, 0.21152295786887407, 0.2240594956576824, 0.23509092042595148, 0.24478134578466415, 0.25082467117905616, 0.2530105312243104, 0.25103947277367117, 0.2447706816494465]\n",
      "testing theta = : 1.75\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.2363 - acc: 0.3417\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2367 - acc: 0.3396\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2364 - acc: 0.3395\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2366 - acc: 0.3390\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702, 0.21152295786887407, 0.2240594956576824, 0.23509092042595148, 0.24478134578466415, 0.25082467117905616, 0.2530105312243104, 0.25103947277367117, 0.2447706816494465, 0.2362952461540699]\n",
      "testing theta = : 2.0\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.2284 - acc: 0.3438\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2269 - acc: 0.3416\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2273 - acc: 0.3436\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2271 - acc: 0.3444\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2270 - acc: 0.3440\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702, 0.21152295786887407, 0.2240594956576824, 0.23509092042595148, 0.24478134578466415, 0.25082467117905616, 0.2530105312243104, 0.25103947277367117, 0.2447706816494465, 0.2362952461540699, 0.22691416384279728]\n",
      "[0.11448619325086475, 0.12735259186849, 0.14110893625020982, 0.15553031770139933, 0.17061586268246173, 0.18487819803506136, 0.1982151596918702, 0.21152295786887407, 0.2240594956576824, 0.23509092042595148, 0.24478134578466415, 0.25082467117905616, 0.2530105312243104, 0.25103947277367117, 0.2447706816494465, 0.2362952461540699, 0.22691416384279728]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2,2,17)\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train, \n",
    "              epochs=100, batch_size=1000,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1, callbacks = [earlystopping])\n",
    "    lvals+=[np.min(model.history.history['loss'])]\n",
    "    print(lvals)\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals)\n",
    "plt.title(\"$\\mu$ Cross Section\\nFixed $\\sigma = \\sigma_{Truth}$\")\n",
    "plt.xlabel(r'$\\theta_{\\mu}$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param[0], ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.legend()\n",
    "#plt.savefig(\"GaussianAltFit-2D-\\mu cross section.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Section for $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',\n",
    "                              patience = 3, \n",
    "                              min_delta = 0.00005,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0):\n",
    "    x  = inputs\n",
    "    x = K.gather(x, np.arange(batch_size))\n",
    "\n",
    "    theta = 0. #starting value\n",
    "    theta_prime = [theta1_param[0], val] #fixed theta_mu = mu_truth\n",
    "    #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    concat_input_and_params = K.ones(shape = (x.shape[0], 2))*theta_prime\n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "    \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(events = x, \n",
    "                                mu1 = theta_prime[0], \n",
    "                                sigma1 = theta_prime[1]) \n",
    "    \n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : 0.5\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.1732 - acc: 0.4910\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1717 - acc: 0.4887\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1717 - acc: 0.4884\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1716 - acc: 0.4886\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.1716 - acc: 0.4886\n",
      "[0.1716344181969762]\n",
      "testing theta = : 0.75\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.2126 - acc: 0.4916\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2125 - acc: 0.4918\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2125 - acc: 0.4919\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2125 - acc: 0.4920\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2125 - acc: 0.4919\n",
      "[0.1716344181969762, 0.21246619226783514]\n",
      "testing theta = : 1.0\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.2349 - acc: 0.4916\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2348 - acc: 0.4913\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2348 - acc: 0.4894\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2348 - acc: 0.4906\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2347 - acc: 0.4894\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133]\n",
      "testing theta = : 1.25\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.2488 - acc: 0.4957\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2487 - acc: 0.4956\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2487 - acc: 0.5009\n",
      "Epoch 4/100\n",
      " 757000/2000000 [==========>...................] - ETA: 5s - loss: 0.2490 - acc: 0.4971"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2529 - acc: 0.4957\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2531 - acc: 0.4928\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766]\n",
      "testing theta = : 1.75\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.2502 - acc: 0.4709\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2509 - acc: 0.4654\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2500 - acc: 0.4629\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2500 - acc: 0.4625\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2502 - acc: 0.4650\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2499 - acc: 0.4584\n",
      "Epoch 7/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2500 - acc: 0.4624\n",
      "Epoch 8/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2499 - acc: 0.4606\n",
      "Epoch 9/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2503 - acc: 0.4594\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928]\n",
      "testing theta = : 2.0\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.2448 - acc: 0.4693\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2433 - acc: 0.4706\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2437 - acc: 0.4712\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2433 - acc: 0.4697\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2430 - acc: 0.4709\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2436 - acc: 0.4700\n",
      "Epoch 7/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2434 - acc: 0.4669\n",
      "Epoch 8/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2438 - acc: 0.4703\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868]\n",
      "testing theta = : 2.25\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.2391 - acc: 0.4760\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2358 - acc: 0.4766\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2355 - acc: 0.4801\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2353 - acc: 0.4781\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2360 - acc: 0.4772\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2363 - acc: 0.4788\n",
      "Epoch 7/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2358 - acc: 0.4833\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217]\n",
      "testing theta = : 2.5\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.2269 - acc: 0.4823\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2289 - acc: 0.4790\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2288 - acc: 0.4808\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2278 - acc: 0.4806\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831]\n",
      "testing theta = : 2.75\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.2231 - acc: 0.4770\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2198 - acc: 0.4835\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2205 - acc: 0.4856\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2200 - acc: 0.4867\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2197 - acc: 0.4883\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2211 - acc: 0.4866\n",
      "Epoch 7/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2214 - acc: 0.4844\n",
      "Epoch 8/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2195 - acc: 0.4896\n",
      "Epoch 9/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2195 - acc: 0.4905\n",
      "Epoch 10/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2212 - acc: 0.4872\n",
      "Epoch 11/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2195 - acc: 0.4892\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628]\n",
      "testing theta = : 3.0\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.2129 - acc: 0.4858\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2123 - acc: 0.4868\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2117 - acc: 0.4894\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2118 - acc: 0.4829\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: 0.2118 - acc: 0.4884\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2122 - acc: 0.4775\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628, 0.21167879273742438]\n",
      "testing theta = : 3.25\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.2124 - acc: 0.4862\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2056 - acc: 0.4882\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2049 - acc: 0.4866\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2043 - acc: 0.4901\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2061 - acc: 0.4816\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2057 - acc: 0.4816\n",
      "Epoch 7/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.2044 - acc: 0.4829\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628, 0.21167879273742438, 0.2042645138502121]\n",
      "testing theta = : 3.5\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.2042 - acc: 0.4786\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1972 - acc: 0.4789\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1991 - acc: 0.4778\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1984 - acc: 0.4772\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1977 - acc: 0.4805\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628, 0.21167879273742438, 0.2042645138502121, 0.19716691149771212]\n",
      "testing theta = : 3.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.1932 - acc: 0.4812\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1920 - acc: 0.4811\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1900 - acc: 0.4807\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1907 - acc: 0.4803\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1909 - acc: 0.4802\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1903 - acc: 0.4828\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628, 0.21167879273742438, 0.2042645138502121, 0.19716691149771212, 0.19002596306800842]\n",
      "testing theta = : 4.0\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.1901 - acc: 0.4821\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1849 - acc: 0.4797\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1838 - acc: 0.4818\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1838 - acc: 0.4817\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1841 - acc: 0.4823\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1843 - acc: 0.4829\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628, 0.21167879273742438, 0.2042645138502121, 0.19716691149771212, 0.19002596306800842, 0.18377799855172633]\n",
      "testing theta = : 4.25\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.1828 - acc: 0.4799\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1773 - acc: 0.4839\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1799 - acc: 0.4822\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1777 - acc: 0.4825\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1779 - acc: 0.4833\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628, 0.21167879273742438, 0.2042645138502121, 0.19716691149771212, 0.19002596306800842, 0.18377799855172633, 0.17726528476178646]\n",
      "testing theta = : 4.5\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/100\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.1768 - acc: 0.4855\n",
      "Epoch 2/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1715 - acc: 0.4824\n",
      "Epoch 3/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1731 - acc: 0.4827\n",
      "Epoch 4/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1750 - acc: 0.4827\n",
      "Epoch 5/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1713 - acc: 0.4838\n",
      "Epoch 6/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1715 - acc: 0.4851\n",
      "Epoch 7/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1745 - acc: 0.4881\n",
      "Epoch 8/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1711 - acc: 0.4836\n",
      "Epoch 9/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1714 - acc: 0.4852\n",
      "Epoch 10/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1712 - acc: 0.4858\n",
      "Epoch 11/100\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.1714 - acc: 0.4866\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628, 0.21167879273742438, 0.2042645138502121, 0.19716691149771212, 0.19002596306800842, 0.18377799855172633, 0.17726528476178646, 0.1711386542841792]\n",
      "[0.1716344181969762, 0.21246619226783514, 0.2347348095551133, 0.24865210366249085, 0.25277938179671766, 0.24988520641624928, 0.24302658019214868, 0.23533886105567217, 0.22689354773610831, 0.2194884277060628, 0.21167879273742438, 0.2042645138502121, 0.19716691149771212, 0.19002596306800842, 0.18377799855172633, 0.17726528476178646, 0.1711386542841792]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.5,4.5,17)\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train, \n",
    "              epochs=100, batch_size=1000,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1, callbacks = [earlystopping])\n",
    "    lvals+=[np.min(model.history.history['loss'])]\n",
    "    print(lvals)\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals)\n",
    "plt.title(\"$\\sigma$ Cross Section\\nFixed $\\mu = \\mu{Truth}$\")\n",
    "plt.xlabel(r'$\\theta_{\\sigma}$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param[1], ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.legend()\n",
    "#plt.savefig(\"GaussianAltFit-2D-\\sigma cross section.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = [0., 1.]\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',\n",
    "                              patience = 3,\n",
    "                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 16,899\n",
      "Trainable params: 16,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=(2,),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "index_refine = np.array([0])\n",
    "batch_size = 2*N\n",
    "lr_initial = 5e-1 #smaller learning rate yields better precision\n",
    "iterations = 75 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs\n",
    "    \n",
    "    \n",
    "    theta = 0. #starting value\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        theta_prime = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array \n",
    "    else:\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        theta_prime = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "        \n",
    "    #creating tensor with same shape as inputs, with val in every entry \n",
    "    concat_input_and_params = K.ones(shape = (x.shape[0], 2), dtype=tf.float32)*theta_prime\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "    \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(events = x, \n",
    "                                mu1 = theta_prime[0], \n",
    "                                sigma1 = theta_prime[1]) \n",
    "    \n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        #t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        \n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6016 - acc: 0.6669\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5956 - acc: 0.6722\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5953 - acc: 0.6723\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.5952 - acc: 0.6724\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6724\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 14/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6722\n",
      "Epoch 15/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 16/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.5952 - acc: 0.6723\n",
      "Epoch 17/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.5952 - acc: 0.6724\n",
      "Epoch 18/20\n",
      " 588000/2000000 [=======>......................] - ETA: 6s - loss: 0.5953 - acc: 0.6721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.6839 - acc: 0.6681\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6838 - acc: 0.6679\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6837 - acc: 0.6677\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6837 - acc: 0.6678\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6837 - acc: 0.6685\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6837 - acc: 0.6683\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6837 - acc: 0.6681\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6838 - acc: 0.6689\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.6836 - acc: 0.6687\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6839 - acc: 0.6681\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6836 - acc: 0.6686\n",
      "Epoch 14/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6837 - acc: 0.6691\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6834 - acc: 0.6705\n",
      ". theta fit =  [0.87203914 1.8719468 ]\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6880 - acc: 0.5072\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6869 - acc: 0.4981\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6868 - acc: 0.5022\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.6862 - acc: 0.5063\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6868 - acc: 0.4995\n",
      "Epoch 6/20\n",
      "1708000/2000000 [========================>.....] - ETA: 1s - loss: 0.6868 - acc: 0.5040"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6875 - acc: 0.6632\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6876 - acc: 0.6645\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6874 - acc: 0.6647\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6914 - acc: 0.5064\n",
      ". theta fit =  [0.84386647 1.2620392 ]\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6907 - acc: 0.5943\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6898 - acc: 0.5825\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6898 - acc: 0.5822\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6897 - acc: 0.5794\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.6897 - acc: 0.5847\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6898 - acc: 0.5820\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6897 - acc: 0.5839\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6897 - acc: 0.5825\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6897 - acc: 0.5830\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6897 - acc: 0.5838\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6896 - acc: 0.5825\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6897 - acc: 0.5850\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6896 - acc: 0.5831\n",
      "Epoch 14/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6897 - acc: 0.5833\n",
      "Epoch 15/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6896 - acc: 0.5842\n",
      "Epoch 16/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.6897 - acc: 0.5813\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.6897 - acc: 0.5698\n",
      ". theta fit =  [1.1165643 1.5347722]\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7007 - acc: 0.3687\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7003 - acc: 0.3420\n",
      "Epoch 3/20\n",
      "1138000/2000000 [================>.............] - ETA: 3s - loss: 0.6979 - acc: 0.3477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7002 - acc: 0.3396\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7006 - acc: 0.3459\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.7003 - acc: 0.3569\n",
      ". theta fit =  [0.855585 1.27383 ]\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6920 - acc: 0.5705\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.6911 - acc: 0.5819\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6910 - acc: 0.5805\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6910 - acc: 0.5801\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6908 - acc: 0.5828\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6911 - acc: 0.5800\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.6909 - acc: 0.5817\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6910 - acc: 0.5817\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: -0.6907 - acc: 0.5735\n",
      ". theta fit =  [1.1087419 1.527009 ]\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7017 - acc: 0.3717\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7003 - acc: 0.3415\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7002 - acc: 0.3387\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: 0.7004 - acc: 0.3380\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7004 - acc: 0.3399\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7004 - acc: 0.3358\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.7003 - acc: 0.3304\n",
      ". theta fit =  [0.8609046 1.2791934]\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "1832000/2000000 [==========================>...] - ETA: 1s - loss: 0.6924 - acc: 0.5611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6916 - acc: 0.5823\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.6915 - acc: 0.5822\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.6914 - acc: 0.5811\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: -0.6914 - acc: 0.5881\n",
      ". theta fit =  [1.1052521 1.5235598]\n",
      "Refining learning rate\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7006 - acc: 0.5107\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.6999 - acc: 0.5268\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7000 - acc: 0.5120\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7003 - acc: 0.5196\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.6999 - acc: 0.5162\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: -0.7002 - acc: 0.5429\n",
      ". theta fit =  [1.0068355 1.4251169]\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7020 - acc: 0.4728\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7006 - acc: 0.4680\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4860\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7006 - acc: 0.4798\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7011 - acc: 0.4823\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.7008 - acc: 0.4369\n",
      ". theta fit =  [1.0309242 1.4492197]\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7015 - acc: 0.4100\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4284\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4154\n",
      "Epoch 4/20\n",
      "1715000/2000000 [========================>.....] - ETA: 1s - loss: 0.7020 - acc: 0.4068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.4565\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7012 - acc: 0.4746\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4697\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7015 - acc: 0.4533\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.4603\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.4649\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.7007 - acc: 0.5159\n",
      ". theta fit =  [1.0310004 1.4974391]\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7023 - acc: 0.4019\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.3920\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.4041\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7014 - acc: 0.4031\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.3941\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4068\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7021 - acc: 0.3966\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7011 - acc: 0.4064\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.7007 - acc: 0.3833\n",
      ". theta fit =  [1.0068911 1.4732925]\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.7014 - acc: 0.4491\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7015 - acc: 0.4597\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.4664\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4590\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4807\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4678\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4603\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7015 - acc: 0.4627\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4687\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.7012 - acc: 0.5386\n",
      ". theta fit =  [1.0311762 1.4975839]\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.7020 - acc: 0.4208\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.3899\n",
      "Epoch 3/20\n",
      "1199000/2000000 [================>.............] - ETA: 4s - loss: 0.7023 - acc: 0.3994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.3951\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.7019 - acc: 0.3455\n",
      ". theta fit =  [1.0555887 1.5220275]\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.7029 - acc: 0.3888\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.3718\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7005 - acc: 0.3932\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.3711\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.3830\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.3745\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.7004 - acc: 0.3381\n",
      ". theta fit =  [1.0309533 1.4973915]\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.7052 - acc: 0.3886\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4081\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7011 - acc: 0.4364\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.3985\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.3959\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.7006 - acc: 0.4082\n",
      ". theta fit =  [1.0061308 1.4725505]\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.7018 - acc: 0.4712\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7014 - acc: 0.4905\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4718\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: 0.7012 - acc: 0.4792\n",
      "Epoch 5/20\n",
      " 230000/2000000 [==>...........................] - ETA: 8s - loss: 0.7044 - acc: 0.4066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7014 - acc: 0.5002\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.7007 - acc: 0.3786\n",
      ". theta fit =  [0.9810763 1.4975632]\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.7008 - acc: 0.4730\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7014 - acc: 0.5057\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7011 - acc: 0.4994\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7021 - acc: 0.5138\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.7007 - acc: 0.5681\n",
      ". theta fit =  [1.0063975 1.4722416]\n",
      "Refining learning rate\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.7036 - acc: 0.4573\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7016 - acc: 0.4702\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4516\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4633\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4756\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - ETA: 0s - loss: 0.7018 - acc: 0.456 - 10s 5us/step - loss: 0.7018 - acc: 0.4563\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4774\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7025 - acc: 0.4564\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.7008 - acc: 0.3697\n",
      ". theta fit =  [1.0035919 1.4823861]\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.7026 - acc: 0.4658\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4765\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4638\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4707\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7007 - acc: 0.5020\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4679\n",
      "Epoch 7/20\n",
      " 319000/2000000 [===>..........................] - ETA: 8s - loss: 0.7065 - acc: 0.4504"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4759\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.4934\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4842\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7007 - acc: 0.4724\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7018 - acc: 0.4916\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.5010\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4643\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.7011 - acc: 0.5312\n",
      ". theta fit =  [1.0036224 1.4824209]\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.7017 - acc: 0.5041\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4520\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7015 - acc: 0.4773\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7007 - acc: 0.4886\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.4665\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7012 - acc: 0.4647\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4749\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.7010 - acc: 0.5019\n",
      ". theta fit =  [1.0009791 1.479779 ]\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.7042 - acc: 0.4710\n",
      "Epoch 2/20\n",
      "1544000/2000000 [======================>.......] - ETA: 2s - loss: 0.7011 - acc: 0.4690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7021 - acc: 0.4677\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4740\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4774\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7014 - acc: 0.4874\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4533\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.7005 - acc: 0.5032\n",
      ". theta fit =  [1.000966  1.4797518]\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.7030 - acc: 0.4659\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4781\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7016 - acc: 0.4676\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.5011\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7018 - acc: 0.5021\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.7011 - acc: 0.5057\n",
      ". theta fit =  [1.0037011 1.4824907]\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.7011 - acc: 0.4725\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7019 - acc: 0.4715\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4559\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7014 - acc: 0.4893\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7011 - acc: 0.5031\n",
      "Epoch 6/20\n",
      " 694000/2000000 [=========>....................] - ETA: 7s - loss: 0.6976 - acc: 0.4038"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7010 - acc: 0.4779\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7015 - acc: 0.4876\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7013 - acc: 0.4484\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4822\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7006 - acc: 0.4523\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7014 - acc: 0.4737\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4756\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7021 - acc: 0.4843\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.7006 - acc: 0.4146\n",
      ". theta fit =  [1.0092492 1.4825283]\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.7060 - acc: 0.4476\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7011 - acc: 0.4498\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4468\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7011 - acc: 0.4763\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7015 - acc: 0.4494\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7006 - acc: 0.4790\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7015 - acc: 0.4276\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7011 - acc: 0.4753\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7007 - acc: 0.4472\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.7016 - acc: 0.5017\n",
      ". theta fit =  [1.0064116 1.4796965]\n",
      "Refining learning rate\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.7022 - acc: 0.4465\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4596\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7013 - acc: 0.4768\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7010 - acc: 0.4570\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7014 - acc: 0.4827\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.7007 - acc: 0.3397\n",
      ". theta fit =  [1.006169  1.4808253]\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.7079 - acc: 0.4500\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7008 - acc: 0.4586\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4471\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7016 - acc: 0.4826\n",
      "Epoch 5/20\n",
      "1472000/2000000 [=====================>........] - ETA: 2s - loss: 0.7030 - acc: 0.4468"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7017 - acc: 0.4495\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.7009 - acc: 0.5025\n",
      ". theta fit =  [1.0055852 1.4802423]\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.7065 - acc: 0.4511\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7010 - acc: 0.4405\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7009 - acc: 0.4703\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7014 - acc: 0.4653\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7008 - acc: 0.4786\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7022 - acc: 0.4612\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7011 - acc: 0.4800\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7008 - acc: 0.4684\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7023 - acc: 0.4589\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7008 - acc: 0.4868\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7029 - acc: 0.4605\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.7006 - acc: 0.5053\n",
      ". theta fit =  [1.0058819 1.4805393]\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.7049 - acc: 0.4818\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7013 - acc: 0.4748\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7006 - acc: 0.4645\n",
      "Epoch 4/20\n",
      "1054000/2000000 [==============>...............] - ETA: 5s - loss: 0.7017 - acc: 0.4867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7010 - acc: 0.4780\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7015 - acc: 0.4540\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7009 - acc: 0.4820\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7005 - acc: 0.4776\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7011 - acc: 0.4701\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7021 - acc: 0.4763\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7012 - acc: 0.4858\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.7004 - acc: 0.5017\n",
      ". theta fit =  [1.0052999 1.4803805]\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.7087 - acc: 0.4610\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7006 - acc: 0.4576\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7010 - acc: 0.4737\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7023 - acc: 0.4603\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7008 - acc: 0.4690\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.7004 - acc: 0.5029\n",
      ". theta fit =  [1.0049899 1.4800704]\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: 0.7074 - acc: 0.4527\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7009 - acc: 0.4759\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7007 - acc: 0.4708\n",
      "Epoch 4/20\n",
      " 429000/2000000 [=====>........................] - ETA: 8s - loss: 0.7101 - acc: 0.4855"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7013 - acc: 0.4649\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7006 - acc: 0.4657\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7009 - acc: 0.4719\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7008 - acc: 0.4720\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7013 - acc: 0.4752\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.7006 - acc: 0.4566\n",
      ". theta fit =  [1.0049902 1.4800644]\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: 0.7024 - acc: 0.4697\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7015 - acc: 0.4615\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7013 - acc: 0.4671\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7010 - acc: 0.4774\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7012 - acc: 0.4884\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7004 - acc: 0.4726\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7026 - acc: 0.4776\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7008 - acc: 0.4833\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7015 - acc: 0.4906\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.7034 - acc: 0.5203\n",
      ". theta fit =  [1.0053079 1.4803822]\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: 0.7061 - acc: 0.4786\n",
      "Epoch 2/20\n",
      " 687000/2000000 [=========>....................] - ETA: 7s - loss: 0.7004 - acc: 0.4813"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.7123 - acc: 0.4548\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7005 - acc: 0.4860\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7001 - acc: 0.4825\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7014 - acc: 0.4713\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7010 - acc: 0.4769\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7016 - acc: 0.5087\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.7027 - acc: 0.5058\n",
      ". theta fit =  [1.0059499 1.4803861]\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.7163 - acc: 0.4826\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.7010 - acc: 0.4712\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7006 - acc: 0.4666\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7028 - acc: 0.4559\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7006 - acc: 0.4889\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7008 - acc: 0.4804\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7008 - acc: 0.4854\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7013 - acc: 0.4922\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7009 - acc: 0.4995\n",
      ". theta fit =  [1.0056225 1.4800587]\n",
      "Refining learning rate\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      " 466000/2000000 [=====>........................] - ETA: 36s - loss: 0.7210 - acc: 0.4997"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7011 - acc: 0.5025\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7008 - acc: 0.4902\n",
      ". theta fit =  [1.0055934 1.4801892]\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.7016 - acc: 0.4739\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7009 - acc: 0.4799\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7011 - acc: 0.4735\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7023 - acc: 0.4779\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7008 - acc: 0.4795\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7009 - acc: 0.4981\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7017 - acc: 0.4884\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7017 - acc: 0.5011\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7009 - acc: 0.5017\n",
      ". theta fit =  [1.0056267 1.4801558]\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.7119 - acc: 0.4599\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7004 - acc: 0.4743\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7008 - acc: 0.4607\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6993 - acc: 0.4874\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7013 - acc: 0.4833\n",
      "Epoch 6/20\n",
      " 517000/2000000 [======>.......................] - ETA: 8s - loss: 0.7020 - acc: 0.4818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7022 - acc: 0.4816\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7020 - acc: 0.4788\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7012 - acc: 0.4820\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7011 - acc: 0.4803\n",
      ". theta fit =  [1.0056945 1.4802237]\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.7026 - acc: 0.4731\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.7005 - acc: 0.4460\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7034 - acc: 0.4923\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7006 - acc: 0.4579\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7031 - acc: 0.4618\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7007 - acc: 0.4649\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7016 - acc: 0.4826\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7007 - acc: 0.4915\n",
      ". theta fit =  [1.0058006 1.4802593]\n",
      "Iteration:  52\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.7111 - acc: 0.4968\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7010 - acc: 0.4350\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7004 - acc: 0.4534\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7011 - acc: 0.4767\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.4748\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7003 - acc: 0.4677\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7011 - acc: 0.4859\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7011 - acc: 0.4818\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7037 - acc: 0.4713\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.7008 - acc: 0.5063\n",
      ". theta fit =  [1.0058366 1.4802953]\n",
      "Iteration:  53\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: 0.7098 - acc: 0.4667\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7011 - acc: 0.4692\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7004 - acc: 0.4681\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7012 - acc: 0.4600\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7012 - acc: 0.4776\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6999 - acc: 0.4756\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7013 - acc: 0.4704\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7007 - acc: 0.4785\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7021 - acc: 0.4832\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.7008 - acc: 0.4986\n",
      ". theta fit =  [1.0058728 1.4803317]\n",
      "Iteration:  54\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.7165 - acc: 0.4484\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7005 - acc: 0.4656\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7002 - acc: 0.4567\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7024 - acc: 0.4599\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7005 - acc: 0.4984\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7033 - acc: 0.4686\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.7024 - acc: 0.5083\n",
      ". theta fit =  [1.0059096 1.4803684]\n",
      "Iteration:  55\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.7073 - acc: 0.4861\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7021 - acc: 0.4748\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.4886\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7005 - acc: 0.4811\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7007 - acc: 0.4711\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7012 - acc: 0.4713\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7014 - acc: 0.4767\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7002 - acc: 0.4917\n",
      ". theta fit =  [1.0058727 1.4803314]\n",
      "Iteration:  56\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.7130 - acc: 0.4770\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7007 - acc: 0.4590\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7008 - acc: 0.4808\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7011 - acc: 0.4784\n",
      "Epoch 5/20\n",
      " 389000/2000000 [====>.........................] - ETA: 9s - loss: 0.6975 - acc: 0.4872"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.4869\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7007 - acc: 0.4704\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7010 - acc: 0.4745\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7022 - acc: 0.4882\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7004 - acc: 0.5025\n",
      ". theta fit =  [1.0058725 1.4803313]\n",
      "Refining learning rate\n",
      "Iteration:  58\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: 0.7076 - acc: 0.4729\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7009 - acc: 0.4842\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.4904\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7016 - acc: 0.4694\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7008 - acc: 0.4861\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7008 - acc: 0.4836\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7004 - acc: 0.4346\n",
      ". theta fit =  [1.005895  1.4803538]\n",
      "Iteration:  59\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: 0.7026 - acc: 0.4588\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7025 - acc: 0.4871\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7026 - acc: 0.4776\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.4871\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.7004 - acc: 0.4888\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7004 - acc: 0.5001\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7004 - acc: 0.4984\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7015 - acc: 0.5017\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7003 - acc: 0.5013\n",
      ". theta fit =  [1.0058988 1.48035  ]\n",
      "Iteration:  60\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "1077000/2000000 [===============>..............] - ETA: 15s - loss: 0.7240 - acc: 0.5015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7019 - acc: 0.4896\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7005 - acc: 0.4888\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7011 - acc: 0.4909\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.7016 - acc: 0.4464\n",
      ". theta fit =  [1.005899  1.4803501]\n",
      "Iteration:  62\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7059 - acc: 0.4647\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7014 - acc: 0.4912\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7008 - acc: 0.4916\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.5039\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7007 - acc: 0.4904\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7007 - acc: 0.4992\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7025 - acc: 0.4979\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7008 - acc: 0.4950\n",
      ". theta fit =  [1.005895  1.4803462]\n",
      "Iteration:  63\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7026 - acc: 0.4720\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7005 - acc: 0.4909\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7013 - acc: 0.5082\n",
      "Epoch 4/20\n",
      " 660000/2000000 [========>.....................] - ETA: 8s - loss: 0.7006 - acc: 0.5187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7003 - acc: 0.4908\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7008 - acc: 0.4806\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.4834\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7009 - acc: 0.4877\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.7005 - acc: 0.4137\n",
      ". theta fit =  [1.0059029 1.4803541]\n",
      "Iteration:  65\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7172 - acc: 0.4521\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7009 - acc: 0.4537\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7035 - acc: 0.4928\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7005 - acc: 0.4848\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7010 - acc: 0.4901\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7017 - acc: 0.4993\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7007 - acc: 0.5012\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.7003 - acc: 0.4968\n",
      ". theta fit =  [1.0059068 1.4803501]\n",
      "Iteration:  66\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7092 - acc: 0.4857\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7001 - acc: 0.4820\n",
      "Epoch 3/20\n",
      "1536000/2000000 [======================>.......] - ETA: 2s - loss: 0.7028 - acc: 0.4764"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7010 - acc: 0.4969\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.7014 - acc: 0.5013\n",
      ". theta fit =  [1.0059068 1.4803501]\n",
      "Iteration:  68\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.7218 - acc: 0.4593\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7013 - acc: 0.4569\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7009 - acc: 0.4959\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7008 - acc: 0.5011\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7015 - acc: 0.5016\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7008 - acc: 0.5034\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.4867\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7027 - acc: 0.5060\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7013 - acc: 0.4834\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.7017 - acc: 0.4836\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.7010 - acc: 0.5153\n",
      ". theta fit =  [1.0059109 1.4803542]\n",
      "Refining learning rate\n",
      "Iteration:  69\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 25s 12us/step - loss: 0.7068 - acc: 0.4859\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7003 - acc: 0.4656\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7015 - acc: 0.4981\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7002 - acc: 0.5051\n",
      "Epoch 5/20\n",
      "  51000/2000000 [..............................] - ETA: 16s - loss: 0.6987 - acc: 0.5038"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7017 - acc: 0.5034\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7010 - acc: 0.4949\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.7004 - acc: 0.4566\n",
      ". theta fit =  [1.0059096 1.4803522]\n",
      "Iteration:  71\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: 0.7137 - acc: 0.4781\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7017 - acc: 0.5090\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7002 - acc: 0.4817\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7024 - acc: 0.4820\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7006 - acc: 0.5111\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7012 - acc: 0.4962\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.7031 - acc: 0.5092\n",
      ". theta fit =  [1.0059099 1.4803525]\n",
      "Iteration:  72\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: 0.7146 - acc: 0.4799\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7006 - acc: 0.4623\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7015 - acc: 0.5043\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7018 - acc: 0.5030\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.7003 - acc: 0.5009\n",
      "Epoch 6/20\n",
      " 101000/2000000 [>.............................] - ETA: 14s - loss: 0.7034 - acc: 0.5087"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 26s 13us/step - loss: 0.7121 - acc: 0.4343\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7022 - acc: 0.4996\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7006 - acc: 0.4528\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7015 - acc: 0.4997\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7014 - acc: 0.5007\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7014 - acc: 0.5193\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.7006 - acc: 0.5041\n",
      ". theta fit =  [1.0059109 1.4803534]\n",
      "Iteration:  74\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: 0.7219 - acc: 0.4381\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7014 - acc: 0.4728\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7010 - acc: 0.4582\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7004 - acc: 0.4675\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7010 - acc: 0.4727\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.7013 - acc: 0.4778\n",
      "Epoch 7/20\n",
      "1691000/2000000 [========================>.....] - ETA: 1s - loss: 0.7008 - acc: 0.4918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):    \n",
    "    print(\"Iteration: \", iteration )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=20, batch_size=1000,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "    \n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "    \n",
    "    fit_vals_mu = np.array(fit_vals)[(index_refine[-1]):,0]\n",
    "    fit_vals_sigma = np.array(fit_vals)[(index_refine[-1]):,1]\n",
    "    \n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "    extrema_mu = np.concatenate((argrelmin(fit_vals_mu)[0], argrelmax(fit_vals_mu)[0]))\n",
    "    extrema_mu = extrema_mu[extrema_mu>= iteration - index_refine[-1] -20]\n",
    "            \n",
    "    extrema_sigma = np.concatenate((argrelmin(fit_vals_sigma)[0], argrelmax(fit_vals_sigma)[0]))\n",
    "    extrema_sigma = extrema_sigma[extrema_sigma >= iteration - index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema_mu\", extrema_mu)\n",
    "    print(\"extrema_sigma\", extrema_sigma)\n",
    "    '''\n",
    "    \n",
    "    if (len(extrema_mu) == 0) or (len(extrema_sigma) == 0): # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema_mu) >= 6) and (len(extrema_sigma) >= 6): #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print(\"Refining learning rate\")\n",
    "        optimizer.lr = optimizer.lr/10\n",
    "        \n",
    "        mean_fit = np.array([[np.mean(fit_vals_mu[len(fit_vals_mu)-4:len(fit_vals_mu)]),\n",
    "                              np.mean(fit_vals_sigma[len(fit_vals_sigma)-4:len(fit_vals_sigma)])]])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex= True,  constrained_layout=True)\n",
    "fig.suptitle(\"GaussianAltFit-2D (DCTR Reweight):\\n N = {:.0e}, Iterations = {:.0f}\".format(N, len(fit_vals)))\n",
    "axs[0].plot(fit_vals[:,0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], 0, len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(fit_vals[:,1], label='Model $\\sigma$ Fit', color ='orange')\n",
    "axs[1].hlines(theta1_param[1], 0, len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.savefig(\"GaussianAltFit-2D (DCTR Reweight):\\n N = {:.0e}, Iterations = {:.0f}.png\".format(N, len(fit_vals)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "\n",
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex= True,  constrained_layout=True)\n",
    "fig.suptitle(\"GaussianAltFit-2D Zoomed (DCTR Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\".format(N,index_refine[1], len(fit_vals)))\n",
    "axs[0].plot(np.arange(index_refine[1], len(fit_vals[:,0])), fit_vals[index_refine[1]:,0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], index_refine[1], len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.arange(index_refine[1], len(fit_vals[:,1])), fit_vals[index_refine[1]:,1], label='Model $\\sigma$ Fit', color = 'orange')\n",
    "axs[1].hlines(theta1_param[1], index_refine[1], len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.savefig(\"GaussianAltFit-2D Zoomed (DCTR Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\".format(N,index_refine[1], len(fit_vals)))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Fitting between DCTR Reweighting and Analytical Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweight_analytically = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = [0., 1.]\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',\n",
    "                              patience = 3, \n",
    "                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 16,899\n",
      "Trainable params: 16,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=(2,),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "index_refine = np.array([0])\n",
    "batch_size = 2*N\n",
    "lr_initial = 5e-1 #smaller learning rate yields better precision\n",
    "iterations = 75 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs\n",
    "    \n",
    "    \n",
    "    theta = 0. #starting value\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        theta_prime = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array \n",
    "    else:\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        theta_prime = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "        \n",
    "    #creating tensor with same shape as inputs, with val in every entry \n",
    "    concat_input_and_params = K.ones(shape = (x.shape[0], 2), dtype=tf.float32)*theta_prime\n",
    "    data = K.concatenate((x, concat_input_and_params), axis=-1)\n",
    "    \n",
    "    if reweight_analytically == False: #NN reweight\n",
    "        w = reweight(data)\n",
    "    else: # analytical reweight\n",
    "        w = analytical_reweight(events = x, \n",
    "                                mu1 = theta_prime[0], \n",
    "                                sigma1 = theta_prime[1]) \n",
    "    \n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        #t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        \n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: 0.6016 - acc: 0.6652\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.5946 - acc: 0.6724\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.5944 - acc: 0.6724\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.5944 - acc: 0.6724\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: 0.5943 - acc: 0.6723\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.5943 - acc: 0.6725\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 14/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 15/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 16/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 17/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 18/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 19/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Epoch 20/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.5943 - acc: 0.6724\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.5944 - acc: 0.6724\n",
      ". theta fit =  [0.49998626 1.49999   ]\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.6862 - acc: 0.6624\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6800 - acc: 0.6666\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6798 - acc: 0.6650\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6798 - acc: 0.6647\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6798 - acc: 0.6686\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6800 - acc: 0.6680\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6798 - acc: 0.6671\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6800 - acc: 0.6682\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6800 - acc: 0.6689\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6796 - acc: 0.6688\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6798 - acc: 0.6683\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6797 - acc: 0.6688\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6799 - acc: 0.6688\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.6793 - acc: 0.6701\n",
      ". theta fit =  [0.8720312 1.8719263]\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: 0.6850 - acc: 0.5336\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6808 - acc: 0.5083\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6810 - acc: 0.5254\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6812 - acc: 0.5161\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6810 - acc: 0.5179\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.6808 - acc: 0.5497\n",
      ". theta fit =  [1.1913772 1.5525391]\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: 0.6922 - acc: 0.3791\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6914 - acc: 0.3724\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6914 - acc: 0.3713\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6909 - acc: 0.3657\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6911 - acc: 0.3636\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6913 - acc: 0.3649\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6912 - acc: 0.3582\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6905 - acc: 0.3932\n",
      ". theta fit =  [0.9008696 1.2620674]\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.6864 - acc: 0.5516\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6858 - acc: 0.5567\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.6858 - acc: 0.5560\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6857 - acc: 0.5580\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6856 - acc: 0.5545\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: 0.6856 - acc: 0.5532\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6856 - acc: 0.5546\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6856 - acc: 0.5535\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6856 - acc: 0.5517\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6856 - acc: 0.5553\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6855 - acc: 0.5532\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6855 - acc: 0.5543\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6857 - acc: 0.5509\n",
      "Epoch 14/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6855 - acc: 0.5530\n",
      "Epoch 15/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6855 - acc: 0.5513\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.6857 - acc: 0.5638\n",
      ". theta fit =  [1.1735678 1.5348004]\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: 0.6948 - acc: 0.3716\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6908 - acc: 0.3487\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6915 - acc: 0.3495\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6908 - acc: 0.3557\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6918 - acc: 0.3486\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6906 - acc: 0.3555\n",
      ". theta fit =  [0.9125484 1.2738249]\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.6874 - acc: 0.5507\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6865 - acc: 0.5557\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6865 - acc: 0.5556\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6863 - acc: 0.5540\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6864 - acc: 0.5504\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6864 - acc: 0.5514\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6863 - acc: 0.5505\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6864 - acc: 0.5510\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6865 - acc: 0.5478\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6863 - acc: 0.5506\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.6864 - acc: 0.5571\n",
      ". theta fit =  [1.1656849 1.5270019]\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.6951 - acc: 0.3672\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6913 - acc: 0.3449\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6921 - acc: 0.3526\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: 0.6912 - acc: 0.3478\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6916 - acc: 0.3474\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6909 - acc: 0.3472\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6913 - acc: 0.3444\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6909 - acc: 0.3451\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6919 - acc: 0.3462\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6910 - acc: 0.3461\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6904 - acc: 0.3434\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6912 - acc: 0.3470\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6914 - acc: 0.3502\n",
      "Epoch 14/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6908 - acc: 0.3448\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6909 - acc: 0.3367\n",
      ". theta fit =  [0.917841  1.2791858]\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: 0.6876 - acc: 0.5308\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6867 - acc: 0.5565\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6868 - acc: 0.5492\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6865 - acc: 0.5501\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6867 - acc: 0.5486\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6866 - acc: 0.5498\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6871 - acc: 0.5473\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6865 - acc: 0.5488\n",
      ". theta fit =  [1.1621395 1.5235494]\n",
      "Refining learning rate\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: 0.6938 - acc: 0.4541\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6918 - acc: 0.4415\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6916 - acc: 0.4305\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: 0.6919 - acc: 0.4399\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6921 - acc: 0.4424\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6919 - acc: 0.4400\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.6913 - acc: 0.4525\n",
      ". theta fit =  [1.0153513 1.4251084]\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: 0.6949 - acc: 0.4283\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6925 - acc: 0.4661\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6920 - acc: 0.4589\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6930 - acc: 0.4508\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6925 - acc: 0.4785\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6918 - acc: 0.4544\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6926 - acc: 0.4621\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6922 - acc: 0.4632\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6931 - acc: 0.4670\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6933 - acc: 0.4940\n",
      ". theta fit =  [1.0394408 1.4492123]\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: 0.6922 - acc: 0.4066\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6922 - acc: 0.4021\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6927 - acc: 0.4059\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6926 - acc: 0.4243\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6921 - acc: 0.3390\n",
      ". theta fit =  [1.0153788 1.4732686]\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: 0.6941 - acc: 0.3861\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6925 - acc: 0.4640\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6922 - acc: 0.4195\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6928 - acc: 0.4666\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.6932 - acc: 0.4279\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6929 - acc: 0.4296\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.6922 - acc: 0.4998\n",
      ". theta fit =  [0.9913553 1.4973533]\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 27s 14us/step - loss: 0.6949 - acc: 0.4880\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6925 - acc: 0.4612\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6921 - acc: 0.4939\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6925 - acc: 0.4981\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6929 - acc: 0.5434\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6928 - acc: 0.4552\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6930 - acc: 0.6173\n",
      ". theta fit =  [1.0156368 1.5216408]\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 27s 14us/step - loss: 0.6934 - acc: 0.4332\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6926 - acc: 0.4312\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6924 - acc: 0.4561\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6935 - acc: 0.4279\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6926 - acc: 0.4429\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6930 - acc: 0.4178\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6922 - acc: 0.5095\n",
      ". theta fit =  [1.0397433 1.497381 ]\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.6950 - acc: 0.3894\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.3716\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6923 - acc: 0.3680\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6927 - acc: 0.3924\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6927 - acc: 0.3614\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6926 - acc: 0.3909\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6923 - acc: 0.4506\n",
      ". theta fit =  [1.0153375 1.5212672]\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.6995 - acc: 0.3876\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6922 - acc: 0.4110\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6929 - acc: 0.4283\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.4489\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6921 - acc: 0.4443\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6926 - acc: 0.4191\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6929 - acc: 0.4831\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6922 - acc: 0.4122\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.6920 - acc: 0.4804\n",
      ". theta fit =  [0.99073756 1.496634  ]\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 34s 17us/step - loss: 0.7004 - acc: 0.4248\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6932 - acc: 0.4828\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6926 - acc: 0.5007\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6924 - acc: 0.5696\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6929 - acc: 0.5464\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6924 - acc: 0.4819\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.5134\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: -0.6925 - acc: 0.4700\n",
      ". theta fit =  [0.9659006 1.4718049]\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.6978 - acc: 0.5220\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6933 - acc: 0.6044\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6926 - acc: 0.6072\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6923 - acc: 0.6150\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6928 - acc: 0.6115\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.5676\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.5492\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.6920 - acc: 0.5714\n",
      ". theta fit =  [0.99089265 1.4968657 ]\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.6949 - acc: 0.5237\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.5213\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6922 - acc: 0.4938\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.5411\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6928 - acc: 0.4975\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6921 - acc: 0.5213\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6932 - acc: 0.5146\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6922 - acc: 0.5302\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6930 - acc: 0.4721\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6938 - acc: 0.5008\n",
      ". theta fit =  [1.0162164 1.5221908]\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.7006 - acc: 0.4240\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6922 - acc: 0.4251\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4564\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6922 - acc: 0.4126\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6934 - acc: 0.4204\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: 0.6925 - acc: 0.4285\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6921 - acc: 0.4456\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6922 - acc: 0.4365\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6928 - acc: 0.4394\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6922 - acc: 0.4237\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6918 - acc: 0.4271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  [0.9906519 1.4966017]\n",
      "Refining learning rate\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: 0.6970 - acc: 0.4669\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6924 - acc: 0.4932\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6935 - acc: 0.5136\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6919 - acc: 0.5017\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6932 - acc: 0.5269\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6927 - acc: 0.5336\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6932 - acc: 0.4922\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6930 - acc: 0.6274\n",
      ". theta fit =  [0.9935006 1.4994518]\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: 0.6983 - acc: 0.4670\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6927 - acc: 0.5054\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6942 - acc: 0.5305\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6928 - acc: 0.5115\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6926 - acc: 0.5079\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6926 - acc: 0.5356\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6944 - acc: 0.4731\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.5078\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.5058\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6928 - acc: 0.5085\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6926 - acc: 0.5107\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6921 - acc: 0.5705\n",
      ". theta fit =  [0.99609923 1.4968371 ]\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 29s 14us/step - loss: 0.6955 - acc: 0.4651\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4863\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6927 - acc: 0.5232\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6925 - acc: 0.5293\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6929 - acc: 0.4664\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6919 - acc: 0.5030\n",
      ". theta fit =  [0.9934643 1.494193 ]\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.6936 - acc: 0.5114\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.6919 - acc: 0.4958\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6925 - acc: 0.4900\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6930 - acc: 0.5190\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6922 - acc: 0.5129\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6929 - acc: 0.4859\n",
      ". theta fit =  [0.99613655 1.4968677 ]\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 28s 14us/step - loss: 0.6997 - acc: 0.4339\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6927 - acc: 0.4538\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: 0.6939 - acc: 0.5144\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6920 - acc: 0.5068\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6931 - acc: 0.5214\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: 0.6928 - acc: 0.4907\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6923 - acc: 0.5240\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6920 - acc: 0.4959\n",
      ". theta fit =  [0.9988359 1.4941634]\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 29s 15us/step - loss: 0.6981 - acc: 0.4722\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6921 - acc: 0.4785\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6931 - acc: 0.5117\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6920 - acc: 0.5008\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6953 - acc: 0.5016\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6920 - acc: 0.4530\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6928 - acc: 0.5061\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.4538\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.5299\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6924 - acc: 0.4967\n",
      ". theta fit =  [1.0015723 1.4969001]\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 30s 15us/step - loss: 0.6927 - acc: 0.4574\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6923 - acc: 0.4524\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6926 - acc: 0.4561\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6935 - acc: 0.4635\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.4527\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6921 - acc: 0.4694\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.5039\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6937 - acc: 0.4404\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4670\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6930 - acc: 0.6434\n",
      ". theta fit =  [1.0043424 1.4996713]\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 30s 15us/step - loss: 0.6979 - acc: 0.4409\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6920 - acc: 0.4441\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4837\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6934 - acc: 0.4461\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6928 - acc: 0.4987\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6921 - acc: 0.4785\n",
      ". theta fit =  [1.0015501 1.496878 ]\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 30s 15us/step - loss: 0.7006 - acc: 0.4350\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6928 - acc: 0.4999\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.4777\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6935 - acc: 0.4564\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.5038\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6920 - acc: 0.4811\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6926 - acc: 0.4707\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6930 - acc: 0.4799\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6923 - acc: 0.4777\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6937 - acc: 0.4843\n",
      ". theta fit =  [1.0043869 1.4997162]\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 31s 15us/step - loss: 0.7004 - acc: 0.4243\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6921 - acc: 0.4332\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6955 - acc: 0.4672\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.4660\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6923 - acc: 0.4975\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.6918 - acc: 0.4990\n",
      ". theta fit =  [1.0072333 1.4968464]\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 30s 15us/step - loss: 0.7015 - acc: 0.4183\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.4157\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6925 - acc: 0.4612\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6946 - acc: 0.4773\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6938 - acc: 0.4315\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6917 - acc: 0.5034\n",
      ". theta fit =  [1.004331  1.4939426]\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 31s 15us/step - loss: 0.7228 - acc: 0.4175\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6960 - acc: 0.4043\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6926 - acc: 0.4801\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6914 - acc: 0.4720\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6929 - acc: 0.4886\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6927 - acc: 0.4681\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6933 - acc: 0.4918\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.6964 - acc: 0.5020\n",
      ". theta fit =  [1.0072702 1.4968824]\n",
      "Refining learning rate\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 31s 16us/step - loss: 0.7059 - acc: 0.4248\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6937 - acc: 0.4225\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6920 - acc: 0.4513\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6934 - acc: 0.4556\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6930 - acc: 0.4796\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6918 - acc: 0.4255\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6927 - acc: 0.5012\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6917 - acc: 0.4566\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6941 - acc: 0.4612\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6926 - acc: 0.4599\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6937 - acc: 0.4256\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: -0.6966 - acc: 0.5382\n",
      ". theta fit =  [1.0061027 1.4971443]\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 32s 16us/step - loss: 0.7120 - acc: 0.4383\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6929 - acc: 0.4363\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6929 - acc: 0.4791\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6928 - acc: 0.4700\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.4425\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 31s 15us/step - loss: 0.6918 - acc: 0.4482\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6968 - acc: 0.4251\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6927 - acc: 0.4325\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6936 - acc: 0.4530\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: -0.6957 - acc: 0.4766\n",
      ". theta fit =  [1.0064068 1.4974486]\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 31s 16us/step - loss: 0.6973 - acc: 0.4247\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6925 - acc: 0.4488\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6935 - acc: 0.4433\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6919 - acc: 0.4289\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6923 - acc: 0.4473\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6925 - acc: 0.4681\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6923 - acc: 0.4788\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6918 - acc: 0.4200\n",
      ". theta fit =  [1.006103  1.4971447]\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 32s 16us/step - loss: 0.7199 - acc: 0.4089\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6928 - acc: 0.4392\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.4476\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6936 - acc: 0.4825\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4352\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6923 - acc: 0.4784\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6920 - acc: 0.4345\n",
      ". theta fit =  [1.0057957 1.4968373]\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 31s 16us/step - loss: 0.7053 - acc: 0.4237\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6918 - acc: 0.4264\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.6931 - acc: 0.4736\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6924 - acc: 0.4743\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6916 - acc: 0.4223\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6933 - acc: 0.4625\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4673\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6942 - acc: 0.4064\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.6966 - acc: 0.5601\n",
      ". theta fit =  [1.0061067 1.4971483]\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 31s 15us/step - loss: 0.6950 - acc: 0.4334\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.6926 - acc: 0.4532\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6956 - acc: 0.4639\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6924 - acc: 0.4515\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6932 - acc: 0.4266\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6922 - acc: 0.4431\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6975 - acc: 0.4315\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6923 - acc: 0.4273\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6925 - acc: 0.4468\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.6918 - acc: 0.4309\n",
      ". theta fit =  [1.005793  1.4968343]\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 33s 16us/step - loss: 0.6942 - acc: 0.4631\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4571\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6927 - acc: 0.4197\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6923 - acc: 0.4532\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6932 - acc: 0.4722\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6933 - acc: 0.4470\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6922 - acc: 0.4239\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6925 - acc: 0.4520\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6921 - acc: 0.4910\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6971 - acc: 0.4247\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6917 - acc: 0.4531\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.6922 - acc: 0.4116\n",
      "Epoch 13/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6931 - acc: 0.4216\n",
      "Epoch 14/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6924 - acc: 0.4325\n",
      "Training theta\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6924 - acc: 0.4469\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6925 - acc: 0.4700\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 12us/step - loss: -0.6919 - acc: 0.5027\n",
      ". theta fit =  [1.0057949 1.4961962]\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 33s 17us/step - loss: 0.7101 - acc: 0.4071\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6922 - acc: 0.4064\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6922 - acc: 0.4382\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: 0.6932 - acc: 0.4459\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6938 - acc: 0.4163\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6919 - acc: 0.4998\n",
      ". theta fit =  [1.0054828 1.4958719]\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 34s 17us/step - loss: 0.7050 - acc: 0.4173\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6921 - acc: 0.4153\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6962 - acc: 0.4193\n",
      "Epoch 4/20\n",
      "1786000/2000000 [=========================>....] - ETA: 1s - loss: 0.6926 - acc: 0.4073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 33s 17us/step - loss: 0.7034 - acc: 0.4261\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6917 - acc: 0.4462\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6919 - acc: 0.4343\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6922 - acc: 0.4581\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6924 - acc: 0.4534\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 11us/step - loss: -0.6918 - acc: 0.3389\n",
      ". theta fit =  [1.0044904 1.494879 ]\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 34s 17us/step - loss: 0.7020 - acc: 0.4105\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6918 - acc: 0.4538\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6925 - acc: 0.4193\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6948 - acc: 0.4625\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6928 - acc: 0.4440\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6918 - acc: 0.4969\n",
      ". theta fit =  [1.0041531 1.4945415]\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "1275000/2000000 [==================>...........] - ETA: 16s - loss: 0.7282 - acc: 0.4307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6933 - acc: 0.4559\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: -0.6913 - acc: 0.3504\n",
      ". theta fit =  [1.0038122 1.4942005]\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 32s 16us/step - loss: 0.7089 - acc: 0.3931\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.6920 - acc: 0.4293\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6919 - acc: 0.4728\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6957 - acc: 0.4577\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6923 - acc: 0.4501\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6924 - acc: 0.4531\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 11us/step - loss: -0.6916 - acc: 0.4949\n",
      ". theta fit =  [1.0034689 1.4938565]\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.7218 - acc: 0.4076\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6923 - acc: 0.4550\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6923 - acc: 0.4842\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6922 - acc: 0.4462\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6929 - acc: 0.4743\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6926 - acc: 0.4872\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6922 - acc: 0.4847\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6921 - acc: 0.5029\n",
      ". theta fit =  [1.0031215 1.4935092]\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 34s 17us/step - loss: 0.7142 - acc: 0.4264\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6918 - acc: 0.4225\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6946 - acc: 0.4467\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: 0.6921 - acc: 0.4857\n",
      "Epoch 5/20\n",
      " 310000/2000000 [===>..........................] - ETA: 13s - loss: 0.6940 - acc: 0.4354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6922 - acc: 0.4914\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6915 - acc: 0.5434\n",
      ". theta fit =  [1.0024178 1.4928044]\n",
      "Iteration:  51\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 40s 20us/step - loss: 0.6970 - acc: 0.4380\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6920 - acc: 0.4653\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6934 - acc: 0.4386\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6935 - acc: 0.5074\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6920 - acc: 0.4532\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6922 - acc: 0.4778\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6927 - acc: 0.4642\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6917 - acc: 0.4536\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6953 - acc: 0.4847\n",
      "Epoch 10/20\n",
      " 916000/2000000 [============>.................] - ETA: 8s - loss: 0.6943 - acc: 0.4146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6941 - acc: 0.4654\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6921 - acc: 0.4494\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6931 - acc: 0.4466\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6932 - acc: 0.4463\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6933 - acc: 0.4790\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6917 - acc: 0.4923\n",
      ". theta fit =  [1.0024161 1.4928017]\n",
      "Iteration:  53\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 36s 18us/step - loss: 0.7143 - acc: 0.4064\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6933 - acc: 0.4246\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6919 - acc: 0.4637\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6946 - acc: 0.4953\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6916 - acc: 0.4347\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6925 - acc: 0.4760\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6934 - acc: 0.5358\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6920 - acc: 0.4401\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: -0.6970 - acc: 0.5000\n",
      ". theta fit =  [1.0027798 1.4931655]\n",
      "Refining learning rate\n",
      "Iteration:  54\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      " 262000/2000000 [==>...........................] - ETA: 2:15 - loss: 0.7041 - acc: 0.5281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6923 - acc: 0.4430\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6926 - acc: 0.4326\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6923 - acc: 0.4274\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6918 - acc: 0.4733\n",
      ". theta fit =  [1.0026338 1.4930198]\n",
      "Iteration:  55\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.7170 - acc: 0.4084\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6925 - acc: 0.4097\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6927 - acc: 0.4430\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6949 - acc: 0.4508\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6921 - acc: 0.4439\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6922 - acc: 0.4512\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6926 - acc: 0.4588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6931 - acc: 0.4588\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6952 - acc: 0.4283\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6931 - acc: 0.4764\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 30s 15us/step - loss: -0.6925 - acc: 0.4734\n",
      ". theta fit =  [1.0026342 1.4930941]\n",
      "Iteration:  57\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 36s 18us/step - loss: 0.7052 - acc: 0.4088\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6918 - acc: 0.4483\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6920 - acc: 0.4990\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6980 - acc: 0.4527\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6921 - acc: 0.4505\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 12us/step - loss: -0.6920 - acc: 0.5000\n",
      ". theta fit =  [1.0026717 1.4931315]\n",
      "Iteration:  58\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 36s 18us/step - loss: 0.7049 - acc: 0.4319\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6919 - acc: 0.4270\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6917 - acc: 0.5134\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6934 - acc: 0.4480\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6923 - acc: 0.4840\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6916 - acc: 0.4672\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6952 - acc: 0.4615\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6919 - acc: 0.4888\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6943 - acc: 0.4505\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6980 - acc: 0.4946\n",
      ". theta fit =  [1.0027096 1.4931694]\n",
      "Iteration:  59\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 39s 19us/step - loss: 0.7215 - acc: 0.4090\n",
      "Epoch 2/20\n",
      " 571000/2000000 [=======>......................] - ETA: 11s - loss: 0.7004 - acc: 0.4287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6922 - acc: 0.4622\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6958 - acc: 0.4480\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6929 - acc: 0.4652\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 22s 11us/step - loss: 0.6912 - acc: 0.4256\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: 0.6995 - acc: 0.4175\n",
      "Epoch 11/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6939 - acc: 0.4644\n",
      "Epoch 12/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6926 - acc: 0.4531\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.7199 - acc: 0.5087\n",
      ". theta fit =  [1.0027479 1.4932077]\n",
      "Iteration:  60\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 37s 19us/step - loss: 0.7318 - acc: 0.3827\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6941 - acc: 0.4399\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6921 - acc: 0.4487\n",
      "Epoch 4/20\n",
      " 510000/2000000 [======>.......................] - ETA: 13s - loss: 0.7000 - acc: 0.4181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6922 - acc: 0.4468\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6924 - acc: 0.4744\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6924 - acc: 0.4833\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6928 - acc: 0.4582\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: -0.6921 - acc: 0.5479\n",
      ". theta fit =  [1.0028251 1.4932075]\n",
      "Iteration:  62\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.7253 - acc: 0.4278\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: 0.6939 - acc: 0.4561\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6922 - acc: 0.5140\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6936 - acc: 0.4598\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6928 - acc: 0.4946\n",
      "Epoch 6/20\n",
      "1465000/2000000 [====================>.........] - ETA: 8s - loss: 0.6889 - acc: 0.5022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 25s 12us/step - loss: -0.6917 - acc: 0.4416\n",
      ". theta fit =  [1.002786  1.4931684]\n",
      "Iteration:  63\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.7015 - acc: 0.4443\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6928 - acc: 0.4694\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6937 - acc: 0.5226\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6926 - acc: 0.4817\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6909 - acc: 0.4795\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6932 - acc: 0.4956\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6970 - acc: 0.4593\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6925 - acc: 0.4509\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 25s 13us/step - loss: -0.6950 - acc: 0.4662\n",
      ". theta fit =  [1.0028255 1.4932078]\n",
      "Iteration:  64\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.7108 - acc: 0.4106\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6931 - acc: 0.4313\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6921 - acc: 0.4534\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6943 - acc: 0.4531\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6922 - acc: 0.4700\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6956 - acc: 0.4826\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 24s 12us/step - loss: -0.6921 - acc: 0.5064\n",
      ". theta fit =  [1.0028652 1.4932474]\n",
      "Iteration:  65\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 42s 21us/step - loss: 0.7186 - acc: 0.4268\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6957 - acc: 0.4580\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6916 - acc: 0.4608\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6926 - acc: 0.4732\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6924 - acc: 0.4764\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6929 - acc: 0.4551\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6932 - acc: 0.5000\n",
      ". theta fit =  [1.0029051 1.4932873]\n",
      "Iteration:  66\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 37s 19us/step - loss: 0.7335 - acc: 0.4031\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: 0.6952 - acc: 0.4591\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6927 - acc: 0.4536\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6922 - acc: 0.4657\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6932 - acc: 0.4835\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6930 - acc: 0.4389\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6928 - acc: 0.4731\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6921 - acc: 0.5100\n",
      ". theta fit =  [1.0029454 1.4933276]\n",
      "Iteration:  67\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 39s 19us/step - loss: 0.7125 - acc: 0.4392\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6919 - acc: 0.4614\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6928 - acc: 0.4653\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6929 - acc: 0.4722\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6925 - acc: 0.4670\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6916 - acc: 0.4913\n",
      ". theta fit =  [1.0029054 1.4932871]\n",
      "Iteration:  68\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 39s 19us/step - loss: 0.7121 - acc: 0.4384\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6923 - acc: 0.4310\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6923 - acc: 0.4445\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6946 - acc: 0.4753\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6928 - acc: 0.4648\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: 0.6931 - acc: 0.4686\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6915 - acc: 0.4587\n",
      ". theta fit =  [1.0029458 1.4932463]\n",
      "Iteration:  69\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.7059 - acc: 0.4347\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6922 - acc: 0.4841\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6933 - acc: 0.4654\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6924 - acc: 0.4844\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6938 - acc: 0.4709\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6917 - acc: 0.4964\n",
      ". theta fit =  [1.0029048 1.4932052]\n",
      "Iteration:  70\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 39s 19us/step - loss: 0.7181 - acc: 0.4447\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6932 - acc: 0.4795\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6926 - acc: 0.5032\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6924 - acc: 0.4867\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6954 - acc: 0.4866\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6923 - acc: 0.4781\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6929 - acc: 0.4872\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6941 - acc: 0.4954\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6924 - acc: 0.4666\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6922 - acc: 0.4982\n",
      ". theta fit =  [1.0029461 1.4932464]\n",
      "Iteration:  71\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.7279 - acc: 0.4303\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6958 - acc: 0.4548\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6926 - acc: 0.4961\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6938 - acc: 0.4826\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6922 - acc: 0.4918\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6969 - acc: 0.4606\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6915 - acc: 0.4993\n",
      "Epoch 8/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6931 - acc: 0.4648\n",
      "Epoch 9/20\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: 0.6930 - acc: 0.4830\n",
      "Epoch 10/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6926 - acc: 0.4794\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: -0.6932 - acc: 0.5061\n",
      ". theta fit =  [1.0029877 1.4932882]\n",
      "Iteration:  72\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 38s 19us/step - loss: 0.7244 - acc: 0.4294\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6924 - acc: 0.4532\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 26s 13us/step - loss: 0.6931 - acc: 0.4802\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6911 - acc: 0.4731\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6938 - acc: 0.4671\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6928 - acc: 0.4706\n",
      "Epoch 7/20\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: 0.6939 - acc: 0.4919\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: -0.7053 - acc: 0.5063\n",
      ". theta fit =  [1.0030297 1.4933301]\n",
      "Iteration:  73\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 39s 20us/step - loss: 0.7027 - acc: 0.4459\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: 0.6923 - acc: 0.4520\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6922 - acc: 0.4919\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6927 - acc: 0.5003\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6943 - acc: 0.4599\n",
      "Epoch 6/20\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: 0.6926 - acc: 0.4621\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 27s 14us/step - loss: -0.6921 - acc: 0.5000\n",
      ". theta fit =  [1.0030719 1.4932879]\n",
      "Refining learning rate\n",
      "Iteration:  74\n",
      "Training g\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "2000000/2000000 [==============================] - 40s 20us/step - loss: 0.7194 - acc: 0.4350\n",
      "Epoch 2/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6921 - acc: 0.4409\n",
      "Epoch 3/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6933 - acc: 0.4640\n",
      "Epoch 4/20\n",
      "2000000/2000000 [==============================] - 23s 12us/step - loss: 0.6926 - acc: 0.4680\n",
      "Epoch 5/20\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: 0.6930 - acc: 0.4474\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 27s 13us/step - loss: -0.6917 - acc: 0.4968\n",
      ". theta fit =  [1.0030046 1.4932839]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):    \n",
    "    print(\"Iteration: \", iteration )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=20, batch_size=1000,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "    \n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "    \n",
    "    fit_vals_mu = np.array(fit_vals)[(index_refine[-1]):,0]\n",
    "    fit_vals_sigma = np.array(fit_vals)[(index_refine[-1]):,1]\n",
    "    \n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "    extrema_mu = np.concatenate((argrelmin(fit_vals_mu)[0], argrelmax(fit_vals_mu)[0]))\n",
    "    extrema_mu = extrema_mu[extrema_mu>= iteration - index_refine[-1] -20]\n",
    "            \n",
    "    extrema_sigma = np.concatenate((argrelmin(fit_vals_sigma)[0], argrelmax(fit_vals_sigma)[0]))\n",
    "    extrema_sigma = extrema_sigma[extrema_sigma >= iteration - index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema_mu\", extrema_mu)\n",
    "    print(\"extrema_sigma\", extrema_sigma)\n",
    "    '''\n",
    "    \n",
    "    if (len(extrema_mu) == 0) or (len(extrema_sigma) == 0): # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema_mu) >= 6) and (len(extrema_sigma) >= 6): #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print(\"Refining learning rate\")\n",
    "        optimizer.lr = optimizer.lr/10\n",
    "        \n",
    "        mean_fit = np.array([[np.mean(fit_vals_mu[len(fit_vals_mu)-4:len(fit_vals_mu)]),\n",
    "                              np.mean(fit_vals_sigma[len(fit_vals_sigma)-4:len(fit_vals_sigma)])]])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAElCAYAAACWMvcuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xl4HNWV8P/vaS3W6k2yDV4lsLExwhbGNkswmDgwYIjZkoATBgjwIwSYMPxemJCQsA2ETODNNiEBhrANYEhITNgSAiQGAsHYMt6N900Gy7b2fevz/nGrpVarW2qt3ZLP53n6UXfVrarTJalP31u37hVVxRhjjBlsfLEOwBhjjOkLluCMMcYMSpbgjDHGDEqW4IwxxgxKluCMMcYMSpbgjDHGDEqW4EyHROTPInJlPx1LRWRyN7abKCJVIpLQF3F1MZYHROTf++lYu0TkS93c9hER+WEPjz9fRAp7so++1JW/CxHJ8f7+Eruw/z+IyLk9i9L0JUtwcUZELhOR5SJSLSIHvOc3iIjEIh5VPVdVn+6t/YlIroj4ReQ3nZR7SkTuC1m2S0RqvQ+twGOsqu5R1QxVbfbKLRORazvZ/20isl5EKkVkp4jcFrJevd9BlYgUi8g7InJpJ/scBVwBPNqd99xXROQqEflH8DJVvV5V/7OPjxt8DveJyE/780tI6N9FT4jI3SLybMji/wLuC1fexAdLcHFERP4P8AvgQeAIYAxwPfAFIDmGofWmK4BS4FIRGdKN7b/sfWgFHp91Mw7xYhkBnAPcJCKXhZSZqaoZwFTgKeBXInJXB/u8CnhDVWtDlvf0PQ9kgXN4BnApcHWM4+k1qvoxMFREZsc6FhOBqtojDh7AMKAauKSTcucBnwAVwF7g7qB184HCkPK7gC95z+cCK71ti4CfestTgGeBYqAMWAGM8dYtA671nh8N/M0rdwh4DhgecqxbgbVAOfAikBK0XoDtwLe9438lJFYFJgPXAY1AA1AFvBr6XkK2y/G2TQTuB5qBOm/bX0V5/n8J/HdoLCFlvuLtNyvCPv4GXB6yLJr3fD2w1Tv3DwPShfP9JdyXoZrguIBZwEHgeC/mZu98lHnrnwLuCyp/AbDa+9vYDpzjLf8msAmoBHYA3+ro7y3c7zPo9e+Ah0P+5n8LfA7sw9WGErx1u4ETveff8PZ1nPf6GuBl77kPuN2Ludg7xsjQvwvvdS7wnvde3vbO9bMhZa8E9njn+w5v3Tm4v8VG7xyuCXoP/wPcFevPD3uEf1gNLn6cAgwB/tRJuWpcjWA4Ltl9W0QujPIYvwB+oapDcR+ev/OWX4n7sJkAZOE+cENrIeA+rB8AxgLHeuXvDinzNdwHQi4wA1erCTgNGA+84B077LU9VX0M92H+E3W1tC9H+f5Q1TuA94GbvG1v6mwbr/l3HrChk6J/wiXRuRHWHw9sDlkWzXs+H5iDO19fA/4lEBqdn29UdT/ui8jXghb/K/CCqq7D/T7/6Z2P4aHbi8hc4BngNtzf1em45AlwwItvKC7Z/UxEZoV/+5GJyDTcOd4WtPgpoAn3peYE4Gwg0LT8Li6Bgqv97fDiCrx+13v+b8CF3rKxuJrywxHCeB74GPc3fjfuHIU6DVdjXwDcKSLHqupfgB8BL3rncGZQ+U1Ay2sRWSsiX49wfNPPLMHFj2zgkKo2BRaIyIciUuZddzodQFWXqeo6VfWr6lpgCe6fOxqNwGQRyVbVKlX9KGh5Fu7bdrOqFqhqRejGqrpNVd9S1XpVPQj8NMyxf6mqn6lqCfAqkB+07krgz6paivuwOUdERkcZe8DL3jkpE5GXu7htJHfj/hee7KiQqjbivtmPjFBkOK52ECya9/xjVS1T1T3A3/HOWZTnO+Bp4HIA7zrXYuB/O3o/Qa4BnvCO5VfVfar6qRfD66q6XZ13gb/iElW0VolINS4RLAN+7cU4BlgI/LuqVqvqAeBnQKCZ+N2g9zoPl+gDr4MT3PW4mlahqtbjfpdfCe0sIiITcV8i7lTVBlX9B/BKmHjvUdVaVV0DrCEoeUVQifu9A6CqM1T1+U62Mf3EElz8KAayg/8xVfVU7xt3Md7vSkROEpG/i8hBESnH/YNnR3mMa4BjgE9FZIWInO8t/1/gTeAFEflMRH4iIkmhG4vIGBF5weswUIFr1gw99v6g5zVAhrdtKvBVXM0MVf0nrimoq992L1TV4d4jqpqriHw/qFPKIyHrbsLViM/zPiA72k8SMAooiVCkFMgMKh/te450zqI53wF/AqaLSC5wFlCu7hpRNCbgmvjaEZFzReQjESkRkTJcUor27w1cU2kG7vrbSUC6t3wSkAR8HvjCguucE0j+7wLzRORIIAFX+/2CiOTgWhtWB+1nadA+NuGaY8eExDEWKFHVmqBle8PEG/Z30YFMXNOyiUOW4OLHP4F63LWQjjyP++Y5QVWHAY/gmrLANV+mBQp63+RHBV6r6lZVXYz7EPkv4CURSVfVRlW9R1WnA6fimqSuCHPsH+GuUxzvNXNeHnTszlyEa+b6tYjsF5H9wDgiNFN6x+muNtuq6o+0tVPK9YHlInI17vrNAlWNprv7BbgmtUiJYy3uC0RAV99zqKjPt6rW4ZLA5bimt+DaW2fnci+uyboNr0PMH4CHcNdkhwNvRIohEq/29zvc3/idQcesB7KDvrAMVdXjvG224RLMvwHveS0K+3HXZ/+hqv6g/ZwbtI/hqpqiqvtCwvgcGCkiaUHLJnTlbURYfiyupmfikCW4OKGqZcA9uA/Dr4hIpoj4RCSf1m+94L4xlqhqnXftJLg2sAVIEZHzvNrGD3DX9QAQkctFZJT34RD41ukXkTNF5HgvIVbgmiz9tJeJu8heLiLjcNdsonUl8ATuOlW+9/gCMFNEjg9Tvgg4qgv779K2IvINXAI5S1V3dFJ2pFf+YeC/VLU4QtE3aNuE2NX3HKqr5/sZ3DXPRbRNcEXAeBGJ1BP3t8A3RWSB9zc3zrtmloz7+zkINIm75+vsKOKO5MfA/yciR6jq57jmzv8rIkO94x4tIsHn713gJlqbI5eFvAb3Be9+EZkE7lYNEWn3JVFVd+M6WN0tIskicgoQ9bVd3DnMEZHQz8wzgD93YT+mH1mCiyOq+hPg/wf+A/cPVYRrtvku8KFX7AbgXhGpxH0b/l3Q9uXe+sdxvdKqgeCayTnABhGpwnU4uUxdl/YjgJdwyW0T7gMk3PWbe3BNTuXA68Afo3lf3ofzAuDnqro/6FEA/IXwNZrf4prcunOt7Re46zClIvLLCGXuw113XBGp+RJY452rbbjOD7eo6p2hOwryDLBQRFK7+Z5Ddel8q+oHuC8mq7wP9IC/4TrQ7BeRQ2G2+xivA4l3rHeBSapaCXwH9zdWivsyFe66VVS8Di/v0Zqor8Al0Y3e/l8Cjgza5F1ckn8vwmtwv+tXgL96/xMf4ZpCw/kGrjNXMe73/yKuFhmN33s/i0VkFYCIzAGqgpuCRWSD92XIxIFAd2RjTC8QkR8BB1T15zE6/t+A51X18VgcfyARkReBT1W1o3sbO9r+D8BvVfWN3o3M9BZLcMYMEl6N4i3c9dnQ3pyHPe/8lAA7cU2tLwOnqOonMQ3M9Jmox10zxsQvEXkadz/YzZbcIjoC18ybhWu6/7Ylt8HNanDGGGMGJetkYowxZlCyBGfMICb9ON2RMfHGEpzpMnHToKwLvidIRO4Tkad6+ThHisgr3ugq6o1i0etE5Osislvc1C4vi8jIkPWXicgmb/12EYlqqCoJmvJHujHfWFdJmCldtJenO+orIaPNVIkbns4vItlBZb4kIqu830OhiHyto30aYwnOdNdYWscN7Ct+3D1jl/R0RyIS9mKziByHu9fwX3HDO9XgjZforT8LN+rLN3H3YJ2OG/i3X/VlYowHIaPNZODO+TJVPQQgItNxo/jcgRuqayZQELOAzYBgCc5010+Ae/ryg1dVi1T117jpe9oRkWEi8lsR+VzceI33Sdcn1PwGbjqe91S1CvghcLGIBMaUvAe4V1U/ChqIOHQYqGgEbk4u82oop3jv4WqvdlgqIm8GRuTw1qmI3CgiW3HT6SAivxCRvSJSISIFgdqkiJwDfB8351yViKzxlrdM/uqNFvIDr7Z6QESeEZFh3rpADfNKEdkjIodE5I6gWOaKyErvuEUi8tNunIOoiEhgrr7gmucPgEdV9c+q2qSqxaoadvxMYwIswZnu+iNu5JOrOisoIhOldQaAcI/uTi/yFJGnW4nWcQSNJeh9aDYAx3jJcjYwSkS2ec1ivxI3iHJXBaZ6Ge7VUv4pbkip7wMX48YMfR83O0SwC3Ejc0z3Xq/ADfk1Elej+b2IpGjHU7oEXOU9zsQNZZYB/CqkTLvpYrzlkaZaaqOXftfzcOOl/iFo2cne/td5X2ieDW1KNiaUJTjTXYqr7fxQIo9x6Aqq7gkZDDf00eXpRaTz6VailYEbnipYOa45cgxuxPuv4D5083GJ9AddjTeC64EHVHWTummSfgTkB9fivPUl3pBqqOqzXu2lSVX/L26syKlRHu8buElud3i11e8Bl4XUwiNNFxNpqqU2eul3fSXwkhdjwHhcM/IlwBQgFfjvKN+3OUxZgjPd5g1RVAh8KwaH73C6FRE5Lbjm4C0Lrkmc5u2nCjfif7ChuHm+ApO+/reqfu5dD/opLrH21nv4RVCMJbiR+scFlWkzpYuI3Oo1aZZ72wwj+ulrxuJmyg7YjRvsIXhqmUjTxUSaaqlXiRvt/6u0bZ4E97t4UlW3eInvR/Te78EMUoP6wrXpF3fgmtVCm9ZaiJtscmMH+/iWqj7XxeMGT7fSFLpS3YSWLRNRiohqmNmscYMQB8/IfBSuVrRFVStFpJC2U6V0d2SEcNvtBe7v5L23bOddb/sPXPPhBlX1i0gprdPXdBbbZ7ikGjAR18RbhKshRQ5CdSuwWFzP2YtxUy1lqWp1cLle+F1fhEv0y0KWr6V3fg/mMGI1ONMjqroMWE8Ho+N7zVYZHTwifuCJSAqtU/4M8V4T5XQr0XgO+LKIzBORdOBe4I9Bw109CfybiIwWkRHALcBrQfGpiMyP4jgHcb1Cg6fxeQT4nteTM9Bp5qsd7CMTl5AOAokicidta5+RpnQJWALcIiK5IpJB6zW7dl8QQkmEqZZCy/Xkd+25EnhG2w+x9CRuSp+jvFre7QT9HowJxxKc6Q0/wHV66Au1uGZEgE9pbTaEzqdb6ZSqbsBdC3sOOIBLIjcEFflPXMeOLbiphD4B7gcQkQm4psx1URynxtvuA69J8mRVXYrrDv+CuBm71wPndrCbN3G3TWzBNS/W0bYJs92ULiGewE2D9B5uwOE63ISi0Yg01VKvETfF0Bdx0w61oapPeMuX4957PW4qH2MisrEojekmEbkcOE5VvxfrWIwx7VmCM8YYMyhZE6UxxphByRKcMcaYQckSnDHGmEHJEpwxxphByRKcMcaYQckSnDHGmEHJEpwxxphByRKcMcaYQckSnDHGmEHJEpwxxphBaVBNl5Odna05OTmxDsMYY0wfKigoOKSqozorN6gSXE5ODitXrox1GMYYY/qQiOzuvJQ1URpjjBmkLMEZY4wZlCzB9VDB7lJm3/c2ByrqYh2KMcaYIJbgeujdLQc5VFXPRztLIpYprW7g+0vXUVHX2OG+6hqbezs8Y4w5bFmC66GNn5UDsHZvWcQyb27Yz/PL9/DOpqKIZbYfrGLG3X9l+Y7iiGWq6pv4xuMfsaaDYxljjHEswfXQhs8qAFhTGDnpFOwuBWDFrtKIZd7bcpCGZj9/+/RAxDL/3F7MB9uK+dXft3UzWmOMOXxYguuBkuoGPi+vIzUpgXX7ymlq9octt2qPS2wrd0VuxvzYa+LsqKnzI692986mIvaV1XY3bGOMOSxYguuBDV7z5JdnHkldo58tRVXtypTVNLD9YDUj0pLYUlRFWU1DuzKqysc7SxCB9fvKqapvCnu8j3YUc/SodBR4fnlUt4EYY8xhyxJcDwSaJxfPnQiEb6b8xLtedsUpOUBrbS7YjkPVFFc3sPD4I2n2a0uTZrDymkY2fl7BopnjWDBtNC+u2Et9k3VKMcaYSCzB9cCGzyoYOyyF/AnDGZaaFLbzx6rdpfgErjhlEkkJEvY6XKB58ttnHE2iT8J2NPl4VwmqcPJRI/nXU3I4VNXAX9bv7/03ZYwxg4QluB7Y+Fk508cOQ0SYOWE4awrL25VZtaeUY48cSlbGEPLGDQt7He7jnSVkZwzhuLFDOX78MJaHuQ730Y5ihiT6mDlhOPMmZ5OTlcb//tOaKY0xJhJLcN1U09DEjkPVHDd2KAD544expaiSmobW62fNfmX1njJmTRwBwJyckazZW97ufrePd5ZwUu5IRISTcrNYs7eszX7AJbhZE0eQkpSAzydcfvIkVu4uZaPXTGqMMaYtS3DdtOnzSlRpSXAzxg+n2a8t1+UANu+vpLqhmVmThgMwe9IIGpr9rN/XWtMrLK1hX1ktc3NHAnDSUSNp8iurdrc2dwauv518VFbLsq+eOIGUJB//+5HV4owxJhxLcN0UuMH7uHHDAJgxwf0Mvg4X6FBy4kSXvE6c5GpywdfhAtff5uS4MrMnjcAnsHxn63W44OtvAcPSklg0cywvf7KvZYSUhiY/r6/9nBufX8V/v7OV/eXdGz6sqdnP2xuLuOn5Vdz76ka2FlV2az/GGBNLMZkuR0SeAM4HDqhqXpj1AvwCWAjUAFep6qr+jbJjGz6rYHhaEmOHpQAwOjOFccNTWR2S4LIzkpkwMhWArIwhHDUqnYLdJcDRAKzYVcLQlESmHpEJQGZKEnnjhrF8R+t1uODrb8GuOCWH360s5OG/b0MV/lBQSHF1AyPTk3l97ef87O0tnDl1NJfNnciZU0eRmNDx95m9JTX8buVefrdyL0UV9WSlJ1NR18gTH+xkTs4Ivn7SRM7NO5KUpIQenz9jjOlrsZoP7ingV8AzEdafC0zxHicBv/F+xo0Nn1Vw3NihuFzszJwwjLVBHU0+2VPGCRNHtCkzZ9JI3ty4H79f8fmE5TtLmJMzkgRfa5mTckfy9Ie7qWtsJiUpoc31t2B544aRP2E4j767g0Sf8KVjx3DZ3AnMmzKKvSU1vLhyL79fWcg7n65kzNAhfPXECVw6ZwITRqa17KOx2c87m4p4/uO9vL/1IABnHDOKexZNZMGxo6mobeSlgkKWfLyHW15cw92vbOSSWeP5+kkTmTw6o0vnTFX5aEcJz360m3e3HOT0Y7L515NzOPmokW3OUX9RVeqb/FTXN1HT0ExigpCWnEh6ckLLl4HQMtUN7mdNvXte3+QnNSmB9CEJpCcnkj4kAb9CdX0T1V6ZusZmfCIkJQiJPh+JCYJPBL8qCqDgPUNEEO+nT0AQRHAPxJVT8AdvE1JGBHziLcMt9yuouvej3vsXwOeTljIdHTuwP59EVz6wLFCeNtsGlfd5cYi0xhz6Przjud8HKG3fR6QyflU0UCgQc2/+/YS+Dl3Qo3233Vlv7rvTY0dxrND4IvGJxPQLsWh/nrngA4vkAK9FqME9CixT1SXe683AfFX9vKN9zp49W3s64en8+fM7LaPiY/ecmxm6fxUj97zbsrz8yDmUTprPhJW/AmDv7JsYsftdhn3+cUuZylHHUXz0QsaueYKEplr2nngjI3YvY9jnK1rK1Aw/mgPTLmbMxhdIrj7A3tn/xvDCDxi+75/tYmlIG0Xd0AmkFW8msbE6bKw1w4+iavQMaofngvhIKdtFxqENNKZmUTnqePzJ6STUV5BxcD2ZB9aS2NC+SVKBuqETqByTT82IKeBLYEjFXjKL1pBesgXRyPfk+ROSqco+jsox+TSmZeNrrCW1fCe1w3LxJ6WSVHOQzKLVpJbvxu9LQhOS8Ccko74k/AlJqK/1OYD4m/D5G5HmRsTfCJIQUt7bR/BPX+s+g9chEWq1/ibE39xxGWNMh9JKtjB6y59aXi9btqxX9isiBao6u7Ny8Tqj9zhgb9DrQm9ZuwQnItcB1wFMnDixX4JrTM0CXyLJ1W3HjUyudvel1WccQeDy5pCqfW3KpFS41/WZ4/A1uWtkQyoL25QZUlkIqtRnTkATkkGElIq9hJNcc5DkmoMRYxX1k166jfTSbTQlZ1I1Ko/K0cdzaPJ5oH5SS7eTeWAtqWU7kQ6+lQmQWrGX1Iq9NCemUTU6j8rRMzk05XxKGmtccixaQ1J9axNtfdpoKsfkU519LJqQTHLV52Rve4O04s34tAm/JFKdPY3KMSdQkntWxGO3odr6VT1SrM2NiL8Bn5cApbkRn7+RxPrylsTofjaEJEqfl/iS8fuSwJfQbl++5oaQ7ZrQhNYE6k9IRlS9bRpajqXiA/Gh4kMl+ButIiHVDA3UM1rep/up3mtXPvR3FVwmePvAPrxqDYFtg/bt7VfbbBOo7oSJpyXG9nFqoLrWss4XtF/abKPt4gzZPuh5YHnre9egt9FJmUDBlj8bCXP+QkVTJlyRvqwwtN93ZzVS7W6ZqN5G54US6yKPv9sf4jXBRU1VHwMeA1eD6+n+ovmG8VJBIbf+fg0v/OYnTB6d2bK8qr6J4+9+k6/f9D0amvw89t4Olv3xmTZVdFVlzv3vcPolVzM0NYkXV+zlvZefIzmxbS3hvF++z9CjFzF97FCe/Wg3y/74dK9V9Zv9yid7Shk/Io0jhn252/vx+5UPth/i+eV7+OvGdCrGzmXelGxOnzKK19d9zuq9ZaQk+fjqzLF846RJ7a4hBqgqawvL2XGoymsiTCRtSAJpya7ZLzXZPU9JTEAE6pv8rpnQa/5LTkhoKZPq3UZhjDHxmuD2AROCXo/3lsWFDZ+Vk5qUQG5222tQGUMSmTI6w7uPrZnjxg5tl5REhDk5I1ixu4TMIUnMmjS8XXIDOCk3i+eW76a4uj7s9beeSPAJs3NGdl6wEz6fMG/KKOZNGUVRRR2/W7GXJR/v4f2thzh6VDp3nj+dS2aNZ1haUof7CdwoHykBhkpJSiAlKYGR6ck9fg/GmMErXhPcK8BNIvICrnNJeWfX3/rThs8qmHZkZpuOIQEzxw/n7U1F1DX6uXTOhDBbw+yckfx5/X6gllu+dEzYMicdNZInPtjJlqIqbvnS2N4Mv0+MGZrCvy2Ywg1nTmZvSQ2TstJi0nHEGGMCYnL1XESWAP8EpopIoYhcIyLXi8j1XpE3gB3ANuB/gBtiEWc4fr+yyetBGc7MCcMprWmktrGZWd59b6Hm5LQun5MbvszcoBpW8P1v8S7BJ+Rkp1tyM8bEXExqcKq6uJP1CtzYT+F0yd7SGirrmzhu7LCw6/ODmtlOjJDgph85lLTkBBqb/ZwwIXyZEenJTDsik52HqqNuujPGGNMqXpso41ZgKK5INbipR2SSnOhjRNBN4KESE3ycenQ29U3NpCZHvrZ23elHUVhaazdWG2NMN1iC66INn5WT4BOOGZMZdn1Sgo8zp45idGZKh810/734hE5vlrx41vgexWqMMYczS3BdtOGzCqaMzuiwVvXov3Z6/2GHNTdjjDE9Z0M0dNGmzyuYfmT45kljjDHxwxJcF5TXNlJUUc8xR4RvnjTGGBM/LMF1wbYDVQBM6eIgw8YYY/qfJbgu2HbADUI8ZbTV4IwxJt5ZguuCrUVVpCT5GDciNdahGGOM6YQluC7YeqCKo0dlhB2iyxhjTHyxBNcF2w5U2fU3Y4wZICzBRamqvol9ZbVMiXCDtzHGmPhiCS5K270elJOtBmeMMQOCJbgobbVbBIwxZkCxBBelrQcqSU7wMXFkWqxDMcYYEwVLcFHaVlTFUaPSSUywU2aMMQOBfVpHaeuBKrv+ZowxA4gluCjUNjSzt7TGRjAxxpgBxBJcFLYfrEIVpoyxGpwxxgwUluCiYIMsG2PMwGMJLgpbD1SS6BMmZaXHOhRjjDFRsgQXha1FVeRkp5OcaKfLGGMGCvvEjoKNQWmMMQOPJbhO1Dc1s6u42hKcMcYMMInd2UhE1gFrgx7rgCtV9f5ejC0u7DxUjV9hsg2ybIwxA0p3a3BnAP8D1AKXAeuBhb0VVDzZWmQ9KI0xZiDqVg1OVUuAZd4DEZkC/KDXooojWw9U4RPIzbYelMYYM5B0qwYnIscEv1bVrcCMXokozmw7UMmkrHRSkhJiHYoxxpgu6FYNDnhURI4G9uGuwaUA60UkTVVrei26OLC1yMagNMaYgahbNThVPVNVJwKXAq8B24BUYLWIfNrZ9iJyjohsFpFtInJ7mPVXichBEVntPa7tTpw91djsZ+ch60FpjDEDUXdrcACo6h5gD/BqYJmIdJgNRCQBeBg4CygEVojIK6q6MaToi6p6U0/i66ndxdU0+dXGoDTGANDY2EhhYSF1dXWxDuWwkJKSwvjx40lKSurW9j1KcOGoalUnReYC21R1B4CIvABcAIQmuJhr7UFptwgYY6CwsJDMzExycnIQkViHM6ipKsXFxRQWFpKbm9utfcTiRu9xwN6g14XeslCXiMhaEXlJRCZE2pmIXCciK0Vk5cGDB3s10J3F1YD1oDTGOHV1dWRlZVly6wciQlZWVo9qy/E6ksmrQI6qzgDeAp6OVFBVH1PV2ao6e9SoUb0axL7SWkakJZE+pNcrusaYAcqSW//p6bmORYLbBwTXyMZ7y1qoarGq1nsvHwdO7KfY2thXVsu4EamxOLQxxpgeikWCWwFMEZFcEUnGjYTySnABETky6OUiYFM/xteisLSWccMtwRljzEDU7wlOVZuAm4A3cYnrd6q6QUTuFZFFXrHviMgGEVkDfAe4KgZxsq+0lvEj0vr70MYY0yER4fLLL2953dTUxKhRozj//POj3sfdd9/NQw891Gm5jIzu9yJPSEggPz+/5bFr1y4ATj31VADKysr49a9/3e39dyYmF5dU9Q3gjZBldwY9/x7wvf6OK1hpTSO1jc1WgzPGxJ309HTWr19PbW0tqampvPXWW4wbF66vXmylpqayevXqdss//PBDoDXB3XDDDX1y/HjtZBJzhaVuQBa7BmeMiUcLFy7k9ddfB2DJkiUsXry4Zd1Pf/pT8vLyyMvL4+c//3nL8vs9Z8vfAAAgAElEQVTvv59jjjmG0047jc2bN7fZ37PPPsvcuXPJz8/nW9/6Fs3NzR0ef/78+Xz6qRvXo7i4mLy8vKhjD9QKb7/9drZv305+fj633XZb1NtHy7oHRrCvtBbAanDGmLDueXUDGz+r6NV9Th87lLu+fFxUZS+77DLuvfdezj//fNauXcvVV1/N+++/T0FBAU8++STLly9HVTnppJM444wz8Pv9vPDCC6xevZqmpiZmzZrFiSe6/nubNm3ixRdf5IMPPiApKYkbbriB5557jiuuuCLi8bdt28Yxx7hhideuXcvxxx/frkxtbS35+fkA5ObmsnTp0jbrf/zjH7N+/fqwtbzeYAkugn1lLsFNsGtwxpg4NGPGDHbt2sWSJUtYuLB1trJ//OMfXHTRRaSnu/t3L774Yt5//338fj8XXXQRaWnuM23RokUt27zzzjsUFBQwZ84cwCWm0aNHRzz27t27GTduHD6fawRcu3YtM2a0H28/UhNlf7EEF0FhaS0ZQxIZmmqnyBjTXrQ1rb60aNEibr31VpYtW0ZxcXG396OqXHnllTzwwANRlV+zZk2bhFZQUMCll17a7eP3FbsGF0HgFgG7qdMYE6+uvvpq7rrrrjbNg/PmzePll1+mpqaG6upqli5dyrx58zj99NN5+eWXqa2tpbKykldfbRlCmAULFvDSSy9x4MABAEpKSti9e3fE465evbplhJGtW7fypz/9KWwTZWcyMzOprKzs8nbRsupJBPvKahlvHUyMMXFs/PjxfOc732mzbNasWVx11VXMnTsXgGuvvZYTTjgBgEsvvZSZM2cyevToluZIgOnTp3Pfffdx9tln4/f7SUpK4uGHH2bSpElhj7tmzRpSUlKYOXMmM2bMYPr06Tz99NP88Ic/7FL8WVlZfOELXyAvL49zzz2XBx98sEvbd0ZUtVd3GEuzZ8/WlStX9sq+Ztz9JheeMI57L4i+Z5AxZnDbtGkTxx57bKzDiLkpU6awatUqMjP7fiD6cOdcRApUdXZn21oTZRgVdY1U1DVZD0pjjAlRWVmJiPRLcuspS3BhtNwiYE2UxhjTRmZmJlu2bIl1GFGxBBdGIMHZMF3GGDNwWYILI3APnDVRGmPMwGUJLozC0hqGJPrIzkiOdSjGGGO6yRJcGIF54OweOGOMGbgswYWxz+aBM8aYAc8SXBh2k7cxxgx8luBC1DY0c6iqwWpwxhgzwFmCCxHoQWm3CBhjzMBmCS5Eyy0C1kRpjIlTPZls9HBigy2HaJnJ25oojTGdmD9/fq/ub9myZVGVi2ay0WiUlpYyYsSIbm07EFgNLsS+0loSfcKYoSmxDsUYY9qJNNnok08+yfXXX09ubi7XX389jz76aMs2kQbVv+WWWwA348BgZDW4EPvKajlyeAoJPrsHzhjTsWhrXL0p0mSj5513HhdccAGNjY088sgj7N+/n1NOOYULL7yQU089leXLl3Prrbdy44038uCDD/Lee+/x6aefcs8997Bt2zbuuOMONm7cyNKlS/v9PfUVq8GFsHvgjDHxrKPJRgsKCjjxxBNbyi1evJjvfve77Ny5k5kzZwJQVVVFWloa2dnZXH755SxYsIBLLrmE+++/n/T09Ni8qT5iCS6Em8nbelAaY+LTmjVr8Pv9zJw5k3vvvbdlslFon+DOOussANatW8eMGTOoqKhoGaFp7dq1zJw5kxUrVrBgwQIAEhISYvCO+o41UQZpaPJTVFlnN3kbY+LW2rVrI042umbNGm6++WbA1e6mTp0KwLRp03jooYdITExk2rRpAGRnZ/P444/z2WefcfPNN3Po0CFGjRrVf2+kH1iCC7K/vA5Vu0XAGBOfOptsdMmSJS3Pf/vb37Y8v+aaa9qVXbRoEYsWLWp5nZ2dzUMPPdSL0caeNVEGCdwiMN6uwRlj4tBAmmw0HliCC1JoN3kbY8ygYQkuyL7SWkTgyGGW4IwxZqCLSYITkXNEZLOIbBOR28OsHyIiL3rrl4tITn/Eta+sljGZKSQnWt43xpiBrt8/yUUkAXgYOBeYDiwWkekhxa4BSlV1MvAz4L/6I7bC0hprnjTGdCjSqCCm9/X0XMeiqjIX2KaqO1S1AXgBuCCkzAXA097zl4AF0g/Ta+8rs5u8jTGRpaSkUFxcbEmuH6gqxcXFpKR0f9jEWNwmMA7YG/S6EDgpUhlVbRKRciALOBS6MxG5DrgOYOLEid0OSlVpblYmjLQEZ4wJb/z48RQWFnLw4MFYh3JYSElJYfz48d3efsDfB6eqjwGPAcyePbvbX6tEhA+/t8C+mRljIkpKSiI3NzfWYZgoxaKJch8wIej1eG9Z2DIikggMA4r7I7h+aAk1xhjTD2KR4FYAU0QkV0SSgcuAV0LKvAJc6T3/CvA3taqVMcaYLuj3JkrvmtpNwJtAAvCEqm4QkXuBlar6CvBb4H9FZBtQgkuCxhhjTNRkMFWMROQgsLuHu8kmTGeWODcQY4aBGbfF3H8GYtwWc/+YpKqdjgw9qBJcbxCRlao6O9ZxdMVAjBkGZtwWc/8ZiHFbzPHFhuwwxhgzKFmCM8YYMyhZgmvvsVgH0A0DMWYYmHFbzP1nIMZtMccRuwZnjDFmULIanDHGmEHJEpwxxphByRKcMcaYQckSnDHGmEHJEpwxxphByRKcMcaYQckSnDHGmEHJEpwxxphByRKcMcaYQanf54PrS9nZ2ZqTkxPrMIwxxvShgoKCQ9FMlzOoElxOTg4rV66MdRjGGGP6kIhENe+nNVEaY4wZlCzBGWOMGZQswXXVJ7fBzmdjHYUxxphODKprcH1OFbb+BkbNg9zLYx2NMaafNTY2UlhYSF1dXaxDOSykpKQwfvx4kpKSurW9JbiuaCiFpmqo3BbrSIwxMVBYWEhmZiY5OTmISKzDGdRUleLiYgoLC8nNze3WPqyJsiuqvY471bvA3xjTUIwx/a+uro6srCxLbv1ARMjKyupRbdkSXFcEEpw2QfWe2MZijIkJS279p6fn2hJcV1QH3XphzZTGGBPXLMF1Rc0ewPtGUWUJzhhj4pkluK6o3g1Dj4GEVKjcHutojDHGdMASXFdU74b0HMicbDU4Y0zMiAiXX956q1JTUxOjRo3i/PPPj3ofd999Nw899FCn5TIyMroVI0BCQgL5+fktj127dgFw6qmnAlBWVsavf/3rbu+/M3abQFdU74YRJ0BCGlR8GutojDGHqfT0dNavX09tbS2pqam89dZbjBs3LtZhtZOamsrq1avbLf/www+B1gR3ww039Mnx+6wGJyJPiMgBEVkfYf1tIrLae6wXkWYRGemt2yUi67x18TF6clMt1B+E9EleDW47+JtjHZUx5jC1cOFCXn/9dQCWLFnC4sWLW9b99Kc/JS8vj7y8PH7+85+3LL///vs55phjOO2009i8eXOb/T377LPMnTuX/Px8vvWtb9Hc3PHn25o1azj99NOZPn06Pp8PEeHOO++MKvZArfD2229n+/bt5Ofnc9ttt0W1bVf0ZQ3uKeBXwDPhVqrqg8CDACLyZeAWVS0JKnKmqh7qw/i6psa7LSB9krvZ298AtfsgfWJs4zLGxEbBv0Np+9pJj4zIhxN/3nk54LLLLuPee+/l/PPPZ+3atVx99dW8//77FBQU8OSTT7J8+XJUlZNOOokzzjgDv9/PCy+8wOrVq2lqamLWrFmceOKJAGzatIkXX3yRDz74gKSkJG644Qaee+45rrjiirDHrqur49JLL+WZZ55h7ty5/PCHP6Suro577rmnTbna2lry8/MByM3NZenSpW3W//jHP2b9+vVha3m9oc8SnKq+JyI5URZfDCzpq1h6ReAWgfRJLrmBu1XAEpwxJgZmzJjBrl27WLJkCQsXLmxZ/o9//IOLLrqI9PR0AC6++GLef/99/H4/F110EWlpaQAsWrSoZZt33nmHgoIC5syZA7jENHr06IjHfvvtt5k1axZz585tieUvf/lLu/vWIjVR9peYX4MTkTTgHOCmoMUK/FVEFHhUVR/rYPvrgOsAJk7sw2TTkuCCjlG1Hfhi3x3TGBO/oqxp9aVFixZx6623smzZMoqLi7u9H1Xlyiuv5IEHHoiq/Pr16zn++ONbXq9atYpZs2Z1+/h9JR56UX4Z+CCkefI0VZ0FnAvcKCKnR9pYVR9T1dmqOnvUqE4neO2+6t0gCZA6DlLHgy/ZbvY2xsTU1VdfzV133dUm2cybN4+XX36ZmpoaqqurWbp0KfPmzeP000/n5Zdfpra2lsrKSl599dWWbRYsWMBLL73EgQMHACgpKWH37shzimZlZbF27VoAtmzZwh//+Ecuu+yyLsefmZlJZWVll7eLVsxrcMBlhDRPquo+7+cBEVkKzAXei0Fsrar3uOTm805ZxlF2q4AxJqbGjx/Pd77znTbLZs2axVVXXdXSfHjttddywgknAHDppZcyc+ZMRo8e3dIcCTB9+nTuu+8+zj77bPx+P0lJSTz88MNMmjQp7HEXL17MK6+8Ql5eHtnZ2SxZsoSsrKwux5+VlcUXvvAF8vLyOPfcc3nwwQe7vI+OiKr26g7b7Nxdg3tNVfMirB8G7AQmqGq1tywd8Klqpff8LeBeVf1LZ8ebPXu2rlzZR50u3z7DTZdzlpdnl33ZdTxZuKZvjmeMiTubNm3i2GOPjXUYh5Vw51xEClR1dmfb9lkNTkSWAPOBbBEpBO4CkgBU9RGv2EXAXwPJzTMGWOpdrEwEno8mufW56t0w6rTW15mToehvLunZ4KvGGBN3+rIX5eIoyjyFu50geNkOYGbfRNVN/iaoKXQ9KAMyjobmGqjbD6lHxi42Y4wxYcVDJ5P4V/s5aHPbBJc52f20MSmNMSYuWYKLRuAWgbQwCc46mhhjTFyyBBeNcPfApU9ytw3YrQLGGBOXLMFFoyZMgvMluZkFLMEZY0xcsgQXjeo9MCQbEtPbLrdpc4wxJm5ZgotG9e62HUwCMo52Nbg+vJfQGGNM91iCi0b1bkgLM85l5mRoLIeGkvbrjDGmj/RkqprDSTwM1RXfVF2CO/Jf2q/LCNwqsA2GdH2YGmPMwDZ//vxe3d+yZcs6LRPtVDWdKS0tZcSIEd2MdGCwGlxnGkrcDd3hmigzgxKcMcb0g3BT1ZSUlPDUU09x/fXXk5uby/XXX8+jjz7ask24IRlvueWWlufXXntt3wceA1aD60zwPHChMnIBsY4mxhymoqlx9bZIU9V885vf5IILLqCxsZFHHnmE/fv3c8opp3DhhRdy6qmnsnz5cm699VZuvPFGzjvvPD799FMefPBBbrzxRrZt28Ydd9zBxo0b201KOpBZDa4z4e6BC0hIgbQJVoMzxvSbjqaqKSgoaJmle/Xq1SxevJjvfve77Ny5k5kz3QiIVVVVjB49mssvv5zbbruNVatWcckll3D//fe3TJI6WFiC60y4UUyCZU62BGeM6TeLFy+mqqqKvLw8rrvuujZT1YQmuLPOOguAdevWMWPGDCoqKhAR1q5d25LwVqxYwYIFCwBISEiIwTvqO9ZE2ZnqPZCQFrkTScbRUPhy/8ZkjDlsZWRktJmsNNiaNWu4+eabAdi6dStTp04FYNq0aTz00EMkJiYybdo0srOzefzxx8nOzmbjxo3cfPPNHDp0iD6dNDoG+nQ+uP7WJ/PBvX8JlG+C8zeGX7/xJ7D6u/CVMkge1rvHNsbEFZsPrv/1ZD44a6LsTPXu8NffAlp6Um7tn3iMMcZExRJcZyKNYhIwdJr7WbE5cpnGKlhzBzTV9G5sxhhjIuqzBCciT4jIARFZH2H9fBEpF5HV3uPOoHXniMhmEdkmIrf3VYydaqqB+kMdJ7iMo0F8UNlBgtv3Gmz4Eex/K3KZ+mL429lQtTNyGVXY8Qw0lHcee1c1VkDZuo6HHWuud4m8p83azXVQ9K77aYwxfaQva3BPAed0UuZ9Vc33HvcCiEgC8DBwLjAdWCwi0/swzsiq97ifkXpQAiQMgfRcqPg0cpkK7/pdWdhc7xx41yXAz96IXKZsHXx0Jazr2ogFHWpugM2/hFeOhjdmwDtnwqHlbcv4m11ifW0qvDYN3vkilBR0/ViqsOf38Np0eGc+vHYs7HnJxvI0A8pg6rcQ73p6rvsswanqe0B3BmmcC2xT1R2q2gC8AFzQq8FFq6N74IINndZxE2W5l+DKN0QuU7bO+7k2cpnS1e7n9v+BhrKOY+qM+mH3i/D6sVBwMwyfAfk/hopN8NeT4f2vQsVWV/v8c75LrMlZcPw9UL4e/jIbPvgGVO2K7njFK+Ht0+EfX4OkDJjzCCRlwj++6pJdySch8amr4am/Z+8zGqrQWOlqqJ2Vaaq1hHwYS0lJobi42JJcP1BViouLSUlJ6fY+Yn2bwCkisgb4DLhVVTcA44C9QWUKgZP6K6DgseXOm/Y5t50Ol159G0VVkU/yt0/ezoXTP+Oc+WegSLv1T35lBbkjYfuqV7jmjvntdwDc86UNnHEUbPhgCTf+R/hk+e2Tt/OVPEhoquKR22bxwppOEm8EJ4wt5VtzdzJtdCXbitN5bPnxfFzYBPyZ1KQpXDojlUsb/0jq3pcAKCxP5fEVx/LujgyUv5GeNJXF+al8tekFZMcS/rB+HM99MomqhvZ/TqPS67h2zi7+5ZgiSmqS+O3KY/jz5hH4dQkJMoKF06Zwzex/MrRoFtuKM0hNbCZzSBMZQ5pI9Cl+hZqGBKoaEqlpTKTae15Vn0h1QyI1jQkMSfSTntxERnIT6cluu6oGt766IYHqhkSSE1rLZAxpIj252T33tknwvurVN/la9t/Q7CMtsE1QmYZmoao+saVcZX0ilQ1J7nlDIgmiLcfJHNJEelITPgERRQR8uJ8iIGjLOlXBr6AIqi6P+lueC35AAJ+3H8FtH1pGFfzq/g79Ki2vg/fnV0GDyrZbF24fYdZrm2MExyCtr733FPq6s2Mr7WMRUXyBn0JUZQIC7zeUtP+XDfNf7CSlDGX6/G+TPnIiEm7DvtKbh+osN/fisZr9Qn1Taz3qiCOO6NL2KSkpjB8/vtvHj2WCWwVMUtUqEVkIvAxM6epOROQ64DqAiRO794EfycjUBgCKa5I7LLe3LJUhiX7GZNSzPyQRJoifCcNrafILE4fXkCB+mrV9xTl3ZDUAR42sRtCwiXLyyCq2HsqkqiGBS/L28dK68TT5o6+ET86q4rq5O5g7oZSiqiE88PepvLVtTMsHA0BtYyJPFeTwysaxXHTcPoqqhvDnzUe0ibm6MZHHVxzFnzaO5ZrZu7h0RiELp+7n6YJJvLJpLE1+H6mJzSzO38OlMwoRlOdXT+DZTyZS09j6J9eswqubxvL37aNZPHMPk7Oq2NuQSmV9ElUNCdQ0JpKS2Ex6cjPpyS5RpCc3MzK1gYnDakhPbiYtuYmGQFLyklpjs4+RqQ1MGFZLRnKTK9Psa0mKVQ2JHKpOZldpWkuiqm5IJNHnJ2NIExle8huS2Ex1aVpLIqsKJC8vAWcmN5E5pJFhKU2MG1ZH5pBGMpKb8KtQGZQAqxoSaVb3aRz4IA+8/+APZmhNXsFJMPDcJ9ryYR74YIf2ZYL3E0iICT51ywCfT1uWu7LB5bXta2kt6/PWB47jC1kv0j7+1uVR/5nGv+proTrWQQwM7+3M5p63jmt53d9Dm/XpfXAikgO8pqp5UZTdBczGJbm7VfVfvOXfA1DVBzrbR6/fB7fiJtj9PHylk5bWA+/B22fA/L/A2JBZB8o3wevT3WwEn78J522AYSGXFJtq4PeZkDoOavbCl7e23n4QoAp/HA3jL4AJX4Vl58DJT8FRV3b+Pqp2wtofwq7nIHkkHPd9OOZGN9RYbyj5BD65FYr+5mZYyL0ctj4Cdfth0mUw8wHIyOmdY8W7wP9Tf367HwhUAXVNztoc9NzvPQ9Z1q6M91MSXKcufK3nuNMy4u0r8FmntKumhP19hS4LV9WLokzbExFmkfbi8cPsu7PjhzsfvXIsICEZkoZ2vF03RHsfXMxqcCJyBFCkqioic3HXA4uBMmCKiOQC+4DLgK/HJMi6IkgZ03m5TDdagOtJGZrgvOtvE7/mElzZ+vYJrnyj+8fM+QZs/LG7Dhea4Go/dz06h8+EI8+GYXmw6SHIvSLyH17dQVh/H2z7DUgiTP8eTP8PSB7e+XvqipEnwBffhs/+DKtvg3V3Q/YpcPpSyD65d48V7yyxhRdINOIj9ldGzOGiL28TWAL8E5gqIoUico2IXC8i13tFvgKs967B/RK4TJ0m4CbgTWAT8Dvv2lz/qyuClNGdl0sZDUnDw/ekLN8ICIy/0P1zl4fpSRnoYDLpMlemNExHk7I17ueIme7D4thb3b4+/2v7so1VsO4/Xc/IrQ9D7lWuVpj/o95PbgEiMG4hnLsGzv8Uzvrg8Etuxpi40ulXKRG5W1Xv7uqOVXVxJ+t/Bfwqwro3gA76y/eTuiKXUDojAkOnhu9JWbER0nNgyEjInBK+J2XZOtdcOCwPMo8J35MysGz4DPdz0mJY83349KHWZlF/I2x/3N1GUFcEEy6GGffDsGlRvd1e4Ut058IYY2IsmraCO0UkFRiJ6xjygqqW9m1YcSLaJkpwH+r7326/vHxja5PksLzW2lqbMutg6HTwJbgEVhLmOmLpGkib2FoDS0iGqTe7cTBLPnFz0q3+vvs5ah7MWwqjTokudmOMGYSiaaJUoA7XZDgB+FBEoqjWDHDNddBY3oUENw1qP3P3SgX4m1ytriXBHecSUFNt223L1sEIr2Y2fAZU7Wi7H3BNlKG1ycnXQWIGvD3P3V+WkAJnvAZfeteSmzHmsBdNgvtUVe9S1ZdU9fu4m65/1sdxxV7dAfezKzU4gMotrcuqdoK/vjXBDc9znUmCr9XVHXA1xWHeDL2BJsjgUU+a61yiHB6S4JKHw/TvQsqRrkfluath3HnW0cEYY4guwR0SkRMDL1R1CzC4Jg0Kp8sJzrvOVR6UvAJDdA0NaqKEth1NAk2Ww70EF6jJBV+HK9/guk2Hux6Y9wNYtNXdLuAbXJMVGmNMT0RzDe47wAsiUgCsA2YAHYwIPEjUFbmf0Sa4cIMuB24RGObNZZQ5GXzJbWtnoQkubSIkDWvtNQnu+hu0r8EZY4yJqNManKquAfKBJd6ivwMd9pAcFLqa4MINuly+EdImuDEXAXxJrqYX3JOybB0MyW49johrpgyuwZWtdbOKZx7d/fdjjDGHmajuuFTVeuB173F4aElwUdwHFxA66HJwD8qAYXlw6IPW12XrXEILvm42fAbsfKZ1hIPSNa6GJzZ9nzHGRMs+MSOpK3I9FBPTot9m6FTXySQwdFDFptbrbwHDj3OzFDRWuGloyje0Nk8GjJgBTZWunGr4HpTGGGM6ZGPmRNKVe+AChk51PR6r9wAKzbXha3DganfJWdBc0z7BDQ/qaCIJ0FBq19+MMaaLLMFF0q0E5/WkrNjsDRZL+wQ33EtwZevd6CbQeotAwLA8wGuaDDRLWg3OGGO6xBJcJHVFrYMoRytQvuJT0Eb3PNCDMiA9x3UYKd/gjUoirtkyWFKG65VZtrY1wQVqdcYYY6JiCS6SuiIYdXrXtgkMuly52TVVphwBySPalhGfG9GkfL27HSDjaEhMb7+vETNaE1zGUa09MY0xxkTFElw4/kaoL+56E2XwoMtN1e2bJwOG57mpZZKGtr/+1lJmBuxd6hLlyBPDlzHGGBOR9aIMp/6Q+5naxQQHXoLbFP4WgYBhx7nJQCu3dJzgUKjZY82TxhjTDZbgwunqTd7Bhk5zk5M2VXaQ4IImOO8wwXmsg4kxxnSZJbhwanuS4II6poTeAxcwPIoEl5Hr7sMDS3DGGNMNluDCCdTghnRhFJOAoUGTi0aqwaWOdZ1RElIgY3L4MuJzyS8x0/W8NMYY0yV91slERJ4AzgcOqGpemPXfAL4LCFAJfNsb9xIR2eUtawaaVHV2X8UZViDBdecaXGDQ5eSRkBJh0gURGHmCmxeuoxkAJl8P1btsiC5jjOmGvuxF+RTwK+CZCOt3AmeoaqmInAs8BpwUtP5MVT3Uh/FFVlfkaleJ3eianzAE0o+CtLEdlzv5acDfcZmjruj68Y0xxgB9mOBU9T0Ryelg/YdBLz8CxvdVLF0WGMWkuxOHnvRY6/WzSNIndG/fxhhjohIvbV/XAH8Oeq3AX0WkQESu62hDEblORFaKyMqDBw/2TjTdGaYr2JgzIWtO78RijDGmW2J+o7eInIlLcKcFLT5NVfeJyGjgLRH5VFXfC7e9qj6Ga95k9uzZ2itB1RVB+qRe2ZUxxpjYiGkNTkRmAI8DF6hqcWC5qu7zfh4AlgJz+zWwugM9q8EZY4yJuZglOBGZCPwR+FdV3RK0PF1EMgPPgbOB9f0WmPqh/qAlOGOMGeD68jaBJcB8IFtECoG7gCQAVX0EuBPIAn4trjNH4HaAMcBSb1ki8Lyq/qWv4mynvthNdWMJzhhjBrS+7EW5uJP11wLXhlm+A4jd0B0tw3R14yZvY4wxcSNeelHGj56MQ2mMMSZuWIILZQnOGGMGBUtwoSzBGWPMoGAJLlRdEfiS2s/EbYwxZkCxBBeqrsjNItDdYbqMMcbEBUtwoWp7OEyXMcaYuGAJLlS9jWJijDGDgSW4UHVF3ZsHzhhjTFyxBBdM1Y1D2Z2ZvI0xxsQVS3DBGsvA32BNlMYYMwhYggtWa/fAGWPMYGEJLljgJm+7BmeMMQOeJbhgNoqJMcYMGpbgglmCM8aYQcMSXLC6IhAfJGfFOhJjjDE9ZAkuWF0RDBkFvoRYR2KMMaaH+jTBicgTInJARNZHWC8i8ksR2SYia0VkVtC6K0Vkq/e4si/jbFFno5gYY8xg0dc1uKeAczpYfy4wxXtcB/wGQERGAncBJwFzgbtEpO+H968rspm8jTFmkOjTBKeq7wElHRS5AHhGnY+A4ZSHK14AAAeCSURBVCJyJPAvwFuqWqKqpcBbdJwoe0edDbRsjDGDRayvwY0D9ga9LvSWRVrejohcJyIrRWTlwYMHux+JqiU4Y4wZRBJjHUBPqepjwGMAs2fP1h7t7MvbwDfgT4kxxhhiX4PbB0wIej3eWxZped8RgbSxdg3OGGMGiVgnuFeAK7zelCcD5ar6OfAmcLaIjPA6l5ztLTPGGGOi0qftcSKyBJgPZItIIa5nZBKAqj4CvAEsBLYBNcA3vXUlIvKfwApvV/eqakedVYwxxpg2RLVnl63iiYgcBHb3cDfZwKFeCKc/DcSYYWDGbTH3n4EYt8XcPyap6qjOCg2qBNcbRGSlqs6OdRxdMRBjhoEZt8XcfwZi3BZzfIn1NThjjDGmT1iCM8YYMyhZgmvvsVgH0A0DMWYYmHFbzP1nIMZtMccRuwZnjDFmULIanDHGmEHJEpxHRM4Rkc3e1D23xzqeSMJNQSQiI0XkLW9qobf6ZeaFLhCRCSLydxHZKCIbRORmb3ncxi0iKSLysYis8WK+x1ueKyLLvb+TF0UkOdaxhhKRBBH5RERe814PhJh3icg6EVktIiu9ZXH79wEgIsNF5CUR+VRENonIKQMg5qneOQ48KkTk3+M97u6yBIf7QAAexk3fMx1YLCLTYxtVRE/RfmaF24F3VHUK8I73Op40Af9HVacDJwM3euc3nuOuB76oqjOBfOAcb7Sd/wJ+pqqTgVLgmhjGGMnNwKag1wMhZoAzVTU/qMt6PP99APwC+IuqTgNm4s55XMesqpu9c5wPnIgbYGMpcR53t6nqYf8ATgHeDHr9PeB7sY6rg3hzgPVBrzcDR3rPjwQ2xzrGTuL/E3DWQIkbSANW4eYnPAQkhvu7iYcHbtzWd4AvAq8BEu8xe3HtArJDlsXt3wcwDNiJ149hIMQc5j2cDXww0OLuysNqcE7U0/PEqTHqxvAE2A//r717C5W6iuI4/v1BZWKiXXzz4RhYQlEpGJUmQhEUIiWBXSChoAtdyAixgp6FIuopiKRexEAtkwiT7mKk5u2gSUUUZKVHKq2MwsvqYa+xaVLr6OnMnj+/Dwzzv8z8zzqHfVjz33tYi2p7/kjqAyYD66k87pzq2woMUHoSfgnsi4hD+ZIax8mzwALgSO6fS/0xAwSwRtImSXfnsZrHxwRgL/BSTge/KGkUdcfc6RZgaW73Utz/mRNcw0T5CFblV2MlnQWsAB6OiJ/bz9UYd0QcjjKVM57SWX5Sl0M6IUmzgIGI2NTtWE7C9IiYQlkmuF/SjPaTFY6P04ApwPMRMRk4QMe0XoUxH5XrsLOBZZ3nao57sJzgiuFvzzO09mQndPJ5oMvx/IOk0ynJbUlEvJqHq48bICL2Ae9RpvfGSmoVKa9tnEwDZkv6GniFMk35HHXHDEBEfJvPA5Q1ocupe3zsAnZFxPrcX05JeDXH3O56YHNE7Mn9Xol7UJzgio3AxPy22RmUW/dVXY5pMFYB83J7HmWNqxqSBCwGdkbEM22nqo1b0jhJY3N7JGXNcCcl0d2cL6sq5oh4LCLGR0QfZQy/GxG3U3HMAJJGSRrd2qasDW2n4vEREbuBbyRdmIeuAT6l4pg73Mpf05PQO3EPTrcXAWt5UNr2fE5ZZ3mi2/GcIM6lwPfAQcqnyLso6yzvAF8AbwPndDvOjpinU6Y8+oGt+bih5riBS4AtGfN24Mk8fj6wgdLiaRkwotuxHif+mcAbvRBzxrctHzta/381j4+M7zLgkxwjK4Gza4854x4F/ACMaTtWfdwn83AlEzMzayRPUZqZWSM5wZmZWSM5wZmZWSM5wZmZWSM5wZmZWSM5wZn9zyT9ms99km4b4ms/3rH/0VBe36yXOcGZDZ8+YFAJrq0CyfH8LcFFxFWDjMmssZzgzIbPIuDq7MM1P4s5PyVpo6R+SfcASJopaa2kVZTqGEhamYWId7SKEUtaBIzM6y3JY627ReW1t2eftblt136/rY/Zkqw0g6RFKj37+iU9Pex/HbMh9m+fDs1s6CwEHo2IWQCZqPZHxFRJI4B1ktbka6cAF0fEV7l/Z0T8mGXDNkpaERELJT0QpSB0pzmUShuXAuflez7Mc5OBi4DvgHXANEk7gZuASRERrTJlZr3Md3Bm3XMdcEe25FlPKZc0Mc9taEtuAA9J2gZ8TCkMPpETmw4sjdIRYQ/wATC17dq7IuIIpWxaH7Af+B1YLGkOpRGmWU9zgjPrHgEPRnZYjogJEdG6gztw9EXSTOBa4MooHca3AGeews/9o237MKUZ6iFKBf/lwCxg9Slc36wKTnBmw+cXYHTb/lvAfdlKCEkXZDX9TmOAnyLiN0mTgCvazh1svb/DWmBurvONA2ZQCi4fU/bqGxMRbwLzKVObZj3Na3Bmw6cfOJxTjS9TerX1AZvzix57gRuP8b7VwL25TvYZZZqy5QWgX9LmKK1xWl6j9K/bRunksCAidmeCPJbRwOuSzqTcWT5ycr+iWT3cTcDMzBrJU5RmZtZITnBmZtZITnBmZtZITnBmZtZITnBmZtZITnBmZtZITnBmZtZITnBmZtZIfwK8kf6UOru3PQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex= True,  constrained_layout=True)\n",
    "fig.suptitle(\"GaussianAltFit-2D (Analytical Reweight):\\n N = {:.0e}, Iterations = {:.0f}\".format(N, len(fit_vals)))\n",
    "axs[0].plot(fit_vals[:,0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], 0, len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(fit_vals[:,1], label='Model $\\sigma$ Fit', color ='orange')\n",
    "axs[1].hlines(theta1_param[1], 0, len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.savefig(\"GaussianAltFit-2D (Analytical Reweight):\\n N = {:.0e}, Iterations = {:.0f}.png\".format(N, len(fit_vals)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "\n",
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex= True,  constrained_layout=True)\n",
    "fig.suptitle(\"GaussianAltFit-2D Zoomed (Analytical Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\".format(N,index_refine[1], len(fit_vals)))\n",
    "axs[0].plot(np.arange(index_refine[1], len(fit_vals[:,0])), fit_vals[index_refine[1]:,0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], index_refine[1], len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.arange(index_refine[1], len(fit_vals[:,1])), fit_vals[index_refine[1]:,1], label='Model $\\sigma$ Fit', color = 'orange')\n",
    "axs[1].hlines(theta1_param[1], index_refine[1], len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.savefig(\"GaussianAltFit-2D Zoomed (Analytical Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\".format(N,index_refine[1], len(fit_vals)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
