{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:17.743398Z",
     "start_time": "2020-06-09T07:59:15.333484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import argrelmin, argrelmax\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:17.754194Z",
     "start_time": "2020-06-09T07:59:17.747529Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__)  #1.15.0\n",
    "print(keras.__version__)  #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative DCTR fitting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{1-f(x_{T,i},\\theta')}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the original Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it $\\mu$ and $\\sigma$ values in addition to $X_i\\sim\\mathcal{N}(\\mu, \\sigma)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ and $\\sigma$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:17.765943Z",
     "start_time": "2020-06-09T07:59:17.757584Z"
    }
   },
   "outputs": [],
   "source": [
    "mu_min = -2\n",
    "mu_max = 2\n",
    "\n",
    "sigma_min = 0.5\n",
    "sigma_max = 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with these $\\mu$ and $\\sigma$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0, \\sigma = 1$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu, \\sigma$ values uniformly sampled in the specified range [$\\mu_{min}, \\mu_{max}$] and [$\\sigma_{min}, \\sigma_{max}$].\n",
    "\n",
    "For every value of $\\mu$ in mu_values and every value of $\\sigma$ in sigma_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ and $\\sigma$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T04:49:41.467094Z",
     "start_time": "2020-06-08T04:49:41.463444Z"
    }
   },
   "source": [
    "## Build and Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:17.822098Z",
     "start_time": "2020-06-09T07:59:17.771086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Either load or train DCTR\n",
    "\n",
    "\n",
    "def make_dctr(load=False, n_data_points=10**7):\n",
    "    if load:\n",
    "        json_file = open('2d_gaussian_dctr_model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        dctr_model.load_weights(\"2d_gaussian_dctr_model.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "    else:\n",
    "        # Generate training & validation data\n",
    "        mu_values = np.random.uniform(mu_min, mu_max, n_data_points)\n",
    "        sigma_values = np.random.uniform(sigma_min, sigma_max, n_data_points)\n",
    "\n",
    "        X0 = [(np.random.normal(0, 1), mu_values[i], sigma_values[i])\n",
    "              for i in range(n_data_points)]  # Note the zero in normal(0, 1)\n",
    "        X1 = [(np.random.normal(mu_values[i], sigma_values[i]), mu_values[i],\n",
    "               sigma_values[i]) for i in range(n_data_points)]\n",
    "\n",
    "        Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "        Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "        X = np.concatenate((X0, X1))\n",
    "        Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
    "                                                            Y,\n",
    "                                                            test_size=0.5)\n",
    "\n",
    "        # Build Model\n",
    "        inputs = Input((3, ))\n",
    "        hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "        hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "        hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "        outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "        dctr_model = Model(inputs=inputs, outputs=outputs)\n",
    "        dctr_model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='Adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        # Train Model\n",
    "        earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        dctr_model.fit(X_train,\n",
    "                       Y_train,\n",
    "                       epochs=200,\n",
    "                       batch_size=10000,\n",
    "                       validation_data=(X_test, Y_test),\n",
    "                       callbacks=[earlystopping])\n",
    "\n",
    "    return dctr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:19.062349Z",
     "start_time": "2020-06-09T07:59:17.826945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "dctr_model = make_dctr(load=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining reweighting functions\n",
    "\n",
    "For a fully trained DCTR $f(x, \\theta)$, the reweighting function is: $w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$.\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$.\n",
    "\n",
    "The expression for the analytical reweighting function is : $w(x_T, \\mu, \\sigma) = \\frac{\\sigma_0}{\\sigma}\\exp{(-((\\frac{x_T-\\mu}{\\sigma})^2-(\\frac{x_T-\\mu_0}{\\sigma_0})^2)/2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:19.082837Z",
     "start_time": "2020-06-09T07:59:19.066872Z"
    }
   },
   "outputs": [],
   "source": [
    "theta0_param = (0, 1)\n",
    "\n",
    "\n",
    "# from NN (DCTR)\n",
    "def reweight(events, param):\n",
    "\n",
    "    # creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    concat_input_and_params = K.ones(shape=(events.shape[0], 2)) * param\n",
    "    # combining and reshaping into correct format:\n",
    "    model_inputs = K.concatenate((events, concat_input_and_params), axis=-1)\n",
    "    f = dctr_model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "# from analytical formula for normal distributions\n",
    "def analytical_reweight(events, param, param0=theta0_param):\n",
    "    weights = (param0[1] / param[1]) * K.exp(-0.5 * (\n",
    "        ((events - param[0]) / param[1])**2 -\n",
    "        ((events - param0[0]) / param0[1])**2))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate DCTR for any $\\mu$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:19.228920Z",
     "start_time": "2020-06-09T07:59:19.087658Z"
    }
   },
   "outputs": [],
   "source": [
    "mu1 = 1\n",
    "sigma1 = 1.5\n",
    "assert mu1 >= mu_min and mu1 <= mu_max  # choose mu1 in valid range\n",
    "assert sigma1 >= sigma_min and sigma1 <= sigma_max  # choose mu1 in valid range\n",
    "X0_val = np.random.normal(0, 1, 10**6)\n",
    "X1_val = np.random.normal(mu1, sigma1, 10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:19.954817Z",
     "start_time": "2020-06-09T07:59:19.233425Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = reweight(\n",
    "    K.expand_dims(tf.convert_to_tensor(X0_val, dtype=tf.float32)),\n",
    "    np.array([mu1, sigma1]))\n",
    "analytical_weights = analytical_reweight(X0_val, np.array([mu1, sigma1]))\n",
    "weights = K.eval(weights)\n",
    "analytical_weights = K.eval(analytical_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:21.541517Z",
     "start_time": "2020-06-09T07:59:19.958689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt8VdWZ//HPI4hgQZGbYoIEFCkEuQURtCAFBaQi8qsXsFWsaOpUpmo706JWAS/VmXqp/Kw6qTJCR6FeKzCAokhLfwqSICCiQkSQIAKCgCgRgef3x1mJJyEkO9dzQr7v1+u8cs6z117rOQnkyV57n73M3REREYniqEQnICIitYeKhoiIRKaiISIikaloiIhIZCoaIiISmYqGiIhEpqIhSc/MTjOzpL02PNnzK87M6pnZHjM7JULb+mbmZpZWjv6fMLNbK5OjJC8VDamU8Mun4HHQzPbGvf5JBfvMM7MBlcjpf8xsYkX3r27h/RV8nz4zsylm9r2aGt/dD7h7Y3f/pLJ9mdm1ZrawWP/XuvvvK9u3JCcVDamU8Munsbs3Bj4BhsfFni7e3szq13yWSemC8D3rCfQGfpPgfEQiUdGQamVmd5vZX81supl9Cfy0+JGAmZ1nZuvD8+nAycDc8Jf4r+LaXRX+St9mZuMrmE9nM3vNzHaY2Qdm9uMQP8fMNpnZUXFtLzWzZeH5UWZ2q5l9ZGafm9kMMzuhIjnEc/dPgVeB7nHjNjSzB81so5ltMbNHzaxh2Pb/zGxEeH5umDoaEl4PMbPsuH6uDe/xCzOba2ZtQrzIlJOZtTSz/zWz3Wb2tpn9vvjRAzDEzHJDX5PDfmcAjwD9ws/q8xAv/PkW/GzN7Dfh5/apmV0Vl2OUsSWJqGhITRgJPAMcD/y1tIbuPhr4lPCXuLs/GLf5bOA0YAgwycw6lCcJM2sMzAemAa2AnwBZZtYReBP4Fjg3bpcrQt4ANwM/AvoDqcAeYPJhxrnNzP4WMac2wFAgNy78B6Ad0BXoAKQBt4VtfwcGhOfnAutCTgWv/x76/THw78AIoCWwJO69FPcYsBM4EbgGGFNCm2FABtCDWOE/z93fBcYBi8LPqsVh+k8FGhH7Y+B64DEzO64cY0sSUdGQmvBPd5/l7gfdfW8l+pno7vnuvgx4D+hWzv1HAGvcfZq773f3HOBvwCUeuwnbDGA0gJk1JVacZoR9rwdudfdN7p4PTAIujT8yKeDu97j7xWXkMjsceX0C5AF3hnGPAq4DbnL3L9x9N3AvMCrs93e+K2z9w7aC14VFI+T7e3f/0N33A3cDvc0sJT4JMzsauBi4w933uvsq4C8l5Huvu+9y9/XAQuKOjCLIB+5292/dfSbwDXB6OcaWJKKiITVhY1V04u6fxb38Gmhczi7aAueY2c6CB3A50Dpsfwb4cfhl9mNgibvnhW2nALPi9ns3xFtV5L0AF7p7E2AQ0BloFuInAccAK+LGmh03zv8D0s2sJdAFmAq0N7PmxI4EFsW91z/F9fE5cJDYX/3xTgTqUfRnVNLPqzLf+8/d/UAJ+0cdW5KIiobUhOKXo34FHBv3+qQy2leVjcDr7t407tHY3ccBuPtKYr8ch1B0agpiRwPnF9u3YbFCVm7uvgB4mtiUFMAWYB/QMW6c4939+NB+D7Cc2HTZcnf/ltjU06+BD9z9i7j3OrZYvo3cfUmxFLZwaDFpU563UI62xVV2bEkAFQ1JhOXAj8zsBDNrDfyy2PYtQPtKjlE/nFAueDQAZhL7K/0KMzs6PHqHcxoFniH2C7kv8Hxc/HHg9xY+22BmrczsokrmWOAhYJiZdQl/kT8B/DGcJDYzSzWzwXHt/07sXELBVNTCYq8L8r3NzDqFfJua2SXFBw5F52/EzhE1MrN04KflyH0LkBqOzsqlCsaWBFDRkER4Cngf2ADM47vzBgV+T+wXyU4zu6mCY9wG7I17vOruu4gdRfwU2EzsqOJeYtNBBZ4BBgLz4/5qB3gw5Pp6OBfxJnBmSQOb2e1mNitqouFo5Wng9hD6NbHvzdvALmJXV8Wf9P870AT4x2Fe4+7PhZyfM7PdwMrw3kvyL0BzYgXgv4HpxM47RDEfWAtsMbOKHHVVZmxJANMiTCISz8weAJq6+9i6NLZEoyMNkTrOYp9dOSNMhfUBfga8dKSPLRWjT+eKyHHEpsdaE5smus/dZ9eBsaUCND0lIiKRaXpKREQiO+Kmp1q0aOFpaWmJTkNEpFbJycn53N1bltXuiCsaaWlpZGdnl91QREQKmdmGKO00PSUiIpGpaIiISGQqGiIiEtkRd06jJN9++y15eXnk5+cnOhWREjVs2JDU1FSOPrrct3ASqVF1omjk5eXRpEkT0tLSMLNEpyNShLuzfft28vLyaNeuXaLTESlVnZieys/Pp3nz5ioYkpTMjObNm+tIWGqFOlE0ABUMSWr69ym1RZ0pGiIiUnkqGiIiElmZJ8LNrA0wjdh6vg5kufvDZtYM+CuQBqwHLnP3Lyx2nP0wMIzYWsBXu/uy0NcY4Heh67vdfWqIZxBbmKcRMAe40d39cGNU+l2LVKEtu6vmXMTuvd/y0Pw1ANx8/ulV0qdIVYty9dR+4NfuvszMmgA5ZjYfuJrYesv3mdl4YDzwW+ACYquMdQDOAh4DzgoFYALQi1jxyTGzmaEIPAZcR2yt4znAUGBu6LOkMSql4D9mVanp/+Dz5s3jxhtv5MCBA1x77bWMHz++Rse/5pprmD17Nq1atWLVqlU1OraIJFaZ01PuvrngSMHdvyS2TGcKMAKYGppNBS4Oz0cA0zxmMdA0rAM9hNgSmjtCoZgPDA3bjnP3xR67T/u0Yn2VNEaddeDAAW644Qbmzp3L6tWrmT59OqtXr67RHK6++mrmzZtXo2OKSHIo1zkNM0sDehA7IjjR3TeHTZ8Rm76CWEHZGLdbXoiVFs8rIU4pYxTPK9PMss0se9u2beV5SzVqwIABfPDBBwBs376dLl26lLuPt99+m9NOO4327dvToEEDRo0axcsvv1zmfitWrKB///507tyZo446CjPjjjvuKPf4AP3796dZs2YV2ldEarfIH+4zs8bAC8BN7r47/hLBcP6hWldzKm0Md88CsgB69eqVtKtK5ebmcvrpsamslStXcsYZZxTZ3q9fP7788stD9rv//vs577zzANi0aRNt2rQp3JaamsqSJUtKHTc/P5/LL7+cadOm0bt3b26//Xby8/OZNGlSucYWEYlUNMzsaGIF42l3fzGEt5hZa3ffHKaYtob4JqBN3O6pIbYJGFAsvjDEU0toX9oYtc6GDRtISUnhqKNiB3crV66ka9euRdosWrSoWsZ+7bXX6NmzJ7179waga9euzJs3r8hnA6prbBE5skS5esqAJ4H33f3BuE0zgTHAfeHry3HxcWY2g9iJ8F3hl/4rwO/N7ITQbjBwi7vvMLPdYVH5JcBVwP8tY4xaZ8WKFUWKRE5ODpdffnmRNlH+2k9JSWHjxu9m+fLy8khJSTlkn3irVq0qclSzbNkyevbsWe6xRUSiHGmcA1wJvGtmy0PsVmK/yJ81s7HABuCysG0Oscttc4ldcvszgFAc7gKWhnZ3uvuO8PwXfHfJ7dzwoJQxap3ly5cX3iZi7dq1vPzyy9x9991F2kT5a//MM89k7dq1fPzxx6SkpDBjxgyeeeaZwu2DBg1i2rRpRQpJ8+bNWbBgAQBr1qzhxRdf5M033yz32CIiZRYNd/8ncLh7HAwqob0DNxymrynAlBLi2cAhZ4XdfXtJY1RWIq6BX7FiBQ0bNqRbt2507dqVzp07M3XqVG6//fZy9VO/fn0eeeQRhgwZwoEDB7jmmmtIT08H4ODBg+Tm5h5yknr06NHMnDmTLl260KJFC6ZPn07z5s0r/F5Gjx7NwoUL+fzzz0lNTWXSpEmMHTu2wv2JSO1RJ+5ymwxWrlzJsmXLaNKkSaX7GjZsGMOGDTskvnr1an784x/TqFGjIvHGjRsza9asSo9bYPr06VXWl4jULrqNSA348ssvMbMqKRil6dKlCw8++GDZDUVEKkhFowY0adKENWuq9lPoIiKJoKIhIiKRqWiIiEhkKhoiIhKZioaIiESmoiEiIpGpaIiISGQqGiIiEpmKhoiIRFYni0ZaWhpmVmWPtLS0Gs1/3rx5dOzYkdNOO4377ruvRseG2HKvrVq1KnURqXr16tG9e3fS09Pp1q0bDzzwAAcPHizc/tlnnzFq1ChOPfVUMjIyGDZsGEuWLKF79+50796dk046iZSUlMLX+/btK+yzS5cuDB8+nJ07d1bp+9q5cyePPvpolfYpcqSpk0Vjw4YNuHuVPTZs2FBjudeW5V4bNWrE8uXLee+995g/fz5z584tXPTJ3Rk5ciQDBgzgo48+Iicnh3vvvZd9+/axfPlyli9fzvXXX8/NN99c+LpBgwaFfa5atYpmzZrxpz/9qUrfl4qGSNnqZNFIlLq63GurVq3IysrikUcewd154403OProo7n++usL23Tr1o1+/fpF7rNv375s2rTpkPgf/vAHJk+eDMDNN9/MwIEDAViwYAE/+clPALjrrrvo2LEjP/jBDxg9ejT3338/AOPHj+ejjz6ie/fu/Pu//3vkXETqEt3ltgbV5eVe27dvz4EDB9i6dSurVq0iIyOjwn0dOHCA119/vcTbsffr148HHniAX/7yl2RnZ/PNN9/w7bffsmjRIvr378/SpUt54YUXWLFiBd9++y09e/YszOW+++5j1apVLF++/JB+RSRGRaOGaLnXytu7dy/du3dn06ZNdOrUifPPP/+QNhkZGeTk5LB7926OOeYYevbsSXZ2NosWLWLy5Mm8+uqrjBgxgoYNG9KwYUOGDx+egHciUnuVOT1lZlPMbKuZrYqL/dXMlofH+oIV/cwszcz2xm17PG6fDDN718xyzWxyWEYWM2tmZvPNbG34ekKIW2iXa2Yrzaxn8dxqk5KWey1eNPr161d44jf+8dprrxW2qc7lXssauzLWrVtHvXr1aNWqFenp6eTk5JS7j4JzGgXnpEo6p3H00UfTrl07nnrqKc4++2z69evHG2+8QW5uLp06daqKtyJSp0U5p/EUMDQ+4O6Xu3t3d+8OvAC8GLf5o4Jt7n59XPwx4DqgQ3gU9DkeeN3dOwCvh9cAF8S1zQz711olLfdafHpq0aJFhSd+4x/x00Pxy73u27ePGTNmcNFFFxVuHzRo0CFz/c2bN2flypXAd8u9jho1qtxjV9S2bdu4/vrrGTduHGbGwIED+eabb8jKyipss3LlyshHO8ceeyyTJ0/mgQceYP/+/Yds79evH/fffz/9+/enX79+PP744/To0QMz45xzzmHWrFnk5+ezZ88eZs+eXbhfkyZNSpyiE5HvlFk03P0fwI6StoWjhcuAUpdyM7PWwHHuvjgsBzsNuDhsHgFMDc+nFotP85jFQNPQT6W1bdu2Si+5bdu2bZljrlixgoMHD9KtWzfuvPPOwuVeyyt+uddOnTpx2WWXRVrudc+ePXTp0oXMzMwqWe61b9++fPjhh6SmpvLkk08e0qZgKik9PZ3zzjuPwYMHM2HCBADMjJdeeonXXnuNU089lfT0dG655RZOOumkyDn06NGDrl27lriKYL9+/di8eTN9+/blxBNPpGHDhoUn2c8880wuuugiunbtygUXXMAZZ5zB8ccfD8SK6znnnEOXLl0KT4QPGzaMTz/9tNzfI5EjlcV+h5fRyCwNmO3uXYrF+wMPunuvuHbvAWuA3cDv3H2RmfUC7nP380K7fsBv3f1CM9vp7k1D3IAv3L2pmc0O+/wzbHs97JNdWq69evXy7OyiTd5///2ET0106NChypZ7PZxVq1YxZcoUrd5Xhj179tC4cWO+/vpr+vfvT1ZW1iHTdeWxZXd+leS1PncNb25vCCRmHXup28wsp+B3eWkqeyJ8NEWPMjYDp7j7djPLAP5mZulRO3N3N7Oyq1gxZpZJbAqLU045pby7Vzst95pcMjMzWb16Nfn5+YwZM6ZSBUOkrqlw0TCz+sD/AQqvnXT3b4BvwvMcM/sIOB3YBKTG7Z4aYgBbzKy1u28O009bQ3wT0OYw+xTh7llAFsSONCr6nqqLlntNLs8880yiUxCptSrz4b7zgA/cPa8gYGYtzaxeeN6e2Ensde6+GdhtZn3CFNRVQMEn0mYCY8LzMcXiV4WrqPoAu0I/IiKSIFEuuZ0OvAV0NLM8Myv4RNUoDj0B3h9YGS7BfR643t0LTqL/AngCyAU+AuaG+H3A+Wa2llghKriZ0hxgXWj/57C/iIgkUJnTU+4++jDxq0uIvUDsEtyS2mcDh9w3w923A4NKiDtwQ1n5iYhIzdG9p0REJDIVDRERiUxFQ0REIlPREBGRyFQ0REQksjpbNDIzM4vcP+rTTz9l1qxZRWIFN9SLjxXcSnv48OFF4jWpNiz3WpUaN2582G0lrbZ39tlnV8tY5bVr507++8//VWX9iSSDOlk0MjIyyMrKKrJk68knn8zw4cOLxDIzMwGKxGbNmgXArFmzisRrSm1Z7rWmlFQ03nzzzQRlU9SuXbt46smsshuK1CJ1smgsW7asSvuLv8V3aerScq8XX3wxGRkZpKenF35/1q9fT6dOnbjuuutIT09n8ODB7N27t9R94t1xxx388Y9/LHx922230aNHj0OWaI0/Wpg2bRpdu3alW7duXHnllZHHihdlCdkH//NezsnoykVDBnL9NVfx6OSHuGfi79jw8ToG/eAsJv3ullLHEKk14v9aPhIeGRkZXtzq1auLvI697aoTtb+UlBQ/cOCAu7svWLDAR40aVWT7D37wA+/Wrdshj/nz5xe2ee6553zs2LGFr6dNm+Y33HBDqePu3bvXO3bs6EuWLHF399/97nf+b//2b37w4MFyjR3v448/9vT09MOOuX37dnd3//rrrz09Pd0///xz//jjj71evXr+zjvvuLv7pZde6n/5y19K3cfd/Xvf+17hmD169HB39wMHDnj79u09Ozv7kDwK2q9atco7dOjg27ZtK9J/lLHivfXWW37JJZcUfp/OPPNM37dvn0+cONEff/xxn7tgkaef0dXXb/nCc/O2erv2p/odd/3e3175gXfs1Nk/27U30mNxzgp/8NUP/cFXPzzs91WkugDZHuF3bJ1c7rV16ypZlqNc6tpyr5MnT+all14CYOPGjaxdu5aTTjqJdu3a0b17dyA2Tbh+/fpS94lf9yMtLY3mzZvzzjvvsGXLFnr06FHquiALFizg0ksvpUWLFgBFjo7KGiteWUvIvjDzfxky7EIaNmwIDRsy+IJhFfiOidQOdbJoJGJRnZKWe7388suLtOnXr1+JK8fdf//9hSvoVedyr2WNHdXChQt57bXXeOuttzj22GMZMGBA4aqFxxxzTGG7evXqFU5PlbZPvGuvvZannnqKzz77jGuuuaZceUXJryTFl5Dt2rVr0SVkZ/5vhfIQqY3q5DmNiRMnVml/M2fOLLNNXVruddeuXZxwwgkce+yxfPDBByxevLjK9hk5ciTz5s1j6dKlDBkypNQlWgcOHMhzzz3H9u3bAdixY0eF8yttCdkzz+rL/LlzyM/P56s9e5g/L3YvzsZNGrNnj5aPlSNLnSwakyZNKnK5bE5ODjk5OUViBYXl5JNPLoxlZMSWDil+uW5BvDR1abnXoUOHsn//fjp16sT48ePp06dPmX1G3adBgwb88Ic/5LLLLqNevXolLtFaID09ndtuu41zzz2Xbt268atf/arC+ZW2hGyPjF4MHvYjBp59JldcMoJOndM57rjjadasOb3P6su5fTIKT4RfccnFfLZZy8dK7RVpudfaRMu9HtnLvR48eJCePXvy3HPP0aFDh0SnA8SWe/1qzx6+F5aQvfiC87n/4Ufo2r1HufrRcq+SSDW13KtEoOVeq8bq1au58MILGTlyZNIUjAL/duMNrPnwA77Jz+ey0T8td8EQqS1UNGqAlnutGp07d2bdunWJTqNEjz1Z/qlGkdpIRUPkMLbsPvwVVdXtofnR/sjQNJbUtCjLvU4xs61mtiouNtHMNpnZ8vAYFrftFjPLNbMPzWxIXHxoiOWa2fi4eDszWxLifzWzBiF+THidG7anVdWbFhGRioly9dRTwNAS4g+5e/fwmANgZp2JrR2eHvZ51MzqmVk94E/ABUBnYHRoC/Afoa/TgC+AgjXIxwJfhPhDoZ2IiCRQmUXD3f8B7IjY3whghrt/4+4fA7lA7/DIdfd17r4PmAGMsNhHkgcCz4f9pwIXx/VVMFH8PDDIavp2siIiUkRlPqcxzsxWhumrE0IsBdgY1yYvxA4Xbw7sdPf9xeJF+grbd4X2hzCzTDPLNrPsbdu2VeItiYhIaSpaNB4DTgW6A5uBB6osowpw9yx37+XuvVq2bJnIVEREjmgVKhruvsXdD7j7QeDPxKafADYBbeKapobY4eLbgaZmVr9YvEhfYfvxob2IiCRIhYqGmcXfJnYkUHBl1UxgVLjyqR3QAXgbWAp0CFdKNSB2snxmuB3vG8AlYf8xwMtxfY0Jzy8BFviR9vF1EZFapszPaZjZdGAA0MLM8oAJwAAz6w44sB74OYC7v2dmzwKrgf3ADe5+IPQzDngFqAdMcff3whC/BWaY2d3AO0DBjYyeBP5iZrnETsQXvcNeZbxxb5V1BcAPa3aBnWuuuYbZs2fTqlUrVq1aVfYOCRg/LS2NJk2aUK9ePerXr0/xW7uISO1UZtFw99ElhJ8sIVbQ/h7gnhLic4A5JcTX8d30Vnw8H7i0rPzqoquvvppx48Zx1VVXJfX4b7zxRuFaFiJyZKiTd7lNlKpY7hWiLbdaXE0v9yoiRybdRqQG5ebmcvrpsds+rFy58pD1NKpyIaR4+fn5XH755UybNo3evXtz++23k5+fz6RJk6ptbDNj8ODBmBk///nPyczMrHD+IpI8VDRqSF1b7vWf//wnKSkpbN26lfPPP5/vf//79O/fv0rHEJGap6JRQ6pqudeKqOnlXoHCJWhbtWrFyJEjefvtt1U0RI4AKho1pKTlXu++++4ibarir/1BgwYxbdq0IuuGN2/enAULFgDfLff65ptvVvnYBb766isOHjxIkyZN+Oqrr3j11VcrfP6krnrvrQWknp7OpNHfFdo+F1zGZTffxYO/+D/k5cYuPvxD69YJWfNe6q66WTRq+BJZiB1pNGzYkG7dutG1a9fC5V5vv/32cvc1evRoFi5cyOeff05qaiqTJk1i7NixpS73OnPmTLp06UKLFi2qZLnXksYfNmwYTzzxBPn5+YwcORKA/fv3c8UVVzB0aEn3vJTDSV1wE0NSxjHkiV8X3fBJFs+OH0rBPUT7jr2/5pOTOk3LvdYQLfda+9T0ehrxy73uuqdrGa1jJv39m0jt2rZty/r16yuamtQBWu41iWi5V6mIIVeOK7NN06Z/YefOXQBkZl4HQFbWnwu3DxhwLgPOHYANvLV6kpQ6R0WjBmi5V6kuN9140yGxiRMmJCATqSv04T4REYlMRUNERCKrM0XjSDvhL0cW/fuU2qJOFI2GDRuyfft2/ceUpOTu7Nn1BXv2azVjSX514kR4amoqeXl5aClYKY/de7+tsbH27Dfe3310jY0nUlF1omgcffTRtGvXLtFpSC3z0Hxd8SZSXJ2YnhIRkapRZtEwsylmttXMVsXF/mBmH5jZSjN7ycyahniame01s+Xh8XjcPhlm9q6Z5ZrZZAu3WDWzZmY238zWhq8nhLiFdrlhnJ7FcxMRkZoV5UjjKQpudPOd+UAXd+8KrAHib+b0kbt3D4/r4+KPAdcRWze8Q1yf44HX3b0D8Hp4DXBBXNvMsL+IiCRQlOVe/2FmacVir8a9XAxcUlofZtYaOM7dF4fX04CLgbnACGJrkANMBRYSWzd8BDDNY5c8LTazpmbW2t03l/muRJLQXVcO5IstmyK1nXDuMdWcjUjFVMWJ8GuAv8a9bmdm7wC7gd+5+yIgBciLa5MXYgAnxhWCz4ATw/MUYGMJ+6hoSK30y+9/TurwHuStXV0YG/Djq9m9fRvLFv5vYSy9zwBSO1RsKWCR6lapomFmtwH7gadDaDNwirtvN7MM4G9mlh61P3d3Myv3hynMLJPYFBannHJKeXcXqTHpfQaS3mdgkVjLYxtHujmhSDKo8NVTZnY1cCHwkzCFhLt/4+7bw/Mc4CPgdGATkBq3e2qIAWwJ01cF01hbQ3wT0OYw+xTh7lnu3svde7Vs2bKib0nkiNWsWTPMrMxHWlpaolOVJFehomFmQ4HfABe5+9dx8ZZmVi88b0/sJPa6MP2028z6hKumrgJeDrvNBMaE52OKxa8KV1H1AXbpfIZIxRTcEaGsx4YNGxKdqiS5MqenzGw6sRPVLcwsD5hA7GqpY4D54crZxeFKqf7AnWb2LXAQuN7dd4SufkHsSqxGxE6Azw3x+4BnzWwssAG4LMTnAMOAXOBr4GeVeaMidZmZ6TY6UiWiXD01uoTwk4dp+wLwwmG2ZQOHnN0L01mDSog7cENZ+YmISM2pE7cREanrJpx7DLxxb6LTkCOAioZIHXD66R0AeGb6M6xZs7YwPnHCBHJycpg1e3aiUpNaRkVDpA64YvQVRb7Gy8jIICMjA4BJWktcyqAbFoqISGQqGiIiEpmKhoiIRKaiISIikaloiIhIZCoaIiISmYqGiIhEpqIhIiKRqWiIiEhkKhoiIhKZioaIiESmoiEiIpGpaIiISGQqGiIiElmkomFmU8xsq5mtios1M7P5ZrY2fD0hxM3MJptZrpmtNLOecfuMCe3XmtmYuHiGmb0b9pkc1hE/7BgiIpIYUY80ngKGFouNB1539w7A6+E1wAVAh/DIBB6DWAEgtr74WUBvYEJcEXgMuC5uv6FljCEiIgkQqWi4+z+AHcXCI4Cp4flU4OK4+DSPWQw0NbPWwBBgvrvvcPcvgPnA0LDtOHdfHNYFn1asr5LGEBGRBKjMOY0T3X1zeP4ZcGJ4ngJsjGuXF2KlxfNKiJc2RhFmlmlm2WaWvW3btgq+HRERKUuVLPfq7m5mXhV9VWQMd88CsgB69epVrXmIFLfg/emJAAAQN0lEQVTv4XPYu+fLMts1aNiwBrIRqV6VKRpbzKy1u28OU0xbQ3wT0CauXWqIbQIGFIsvDPHUEtqXNoZI0ti750uGXDku0WmI1IjKTE/NBAqugBoDvBwXvypcRdUH2BWmmF4BBpvZCeEE+GDglbBtt5n1CVdNXVWsr5LGEBGRBIh0pGFm04kdJbQwszxiV0HdBzxrZmOBDcBlofkcYBiQC3wN/AzA3XeY2V3A0tDuTncvOLn+C2JXaDUC5oYHpYwhItWgbdu2hCvey2y3fv366k9Ikk6kouHuow+zaVAJbR244TD9TAGmlBDPBrqUEN9e0hgiUj327dtH7L9w6aIUFjky6RPhIlJo8+bNZTeSOk1FQ0REIlPREJFCPXv2LLuR1GlV8jkNETky5Nx/Cbxxb6LTkCSmoiEihWbNnkVOzrLC17/+1a/4dPOnTJ8+ozA2/MILE5GaJAkVDREpNPzC4Qy/cHiRWMcmHZk4YUKxli/UXFKSVFQ0pM55aP6aRKdQZd5atz1Su77tm1dzJlJX6ES4iIhEpqIhIiKRqWiIiEhkKhoiIhKZioaIiESmoiEiIpGpaIiISGQqGiIiEpmKhoiIRFbhomFmHc1sedxjt5ndZGYTzWxTXHxY3D63mFmumX1oZkPi4kNDLNfMxsfF25nZkhD/q5k1qPhbFRGRyqpw0XD3D929u7t3BzKILe36Utj8UME2d58DYGadgVFAOjAUeNTM6plZPeBPwAVAZ2B0aAvwH6Gv04AvgLEVzVdERCqvqqanBgEfufuGUtqMAGa4+zfu/jGxNcR7h0euu69z933ADGCExdaTHAg8H/afClxcRfmKiEgFVFXRGAVMj3s9zsxWmtkUMzshxFKAjXFt8kLscPHmwE53318sfggzyzSzbDPL3rZtW+XfjYiIlKjSRSOcZ7gIeC6EHgNOBboDm4EHKjtGWdw9y917uXuvli1bVvdwIiJ1VlXcGv0CYJm7bwEo+ApgZn8GZoeXm4A2cfulhhiHiW8HmppZ/XC0Ed9eREQSoCqmp0YTNzVlZq3jto0EVoXnM4FRZnaMmbUDOgBvA0uBDuFKqQbEprpmursDbwCXhP3HAC9XQb4iIlJBlTrSMLPvAecDP48L/6eZdQccWF+wzd3fM7NngdXAfuAGdz8Q+hkHvALUA6a4+3uhr98CM8zsbuAd4MnK5CsiIpVTqaLh7l8RO2EdH7uylPb3APeUEJ8DzCkhvo7Y1VUiIpIEtNyryGHs/n03YrOkpYtdHS5SN6hoiByGuzPkynGJTkMkqejeUyIiEpmKhoiIRKaiISIikaloiEi5tW3bFjMr85GWlpboVKWKqWiISLldffXVuHuZjw0bSruHqdRGunpKRMqt6fLHmDjgPgAyM68DICvrz4XbBww4lwHnDkhEalLNVDREpNxuuvGmQ2ITJ0xIQCZS0zQ9JSIikaloiIhIZCoaIiISmYqGiIhEpqIhIiKRqWiIiEhkKhoiIhKZioaIiERW6aJhZuvN7F0zW25m2SHWzMzmm9na8PWEEDczm2xmuWa20sx6xvUzJrRfa2Zj4uIZof/csK9WvBERSZCqOtL4obt3d/de4fV44HV37wC8Hl4DXAB0CI9M4DGIFRlgAnAWseVdJxQUmtDmurj9hlZRziIiUk7VNT01Apgank8FLo6LT/OYxUBTM2sNDAHmu/sOd/8CmA8MDduOc/fFHlt3c1pcXyIiUsOqomg48KqZ5ZhZZoid6O6bw/PPgBPD8xRgY9y+eSFWWjyvhHgRZpZpZtlmlr1t27bKvh8RETmMqrhh4Q/cfZOZtQLmm9kH8Rvd3c3Mq2Ccw3L3LCALoFevXtU6lohIXVbpIw133xS+bgVeInZOYkuYWiJ83RqabwLaxO2eGmKlxVNLiIuISAJUqmiY2ffMrEnBc2AwsAqYCRRcATUGeDk8nwlcFa6i6gPsCtNYrwCDzeyEcAJ8MPBK2LbbzPqEq6auiutLRERqWGWnp04EXgpXwdYHnnH3eWa2FHjWzMYCG4DLQvs5wDAgF/ga+BmAu+8ws7uApaHdne6+Izz/BfAU0AiYGx4iIpIAlSoa7r4O6FZCfDswqIS4Azccpq8pwJQS4tlAl8rkKSIiVUOfCBcRkci03KvUOfsePoe9e77k1K5nclq3s1j4/BS+2fs1AMc1a0nfH13Oe4sXsCvBeYokIxUNqXP27vkSgI9WLuWjlUuLbNu9Yxuv/OURABo1blLjuYkkOxUNqZOGXDku0SmI1Eo6pyEiIpGpaIiISGSanpIjxkPz1yQ6haT11rrtkdr1bd+8Ssdt27YtUVYzaNu2LevXr6/SsaV6qGiISLV5YVwvMjJ+XmY7G3hrDWQjVUFFQ0SqzaJ/LmLW7NkAjB49ipNbn8wDDz5YuD0joyfDLxyeqPSkAlQ0RKTa3HTjTYfEJk6YkIBMpKroRLiIiESmoiEiIpGpaIiISGQqGiIiEpmKhoiIRKaiISIikVW4aJhZGzN7w8xWm9l7ZnZjiE80s01mtjw8hsXtc4uZ5ZrZh2Y2JC4+NMRyzWx8XLydmS0J8b+aWYOK5isiIpVXmSON/cCv3b0z0Ae4wcw6h20PuXv38JgDELaNAtKBocCjZlbPzOoBfwIuADoDo+P6+Y/Q12nAF8DYSuQrIiKVVOGi4e6b3X1ZeP4l8D6QUsouI4AZ7v6Nu39MbJ3w3uGR6+7r3H0fMAMYYbEb1gwEng/7TwUurmi+IiJSeVVyTsPM0oAewJIQGmdmK81sipmdEGIpwMa43fJC7HDx5sBOd99fLF7S+Jlmlm1m2du2bauCdyQiIiWpdNEws8bAC8BN7r4beAw4FegObAYeqOwYZXH3LHfv5e69WrZsWd3DiYjUWZUqGmZ2NLGC8bS7vwjg7lvc/YC7HwT+TGz6CWAT0CZu99QQO1x8O9DUzOoXi4uUqMHUi+jzSRYNpl7Ernu6Fj76fJJF6uu/LHwtyWnixIkAnHzyyZgZZkZGRgYAmZmZhbEot1qX6mPuXrEdYz+5qcAOd78pLt7a3TeH5zcDZ7n7KDNLB54hVkROBl4HOgAGrAEGESsKS4Er3P09M3sOeMHdZ5jZ48BKd3+0tLx69erl2dnZFXpPUrtNHNAwUrtGjZvQf+SYas6mdqrq9TSiSvvZf7Fhw4ZIbVNSUsjLy6vmjOoeM8tx915ltavMXW7PAa4E3jWz5SF2K7Grn7oDDqwHfg4QisCzwGpiV17d4O4HQrLjgFeAesAUd38v9PdbYIaZ3Q28AzxZiXylDtDa37VTeRZg0pFGYlW4aLj7P4kdJRQ3p5R97gHuKSE+p6T93H0d301viYhIgmk9DRFJvDfuTXQGEpFuIyIiIpGpaIiISGQqGiIiEpmKhoiIRKaiISIikaloiIhIZLrkVpLeQ/PXJDqFOuOtddsjtUvUJ8cl8VQ0JOnte/gcTm7/fU7rdhYLn5/CN3u/BuC4Zi3p+6PLeW/xAvLWrk5wliJ1g4qGJL29e77ko5VL+Wjl0iLx3Tu28cpfHil83ahxk5pOTaTOUdGQWkH3lBJJDioaIlKrTDj3mGi3HfnhLdWfTB2kq6dERCQyHWmISK2Tk5PDrNmzC1+PHj2Kk1ufzAMPPlgY+3T6x2RlZSUivSOaioaI1CpNmx5fpGAATJ8+45B2r776ak2lVKeoaEjC3DmwEQcPlr1ypBbdkXg33XhT2Y2ASQNvreZM6iYVDUmYgwediRMmlNku6gfOROLphHn1SPqiYWZDgYeJLQX7hLvfl+CUpAqpINROteGT40cdZUycNKnMdneffzv79++vgYyODEldNMysHvAn4HwgD1hqZjPdXR//TVL3nN+Yb7+N9h+wQcOG1ZyN1GV33H5HtIaTJumIpBySumgQWx88N6wVjpnNAEYAKho1bOKAaL/gzUwfxJNCteGIJLKoS9Ie4cXF3Ms+EZkoZnYJMNTdrw2vrwTOcvdxxdplApnhZUfgwwoO2QL4vIL7ViflVT7Kq/ySNTflVT6Vyautu7csq1GyH2lE4u5ZQKUvyDazbHfvVQUpVSnlVT7Kq/ySNTflVT41kVeyfyJ8E9Am7nVqiImISAIke9FYCnQws3Zm1gAYBcxMcE4iInVWUk9Puft+MxsHvELsktsp7v5eNQ6ZrPccUF7lo7zKL1lzU17lU+15JfWJcBERSS7JPj0lIiJJREVDREQiU9Eoxsy6m9liM1tuZtlm1jvRORUws381sw/M7D0z+89E5xPPzH5tZm5mLRKdC4CZ/SF8r1aa2Utm1jTB+Qw1sw/NLNfMxicylwJm1sbM3jCz1eHf1I2JzimemdUzs3fMbHbZrWuGmTU1s+fDv633zaxvonMCMLObw89wlZlNN7Nqu92Cisah/hOY5O7dgTvC64Qzsx8S+zR8N3dPB+5PcEqFzKwNMBj4JNG5xJkPdHH3rsAaIGEf0427Hc4FQGdgtJl1TlQ+cfYDv3b3zkAf4IYkyavAjcD7iU6imIeBee7+faAbSZCfmaUAvwR6uXsXYhcNjaqu8VQ0DuXAceH58cCnCcwl3r8A97n7NwDuvjXB+cR7CPgNse9dUnD3V9294CZYi4l9xidRCm+H4+77gILb4SSUu29292Xh+ZfEfgGmJDarGDNLBX4EPJHoXAqY2fFAf+BJAHff5+47E5tVofpAIzOrDxxLNf7eUtE41E3AH8xsI7G/5pPlRjKnA/3MbImZ/d3Mzkx0QgBmNgLY5O4rEp1LKa4B5iZw/BRgY9zrPJLkl3MBM0sDegBLEptJoT8S+0PkYKITidMO2Ab8d5g2e8LMvpfopNx9E7HfVZ8Am4Fd7l5tK1Al9ec0qouZvQacVMKm24BBwM3u/oKZXUbsr4rzkiCv+kAzYtMIZwLPmll7r4FrpsvI61ZiU1M1rrS83P3l0OY2YtMwT9dkbrWJmTUGXgBucvfdSZDPhcBWd88xswGJzidOfaAn8K/uvsTMHgbGA7cnMikzO4HYkWs7YCfwnJn91N3/pzrGq5NFw90PWwTMbBqxuVSA56jBw+My8voX4MVQJN42s4PEbk62LVF5mdkZxP6hrgir66UCy8yst7t/lqi84vK7GrgQGFQTxbUUSXs7HDM7mljBeNrdX0x0PsE5wEVmNgxoCBxnZv/j7j9NcF55QJ67FxyNPU+saCTaecDH7r4NwMxeBM4GqqVoaHrqUJ8C54bnA4G1Ccwl3t+AHwKY2elAAxJ8l013f9fdW7l7mrunEftP1bMmCkZZwuJdvwEucvevE5xOUt4Ox2KV/kngfXd/MNH5FHD3W9w9NfybGgUsSIKCQfh3vdHMOobQIJJjmYZPgD5mdmz4mQ6iGk/Q18kjjTJcBzwcTijl890t1xNtCjDFzFYB+4AxCf7rOdk9AhwDzA9HQYvd/fpEJJKA2+FEdQ5wJfCumS0PsVvdfU4Cc0p2/wo8HYr/OuBnCc6HMFX2PLCM2FTsO1Tj7UR0GxEREYlM01MiIhKZioaIiESmoiEiIpGpaIiISGQqGiIiEpmKhoiIRKaiISIikf1/6qdo3tCgplAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-8, 8, 31)\n",
    "plt.hist(X0_val, bins=bins, alpha=0.5, label=r'$\\mu=0$, $\\sigma=1$')\n",
    "plt.hist(X0_val,\n",
    "         bins=bins,\n",
    "         label=r'$\\mu=0$, $\\sigma=1$ DCTR wgt.',\n",
    "         weights=weights,\n",
    "         histtype='step',\n",
    "         color='k')\n",
    "plt.hist(X0_val,\n",
    "         bins=bins,\n",
    "         label=r'$\\mu=0$, $\\sigma=1$ analytical wgt.',\n",
    "         weights=analytical_weights,\n",
    "         histtype='step',\n",
    "         linestyle='--',\n",
    "         color='k')\n",
    "plt.hist(X1_val,\n",
    "         bins=bins,\n",
    "         alpha=0.5,\n",
    "         label=r'$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "plt.legend()\n",
    "plt.title(\"Truth Level: Reweighting\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:21.552251Z",
     "start_time": "2020-06-09T07:59:21.545187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"2d_gaussian_dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"2d_gaussian_dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"2d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"2d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:22.202630Z",
     "start_time": "2020-06-09T07:59:21.557106Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "theta0_param = (\n",
    "    0, 1\n",
    ")  # this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = (1, 1.5)  # this is the data (the target)\n",
    "\n",
    "theta0 = np.random.normal(theta0_param[0], theta0_param[1], N)\n",
    "theta1 = np.random.normal(theta1_param[0], theta1_param[1], N)\n",
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0, theta1])\n",
    "yvals = np.concatenate([labels0, labels1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals,\n",
    "                                                    yvals,\n",
    "                                                    test_size=0.5)\n",
    "X_train_theta, y_train_theta = shuffle(xvals, yvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Section for $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:22.294880Z",
     "start_time": "2020-06-09T07:59:22.207495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1, ), dtype=tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs,\n",
    "                    val=0.,\n",
    "                    reweight_analytically=False,\n",
    "                    MSE_loss=True):\n",
    "    x = inputs\n",
    "    x = K.gather(x, np.arange(batch_size))\n",
    "\n",
    "    theta_prime = [val, theta1_param[1]]  # fixed theta_sigma = sigma_truth\n",
    "\n",
    "    if reweight_analytically:\n",
    "        # analytical reweight\n",
    "        weights = analytical_reweight(events=x, param=theta_prime)\n",
    "    else:\n",
    "        # NN (DCTR) reweight\n",
    "        weights = reweight(events=x, param=theta_prime)\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:21:13.545829Z",
     "start_time": "2020-06-09T07:59:22.298405Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : -2.0\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1155 - acc: 0.5775 - val_loss: 0.1138 - val_acc: 0.5734\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5753 - val_loss: 0.1138 - val_acc: 0.5814\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5759 - val_loss: 0.1138 - val_acc: 0.5726\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5760 - val_loss: 0.1138 - val_acc: 0.5776\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5763 - val_loss: 0.1138 - val_acc: 0.5760\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5759 - val_loss: 0.1138 - val_acc: 0.5749\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5763 - val_loss: 0.1139 - val_acc: 0.5733\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5761 - val_loss: 0.1138 - val_acc: 0.5768\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5763 - val_loss: 0.1138 - val_acc: 0.5772\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5759 - val_loss: 0.1139 - val_acc: 0.5760\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5756 - val_loss: 0.1138 - val_acc: 0.5762\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5764 - val_loss: 0.1138 - val_acc: 0.5768\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5758 - val_loss: 0.1139 - val_acc: 0.5780\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1139 - acc: 0.5762 - val_loss: 0.1138 - val_acc: 0.5808\n",
      "[0.11389913830906152]\n",
      "testing theta = : -1.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1270 - acc: 0.5890 - val_loss: 0.1269 - val_acc: 0.5889\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5888 - val_loss: 0.1269 - val_acc: 0.5876\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5886 - val_loss: 0.1269 - val_acc: 0.5888\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5881 - val_loss: 0.1270 - val_acc: 0.5956\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5892 - val_loss: 0.1269 - val_acc: 0.5865\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5882 - val_loss: 0.1269 - val_acc: 0.5930\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5888 - val_loss: 0.1269 - val_acc: 0.5919\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5891 - val_loss: 0.1269 - val_acc: 0.5891\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5891 - val_loss: 0.1269 - val_acc: 0.5851\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5887 - val_loss: 0.1269 - val_acc: 0.5902\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5889 - val_loss: 0.1269 - val_acc: 0.5862\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5883 - val_loss: 0.1269 - val_acc: 0.5899\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5888 - val_loss: 0.1269 - val_acc: 0.5871\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5887 - val_loss: 0.1269 - val_acc: 0.5863\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1270 - acc: 0.5891 - val_loss: 0.1269 - val_acc: 0.5908\n",
      "[0.11389913830906152, 0.12697613140940667]\n",
      "testing theta = : -1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1408 - acc: 0.6005 - val_loss: 0.1407 - val_acc: 0.5987\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.6007 - val_loss: 0.1407 - val_acc: 0.5995\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.6000 - val_loss: 0.1407 - val_acc: 0.6039\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.6014 - val_loss: 0.1407 - val_acc: 0.6003\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.6000 - val_loss: 0.1407 - val_acc: 0.6004\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.6010 - val_loss: 0.1407 - val_acc: 0.5998\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.5998 - val_loss: 0.1407 - val_acc: 0.6011\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.6005 - val_loss: 0.1407 - val_acc: 0.5983\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.6004 - val_loss: 0.1407 - val_acc: 0.5990\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1408 - acc: 0.6007 - val_loss: 0.1407 - val_acc: 0.5998\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766]\n",
      "testing theta = : -1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1556 - acc: 0.6135 - val_loss: 0.1555 - val_acc: 0.6147\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1556 - acc: 0.6133 - val_loss: 0.1556 - val_acc: 0.6141\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1556 - acc: 0.6142 - val_loss: 0.1555 - val_acc: 0.6115\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1556 - acc: 0.6125 - val_loss: 0.1555 - val_acc: 0.6131\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1556 - acc: 0.6135 - val_loss: 0.1555 - val_acc: 0.6141\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1556 - acc: 0.6137 - val_loss: 0.1556 - val_acc: 0.6134\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1556 - acc: 0.6138 - val_loss: 0.1555 - val_acc: 0.6169\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1556 - acc: 0.6131 - val_loss: 0.1555 - val_acc: 0.6126\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1556 - acc: 0.6129 - val_loss: 0.1555 - val_acc: 0.6110\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1556 - acc: 0.6127 - val_loss: 0.1555 - val_acc: 0.6112\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089]\n",
      "testing theta = : -1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1707 - acc: 0.6260 - val_loss: 0.1706 - val_acc: 0.6264\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1707 - acc: 0.6257 - val_loss: 0.1706 - val_acc: 0.6278\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1707 - acc: 0.6259 - val_loss: 0.1706 - val_acc: 0.6274\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1707 - acc: 0.6259 - val_loss: 0.1706 - val_acc: 0.6261\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1707 - acc: 0.6257 - val_loss: 0.1706 - val_acc: 0.6267\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1707 - acc: 0.6259 - val_loss: 0.1706 - val_acc: 0.6239\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1707 - acc: 0.6248 - val_loss: 0.1706 - val_acc: 0.6250\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1707 - acc: 0.6254 - val_loss: 0.1705 - val_acc: 0.6264\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1707 - acc: 0.6262 - val_loss: 0.1706 - val_acc: 0.6250\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1707 - acc: 0.6257 - val_loss: 0.1706 - val_acc: 0.6264\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1707 - acc: 0.6257 - val_loss: 0.1708 - val_acc: 0.6281\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1707 - acc: 0.6261 - val_loss: 0.1705 - val_acc: 0.6261\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1707 - acc: 0.6261 - val_loss: 0.1706 - val_acc: 0.6250\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434]\n",
      "testing theta = : -0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1859 - acc: 0.6370 - val_loss: 0.1857 - val_acc: 0.6393\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1859 - acc: 0.6376 - val_loss: 0.1858 - val_acc: 0.6391\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1859 - acc: 0.6370 - val_loss: 0.1857 - val_acc: 0.6380\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1859 - acc: 0.6369 - val_loss: 0.1857 - val_acc: 0.6379\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1859 - acc: 0.6371 - val_loss: 0.1857 - val_acc: 0.6395\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1859 - acc: 0.6371 - val_loss: 0.1857 - val_acc: 0.6379\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1859 - acc: 0.6366 - val_loss: 0.1857 - val_acc: 0.6393\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1859 - acc: 0.6370 - val_loss: 0.1857 - val_acc: 0.6379\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1859 - acc: 0.6369 - val_loss: 0.1857 - val_acc: 0.6382\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1859 - acc: 0.6371 - val_loss: 0.1857 - val_acc: 0.6386\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478]\n",
      "testing theta = : -0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2005 - acc: 0.6476 - val_loss: 0.2003 - val_acc: 0.6515\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2004 - acc: 0.6486 - val_loss: 0.2003 - val_acc: 0.6498\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2004 - acc: 0.6484 - val_loss: 0.2003 - val_acc: 0.6495\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2005 - acc: 0.6482 - val_loss: 0.2003 - val_acc: 0.6496\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2004 - acc: 0.6487 - val_loss: 0.2003 - val_acc: 0.6442\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2004 - acc: 0.6486 - val_loss: 0.2003 - val_acc: 0.6490\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2004 - acc: 0.6485 - val_loss: 0.2003 - val_acc: 0.6478\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2004 - acc: 0.6485 - val_loss: 0.2004 - val_acc: 0.6502\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2005 - acc: 0.6484 - val_loss: 0.2004 - val_acc: 0.6514\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2004 - acc: 0.6486 - val_loss: 0.2003 - val_acc: 0.6499\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2004 - acc: 0.6484 - val_loss: 0.2003 - val_acc: 0.6516\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2004 - acc: 0.6486 - val_loss: 0.2003 - val_acc: 0.6511\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265]\n",
      "testing theta = : -0.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2137 - acc: 0.6572 - val_loss: 0.2135 - val_acc: 0.6553\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2137 - acc: 0.6569 - val_loss: 0.2135 - val_acc: 0.6583\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2137 - acc: 0.6575 - val_loss: 0.2135 - val_acc: 0.6562\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2137 - acc: 0.6572 - val_loss: 0.2135 - val_acc: 0.6581\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2137 - acc: 0.6571 - val_loss: 0.2135 - val_acc: 0.6571\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2137 - acc: 0.6570 - val_loss: 0.2135 - val_acc: 0.6561\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2137 - acc: 0.6570 - val_loss: 0.2135 - val_acc: 0.6559\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2137 - acc: 0.6569 - val_loss: 0.2135 - val_acc: 0.6555\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2137 - acc: 0.6566 - val_loss: 0.2135 - val_acc: 0.6589\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2137 - acc: 0.6570 - val_loss: 0.2135 - val_acc: 0.6573\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887]\n",
      "testing theta = : 0.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2262 - acc: 0.6640 - val_loss: 0.2260 - val_acc: 0.6607\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2262 - acc: 0.6634 - val_loss: 0.2259 - val_acc: 0.6617\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2262 - acc: 0.6630 - val_loss: 0.2261 - val_acc: 0.6655\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2262 - acc: 0.6629 - val_loss: 0.2261 - val_acc: 0.6600\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2262 - acc: 0.6629 - val_loss: 0.2260 - val_acc: 0.6650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2262 - acc: 0.6629 - val_loss: 0.2260 - val_acc: 0.6634\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2262 - acc: 0.6630 - val_loss: 0.2260 - val_acc: 0.6607\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625]\n",
      "testing theta = : 0.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2364 - acc: 0.6681 - val_loss: 0.2361 - val_acc: 0.6667\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2364 - acc: 0.6675 - val_loss: 0.2361 - val_acc: 0.6690\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2364 - acc: 0.6682 - val_loss: 0.2363 - val_acc: 0.6678\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2364 - acc: 0.6672 - val_loss: 0.2362 - val_acc: 0.6708\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2364 - acc: 0.6680 - val_loss: 0.2361 - val_acc: 0.6677\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2364 - acc: 0.6671 - val_loss: 0.2362 - val_acc: 0.6675\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2364 - acc: 0.6675 - val_loss: 0.2361 - val_acc: 0.6664\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2364 - acc: 0.6670 - val_loss: 0.2363 - val_acc: 0.6701\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2364 - acc: 0.6671 - val_loss: 0.2361 - val_acc: 0.6655\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2364 - acc: 0.6667 - val_loss: 0.2363 - val_acc: 0.6709\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852]\n",
      "testing theta = : 0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2441 - acc: 0.6702 - val_loss: 0.2437 - val_acc: 0.6705\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2440 - acc: 0.6700 - val_loss: 0.2438 - val_acc: 0.6702\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2440 - acc: 0.6707 - val_loss: 0.2438 - val_acc: 0.6699\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6697 - val_loss: 0.2437 - val_acc: 0.6699\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6700 - val_loss: 0.2438 - val_acc: 0.6687\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6700 - val_loss: 0.2439 - val_acc: 0.6709\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852, 0.24404856051504611]\n",
      "testing theta = : 0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.6649 - val_loss: 0.2485 - val_acc: 0.6549\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.6645 - val_loss: 0.2484 - val_acc: 0.6703\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6653 - val_loss: 0.2485 - val_acc: 0.6670\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6685 - val_loss: 0.2486 - val_acc: 0.6660\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6688 - val_loss: 0.2485 - val_acc: 0.6680\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6671 - val_loss: 0.2486 - val_acc: 0.6695\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6666 - val_loss: 0.2485 - val_acc: 0.6693\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852, 0.24404856051504611, 0.24865865160524844]\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.4996 - val_loss: 0.2500 - val_acc: 0.3513\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.4290 - val_loss: 0.2500 - val_acc: 0.4991\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2501 - acc: 0.4494 - val_loss: 0.2500 - val_acc: 0.3433\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.4498 - val_loss: 0.2503 - val_acc: 0.4864\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.4693 - val_loss: 0.2500 - val_acc: 0.3862\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.4606 - val_loss: 0.2499 - val_acc: 0.5023\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.4411 - val_loss: 0.2500 - val_acc: 0.3842\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.4143 - val_loss: 0.2498 - val_acc: 0.4998\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.4486 - val_loss: 0.2499 - val_acc: 0.5127\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.4411 - val_loss: 0.2500 - val_acc: 0.3690\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2500 - acc: 0.4278 - val_loss: 0.2499 - val_acc: 0.5025\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.4263 - val_loss: 0.2500 - val_acc: 0.3480\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.4259 - val_loss: 0.2499 - val_acc: 0.4984\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852, 0.24404856051504611, 0.24865865160524844, 0.24998557727038861]\n",
      "testing theta = : 1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2491 - acc: 0.3450 - val_loss: 0.2483 - val_acc: 0.3494\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.3415 - val_loss: 0.2479 - val_acc: 0.3391\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.3380 - val_loss: 0.2481 - val_acc: 0.3320\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3367 - val_loss: 0.2479 - val_acc: 0.3381\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2478 - acc: 0.3410 - val_loss: 0.2479 - val_acc: 0.3367\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.3377 - val_loss: 0.2481 - val_acc: 0.3402\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.3391 - val_loss: 0.2482 - val_acc: 0.3343\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.3375 - val_loss: 0.2482 - val_acc: 0.3314\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.3380 - val_loss: 0.2480 - val_acc: 0.3389\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.3384 - val_loss: 0.2481 - val_acc: 0.3374\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852, 0.24404856051504611, 0.24865865160524844, 0.24998557727038861, 0.24784427519142627]\n",
      "testing theta = : 1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2446 - acc: 0.3426 - val_loss: 0.2456 - val_acc: 0.3500\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2434 - acc: 0.3395 - val_loss: 0.2431 - val_acc: 0.3424\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2441 - acc: 0.3396 - val_loss: 0.2428 - val_acc: 0.3386\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2437 - acc: 0.3401 - val_loss: 0.2434 - val_acc: 0.3325\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2432 - acc: 0.3383 - val_loss: 0.2427 - val_acc: 0.3350\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2438 - acc: 0.3374 - val_loss: 0.2430 - val_acc: 0.3373\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2432 - acc: 0.3404 - val_loss: 0.2432 - val_acc: 0.3346\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2435 - acc: 0.3384 - val_loss: 0.2430 - val_acc: 0.3369\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2432 - acc: 0.3393 - val_loss: 0.2429 - val_acc: 0.3384\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2435 - acc: 0.3395 - val_loss: 0.2435 - val_acc: 0.3319\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852, 0.24404856051504611, 0.24865865160524844, 0.24998557727038861, 0.24784427519142627, 0.24318512739241124]\n",
      "testing theta = : 1.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2353 - acc: 0.3427 - val_loss: 0.2353 - val_acc: 0.3354\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2347 - acc: 0.3432 - val_loss: 0.2356 - val_acc: 0.3452\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2369 - acc: 0.3409 - val_loss: 0.2350 - val_acc: 0.3454\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2352 - acc: 0.3433 - val_loss: 0.2346 - val_acc: 0.3476\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2344 - acc: 0.3444 - val_loss: 0.2351 - val_acc: 0.3435\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2367 - acc: 0.3489 - val_loss: 0.2362 - val_acc: 0.3390\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2357 - acc: 0.3443 - val_loss: 0.2355 - val_acc: 0.3325\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2355 - acc: 0.3420 - val_loss: 0.2364 - val_acc: 0.3499\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2359 - acc: 0.3444 - val_loss: 0.2354 - val_acc: 0.3447\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852, 0.24404856051504611, 0.24865865160524844, 0.24998557727038861, 0.24784427519142627, 0.24318512739241124, 0.23436772483587265]\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2284 - acc: 0.3490 - val_loss: 0.2251 - val_acc: 0.3499\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2242 - acc: 0.3484 - val_loss: 0.2243 - val_acc: 0.3407\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2241 - acc: 0.3500 - val_loss: 0.2348 - val_acc: 0.3554\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2268 - acc: 0.3478 - val_loss: 0.2248 - val_acc: 0.3517\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2246 - acc: 0.3501 - val_loss: 0.2250 - val_acc: 0.3532\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2269 - acc: 0.3480 - val_loss: 0.2256 - val_acc: 0.3444\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2255 - acc: 0.3496 - val_loss: 0.2253 - val_acc: 0.3429\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852, 0.24404856051504611, 0.24865865160524844, 0.24998557727038861, 0.24784427519142627, 0.24318512739241124, 0.23436772483587265, 0.22410120792686938]\n",
      "[0.11389913830906152, 0.12697613140940667, 0.14077998942136766, 0.1556224104911089, 0.17069610522687434, 0.18587440206110478, 0.2004408008903265, 0.21366085907816887, 0.22617429941892625, 0.2363775470852852, 0.24404856051504611, 0.24865865160524844, 0.24998557727038861, 0.24784427519142627, 0.24318512739241124, 0.23436772483587265, 0.22410120792686938]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2, 2, 17)\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "earlystopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=my_loss_wrapper(myinputs, theta),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train),\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              batch_size=1000,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1,\n",
    "              callbacks=[earlystopping])\n",
    "    lvals += [np.min(model.history.history['loss'])]\n",
    "    vlvals += [np.min(model.history.history['val_loss'])]\n",
    "    print(lvals)\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:21:13.896996Z",
     "start_time": "2020-06-09T08:21:13.551140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.title(\"$\\mu$ Cross Section\\nFixed $\\sigma = \\sigma_{Truth}$\")\n",
    "plt.xlabel(r'$\\theta_{\\mu}$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param[0],\n",
    "           ymin=np.min(lvals),\n",
    "           ymax=np.max(lvals),\n",
    "           label='Truth')\n",
    "plt.legend()\n",
    "#plt.savefig(\"GaussianAltFit-2D-\\mu cross section.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Section for $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:21:13.989330Z",
     "start_time": "2020-06-09T08:21:13.901485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1, ), dtype=tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs,\n",
    "                    val=0.,\n",
    "                    reweight_analytically=False,\n",
    "                    MSE_loss=True):\n",
    "    x = inputs\n",
    "    x = K.gather(x, np.arange(batch_size))\n",
    "\n",
    "    theta_prime = [theta1_param[0], val]  # fixed theta_mu = mu_truth\n",
    "\n",
    "    if reweight_analytically:\n",
    "        # analytical reweight\n",
    "        weights = analytical_reweight(events=x, param=theta_prime)\n",
    "    else:\n",
    "        # NN (DCTR) reweight\n",
    "        weights = reweight(events=x, param=theta_prime)\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_true * (y_true - y_pred)**2 + weights * (\n",
    "                1. - y_true) * (y_true - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_true) * K.log(y_pred) + weights *\n",
    "                       (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:46:46.937743Z",
     "start_time": "2020-06-09T08:21:13.994154Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : 0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1755 - acc: 0.4915 - val_loss: 0.1731 - val_acc: 0.4885\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1730 - acc: 0.4873 - val_loss: 0.1734 - val_acc: 0.4922\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1730 - acc: 0.4874 - val_loss: 0.1730 - val_acc: 0.4880\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1729 - acc: 0.4876 - val_loss: 0.1733 - val_acc: 0.4879\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1729 - acc: 0.4873 - val_loss: 0.1730 - val_acc: 0.4873\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1729 - acc: 0.4878 - val_loss: 0.1730 - val_acc: 0.4883\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1729 - acc: 0.4872 - val_loss: 0.1730 - val_acc: 0.4874\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1729 - acc: 0.4873 - val_loss: 0.1731 - val_acc: 0.4888\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1729 - acc: 0.4875 - val_loss: 0.1730 - val_acc: 0.4896\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1729 - acc: 0.4877 - val_loss: 0.1731 - val_acc: 0.4884\n",
      "[0.17286657012999057]\n",
      "testing theta = : 0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2119 - acc: 0.4886 - val_loss: 0.2119 - val_acc: 0.4916\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4890 - val_loss: 0.2119 - val_acc: 0.4983\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4892 - val_loss: 0.2118 - val_acc: 0.4883\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4894 - val_loss: 0.2117 - val_acc: 0.4881\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4896 - val_loss: 0.2118 - val_acc: 0.4909\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4897 - val_loss: 0.2117 - val_acc: 0.4938\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4895 - val_loss: 0.2118 - val_acc: 0.4938\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4890 - val_loss: 0.2117 - val_acc: 0.4934\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4895 - val_loss: 0.2119 - val_acc: 0.4830\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4899 - val_loss: 0.2117 - val_acc: 0.4853\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4891 - val_loss: 0.2117 - val_acc: 0.4918\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4896 - val_loss: 0.2118 - val_acc: 0.4939\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4891 - val_loss: 0.2116 - val_acc: 0.4895\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4901 - val_loss: 0.2116 - val_acc: 0.4892\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4894 - val_loss: 0.2117 - val_acc: 0.4870\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4893 - val_loss: 0.2117 - val_acc: 0.4927\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4900 - val_loss: 0.2116 - val_acc: 0.4893\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2117 - acc: 0.4892 - val_loss: 0.2116 - val_acc: 0.4878\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4892 - val_loss: 0.2117 - val_acc: 0.4851\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4884 - val_loss: 0.2117 - val_acc: 0.4953\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4901 - val_loss: 0.2119 - val_acc: 0.4794\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4883 - val_loss: 0.2116 - val_acc: 0.4913\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4894 - val_loss: 0.2116 - val_acc: 0.4871\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4895 - val_loss: 0.2116 - val_acc: 0.4891\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4891 - val_loss: 0.2117 - val_acc: 0.4840\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4891 - val_loss: 0.2116 - val_acc: 0.4870\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2116 - acc: 0.4890 - val_loss: 0.2117 - val_acc: 0.4930\n",
      "[0.17286657012999057, 0.21162624084949494]\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2347 - acc: 0.4913 - val_loss: 0.2344 - val_acc: 0.4939\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2346 - acc: 0.4935 - val_loss: 0.2344 - val_acc: 0.4860\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2346 - acc: 0.4923 - val_loss: 0.2348 - val_acc: 0.4985\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2346 - acc: 0.4927 - val_loss: 0.2347 - val_acc: 0.5041\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2346 - acc: 0.4930 - val_loss: 0.2344 - val_acc: 0.4907\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2346 - acc: 0.4933 - val_loss: 0.2344 - val_acc: 0.4876\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694]\n",
      "testing theta = : 1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2469 - acc: 0.4846 - val_loss: 0.2467 - val_acc: 0.4815\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2468 - acc: 0.4891 - val_loss: 0.2470 - val_acc: 0.4928\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2468 - acc: 0.4850 - val_loss: 0.2468 - val_acc: 0.5060\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4881 - val_loss: 0.2465 - val_acc: 0.4931\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4903 - val_loss: 0.2465 - val_acc: 0.4841\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4896 - val_loss: 0.2465 - val_acc: 0.4842\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2468 - acc: 0.4853 - val_loss: 0.2465 - val_acc: 0.5014\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4919 - val_loss: 0.2466 - val_acc: 0.5003\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4911 - val_loss: 0.2465 - val_acc: 0.4917\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4891 - val_loss: 0.2465 - val_acc: 0.4811\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2468 - acc: 0.4883 - val_loss: 0.2467 - val_acc: 0.5108\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4899 - val_loss: 0.2465 - val_acc: 0.4868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4881 - val_loss: 0.2464 - val_acc: 0.4900\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2467 - acc: 0.4885 - val_loss: 0.2477 - val_acc: 0.4968\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4873 - val_loss: 0.2465 - val_acc: 0.4821\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2468 - acc: 0.4855 - val_loss: 0.2465 - val_acc: 0.4987\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4903 - val_loss: 0.2465 - val_acc: 0.4851\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2468 - acc: 0.4914 - val_loss: 0.2465 - val_acc: 0.4859\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497]\n",
      "testing theta = : 1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2507 - acc: 0.4576 - val_loss: 0.2500 - val_acc: 0.3582\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.4261 - val_loss: 0.2500 - val_acc: 0.4286\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2502 - acc: 0.4224 - val_loss: 0.2502 - val_acc: 0.4872\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2502 - acc: 0.4431 - val_loss: 0.2499 - val_acc: 0.3354\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2500 - acc: 0.4147 - val_loss: 0.2499 - val_acc: 0.4531\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2500 - acc: 0.4449 - val_loss: 0.2501 - val_acc: 0.3579\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2503 - acc: 0.4400 - val_loss: 0.2500 - val_acc: 0.3637\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2499 - acc: 0.4359 - val_loss: 0.2499 - val_acc: 0.4630\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2503 - acc: 0.4105 - val_loss: 0.2499 - val_acc: 0.4134\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958]\n",
      "testing theta = : 1.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2475 - acc: 0.4829 - val_loss: 0.2463 - val_acc: 0.4726\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2469 - acc: 0.4761 - val_loss: 0.2465 - val_acc: 0.4652\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2467 - acc: 0.4805 - val_loss: 0.2462 - val_acc: 0.4654\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2467 - acc: 0.4777 - val_loss: 0.2470 - val_acc: 0.4917\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.4748 - val_loss: 0.2463 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2462 - acc: 0.4795 - val_loss: 0.2461 - val_acc: 0.4962\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2464 - acc: 0.4774 - val_loss: 0.2462 - val_acc: 0.4531\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2470 - acc: 0.4781 - val_loss: 0.2462 - val_acc: 0.4500\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2463 - acc: 0.4782 - val_loss: 0.2463 - val_acc: 0.4910\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2468 - acc: 0.4820 - val_loss: 0.2467 - val_acc: 0.4930\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2465 - acc: 0.4807 - val_loss: 0.2462 - val_acc: 0.4745\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167]\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2423 - acc: 0.4893 - val_loss: 0.2411 - val_acc: 0.4590\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2413 - acc: 0.4810 - val_loss: 0.2462 - val_acc: 0.5427\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2412 - acc: 0.4800 - val_loss: 0.2403 - val_acc: 0.4869\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2412 - acc: 0.4851 - val_loss: 0.2404 - val_acc: 0.4748\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2413 - acc: 0.4827 - val_loss: 0.2406 - val_acc: 0.4891\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2407 - acc: 0.4899 - val_loss: 0.2401 - val_acc: 0.4761\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2406 - acc: 0.4829 - val_loss: 0.2403 - val_acc: 0.4993\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2400 - acc: 0.4876 - val_loss: 0.2405 - val_acc: 0.4806\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2407 - acc: 0.4891 - val_loss: 0.2403 - val_acc: 0.4804\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2404 - acc: 0.4890 - val_loss: 0.2407 - val_acc: 0.5104\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2408 - acc: 0.4881 - val_loss: 0.2406 - val_acc: 0.4841\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347]\n",
      "testing theta = : 2.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2346 - acc: 0.4817 - val_loss: 0.2321 - val_acc: 0.4938\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2339 - acc: 0.4847 - val_loss: 0.2330 - val_acc: 0.4949\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2329 - acc: 0.4898 - val_loss: 0.2325 - val_acc: 0.4765\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2324 - acc: 0.4866 - val_loss: 0.2410 - val_acc: 0.5007\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2327 - acc: 0.4880 - val_loss: 0.2363 - val_acc: 0.5161\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2323 - acc: 0.4886 - val_loss: 0.2319 - val_acc: 0.4879\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2328 - acc: 0.4920 - val_loss: 0.2322 - val_acc: 0.4895\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2340 - acc: 0.4879 - val_loss: 0.2351 - val_acc: 0.4659\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2334 - acc: 0.4849 - val_loss: 0.2329 - val_acc: 0.5001\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2329 - acc: 0.4905 - val_loss: 0.2325 - val_acc: 0.4804\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2340 - acc: 0.4898 - val_loss: 0.2327 - val_acc: 0.4942\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555]\n",
      "testing theta = : 2.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2280 - acc: 0.4903 - val_loss: 0.2272 - val_acc: 0.5143\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2250 - acc: 0.4892 - val_loss: 0.2245 - val_acc: 0.4816\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2243 - acc: 0.4894 - val_loss: 0.2258 - val_acc: 0.4724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2252 - acc: 0.4849 - val_loss: 0.2239 - val_acc: 0.4987\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2247 - acc: 0.4888 - val_loss: 0.2243 - val_acc: 0.4977\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2243 - acc: 0.4880 - val_loss: 0.2242 - val_acc: 0.4917\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2237 - acc: 0.4887 - val_loss: 0.2244 - val_acc: 0.5060\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2258 - acc: 0.4914 - val_loss: 0.2235 - val_acc: 0.4896\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2246 - acc: 0.4888 - val_loss: 0.2240 - val_acc: 0.4882\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2240 - acc: 0.4923 - val_loss: 0.2267 - val_acc: 0.4991\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2247 - acc: 0.4903 - val_loss: 0.2240 - val_acc: 0.4887\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2247 - acc: 0.4882 - val_loss: 0.2237 - val_acc: 0.4840\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2250 - acc: 0.4868 - val_loss: 0.2238 - val_acc: 0.4894\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012]\n",
      "testing theta = : 2.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2174 - acc: 0.4887 - val_loss: 0.2159 - val_acc: 0.4733\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2155 - acc: 0.4907 - val_loss: 0.2156 - val_acc: 0.4979\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2142 - acc: 0.4948 - val_loss: 0.2158 - val_acc: 0.4933\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2168 - acc: 0.4905 - val_loss: 0.2153 - val_acc: 0.4934\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2165 - acc: 0.4899 - val_loss: 0.2152 - val_acc: 0.4893\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2157 - acc: 0.4909 - val_loss: 0.2237 - val_acc: 0.5076\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2172 - acc: 0.4869 - val_loss: 0.2199 - val_acc: 0.5108\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2155 - acc: 0.4902 - val_loss: 0.2173 - val_acc: 0.4739\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2145 - acc: 0.4873 - val_loss: 0.2153 - val_acc: 0.4683\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2157 - acc: 0.4911 - val_loss: 0.2148 - val_acc: 0.4931\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2161 - acc: 0.4917 - val_loss: 0.2152 - val_acc: 0.4801\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2155 - acc: 0.4924 - val_loss: 0.2149 - val_acc: 0.4899\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2166 - acc: 0.4951 - val_loss: 0.2248 - val_acc: 0.5098\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2163 - acc: 0.4939 - val_loss: 0.2148 - val_acc: 0.4943\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2180 - acc: 0.4954 - val_loss: 0.2195 - val_acc: 0.5037\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614]\n",
      "testing theta = : 3.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2097 - acc: 0.4910 - val_loss: 0.2116 - val_acc: 0.5002\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2082 - acc: 0.4881 - val_loss: 0.2075 - val_acc: 0.4854\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2088 - acc: 0.4919 - val_loss: 0.2075 - val_acc: 0.4885\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2080 - acc: 0.4934 - val_loss: 0.2074 - val_acc: 0.4990\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2082 - acc: 0.4927 - val_loss: 0.2085 - val_acc: 0.5159\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2078 - acc: 0.4949 - val_loss: 0.2112 - val_acc: 0.5023\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2105 - acc: 0.4891 - val_loss: 0.2075 - val_acc: 0.4780\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2109 - acc: 0.4879 - val_loss: 0.2078 - val_acc: 0.4810\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2071 - acc: 0.4880 - val_loss: 0.2089 - val_acc: 0.4937\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614, 0.20714900857210158]\n",
      "testing theta = : 3.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2051 - acc: 0.4916 - val_loss: 0.2125 - val_acc: 0.4967\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2028 - acc: 0.4906 - val_loss: 0.2027 - val_acc: 0.4962\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2027 - acc: 0.4904 - val_loss: 0.2014 - val_acc: 0.4814\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2011 - acc: 0.4912 - val_loss: 0.2009 - val_acc: 0.4947\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2007 - acc: 0.4904 - val_loss: 0.2000 - val_acc: 0.4745\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2006 - acc: 0.4881 - val_loss: 0.1998 - val_acc: 0.4882\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2005 - acc: 0.4894 - val_loss: 0.1999 - val_acc: 0.4867\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2011 - acc: 0.4904 - val_loss: 0.2006 - val_acc: 0.4767\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2007 - acc: 0.4904 - val_loss: 0.2018 - val_acc: 0.4780\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2015 - acc: 0.4857 - val_loss: 0.2017 - val_acc: 0.4830\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2029 - acc: 0.4867 - val_loss: 0.2005 - val_acc: 0.4890\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614, 0.20714900857210158, 0.20047596941888332]\n",
      "testing theta = : 3.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1968 - acc: 0.4933 - val_loss: 0.1972 - val_acc: 0.4995\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1951 - acc: 0.4839 - val_loss: 0.1926 - val_acc: 0.4849\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1931 - acc: 0.4894 - val_loss: 0.1952 - val_acc: 0.4834\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1955 - acc: 0.4852 - val_loss: 0.2007 - val_acc: 0.4994\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1953 - acc: 0.4864 - val_loss: 0.1942 - val_acc: 0.4916\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1939 - acc: 0.4892 - val_loss: 0.1942 - val_acc: 0.4728\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1939 - acc: 0.4879 - val_loss: 0.1940 - val_acc: 0.4913\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614, 0.20714900857210158, 0.20047596941888332, 0.19311883345246314]\n",
      "testing theta = : 3.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.1915 - acc: 0.4918 - val_loss: 0.1875 - val_acc: 0.4726\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1874 - acc: 0.4849 - val_loss: 0.1866 - val_acc: 0.4955\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1871 - acc: 0.4885 - val_loss: 0.1889 - val_acc: 0.4972\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1876 - acc: 0.4888 - val_loss: 0.1862 - val_acc: 0.4948\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1864 - acc: 0.4885 - val_loss: 0.1864 - val_acc: 0.4809\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1900 - acc: 0.4886 - val_loss: 0.1881 - val_acc: 0.4873\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1893 - acc: 0.4856 - val_loss: 0.1879 - val_acc: 0.4901\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1882 - acc: 0.4858 - val_loss: 0.1892 - val_acc: 0.4757\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1878 - acc: 0.4854 - val_loss: 0.1866 - val_acc: 0.4802\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614, 0.20714900857210158, 0.20047596941888332, 0.19311883345246314, 0.18642442533373832]\n",
      "testing theta = : 4.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.1843 - acc: 0.4894 - val_loss: 0.1967 - val_acc: 0.5013\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1825 - acc: 0.4890 - val_loss: 0.1806 - val_acc: 0.4823\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1817 - acc: 0.4882 - val_loss: 0.1933 - val_acc: 0.4971\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1808 - acc: 0.4903 - val_loss: 0.1824 - val_acc: 0.4841\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1830 - acc: 0.4880 - val_loss: 0.1816 - val_acc: 0.4799\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1824 - acc: 0.4893 - val_loss: 0.1827 - val_acc: 0.4828\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1842 - acc: 0.4899 - val_loss: 0.1803 - val_acc: 0.4825\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1832 - acc: 0.4855 - val_loss: 0.1807 - val_acc: 0.4801\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1816 - acc: 0.4889 - val_loss: 0.1841 - val_acc: 0.4837\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1827 - acc: 0.4857 - val_loss: 0.1816 - val_acc: 0.4918\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1816 - acc: 0.4860 - val_loss: 0.1821 - val_acc: 0.4652\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1841 - acc: 0.4889 - val_loss: 0.1859 - val_acc: 0.4540\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614, 0.20714900857210158, 0.20047596941888332, 0.19311883345246314, 0.18642442533373832, 0.18079000690579414]\n",
      "testing theta = : 4.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.1762 - acc: 0.4892 - val_loss: 0.1904 - val_acc: 0.4922\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1792 - acc: 0.4854 - val_loss: 0.1752 - val_acc: 0.4891\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1761 - acc: 0.4869 - val_loss: 0.1773 - val_acc: 0.4789\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1778 - acc: 0.4840 - val_loss: 0.1765 - val_acc: 0.4975\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1765 - acc: 0.4868 - val_loss: 0.1771 - val_acc: 0.4972\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1762 - acc: 0.4888 - val_loss: 0.1772 - val_acc: 0.4893\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1766 - acc: 0.4858 - val_loss: 0.1751 - val_acc: 0.4927\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1770 - acc: 0.4862 - val_loss: 0.1763 - val_acc: 0.4918\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1763 - acc: 0.4857 - val_loss: 0.1746 - val_acc: 0.4850\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1764 - acc: 0.4866 - val_loss: 0.1748 - val_acc: 0.4831\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1752 - acc: 0.4870 - val_loss: 0.1768 - val_acc: 0.4858\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1755 - acc: 0.4882 - val_loss: 0.1810 - val_acc: 0.4832\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1772 - acc: 0.4870 - val_loss: 0.1762 - val_acc: 0.4804\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1766 - acc: 0.4865 - val_loss: 0.1747 - val_acc: 0.4859\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614, 0.20714900857210158, 0.20047596941888332, 0.19311883345246314, 0.18642442533373832, 0.18079000690579414, 0.17519741475582123]\n",
      "testing theta = : 4.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.1746 - acc: 0.4898 - val_loss: 0.1724 - val_acc: 0.4798\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1710 - acc: 0.4862 - val_loss: 0.1724 - val_acc: 0.4984\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1703 - acc: 0.4873 - val_loss: 0.1692 - val_acc: 0.4936\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1709 - acc: 0.4867 - val_loss: 0.1719 - val_acc: 0.5006\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1720 - acc: 0.4854 - val_loss: 0.1711 - val_acc: 0.4864\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1699 - acc: 0.4867 - val_loss: 0.1699 - val_acc: 0.4858\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1698 - acc: 0.4882 - val_loss: 0.1696 - val_acc: 0.4936\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1703 - acc: 0.4905 - val_loss: 0.1699 - val_acc: 0.4958\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614, 0.20714900857210158, 0.20047596941888332, 0.19311883345246314, 0.18642442533373832, 0.18079000690579414, 0.17519741475582123, 0.16984318214654923]\n",
      "[0.17286657012999057, 0.21162624084949494, 0.23458432760834694, 0.24673928970098497, 0.2499499090909958, 0.24619582210481167, 0.24002713783085347, 0.2322637508660555, 0.22366215172410012, 0.21417731469869614, 0.20714900857210158, 0.20047596941888332, 0.19311883345246314, 0.18642442533373832, 0.18079000690579414, 0.17519741475582123, 0.16984318214654923]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.5, 4.5, 17)\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "earlystopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=my_loss_wrapper(myinputs, theta),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train),\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              batch_size=1000,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1,\n",
    "              callbacks=[earlystopping])\n",
    "    lvals += [np.min(model.history.history['loss'])]\n",
    "    vlvals += [np.min(model.history.history['val_loss'])]\n",
    "    print(lvals)\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:46:47.380293Z",
     "start_time": "2020-06-09T08:46:46.942735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.title(\"$\\sigma$ Cross Section\\nFixed $\\mu = \\mu{Truth}$\")\n",
    "plt.xlabel(r'$\\theta_{\\sigma}$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param[1],\n",
    "           ymin=np.min(lvals),\n",
    "           ymax=np.max(lvals),\n",
    "           label='Truth')\n",
    "plt.legend()\n",
    "#plt.savefig(\"GaussianAltFit-2D-\\sigma cross section.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:46:47.397522Z",
     "start_time": "2020-06-09T08:46:47.385910Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = [0., 1.]\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:46:47.543297Z",
     "start_time": "2020-06-09T08:46:47.402446Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 16,899\n",
      "Trainable params: 16,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1, ))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers) - 1].add_weight(\n",
    "    name=\"thetaX\",\n",
    "    shape=(2, ),\n",
    "    initializer=keras.initializers.Constant(value=theta_fit_init),\n",
    "    trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = 2 * N\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "\n",
    "def my_loss_wrapper_fit(\n",
    "        inputs,\n",
    "        mysign=1,  # -1 for training theta, +1 for training g\n",
    "        reweight_analytically=False,\n",
    "        MSE_loss=True):\n",
    "    x = inputs\n",
    "\n",
    "    # Getting theta_prime:\n",
    "    if mysign == 1:\n",
    "        # regular batch size\n",
    "        x = K.gather(x, np.arange(1000))\n",
    "        # when not training theta, fetch as np array\n",
    "        theta_prime = model_fit.layers[-1].get_weights()[0]\n",
    "    else:\n",
    "        # special theta batch size\n",
    "        x = K.gather(x, np.arange(batch_size))\n",
    "        # when training theta, fetch as tf.Variable\n",
    "        theta_prime = model_fit.trainable_weights[-1]\n",
    "\n",
    "    if reweight_analytically:\n",
    "        # analytical reweight\n",
    "        weights = analytical_reweight(x, theta_prime)\n",
    "    else:\n",
    "        # NN reweight\n",
    "        weights = reweight(x, theta_prime)\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_true * (y_true - y_pred)**2 + weights *\n",
    "                               (1. - y_true) * (y_true - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_true) * K.log(y_pred) + weights *\n",
    "                                (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T11:46:55.190935Z",
     "start_time": "2020-06-09T08:46:47.549128Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2117 - acc: 0.6624 - val_loss: 0.2074 - val_acc: 0.6724\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2073 - acc: 0.6726 - val_loss: 0.2072 - val_acc: 0.6727\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2071 - acc: 0.6729 - val_loss: 0.2071 - val_acc: 0.6728\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2071 - acc: 0.6728 - val_loss: 0.2071 - val_acc: 0.6727\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2071 - acc: 0.6729 - val_loss: 0.2071 - val_acc: 0.6726\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2071 - val_acc: 0.6727\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6726\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6729\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6727\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6727\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6729\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6730 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2070 - acc: 0.6730 - val_loss: 0.2070 - val_acc: 0.6727\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6727\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6727\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2071 - val_acc: 0.6725\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6726\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6729 - val_loss: 0.2070 - val_acc: 0.6728\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6729\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6727\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2070 - acc: 0.6728 - val_loss: 0.2070 - val_acc: 0.6727\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: -0.2070 - acc: 0.6728\n",
      ". theta fit =  [0.49996716 1.4999754 ]\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2459 - acc: 0.6669 - val_loss: 0.2437 - val_acc: 0.6700\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6698 - val_loss: 0.2437 - val_acc: 0.6692\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6699 - val_loss: 0.2437 - val_acc: 0.6703\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6698 - val_loss: 0.2438 - val_acc: 0.6700\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6703 - val_loss: 0.2438 - val_acc: 0.6667\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2440 - acc: 0.6693 - val_loss: 0.2437 - val_acc: 0.6709\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6699 - val_loss: 0.2437 - val_acc: 0.6701\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2440 - acc: 0.6696 - val_loss: 0.2437 - val_acc: 0.6696\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6698 - val_loss: 0.2438 - val_acc: 0.6705\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2440 - acc: 0.6695 - val_loss: 0.2437 - val_acc: 0.6707\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2440 - acc: 0.6700 - val_loss: 0.2438 - val_acc: 0.6707\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2440 - acc: 0.6696 - val_loss: 0.2437 - val_acc: 0.6708\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.6698 - val_loss: 0.2437 - val_acc: 0.6695\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: -0.2438 - acc: 0.6700\n",
      ". theta fit =  [0.87198853 1.8717372 ]\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2455 - acc: 0.5573 - val_loss: 0.2442 - val_acc: 0.5290\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2443 - acc: 0.5142 - val_loss: 0.2442 - val_acc: 0.4715\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5059 - val_loss: 0.2443 - val_acc: 0.4698\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2445 - acc: 0.5102 - val_loss: 0.2443 - val_acc: 0.4667\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2444 - acc: 0.5048 - val_loss: 0.2444 - val_acc: 0.5647\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2445 - acc: 0.5167 - val_loss: 0.2443 - val_acc: 0.5325\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2443 - acc: 0.5079 - val_loss: 0.2440 - val_acc: 0.5363\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5124 - val_loss: 0.2444 - val_acc: 0.5528\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2444 - acc: 0.5186 - val_loss: 0.2441 - val_acc: 0.5214\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2443 - acc: 0.5171 - val_loss: 0.2440 - val_acc: 0.5041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5167 - val_loss: 0.2440 - val_acc: 0.4972\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5179 - val_loss: 0.2439 - val_acc: 0.5250\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5190 - val_loss: 0.2439 - val_acc: 0.5130\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5218 - val_loss: 0.2440 - val_acc: 0.4887\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5169 - val_loss: 0.2442 - val_acc: 0.5404\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2441 - acc: 0.5196 - val_loss: 0.2439 - val_acc: 0.5027\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5193 - val_loss: 0.2439 - val_acc: 0.5333\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2443 - acc: 0.5233 - val_loss: 0.2440 - val_acc: 0.5087\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5127 - val_loss: 0.2439 - val_acc: 0.5315\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5205 - val_loss: 0.2440 - val_acc: 0.5419\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5183 - val_loss: 0.2441 - val_acc: 0.5550\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5242 - val_loss: 0.2441 - val_acc: 0.4966\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2439 - acc: 0.5195 - val_loss: 0.2444 - val_acc: 0.5402\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.5208 - val_loss: 0.2438 - val_acc: 0.5316\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5227 - val_loss: 0.2439 - val_acc: 0.5112\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5226 - val_loss: 0.2443 - val_acc: 0.4959\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5194 - val_loss: 0.2440 - val_acc: 0.5612\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5250 - val_loss: 0.2439 - val_acc: 0.4967\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2440 - acc: 0.5189 - val_loss: 0.2441 - val_acc: 0.5338\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2441 - acc: 0.5240 - val_loss: 0.2438 - val_acc: 0.5119\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2439 - acc: 0.5224 - val_loss: 0.2439 - val_acc: 0.5202\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2440 - acc: 0.5239 - val_loss: 0.2439 - val_acc: 0.4961\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2439 - acc: 0.5203 - val_loss: 0.2437 - val_acc: 0.5196\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5206 - val_loss: 0.2439 - val_acc: 0.5117\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5200 - val_loss: 0.2439 - val_acc: 0.5467\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5246 - val_loss: 0.2439 - val_acc: 0.5353\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2437 - acc: 0.5223 - val_loss: 0.2438 - val_acc: 0.5318\n",
      "Epoch 38/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.5255 - val_loss: 0.2442 - val_acc: 0.4902\n",
      "Epoch 39/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5174 - val_loss: 0.2439 - val_acc: 0.5389\n",
      "Epoch 40/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2440 - acc: 0.5214 - val_loss: 0.2438 - val_acc: 0.5408\n",
      "Epoch 41/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2440 - acc: 0.5243 - val_loss: 0.2437 - val_acc: 0.5197\n",
      "Epoch 42/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2440 - acc: 0.5249 - val_loss: 0.2437 - val_acc: 0.5270\n",
      "Epoch 43/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5230 - val_loss: 0.2440 - val_acc: 0.5068\n",
      "Epoch 44/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2439 - acc: 0.5222 - val_loss: 0.2437 - val_acc: 0.5150\n",
      "Epoch 45/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5229 - val_loss: 0.2437 - val_acc: 0.5080\n",
      "Epoch 46/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2438 - acc: 0.5210 - val_loss: 0.2438 - val_acc: 0.5336\n",
      "Epoch 47/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2439 - acc: 0.5208 - val_loss: 0.2444 - val_acc: 0.5511\n",
      "Epoch 48/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2438 - acc: 0.5266 - val_loss: 0.2437 - val_acc: 0.5120\n",
      "Epoch 49/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5236 - val_loss: 0.2438 - val_acc: 0.5318\n",
      "Epoch 50/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2438 - acc: 0.5268 - val_loss: 0.2437 - val_acc: 0.5109\n",
      "Epoch 51/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5213 - val_loss: 0.2440 - val_acc: 0.5035\n",
      "Epoch 52/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5199 - val_loss: 0.2441 - val_acc: 0.5388\n",
      "Epoch 53/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2437 - acc: 0.5212 - val_loss: 0.2445 - val_acc: 0.5378\n",
      "Epoch 54/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2438 - acc: 0.5250 - val_loss: 0.2437 - val_acc: 0.5351\n",
      "Epoch 55/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2438 - acc: 0.5252 - val_loss: 0.2437 - val_acc: 0.5125\n",
      "Epoch 56/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5222 - val_loss: 0.2436 - val_acc: 0.5259\n",
      "Epoch 57/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5221 - val_loss: 0.2446 - val_acc: 0.5450\n",
      "Epoch 58/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2436 - acc: 0.5258 - val_loss: 0.2438 - val_acc: 0.5046\n",
      "Epoch 59/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5288 - val_loss: 0.2449 - val_acc: 0.4816\n",
      "Epoch 60/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5167 - val_loss: 0.2440 - val_acc: 0.5334\n",
      "Epoch 61/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2438 - acc: 0.5241 - val_loss: 0.2436 - val_acc: 0.5204\n",
      "Epoch 62/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5221 - val_loss: 0.2438 - val_acc: 0.5276\n",
      "Epoch 63/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2434 - acc: 0.5228 - val_loss: 0.2437 - val_acc: 0.5096\n",
      "Epoch 64/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2438 - acc: 0.5236 - val_loss: 0.2437 - val_acc: 0.5121\n",
      "Epoch 65/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5213 - val_loss: 0.2441 - val_acc: 0.5318\n",
      "Epoch 66/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2438 - acc: 0.5222 - val_loss: 0.2439 - val_acc: 0.5370\n",
      "Epoch 67/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2434 - acc: 0.5261 - val_loss: 0.2440 - val_acc: 0.5306\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5243 - val_loss: 0.2436 - val_acc: 0.5075\n",
      "Epoch 69/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5209 - val_loss: 0.2437 - val_acc: 0.5230\n",
      "Epoch 70/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2435 - acc: 0.5218 - val_loss: 0.2436 - val_acc: 0.5363\n",
      "Epoch 71/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5263 - val_loss: 0.2438 - val_acc: 0.5239\n",
      "Epoch 72/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5236 - val_loss: 0.2437 - val_acc: 0.5250\n",
      "Epoch 73/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2435 - acc: 0.5279 - val_loss: 0.2436 - val_acc: 0.5217\n",
      "Epoch 74/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5235 - val_loss: 0.2437 - val_acc: 0.5152\n",
      "Epoch 75/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2435 - acc: 0.5215 - val_loss: 0.2437 - val_acc: 0.5373\n",
      "Epoch 76/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5272 - val_loss: 0.2438 - val_acc: 0.5168\n",
      "Epoch 77/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5236 - val_loss: 0.2435 - val_acc: 0.5176\n",
      "Epoch 78/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5231 - val_loss: 0.2435 - val_acc: 0.5344\n",
      "Epoch 79/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5271 - val_loss: 0.2437 - val_acc: 0.5019\n",
      "Epoch 80/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2435 - acc: 0.5213 - val_loss: 0.2439 - val_acc: 0.5190\n",
      "Epoch 81/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2438 - acc: 0.5249 - val_loss: 0.2437 - val_acc: 0.5143\n",
      "Epoch 82/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2433 - acc: 0.5249 - val_loss: 0.2439 - val_acc: 0.5100\n",
      "Epoch 83/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5185 - val_loss: 0.2437 - val_acc: 0.5311\n",
      "Epoch 84/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2434 - acc: 0.5229 - val_loss: 0.2438 - val_acc: 0.5341\n",
      "Epoch 85/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2436 - acc: 0.5287 - val_loss: 0.2436 - val_acc: 0.5109\n",
      "Epoch 86/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2437 - acc: 0.5200 - val_loss: 0.2435 - val_acc: 0.5258\n",
      "Epoch 87/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2435 - acc: 0.5255 - val_loss: 0.2435 - val_acc: 0.5042\n",
      "Epoch 88/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2435 - acc: 0.5197 - val_loss: 0.2435 - val_acc: 0.5299\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.2434 - acc: 0.5341\n",
      ". theta fit =  [1.1911893 1.5523729]\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2494 - acc: 0.3837 - val_loss: 0.2490 - val_acc: 0.3761\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.3591 - val_loss: 0.2486 - val_acc: 0.3508\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.3506 - val_loss: 0.2486 - val_acc: 0.3472\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.3500 - val_loss: 0.2492 - val_acc: 0.3446\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.3505 - val_loss: 0.2503 - val_acc: 0.3558\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.3502 - val_loss: 0.2487 - val_acc: 0.3442\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.3486 - val_loss: 0.2487 - val_acc: 0.3448\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.3486 - val_loss: 0.2487 - val_acc: 0.3387\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.3505 - val_loss: 0.2486 - val_acc: 0.3469\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.3492 - val_loss: 0.2502 - val_acc: 0.3575\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3502 - val_loss: 0.2487 - val_acc: 0.3427\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3491 - val_loss: 0.2485 - val_acc: 0.3463\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2484 - acc: 0.3484 - val_loss: 0.2492 - val_acc: 0.3436\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.3495 - val_loss: 0.2488 - val_acc: 0.3456\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3486 - val_loss: 0.2491 - val_acc: 0.3565\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3500 - val_loss: 0.2487 - val_acc: 0.3499\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3506 - val_loss: 0.2485 - val_acc: 0.3547\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3480 - val_loss: 0.2489 - val_acc: 0.3517\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3492 - val_loss: 0.2489 - val_acc: 0.3584\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3486 - val_loss: 0.2485 - val_acc: 0.3490\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3518 - val_loss: 0.2491 - val_acc: 0.3510\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3495 - val_loss: 0.2491 - val_acc: 0.3573\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3504 - val_loss: 0.2487 - val_acc: 0.3672\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3506 - val_loss: 0.2486 - val_acc: 0.3626\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.3503 - val_loss: 0.2485 - val_acc: 0.3437\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3488 - val_loss: 0.2485 - val_acc: 0.3492\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.3515 - val_loss: 0.2491 - val_acc: 0.3583\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.3499 - val_loss: 0.2493 - val_acc: 0.3448\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3491 - val_loss: 0.2485 - val_acc: 0.3471\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3484 - val_loss: 0.2487 - val_acc: 0.3500\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3489 - val_loss: 0.2488 - val_acc: 0.3588\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3493 - val_loss: 0.2484 - val_acc: 0.3512\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3492 - val_loss: 0.2485 - val_acc: 0.3604\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.3473 - val_loss: 0.2487 - val_acc: 0.3532\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.3502 - val_loss: 0.2489 - val_acc: 0.3541\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3508 - val_loss: 0.2485 - val_acc: 0.3390\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3473 - val_loss: 0.2492 - val_acc: 0.3579\n",
      "Epoch 38/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3496 - val_loss: 0.2494 - val_acc: 0.3621\n",
      "Epoch 39/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.3504 - val_loss: 0.2485 - val_acc: 0.3484\n",
      "Epoch 40/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3498 - val_loss: 0.2485 - val_acc: 0.3491\n",
      "Epoch 41/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3492 - val_loss: 0.2487 - val_acc: 0.3471\n",
      "Epoch 42/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.3496 - val_loss: 0.2488 - val_acc: 0.3468\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.2484 - acc: 0.3507\n",
      ". theta fit =  [0.90071124 1.2619312 ]\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2474 - acc: 0.5183 - val_loss: 0.2466 - val_acc: 0.5449\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2469 - acc: 0.5474 - val_loss: 0.2466 - val_acc: 0.5463\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5442 - val_loss: 0.2466 - val_acc: 0.5363\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5430 - val_loss: 0.2466 - val_acc: 0.5394\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5440 - val_loss: 0.2466 - val_acc: 0.5333\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5412 - val_loss: 0.2466 - val_acc: 0.5316\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5420 - val_loss: 0.2466 - val_acc: 0.5393\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5417 - val_loss: 0.2466 - val_acc: 0.5425\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5399 - val_loss: 0.2466 - val_acc: 0.5405\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5411 - val_loss: 0.2466 - val_acc: 0.5302\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5414 - val_loss: 0.2466 - val_acc: 0.5461\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5416 - val_loss: 0.2466 - val_acc: 0.5451\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.5403 - val_loss: 0.2466 - val_acc: 0.5409\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2467 - acc: 0.5419 - val_loss: 0.2467 - val_acc: 0.5388\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.2466 - acc: 0.5399\n",
      ". theta fit =  [1.1732231 1.5346465]\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2497 - acc: 0.3742 - val_loss: 0.2489 - val_acc: 0.3350\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3426 - val_loss: 0.2488 - val_acc: 0.3442\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.3477 - val_loss: 0.2491 - val_acc: 0.3507\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3465 - val_loss: 0.2490 - val_acc: 0.3426\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.3467 - val_loss: 0.2488 - val_acc: 0.3553\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.3451 - val_loss: 0.2487 - val_acc: 0.3461\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.3448 - val_loss: 0.2489 - val_acc: 0.3467\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3486 - val_loss: 0.2497 - val_acc: 0.3477\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3459 - val_loss: 0.2487 - val_acc: 0.3484\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3453 - val_loss: 0.2488 - val_acc: 0.3482\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3449 - val_loss: 0.2488 - val_acc: 0.3378\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3457 - val_loss: 0.2495 - val_acc: 0.3414\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.3447 - val_loss: 0.2488 - val_acc: 0.3483\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3458 - val_loss: 0.2489 - val_acc: 0.3445\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3450 - val_loss: 0.2491 - val_acc: 0.3445\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3461 - val_loss: 0.2487 - val_acc: 0.3419\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2486 - acc: 0.3458\n",
      ". theta fit =  [0.91224843 1.2737343 ]\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2477 - acc: 0.5090 - val_loss: 0.2470 - val_acc: 0.5415\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5405 - val_loss: 0.2470 - val_acc: 0.5312\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2472 - acc: 0.5375 - val_loss: 0.2470 - val_acc: 0.5384\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5369 - val_loss: 0.2471 - val_acc: 0.5289\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5367 - val_loss: 0.2470 - val_acc: 0.5252\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5349 - val_loss: 0.2470 - val_acc: 0.5381\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5368 - val_loss: 0.2470 - val_acc: 0.5406\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.5358 - val_loss: 0.2470 - val_acc: 0.5342\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5342 - val_loss: 0.2471 - val_acc: 0.5492\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5362 - val_loss: 0.2470 - val_acc: 0.5270\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5353 - val_loss: 0.2470 - val_acc: 0.5299\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5344 - val_loss: 0.2470 - val_acc: 0.5383\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5349 - val_loss: 0.2472 - val_acc: 0.5125\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5339 - val_loss: 0.2470 - val_acc: 0.5384\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5358 - val_loss: 0.2470 - val_acc: 0.5378\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2472 - acc: 0.5353 - val_loss: 0.2470 - val_acc: 0.5340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.5351 - val_loss: 0.2470 - val_acc: 0.5301\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2472 - acc: 0.5351 - val_loss: 0.2470 - val_acc: 0.5454\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.2471 - acc: 0.5346\n",
      ". theta fit =  [1.1651871 1.5268962]\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2507 - acc: 0.3899 - val_loss: 0.2492 - val_acc: 0.3391\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.3380 - val_loss: 0.2489 - val_acc: 0.3376\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3405 - val_loss: 0.2488 - val_acc: 0.3429\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3427 - val_loss: 0.2492 - val_acc: 0.3492\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.3431 - val_loss: 0.2488 - val_acc: 0.3445\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3438 - val_loss: 0.2488 - val_acc: 0.3346\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3433 - val_loss: 0.2499 - val_acc: 0.3563\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3451 - val_loss: 0.2492 - val_acc: 0.3488\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3460 - val_loss: 0.2489 - val_acc: 0.3563\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3423 - val_loss: 0.2488 - val_acc: 0.3370\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3428 - val_loss: 0.2488 - val_acc: 0.3540\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3455 - val_loss: 0.2490 - val_acc: 0.3525\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.3452 - val_loss: 0.2487 - val_acc: 0.3506\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3435 - val_loss: 0.2488 - val_acc: 0.3494\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3446 - val_loss: 0.2491 - val_acc: 0.3436\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.3450 - val_loss: 0.2491 - val_acc: 0.3449\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.3427 - val_loss: 0.2490 - val_acc: 0.3520\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3437 - val_loss: 0.2488 - val_acc: 0.3473\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3430 - val_loss: 0.2491 - val_acc: 0.3413\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3435 - val_loss: 0.2495 - val_acc: 0.3425\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3439 - val_loss: 0.2494 - val_acc: 0.3551\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.3447 - val_loss: 0.2497 - val_acc: 0.3489\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.3436 - val_loss: 0.2488 - val_acc: 0.3433\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.2488 - acc: 0.3501\n",
      ". theta fit =  [0.9173949 1.2791406]\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2481 - acc: 0.5052 - val_loss: 0.2472 - val_acc: 0.5395\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5368 - val_loss: 0.2472 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2474 - acc: 0.5373 - val_loss: 0.2472 - val_acc: 0.5334\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5336 - val_loss: 0.2472 - val_acc: 0.5411\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5355 - val_loss: 0.2472 - val_acc: 0.5261\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.5347 - val_loss: 0.2472 - val_acc: 0.5353\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5326 - val_loss: 0.2472 - val_acc: 0.5367\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5355 - val_loss: 0.2473 - val_acc: 0.5415\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5337 - val_loss: 0.2472 - val_acc: 0.5265\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.5337 - val_loss: 0.2473 - val_acc: 0.5368\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5348 - val_loss: 0.2472 - val_acc: 0.5202\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5332 - val_loss: 0.2472 - val_acc: 0.5261\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5325 - val_loss: 0.2472 - val_acc: 0.5364\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5344 - val_loss: 0.2471 - val_acc: 0.5308\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5343 - val_loss: 0.2472 - val_acc: 0.5231\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5322 - val_loss: 0.2471 - val_acc: 0.5251\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5328 - val_loss: 0.2472 - val_acc: 0.5253\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5341 - val_loss: 0.2472 - val_acc: 0.5330\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2474 - acc: 0.5325 - val_loss: 0.2473 - val_acc: 0.5424\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5352 - val_loss: 0.2472 - val_acc: 0.5329\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5314 - val_loss: 0.2472 - val_acc: 0.5315\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5338 - val_loss: 0.2473 - val_acc: 0.5251\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5313 - val_loss: 0.2471 - val_acc: 0.5399\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.5344 - val_loss: 0.2472 - val_acc: 0.5351\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2472 - acc: 0.5312\n",
      ". theta fit =  [1.1614844 1.5234898]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2498 - acc: 0.4617 - val_loss: 0.2495 - val_acc: 0.4209\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.4260 - val_loss: 0.2498 - val_acc: 0.4565\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4444 - val_loss: 0.2495 - val_acc: 0.4051\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.4278 - val_loss: 0.2495 - val_acc: 0.4286\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4269 - val_loss: 0.2498 - val_acc: 0.4324\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.4375 - val_loss: 0.2495 - val_acc: 0.4279\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.4312 - val_loss: 0.2495 - val_acc: 0.4237\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2496 - acc: 0.4342 - val_loss: 0.2495 - val_acc: 0.4307\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.4296 - val_loss: 0.2495 - val_acc: 0.4498\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.4427 - val_loss: 0.2495 - val_acc: 0.4305\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.4380 - val_loss: 0.2495 - val_acc: 0.4328\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4338 - val_loss: 0.2495 - val_acc: 0.4231\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4340 - val_loss: 0.2495 - val_acc: 0.4225\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4450 - val_loss: 0.2496 - val_acc: 0.4210\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4345 - val_loss: 0.2496 - val_acc: 0.4263\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.4288 - val_loss: 0.2494 - val_acc: 0.4323\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2494 - acc: 0.4321 - val_loss: 0.2500 - val_acc: 0.4498\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4359 - val_loss: 0.2497 - val_acc: 0.4466\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4322 - val_loss: 0.2495 - val_acc: 0.4351\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2494 - acc: 0.4395 - val_loss: 0.2496 - val_acc: 0.4204\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4314 - val_loss: 0.2496 - val_acc: 0.4398\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.4420 - val_loss: 0.2495 - val_acc: 0.4431\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4278 - val_loss: 0.2498 - val_acc: 0.4383\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4362 - val_loss: 0.2498 - val_acc: 0.4344\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4376 - val_loss: 0.2496 - val_acc: 0.4458\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.4360 - val_loss: 0.2496 - val_acc: 0.4174\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2495 - acc: 0.4324\n",
      ". theta fit =  [1.0148728 1.4250265]\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2501 - acc: 0.4413 - val_loss: 0.2502 - val_acc: 0.4361\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4441 - val_loss: 0.2497 - val_acc: 0.4664\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4570 - val_loss: 0.2497 - val_acc: 0.4186\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4485 - val_loss: 0.2497 - val_acc: 0.4474\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.4558 - val_loss: 0.2497 - val_acc: 0.4207\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.4416 - val_loss: 0.2499 - val_acc: 0.4585\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4495 - val_loss: 0.2497 - val_acc: 0.4719\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4481 - val_loss: 0.2498 - val_acc: 0.4755\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4634 - val_loss: 0.2498 - val_acc: 0.4493\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4466 - val_loss: 0.2501 - val_acc: 0.4448\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4447 - val_loss: 0.2501 - val_acc: 0.4691\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4548 - val_loss: 0.2497 - val_acc: 0.4329\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4415 - val_loss: 0.2497 - val_acc: 0.4399\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4615 - val_loss: 0.2497 - val_acc: 0.4517\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4569 - val_loss: 0.2497 - val_acc: 0.4697\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.4424 - val_loss: 0.2496 - val_acc: 0.4552\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4553 - val_loss: 0.2497 - val_acc: 0.4662\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.4459 - val_loss: 0.2501 - val_acc: 0.4640\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4469 - val_loss: 0.2497 - val_acc: 0.4536\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4610 - val_loss: 0.2497 - val_acc: 0.4284\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4527 - val_loss: 0.2497 - val_acc: 0.4396\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4482 - val_loss: 0.2498 - val_acc: 0.4629\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4584 - val_loss: 0.2500 - val_acc: 0.4830\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4612 - val_loss: 0.2499 - val_acc: 0.4534\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4579 - val_loss: 0.2496 - val_acc: 0.4412\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.4430 - val_loss: 0.2505 - val_acc: 0.4698\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4441 - val_loss: 0.2498 - val_acc: 0.4773\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4552 - val_loss: 0.2499 - val_acc: 0.4707\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4673 - val_loss: 0.2497 - val_acc: 0.4380\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4443 - val_loss: 0.2497 - val_acc: 0.4445\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4672 - val_loss: 0.2497 - val_acc: 0.4387\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4473 - val_loss: 0.2503 - val_acc: 0.4631\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4470 - val_loss: 0.2501 - val_acc: 0.4720\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4635 - val_loss: 0.2500 - val_acc: 0.4586\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.4526 - val_loss: 0.2497 - val_acc: 0.4243\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2497 - acc: 0.4413\n",
      ". theta fit =  [0.99078804 1.4491132 ]\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2503 - acc: 0.4655 - val_loss: 0.2499 - val_acc: 0.4835\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4701 - val_loss: 0.2499 - val_acc: 0.4516\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4815 - val_loss: 0.2498 - val_acc: 0.4331\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.4622 - val_loss: 0.2500 - val_acc: 0.4952\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.4818 - val_loss: 0.2498 - val_acc: 0.4558\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4815 - val_loss: 0.2500 - val_acc: 0.4852\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.4688 - val_loss: 0.2498 - val_acc: 0.4970\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4801 - val_loss: 0.2500 - val_acc: 0.5010\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4856 - val_loss: 0.2502 - val_acc: 0.4735\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4672 - val_loss: 0.2503 - val_acc: 0.4842\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4848 - val_loss: 0.2499 - val_acc: 0.4896\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4653 - val_loss: 0.2506 - val_acc: 0.4934\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4847 - val_loss: 0.2502 - val_acc: 0.4682\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2498 - acc: 0.4333\n",
      ". theta fit =  [0.96674705 1.4727341 ]\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2501 - acc: 0.4989 - val_loss: 0.2501 - val_acc: 0.5108\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5172 - val_loss: 0.2507 - val_acc: 0.4987\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5014 - val_loss: 0.2509 - val_acc: 0.5352\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5169 - val_loss: 0.2498 - val_acc: 0.5194\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5352 - val_loss: 0.2498 - val_acc: 0.5052\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.4972 - val_loss: 0.2507 - val_acc: 0.5197\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.5037 - val_loss: 0.2512 - val_acc: 0.5469\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5280 - val_loss: 0.2501 - val_acc: 0.4869\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5176 - val_loss: 0.2499 - val_acc: 0.4873\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5195 - val_loss: 0.2498 - val_acc: 0.5223\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5101 - val_loss: 0.2507 - val_acc: 0.5441\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5168 - val_loss: 0.2501 - val_acc: 0.5308\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5205 - val_loss: 0.2498 - val_acc: 0.5260\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5147 - val_loss: 0.2504 - val_acc: 0.5347\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5263 - val_loss: 0.2503 - val_acc: 0.5207\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5179 - val_loss: 0.2501 - val_acc: 0.5254\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5225 - val_loss: 0.2498 - val_acc: 0.5188\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5212 - val_loss: 0.2504 - val_acc: 0.5442\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5254 - val_loss: 0.2508 - val_acc: 0.4979\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5250 - val_loss: 0.2504 - val_acc: 0.5175\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5097 - val_loss: 0.2505 - val_acc: 0.5369\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.5260 - val_loss: 0.2498 - val_acc: 0.5232\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5177 - val_loss: 0.2498 - val_acc: 0.5338\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5245 - val_loss: 0.2498 - val_acc: 0.5206\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5300 - val_loss: 0.2499 - val_acc: 0.5175\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5271 - val_loss: 0.2498 - val_acc: 0.5159\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5141 - val_loss: 0.2498 - val_acc: 0.4847\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5172 - val_loss: 0.2498 - val_acc: 0.5161\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5183 - val_loss: 0.2501 - val_acc: 0.5229\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5134 - val_loss: 0.2506 - val_acc: 0.5160\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5181 - val_loss: 0.2507 - val_acc: 0.5124\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5150 - val_loss: 0.2498 - val_acc: 0.5234\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5189 - val_loss: 0.2498 - val_acc: 0.5184\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5170 - val_loss: 0.2503 - val_acc: 0.5384\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2498 - acc: 0.5208\n",
      ". theta fit =  [0.9899979 1.4485964]\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2503 - acc: 0.4825 - val_loss: 0.2506 - val_acc: 0.4819\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4612 - val_loss: 0.2503 - val_acc: 0.4901\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.4775 - val_loss: 0.2504 - val_acc: 0.4672\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4665 - val_loss: 0.2506 - val_acc: 0.4686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4818 - val_loss: 0.2500 - val_acc: 0.4955\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4854 - val_loss: 0.2505 - val_acc: 0.4532\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4719 - val_loss: 0.2505 - val_acc: 0.4471\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4798 - val_loss: 0.2499 - val_acc: 0.4568\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4744 - val_loss: 0.2501 - val_acc: 0.4874\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4871 - val_loss: 0.2498 - val_acc: 0.4750\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4883 - val_loss: 0.2500 - val_acc: 0.4874\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4733 - val_loss: 0.2497 - val_acc: 0.4627\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4884 - val_loss: 0.2501 - val_acc: 0.4770\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4842 - val_loss: 0.2498 - val_acc: 0.4892\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4772 - val_loss: 0.2499 - val_acc: 0.4888\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.4808 - val_loss: 0.2497 - val_acc: 0.4713\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4840 - val_loss: 0.2498 - val_acc: 0.4868\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4776 - val_loss: 0.2505 - val_acc: 0.4902\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4844 - val_loss: 0.2498 - val_acc: 0.4844\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4782 - val_loss: 0.2503 - val_acc: 0.4693\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4811 - val_loss: 0.2504 - val_acc: 0.4741\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4651 - val_loss: 0.2503 - val_acc: 0.4878\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4912 - val_loss: 0.2499 - val_acc: 0.4816\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4860 - val_loss: 0.2498 - val_acc: 0.4611\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4697 - val_loss: 0.2497 - val_acc: 0.4920\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4848 - val_loss: 0.2499 - val_acc: 0.4841\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4847 - val_loss: 0.2503 - val_acc: 0.4771\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4624 - val_loss: 0.2498 - val_acc: 0.4801\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4896 - val_loss: 0.2501 - val_acc: 0.5051\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.4941 - val_loss: 0.2499 - val_acc: 0.4644\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.4761 - val_loss: 0.2504 - val_acc: 0.4767\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2499 - acc: 0.4793 - val_loss: 0.2498 - val_acc: 0.4764\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2498 - acc: 0.4844 - val_loss: 0.2503 - val_acc: 0.5003\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2500 - acc: 0.4788 - val_loss: 0.2499 - val_acc: 0.4910\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.4883 - val_loss: 0.2508 - val_acc: 0.4905\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.2498 - acc: 0.4921\n",
      ". theta fit =  [0.96588254 1.4721836 ]\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2507 - acc: 0.5075 - val_loss: 0.2508 - val_acc: 0.5190\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5195 - val_loss: 0.2502 - val_acc: 0.5236\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5186 - val_loss: 0.2499 - val_acc: 0.5439\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5272 - val_loss: 0.2501 - val_acc: 0.5476\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5340 - val_loss: 0.2503 - val_acc: 0.5066\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.4991 - val_loss: 0.2498 - val_acc: 0.5480\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5304 - val_loss: 0.2499 - val_acc: 0.5190\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5276 - val_loss: 0.2501 - val_acc: 0.5294\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5160 - val_loss: 0.2499 - val_acc: 0.5177\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5303 - val_loss: 0.2499 - val_acc: 0.5067\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5194 - val_loss: 0.2504 - val_acc: 0.5256\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5175 - val_loss: 0.2502 - val_acc: 0.5010\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5283 - val_loss: 0.2498 - val_acc: 0.5269\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5221 - val_loss: 0.2503 - val_acc: 0.5324\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5232 - val_loss: 0.2499 - val_acc: 0.5225\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5252 - val_loss: 0.2498 - val_acc: 0.5044\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5117 - val_loss: 0.2501 - val_acc: 0.5192\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5247 - val_loss: 0.2498 - val_acc: 0.5155\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5194 - val_loss: 0.2502 - val_acc: 0.4977\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5160 - val_loss: 0.2502 - val_acc: 0.5148\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5326 - val_loss: 0.2499 - val_acc: 0.4985\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5161 - val_loss: 0.2506 - val_acc: 0.5275\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5199 - val_loss: 0.2504 - val_acc: 0.5347\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2498 - acc: 0.5273\n",
      ". theta fit =  [0.9899282 1.447912 ]\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2503 - acc: 0.4831 - val_loss: 0.2500 - val_acc: 0.4537\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4590 - val_loss: 0.2500 - val_acc: 0.4789\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4767 - val_loss: 0.2498 - val_acc: 0.5023\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4949 - val_loss: 0.2498 - val_acc: 0.4740\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4803 - val_loss: 0.2499 - val_acc: 0.4705\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4694 - val_loss: 0.2506 - val_acc: 0.4987\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4774 - val_loss: 0.2503 - val_acc: 0.4676\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4860 - val_loss: 0.2500 - val_acc: 0.4792\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4715 - val_loss: 0.2500 - val_acc: 0.4928\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4818 - val_loss: 0.2499 - val_acc: 0.4799\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4836 - val_loss: 0.2501 - val_acc: 0.4733\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4760 - val_loss: 0.2498 - val_acc: 0.4898\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4846 - val_loss: 0.2498 - val_acc: 0.4652\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2498 - acc: 0.5026\n",
      ". theta fit =  [0.9656903 1.4721771]\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2500 - acc: 0.5174 - val_loss: 0.2500 - val_acc: 0.5194\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5241 - val_loss: 0.2499 - val_acc: 0.5298\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5221 - val_loss: 0.2499 - val_acc: 0.5184\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5219 - val_loss: 0.2502 - val_acc: 0.5141\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5088 - val_loss: 0.2507 - val_acc: 0.5418\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5303 - val_loss: 0.2506 - val_acc: 0.5129\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5092 - val_loss: 0.2498 - val_acc: 0.5413\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5375 - val_loss: 0.2501 - val_acc: 0.5079\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5197 - val_loss: 0.2499 - val_acc: 0.5066\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5258 - val_loss: 0.2499 - val_acc: 0.4997\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5065 - val_loss: 0.2509 - val_acc: 0.5338\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5156 - val_loss: 0.2507 - val_acc: 0.5144\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5266 - val_loss: 0.2500 - val_acc: 0.5112\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5133 - val_loss: 0.2507 - val_acc: 0.5297\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5294 - val_loss: 0.2498 - val_acc: 0.4966\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5133 - val_loss: 0.2497 - val_acc: 0.5265\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5169 - val_loss: 0.2501 - val_acc: 0.5463\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5292 - val_loss: 0.2506 - val_acc: 0.5167\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5179 - val_loss: 0.2498 - val_acc: 0.5221\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5250 - val_loss: 0.2498 - val_acc: 0.5221\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5221 - val_loss: 0.2501 - val_acc: 0.5311\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5170 - val_loss: 0.2498 - val_acc: 0.5201\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5384 - val_loss: 0.2498 - val_acc: 0.5128\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5213 - val_loss: 0.2500 - val_acc: 0.5189\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5027 - val_loss: 0.2498 - val_acc: 0.5204\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5232 - val_loss: 0.2501 - val_acc: 0.5358\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2497 - acc: 0.5268\n",
      ". theta fit =  [0.99007565 1.4475669 ]\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2499 - acc: 0.4877 - val_loss: 0.2503 - val_acc: 0.4767\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4773 - val_loss: 0.2499 - val_acc: 0.4706\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4874 - val_loss: 0.2504 - val_acc: 0.4873\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4777 - val_loss: 0.2498 - val_acc: 0.4671\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.4749 - val_loss: 0.2498 - val_acc: 0.4621\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4855 - val_loss: 0.2498 - val_acc: 0.4652\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4793 - val_loss: 0.2498 - val_acc: 0.4710\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4790 - val_loss: 0.2498 - val_acc: 0.4834\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4804 - val_loss: 0.2500 - val_acc: 0.4837\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4825 - val_loss: 0.2506 - val_acc: 0.4786\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4788 - val_loss: 0.2500 - val_acc: 0.4827\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4758 - val_loss: 0.2502 - val_acc: 0.5017\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4877 - val_loss: 0.2499 - val_acc: 0.4654\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4711 - val_loss: 0.2503 - val_acc: 0.4902\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.4749 - val_loss: 0.2501 - val_acc: 0.4590\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4842 - val_loss: 0.2502 - val_acc: 0.4900\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4762 - val_loss: 0.2507 - val_acc: 0.4745\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2498 - acc: 0.4713\n",
      ". theta fit =  [0.96526754 1.4723394 ]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2503 - acc: 0.4728 - val_loss: 0.2498 - val_acc: 0.4784\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4839 - val_loss: 0.2506 - val_acc: 0.5046\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5032 - val_loss: 0.2507 - val_acc: 0.4834\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4961 - val_loss: 0.2499 - val_acc: 0.5051\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5036 - val_loss: 0.2506 - val_acc: 0.4794\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4618 - val_loss: 0.2505 - val_acc: 0.5202\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.5064 - val_loss: 0.2498 - val_acc: 0.5107\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5057 - val_loss: 0.2499 - val_acc: 0.4943\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2497 - acc: 0.4999 - val_loss: 0.2498 - val_acc: 0.4993\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5065 - val_loss: 0.2499 - val_acc: 0.4826\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5040 - val_loss: 0.2498 - val_acc: 0.4973\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4970 - val_loss: 0.2498 - val_acc: 0.5060\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5074 - val_loss: 0.2498 - val_acc: 0.5020\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4958 - val_loss: 0.2501 - val_acc: 0.4953\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4971 - val_loss: 0.2499 - val_acc: 0.5107\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5038 - val_loss: 0.2497 - val_acc: 0.4949\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4918 - val_loss: 0.2502 - val_acc: 0.5011\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4937 - val_loss: 0.2497 - val_acc: 0.5040\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4999 - val_loss: 0.2499 - val_acc: 0.5066\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5076 - val_loss: 0.2502 - val_acc: 0.4885\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4986 - val_loss: 0.2508 - val_acc: 0.4933\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5020 - val_loss: 0.2501 - val_acc: 0.4969\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5054 - val_loss: 0.2499 - val_acc: 0.4905\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.4941 - val_loss: 0.2498 - val_acc: 0.5092\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.4973 - val_loss: 0.2498 - val_acc: 0.4512\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5123 - val_loss: 0.2497 - val_acc: 0.4850\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4829 - val_loss: 0.2503 - val_acc: 0.4958\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5035 - val_loss: 0.2499 - val_acc: 0.5159\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5055 - val_loss: 0.2498 - val_acc: 0.5159\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5132 - val_loss: 0.2506 - val_acc: 0.4979\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4886 - val_loss: 0.2499 - val_acc: 0.4805\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5100 - val_loss: 0.2500 - val_acc: 0.5010\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4974 - val_loss: 0.2500 - val_acc: 0.4969\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5064 - val_loss: 0.2500 - val_acc: 0.5036\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.4958 - val_loss: 0.2508 - val_acc: 0.5047\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5002 - val_loss: 0.2506 - val_acc: 0.4950\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2498 - acc: 0.4853\n",
      ". theta fit =  [0.9752392 1.4574971]\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2498 - acc: 0.4960 - val_loss: 0.2498 - val_acc: 0.4876\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4951 - val_loss: 0.2500 - val_acc: 0.5189\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5096 - val_loss: 0.2503 - val_acc: 0.4989\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5022 - val_loss: 0.2498 - val_acc: 0.5072\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5018 - val_loss: 0.2498 - val_acc: 0.4947\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5072 - val_loss: 0.2502 - val_acc: 0.5004\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.4946 - val_loss: 0.2498 - val_acc: 0.5111\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5044 - val_loss: 0.2502 - val_acc: 0.5165\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5040 - val_loss: 0.2507 - val_acc: 0.5035\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5024 - val_loss: 0.2499 - val_acc: 0.5070\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5045 - val_loss: 0.2498 - val_acc: 0.5046\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5054 - val_loss: 0.2498 - val_acc: 0.5128\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5041 - val_loss: 0.2504 - val_acc: 0.5212\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5093 - val_loss: 0.2500 - val_acc: 0.5053\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5013 - val_loss: 0.2499 - val_acc: 0.5007\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.4994 - val_loss: 0.2509 - val_acc: 0.5024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5007 - val_loss: 0.2500 - val_acc: 0.4921\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2498 - acc: 0.5113\n",
      ". theta fit =  [0.97272843 1.4549849 ]\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2500 - acc: 0.5030 - val_loss: 0.2504 - val_acc: 0.5097\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5166 - val_loss: 0.2502 - val_acc: 0.5053\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.4991 - val_loss: 0.2498 - val_acc: 0.5102\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5061 - val_loss: 0.2502 - val_acc: 0.5212\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5108 - val_loss: 0.2502 - val_acc: 0.5105\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5116 - val_loss: 0.2499 - val_acc: 0.5047\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5037 - val_loss: 0.2498 - val_acc: 0.5150\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5097 - val_loss: 0.2498 - val_acc: 0.5057\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5012 - val_loss: 0.2501 - val_acc: 0.5168\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5059 - val_loss: 0.2498 - val_acc: 0.5150\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5069 - val_loss: 0.2498 - val_acc: 0.5039\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5168 - val_loss: 0.2499 - val_acc: 0.5036\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5075 - val_loss: 0.2501 - val_acc: 0.5002\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5089 - val_loss: 0.2497 - val_acc: 0.5088\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5126 - val_loss: 0.2499 - val_acc: 0.4975\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.4899 - val_loss: 0.2501 - val_acc: 0.4955\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5106 - val_loss: 0.2499 - val_acc: 0.4927\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5092 - val_loss: 0.2498 - val_acc: 0.4998\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5045 - val_loss: 0.2499 - val_acc: 0.5184\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5063 - val_loss: 0.2498 - val_acc: 0.5096\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5202 - val_loss: 0.2498 - val_acc: 0.5049\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5040 - val_loss: 0.2508 - val_acc: 0.5155\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5105 - val_loss: 0.2499 - val_acc: 0.5113\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5076 - val_loss: 0.2502 - val_acc: 0.5089\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2498 - acc: 0.5090\n",
      ". theta fit =  [0.9704996 1.4524571]\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2500 - acc: 0.4998 - val_loss: 0.2499 - val_acc: 0.5154\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5111 - val_loss: 0.2502 - val_acc: 0.5134\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5119 - val_loss: 0.2497 - val_acc: 0.5110\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5133 - val_loss: 0.2505 - val_acc: 0.5098\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5117 - val_loss: 0.2501 - val_acc: 0.5046\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5091 - val_loss: 0.2499 - val_acc: 0.5071\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5181 - val_loss: 0.2499 - val_acc: 0.5084\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5057 - val_loss: 0.2498 - val_acc: 0.5110\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5064 - val_loss: 0.2504 - val_acc: 0.5213\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5185 - val_loss: 0.2498 - val_acc: 0.5091\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5073 - val_loss: 0.2498 - val_acc: 0.5087\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5057 - val_loss: 0.2504 - val_acc: 0.5164\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5078 - val_loss: 0.2505 - val_acc: 0.5083\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2497 - acc: 0.5113\n",
      ". theta fit =  [0.9679535 1.4550233]\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2508 - acc: 0.5132 - val_loss: 0.2504 - val_acc: 0.5086\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5044 - val_loss: 0.2499 - val_acc: 0.5293\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5187 - val_loss: 0.2498 - val_acc: 0.5195\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5141 - val_loss: 0.2499 - val_acc: 0.5091\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5127 - val_loss: 0.2506 - val_acc: 0.5144\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5168 - val_loss: 0.2504 - val_acc: 0.5094\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5149 - val_loss: 0.2508 - val_acc: 0.5139\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5114 - val_loss: 0.2498 - val_acc: 0.5149\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5161 - val_loss: 0.2498 - val_acc: 0.5206\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5241 - val_loss: 0.2498 - val_acc: 0.5130\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5138 - val_loss: 0.2501 - val_acc: 0.5070\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5091 - val_loss: 0.2506 - val_acc: 0.5286\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5264 - val_loss: 0.2498 - val_acc: 0.5210\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5167 - val_loss: 0.2501 - val_acc: 0.5208\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5191 - val_loss: 0.2506 - val_acc: 0.5242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5184 - val_loss: 0.2499 - val_acc: 0.5085\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5148 - val_loss: 0.2498 - val_acc: 0.5287\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5229 - val_loss: 0.2498 - val_acc: 0.5240\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5197 - val_loss: 0.2498 - val_acc: 0.5243\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5190 - val_loss: 0.2498 - val_acc: 0.5239\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2498 - acc: 0.5132\n",
      ". theta fit =  [0.965537  1.4525968]\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2502 - acc: 0.5139 - val_loss: 0.2513 - val_acc: 0.5165\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5199 - val_loss: 0.2498 - val_acc: 0.5195\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5243 - val_loss: 0.2498 - val_acc: 0.5187\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5195 - val_loss: 0.2501 - val_acc: 0.5227\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5247 - val_loss: 0.2500 - val_acc: 0.5322\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5247 - val_loss: 0.2498 - val_acc: 0.5283\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5315 - val_loss: 0.2498 - val_acc: 0.5127\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5184 - val_loss: 0.2500 - val_acc: 0.5257\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5183 - val_loss: 0.2510 - val_acc: 0.5344\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5306 - val_loss: 0.2502 - val_acc: 0.5273\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5287 - val_loss: 0.2498 - val_acc: 0.5204\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5210 - val_loss: 0.2500 - val_acc: 0.5278\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5247 - val_loss: 0.2498 - val_acc: 0.5070\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2497 - acc: 0.5189\n",
      ". theta fit =  [0.9668453 1.4552346]\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2500 - acc: 0.5148 - val_loss: 0.2507 - val_acc: 0.5177\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5067 - val_loss: 0.2508 - val_acc: 0.5345\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5211 - val_loss: 0.2502 - val_acc: 0.5271\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5234 - val_loss: 0.2497 - val_acc: 0.5080\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5156 - val_loss: 0.2497 - val_acc: 0.5246\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5242 - val_loss: 0.2498 - val_acc: 0.5218\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5213 - val_loss: 0.2501 - val_acc: 0.5181\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5211 - val_loss: 0.2498 - val_acc: 0.5172\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5177 - val_loss: 0.2511 - val_acc: 0.5150\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.5197 - val_loss: 0.2497 - val_acc: 0.5205\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5228 - val_loss: 0.2498 - val_acc: 0.5217\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5244 - val_loss: 0.2497 - val_acc: 0.5189\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5211 - val_loss: 0.2498 - val_acc: 0.5263\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5206 - val_loss: 0.2500 - val_acc: 0.5290\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2498 - acc: 0.5082\n",
      ". theta fit =  [0.96418667 1.4525763 ]\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2506 - acc: 0.5179 - val_loss: 0.2501 - val_acc: 0.5116\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5196 - val_loss: 0.2498 - val_acc: 0.5142\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5179 - val_loss: 0.2497 - val_acc: 0.5077\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5221 - val_loss: 0.2499 - val_acc: 0.5262\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5226 - val_loss: 0.2498 - val_acc: 0.5289\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5282 - val_loss: 0.2498 - val_acc: 0.5110\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5186 - val_loss: 0.2502 - val_acc: 0.5277\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5277 - val_loss: 0.2504 - val_acc: 0.5183\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5213 - val_loss: 0.2499 - val_acc: 0.5266\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5197 - val_loss: 0.2498 - val_acc: 0.5327\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5267 - val_loss: 0.2498 - val_acc: 0.5242\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5241 - val_loss: 0.2498 - val_acc: 0.5178\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5196 - val_loss: 0.2498 - val_acc: 0.5316\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2498 - acc: 0.5079\n",
      ". theta fit =  [0.96159905 1.4552107 ]\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2508 - acc: 0.5291 - val_loss: 0.2499 - val_acc: 0.5183\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5212 - val_loss: 0.2502 - val_acc: 0.5302\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5287 - val_loss: 0.2498 - val_acc: 0.5312\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5314 - val_loss: 0.2506 - val_acc: 0.5184\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5264 - val_loss: 0.2499 - val_acc: 0.5275\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5255 - val_loss: 0.2497 - val_acc: 0.5331\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5343 - val_loss: 0.2500 - val_acc: 0.5241\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5286 - val_loss: 0.2502 - val_acc: 0.5252\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5279 - val_loss: 0.2501 - val_acc: 0.5249\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2497 - acc: 0.5269 - val_loss: 0.2499 - val_acc: 0.5230\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5260 - val_loss: 0.2507 - val_acc: 0.5260\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5323 - val_loss: 0.2504 - val_acc: 0.5244\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5287 - val_loss: 0.2497 - val_acc: 0.5076\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5257 - val_loss: 0.2499 - val_acc: 0.5298\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5320 - val_loss: 0.2501 - val_acc: 0.5270\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5286 - val_loss: 0.2499 - val_acc: 0.5297\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5292 - val_loss: 0.2497 - val_acc: 0.5304\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5308 - val_loss: 0.2497 - val_acc: 0.5253\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5304 - val_loss: 0.2499 - val_acc: 0.5252\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5274 - val_loss: 0.2503 - val_acc: 0.5274\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5284 - val_loss: 0.2500 - val_acc: 0.5376\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5336 - val_loss: 0.2499 - val_acc: 0.5226\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5292 - val_loss: 0.2499 - val_acc: 0.5288\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5272 - val_loss: 0.2505 - val_acc: 0.5306\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5294 - val_loss: 0.2497 - val_acc: 0.5291\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5289 - val_loss: 0.2497 - val_acc: 0.5255\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5211 - val_loss: 0.2497 - val_acc: 0.5443\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5348 - val_loss: 0.2498 - val_acc: 0.5227\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5249 - val_loss: 0.2498 - val_acc: 0.5363\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5280 - val_loss: 0.2502 - val_acc: 0.5303\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5309 - val_loss: 0.2504 - val_acc: 0.5226\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5295 - val_loss: 0.2498 - val_acc: 0.5132\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5245 - val_loss: 0.2508 - val_acc: 0.5157\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5192 - val_loss: 0.2498 - val_acc: 0.5220\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5210 - val_loss: 0.2498 - val_acc: 0.5415\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5316 - val_loss: 0.2497 - val_acc: 0.5298\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5315 - val_loss: 0.2501 - val_acc: 0.5178\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2498 - acc: 0.5446\n",
      ". theta fit =  [0.9643292 1.4524788]\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2505 - acc: 0.5226 - val_loss: 0.2500 - val_acc: 0.5374\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5233 - val_loss: 0.2497 - val_acc: 0.5310\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5262 - val_loss: 0.2497 - val_acc: 0.5031\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.5094 - val_loss: 0.2511 - val_acc: 0.5141\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5140 - val_loss: 0.2497 - val_acc: 0.5289\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5182 - val_loss: 0.2505 - val_acc: 0.5256\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5257 - val_loss: 0.2501 - val_acc: 0.5183\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5197 - val_loss: 0.2500 - val_acc: 0.5152\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5208 - val_loss: 0.2504 - val_acc: 0.5143\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5211 - val_loss: 0.2500 - val_acc: 0.4990\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5106 - val_loss: 0.2504 - val_acc: 0.5190\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5215 - val_loss: 0.2502 - val_acc: 0.5158\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2497 - acc: 0.5312\n",
      ". theta fit =  [0.9670928 1.4551005]\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2507 - acc: 0.5157 - val_loss: 0.2498 - val_acc: 0.4962\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5042 - val_loss: 0.2497 - val_acc: 0.5106\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5083 - val_loss: 0.2499 - val_acc: 0.5281\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5172 - val_loss: 0.2502 - val_acc: 0.5077\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5123 - val_loss: 0.2501 - val_acc: 0.5205\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5079 - val_loss: 0.2499 - val_acc: 0.5389\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5168 - val_loss: 0.2510 - val_acc: 0.5209\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2500 - acc: 0.5133 - val_loss: 0.2503 - val_acc: 0.5197\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2497 - acc: 0.5151 - val_loss: 0.2500 - val_acc: 0.5149\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2500 - acc: 0.5184 - val_loss: 0.2498 - val_acc: 0.5143\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2499 - acc: 0.5187 - val_loss: 0.2497 - val_acc: 0.5130\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2499 - acc: 0.5099 - val_loss: 0.2499 - val_acc: 0.5481\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5276 - val_loss: 0.2500 - val_acc: 0.5132\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5170 - val_loss: 0.2497 - val_acc: 0.5201\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5230 - val_loss: 0.2498 - val_acc: 0.5196\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5200 - val_loss: 0.2500 - val_acc: 0.5089\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5177 - val_loss: 0.2498 - val_acc: 0.5022\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5114 - val_loss: 0.2510 - val_acc: 0.5073\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5138 - val_loss: 0.2501 - val_acc: 0.5140\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5095 - val_loss: 0.2502 - val_acc: 0.5301\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5197 - val_loss: 0.2507 - val_acc: 0.5169\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2498 - acc: 0.5132\n",
      ". theta fit =  [0.964402  1.4523115]\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2498 - acc: 0.5158 - val_loss: 0.2500 - val_acc: 0.5267\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5176 - val_loss: 0.2503 - val_acc: 0.5358\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5148 - val_loss: 0.2497 - val_acc: 0.5221\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5240 - val_loss: 0.2497 - val_acc: 0.5168\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5282 - val_loss: 0.2503 - val_acc: 0.5051\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5177 - val_loss: 0.2499 - val_acc: 0.5200\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5192 - val_loss: 0.2499 - val_acc: 0.5348\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5250 - val_loss: 0.2501 - val_acc: 0.5184\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5185 - val_loss: 0.2497 - val_acc: 0.5258\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5320 - val_loss: 0.2501 - val_acc: 0.5067\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5148 - val_loss: 0.2503 - val_acc: 0.5244\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5164 - val_loss: 0.2498 - val_acc: 0.5268\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5260 - val_loss: 0.2497 - val_acc: 0.5218\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5276 - val_loss: 0.2502 - val_acc: 0.5220\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5182 - val_loss: 0.2507 - val_acc: 0.5196\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5279 - val_loss: 0.2507 - val_acc: 0.5447\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5232 - val_loss: 0.2508 - val_acc: 0.5272\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5245 - val_loss: 0.2498 - val_acc: 0.5109\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5166 - val_loss: 0.2509 - val_acc: 0.5374\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2497 - acc: 0.5261\n",
      ". theta fit =  [0.9672205 1.4551394]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2515 - acc: 0.5214 - val_loss: 0.2508 - val_acc: 0.5077\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5035 - val_loss: 0.2499 - val_acc: 0.5404\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5239 - val_loss: 0.2497 - val_acc: 0.5180\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5215 - val_loss: 0.2502 - val_acc: 0.5282\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5220 - val_loss: 0.2497 - val_acc: 0.5057\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5187 - val_loss: 0.2499 - val_acc: 0.5327\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2500 - acc: 0.5268 - val_loss: 0.2498 - val_acc: 0.5126\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2498 - acc: 0.5187 - val_loss: 0.2502 - val_acc: 0.5302\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2497 - acc: 0.5222 - val_loss: 0.2511 - val_acc: 0.5213\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.5226 - val_loss: 0.2506 - val_acc: 0.5150\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5176 - val_loss: 0.2497 - val_acc: 0.5210\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5239 - val_loss: 0.2497 - val_acc: 0.5183\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5258 - val_loss: 0.2505 - val_acc: 0.5077\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2497 - acc: 0.5183\n",
      ". theta fit =  [0.9654741 1.4534693]\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2504 - acc: 0.5219 - val_loss: 0.2500 - val_acc: 0.5024\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5043 - val_loss: 0.2498 - val_acc: 0.5057\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5127 - val_loss: 0.2499 - val_acc: 0.5345\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5274 - val_loss: 0.2504 - val_acc: 0.5145\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5210 - val_loss: 0.2499 - val_acc: 0.5207\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5223 - val_loss: 0.2508 - val_acc: 0.5276\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5251 - val_loss: 0.2505 - val_acc: 0.5331\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5174 - val_loss: 0.2497 - val_acc: 0.5207\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5203 - val_loss: 0.2504 - val_acc: 0.5330\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5281 - val_loss: 0.2500 - val_acc: 0.5074\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5176 - val_loss: 0.2498 - val_acc: 0.5311\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5209 - val_loss: 0.2509 - val_acc: 0.5386\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5252 - val_loss: 0.2502 - val_acc: 0.5276\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5127 - val_loss: 0.2497 - val_acc: 0.5157\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5133 - val_loss: 0.2502 - val_acc: 0.5226\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5192 - val_loss: 0.2501 - val_acc: 0.5186\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5172 - val_loss: 0.2498 - val_acc: 0.5139\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2498 - acc: 0.5258 - val_loss: 0.2504 - val_acc: 0.5186\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2498 - acc: 0.5209\n",
      ". theta fit =  [0.9657547 1.4531813]\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2503 - acc: 0.5206 - val_loss: 0.2497 - val_acc: 0.5096\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2497 - acc: 0.5172 - val_loss: 0.2500 - val_acc: 0.5149\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5157 - val_loss: 0.2500 - val_acc: 0.5250\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5280 - val_loss: 0.2499 - val_acc: 0.5120\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5192 - val_loss: 0.2505 - val_acc: 0.5306\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5283 - val_loss: 0.2504 - val_acc: 0.5097\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5111 - val_loss: 0.2497 - val_acc: 0.5326\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5269 - val_loss: 0.2498 - val_acc: 0.5156\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5340 - val_loss: 0.2499 - val_acc: 0.4946\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5227 - val_loss: 0.2499 - val_acc: 0.5197\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5170 - val_loss: 0.2507 - val_acc: 0.5163\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2497 - acc: 0.5100\n",
      ". theta fit =  [0.96546304 1.45347   ]\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2506 - acc: 0.5134 - val_loss: 0.2498 - val_acc: 0.5025\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5229 - val_loss: 0.2498 - val_acc: 0.5170\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5218 - val_loss: 0.2501 - val_acc: 0.5191\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5181 - val_loss: 0.2506 - val_acc: 0.5380\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5320 - val_loss: 0.2497 - val_acc: 0.5126\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5181 - val_loss: 0.2498 - val_acc: 0.5236\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5275 - val_loss: 0.2497 - val_acc: 0.5184\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5300 - val_loss: 0.2506 - val_acc: 0.5079\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5115 - val_loss: 0.2499 - val_acc: 0.4926\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5193 - val_loss: 0.2502 - val_acc: 0.4934\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5153 - val_loss: 0.2497 - val_acc: 0.5158\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5202 - val_loss: 0.2497 - val_acc: 0.5177\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5222 - val_loss: 0.2505 - val_acc: 0.5259\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5205 - val_loss: 0.2499 - val_acc: 0.5158\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5224 - val_loss: 0.2499 - val_acc: 0.5175\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5227 - val_loss: 0.2499 - val_acc: 0.5092\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5263 - val_loss: 0.2498 - val_acc: 0.5043\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5271 - val_loss: 0.2499 - val_acc: 0.5182\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2498 - acc: 0.5265 - val_loss: 0.2500 - val_acc: 0.4954\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2499 - acc: 0.5167 - val_loss: 0.2498 - val_acc: 0.5053\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2497 - acc: 0.5170 - val_loss: 0.2506 - val_acc: 0.5226\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2499 - acc: 0.5218 - val_loss: 0.2503 - val_acc: 0.5458\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2498 - acc: 0.5179\n",
      ". theta fit =  [0.96519756 1.4537605 ]\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2505 - acc: 0.5226 - val_loss: 0.2502 - val_acc: 0.5312\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5170 - val_loss: 0.2504 - val_acc: 0.5431\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5303 - val_loss: 0.2499 - val_acc: 0.5173\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5216 - val_loss: 0.2499 - val_acc: 0.5373\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5316 - val_loss: 0.2499 - val_acc: 0.5027\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5226 - val_loss: 0.2507 - val_acc: 0.5169\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.5194 - val_loss: 0.2502 - val_acc: 0.5216\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2496 - acc: 0.5191 - val_loss: 0.2507 - val_acc: 0.5380\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5259 - val_loss: 0.2506 - val_acc: 0.5264\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5247 - val_loss: 0.2498 - val_acc: 0.5197\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5241 - val_loss: 0.2503 - val_acc: 0.5320\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5346 - val_loss: 0.2503 - val_acc: 0.5045\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5119 - val_loss: 0.2501 - val_acc: 0.5294\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5263 - val_loss: 0.2508 - val_acc: 0.5328\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5233 - val_loss: 0.2503 - val_acc: 0.5303\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5248 - val_loss: 0.2506 - val_acc: 0.5170\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5301 - val_loss: 0.2502 - val_acc: 0.5300\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5244 - val_loss: 0.2506 - val_acc: 0.5296\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5259 - val_loss: 0.2509 - val_acc: 0.5245\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.5256 - val_loss: 0.2498 - val_acc: 0.5074\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5287 - val_loss: 0.2499 - val_acc: 0.5044\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5264 - val_loss: 0.2499 - val_acc: 0.5076\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5220 - val_loss: 0.2505 - val_acc: 0.5208\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5218 - val_loss: 0.2497 - val_acc: 0.5101\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5248 - val_loss: 0.2499 - val_acc: 0.5137\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5250 - val_loss: 0.2497 - val_acc: 0.5220\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5225 - val_loss: 0.2498 - val_acc: 0.5149\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5296 - val_loss: 0.2501 - val_acc: 0.5059\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5115 - val_loss: 0.2510 - val_acc: 0.5395\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5264 - val_loss: 0.2504 - val_acc: 0.5196\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5185 - val_loss: 0.2497 - val_acc: 0.5310\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5324 - val_loss: 0.2497 - val_acc: 0.5092\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5216 - val_loss: 0.2501 - val_acc: 0.5376\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5219 - val_loss: 0.2505 - val_acc: 0.5563\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2496 - acc: 0.5243 - val_loss: 0.2507 - val_acc: 0.5225\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.5163 - val_loss: 0.2506 - val_acc: 0.5261\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5292 - val_loss: 0.2501 - val_acc: 0.5037\n",
      "Epoch 38/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5133 - val_loss: 0.2499 - val_acc: 0.5458\n",
      "Epoch 39/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5311 - val_loss: 0.2500 - val_acc: 0.5222\n",
      "Epoch 40/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5184 - val_loss: 0.2497 - val_acc: 0.5323\n",
      "Epoch 41/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5234 - val_loss: 0.2508 - val_acc: 0.5056\n",
      "Epoch 42/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5195 - val_loss: 0.2505 - val_acc: 0.5165\n",
      "Epoch 43/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.5204 - val_loss: 0.2502 - val_acc: 0.5139\n",
      "Epoch 44/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5155 - val_loss: 0.2498 - val_acc: 0.5021\n",
      "Epoch 45/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5155 - val_loss: 0.2501 - val_acc: 0.5114\n",
      "Epoch 46/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5201 - val_loss: 0.2499 - val_acc: 0.5028\n",
      "Epoch 47/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5181 - val_loss: 0.2498 - val_acc: 0.5098\n",
      "Epoch 48/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5149 - val_loss: 0.2498 - val_acc: 0.4996\n",
      "Epoch 49/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5196 - val_loss: 0.2497 - val_acc: 0.5003\n",
      "Epoch 50/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5145 - val_loss: 0.2511 - val_acc: 0.5184\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2498 - acc: 0.5326\n",
      ". theta fit =  [0.9654963 1.4534613]\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2510 - acc: 0.5168 - val_loss: 0.2504 - val_acc: 0.5233\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5208 - val_loss: 0.2501 - val_acc: 0.4951\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5115 - val_loss: 0.2499 - val_acc: 0.5006\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2497 - acc: 0.5030 - val_loss: 0.2505 - val_acc: 0.5318\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5185 - val_loss: 0.2497 - val_acc: 0.5050\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2497 - acc: 0.5102 - val_loss: 0.2506 - val_acc: 0.5519\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.5267 - val_loss: 0.2501 - val_acc: 0.5186\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5210 - val_loss: 0.2498 - val_acc: 0.4938\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5150 - val_loss: 0.2497 - val_acc: 0.5040\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5134 - val_loss: 0.2497 - val_acc: 0.5428\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5208 - val_loss: 0.2498 - val_acc: 0.4988\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5160 - val_loss: 0.2502 - val_acc: 0.4954\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5201 - val_loss: 0.2505 - val_acc: 0.5348\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5228 - val_loss: 0.2499 - val_acc: 0.5273\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5216 - val_loss: 0.2498 - val_acc: 0.5297\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2497 - acc: 0.5052\n",
      ". theta fit =  [0.9651951 1.4537625]\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2500 - acc: 0.5103 - val_loss: 0.2497 - val_acc: 0.4960\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5173 - val_loss: 0.2497 - val_acc: 0.5165\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.5167 - val_loss: 0.2510 - val_acc: 0.5397\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5158 - val_loss: 0.2498 - val_acc: 0.5320\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5159 - val_loss: 0.2500 - val_acc: 0.5205\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5141 - val_loss: 0.2500 - val_acc: 0.5302\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5157 - val_loss: 0.2498 - val_acc: 0.4963\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.5189 - val_loss: 0.2500 - val_acc: 0.4903\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5106 - val_loss: 0.2498 - val_acc: 0.4987\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5123 - val_loss: 0.2501 - val_acc: 0.5134\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5127 - val_loss: 0.2498 - val_acc: 0.5235\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5183 - val_loss: 0.2498 - val_acc: 0.5130\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2497 - acc: 0.5167\n",
      ". theta fit =  [0.96499884 1.4534836 ]\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2507 - acc: 0.5123 - val_loss: 0.2510 - val_acc: 0.5134\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5155 - val_loss: 0.2507 - val_acc: 0.5325\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5223 - val_loss: 0.2498 - val_acc: 0.5136\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5224 - val_loss: 0.2497 - val_acc: 0.5060\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5151 - val_loss: 0.2507 - val_acc: 0.5420\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5202 - val_loss: 0.2503 - val_acc: 0.5473\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5268 - val_loss: 0.2498 - val_acc: 0.4962\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5215 - val_loss: 0.2497 - val_acc: 0.5055\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5197 - val_loss: 0.2498 - val_acc: 0.5226\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5198 - val_loss: 0.2498 - val_acc: 0.5406\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5273 - val_loss: 0.2498 - val_acc: 0.5034\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5163 - val_loss: 0.2499 - val_acc: 0.4973\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5126 - val_loss: 0.2506 - val_acc: 0.5353\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5307 - val_loss: 0.2497 - val_acc: 0.5077\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5255 - val_loss: 0.2497 - val_acc: 0.5070\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5181 - val_loss: 0.2499 - val_acc: 0.5365\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5337 - val_loss: 0.2498 - val_acc: 0.4840\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5156 - val_loss: 0.2502 - val_acc: 0.4965\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5118 - val_loss: 0.2499 - val_acc: 0.5409\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5270 - val_loss: 0.2504 - val_acc: 0.5173\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5073 - val_loss: 0.2497 - val_acc: 0.5390\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5250 - val_loss: 0.2497 - val_acc: 0.5206\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5225 - val_loss: 0.2502 - val_acc: 0.5090\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5229 - val_loss: 0.2503 - val_acc: 0.5302\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2497 - acc: 0.5080\n",
      ". theta fit =  [0.9646916 1.4531769]\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2512 - acc: 0.5088 - val_loss: 0.2511 - val_acc: 0.5085\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.5212 - val_loss: 0.2500 - val_acc: 0.5150\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5117 - val_loss: 0.2504 - val_acc: 0.5323\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5284 - val_loss: 0.2498 - val_acc: 0.4968\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.5166 - val_loss: 0.2507 - val_acc: 0.4960\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5126 - val_loss: 0.2497 - val_acc: 0.5116\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5259 - val_loss: 0.2499 - val_acc: 0.5166\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5179 - val_loss: 0.2500 - val_acc: 0.5406\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5223 - val_loss: 0.2497 - val_acc: 0.5270\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5290 - val_loss: 0.2503 - val_acc: 0.5242\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5240 - val_loss: 0.2499 - val_acc: 0.5235\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5226 - val_loss: 0.2500 - val_acc: 0.5117\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5116 - val_loss: 0.2501 - val_acc: 0.5390\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5255 - val_loss: 0.2500 - val_acc: 0.5315\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5287 - val_loss: 0.2507 - val_acc: 0.5308\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5205 - val_loss: 0.2506 - val_acc: 0.5215\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5250 - val_loss: 0.2500 - val_acc: 0.5352\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5257 - val_loss: 0.2503 - val_acc: 0.5215\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5245 - val_loss: 0.2498 - val_acc: 0.5247\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2497 - acc: 0.5272\n",
      ". theta fit =  [0.96439624 1.4528636 ]\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2507 - acc: 0.5190 - val_loss: 0.2513 - val_acc: 0.5150\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.5110 - val_loss: 0.2508 - val_acc: 0.5330\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5069 - val_loss: 0.2498 - val_acc: 0.5365\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.5290 - val_loss: 0.2507 - val_acc: 0.5318\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5282 - val_loss: 0.2506 - val_acc: 0.5398\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5313 - val_loss: 0.2497 - val_acc: 0.5165\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5190 - val_loss: 0.2507 - val_acc: 0.5162\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5173 - val_loss: 0.2499 - val_acc: 0.4990\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5181 - val_loss: 0.2497 - val_acc: 0.5120\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5210 - val_loss: 0.2499 - val_acc: 0.5288\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5291 - val_loss: 0.2498 - val_acc: 0.4885\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5121 - val_loss: 0.2501 - val_acc: 0.5448\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5201 - val_loss: 0.2503 - val_acc: 0.5368\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5223 - val_loss: 0.2504 - val_acc: 0.5273\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5263 - val_loss: 0.2500 - val_acc: 0.5331\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5287 - val_loss: 0.2499 - val_acc: 0.5313\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5252 - val_loss: 0.2504 - val_acc: 0.5056\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5107 - val_loss: 0.2497 - val_acc: 0.5177\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5191 - val_loss: 0.2506 - val_acc: 0.5251\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2497 - acc: 0.5122\n",
      ". theta fit =  [0.9640816 1.4531766]\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2505 - acc: 0.5154 - val_loss: 0.2498 - val_acc: 0.5053\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5102 - val_loss: 0.2496 - val_acc: 0.5309\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5276 - val_loss: 0.2497 - val_acc: 0.5348\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5318 - val_loss: 0.2504 - val_acc: 0.5218\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5277 - val_loss: 0.2511 - val_acc: 0.5162\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5149 - val_loss: 0.2510 - val_acc: 0.5231\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5189 - val_loss: 0.2503 - val_acc: 0.5529\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5335 - val_loss: 0.2500 - val_acc: 0.5135\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5257 - val_loss: 0.2502 - val_acc: 0.5171\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5295 - val_loss: 0.2504 - val_acc: 0.5059\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5232 - val_loss: 0.2501 - val_acc: 0.5416\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5198 - val_loss: 0.2498 - val_acc: 0.5167\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2497 - acc: 0.5311\n",
      ". theta fit =  [0.96439725 1.4528577 ]\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2509 - acc: 0.5154 - val_loss: 0.2504 - val_acc: 0.5057\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5066 - val_loss: 0.2508 - val_acc: 0.5524\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5170 - val_loss: 0.2497 - val_acc: 0.5021\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5214 - val_loss: 0.2506 - val_acc: 0.5262\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5272 - val_loss: 0.2499 - val_acc: 0.4986\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5155 - val_loss: 0.2502 - val_acc: 0.5390\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5179 - val_loss: 0.2507 - val_acc: 0.5192\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5192 - val_loss: 0.2500 - val_acc: 0.5390\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5251 - val_loss: 0.2499 - val_acc: 0.5401\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5314 - val_loss: 0.2499 - val_acc: 0.5215\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5183 - val_loss: 0.2497 - val_acc: 0.5441\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5232 - val_loss: 0.2500 - val_acc: 0.5079\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5045 - val_loss: 0.2498 - val_acc: 0.5312\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2498 - acc: 0.5023\n",
      ". theta fit =  [0.96407515 1.4525359 ]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2504 - acc: 0.5073 - val_loss: 0.2510 - val_acc: 0.4958\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5103 - val_loss: 0.2503 - val_acc: 0.4920\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5052 - val_loss: 0.2497 - val_acc: 0.4882\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.5109 - val_loss: 0.2510 - val_acc: 0.5030\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.5204 - val_loss: 0.2500 - val_acc: 0.5058\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5247 - val_loss: 0.2497 - val_acc: 0.5107\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5223 - val_loss: 0.2500 - val_acc: 0.5332\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5275 - val_loss: 0.2497 - val_acc: 0.5154\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5255 - val_loss: 0.2497 - val_acc: 0.5145\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.5105 - val_loss: 0.2513 - val_acc: 0.5643\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5259 - val_loss: 0.2500 - val_acc: 0.5151\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5275 - val_loss: 0.2503 - val_acc: 0.4978\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.5015 - val_loss: 0.2497 - val_acc: 0.5392\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5188 - val_loss: 0.2498 - val_acc: 0.5325\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5146 - val_loss: 0.2503 - val_acc: 0.5152\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5119 - val_loss: 0.2498 - val_acc: 0.5295\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5224 - val_loss: 0.2504 - val_acc: 0.5055\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5175 - val_loss: 0.2500 - val_acc: 0.5083\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2497 - acc: 0.5156\n",
      ". theta fit =  [0.9642053 1.4528261]\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2509 - acc: 0.5065 - val_loss: 0.2508 - val_acc: 0.5459\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5188 - val_loss: 0.2499 - val_acc: 0.5040\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5154 - val_loss: 0.2509 - val_acc: 0.5267\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5230 - val_loss: 0.2498 - val_acc: 0.5275\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5249 - val_loss: 0.2497 - val_acc: 0.5220\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5317 - val_loss: 0.2507 - val_acc: 0.5043\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5158 - val_loss: 0.2497 - val_acc: 0.5174\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5397 - val_loss: 0.2498 - val_acc: 0.4825\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5105 - val_loss: 0.2497 - val_acc: 0.5153\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5267 - val_loss: 0.2500 - val_acc: 0.5056\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5184 - val_loss: 0.2506 - val_acc: 0.5149\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5107 - val_loss: 0.2497 - val_acc: 0.5290\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5198 - val_loss: 0.2497 - val_acc: 0.5069\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5181 - val_loss: 0.2497 - val_acc: 0.5408\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5296 - val_loss: 0.2497 - val_acc: 0.5033\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2497 - acc: 0.5223\n",
      ". theta fit =  [0.9642379 1.4528592]\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2508 - acc: 0.5113 - val_loss: 0.2499 - val_acc: 0.4941\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5231 - val_loss: 0.2502 - val_acc: 0.4912\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5199 - val_loss: 0.2498 - val_acc: 0.5042\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5182 - val_loss: 0.2497 - val_acc: 0.4992\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5135 - val_loss: 0.2509 - val_acc: 0.5565\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5285 - val_loss: 0.2497 - val_acc: 0.5033\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5178 - val_loss: 0.2511 - val_acc: 0.5304\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5217 - val_loss: 0.2501 - val_acc: 0.5180\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5243 - val_loss: 0.2500 - val_acc: 0.5258\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5219 - val_loss: 0.2504 - val_acc: 0.5182\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5174 - val_loss: 0.2500 - val_acc: 0.5034\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5176 - val_loss: 0.2499 - val_acc: 0.5406\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5198 - val_loss: 0.2498 - val_acc: 0.5484\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5309 - val_loss: 0.2496 - val_acc: 0.5417\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.5302 - val_loss: 0.2513 - val_acc: 0.5225\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5256 - val_loss: 0.2498 - val_acc: 0.5084\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5254 - val_loss: 0.2509 - val_acc: 0.5097\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5132 - val_loss: 0.2502 - val_acc: 0.5637\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5339 - val_loss: 0.2499 - val_acc: 0.4994\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5200 - val_loss: 0.2501 - val_acc: 0.5255\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5276 - val_loss: 0.2500 - val_acc: 0.5138\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5159 - val_loss: 0.2502 - val_acc: 0.5588\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5285 - val_loss: 0.2503 - val_acc: 0.5211\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5214 - val_loss: 0.2508 - val_acc: 0.5146\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2497 - acc: 0.5421\n",
      ". theta fit =  [0.9642712 1.452826 ]\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2503 - acc: 0.5210 - val_loss: 0.2506 - val_acc: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.5191 - val_loss: 0.2502 - val_acc: 0.5104\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5278 - val_loss: 0.2499 - val_acc: 0.4925\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5149 - val_loss: 0.2503 - val_acc: 0.5483\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5303 - val_loss: 0.2499 - val_acc: 0.5060\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5177 - val_loss: 0.2497 - val_acc: 0.5343\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5340 - val_loss: 0.2497 - val_acc: 0.5292\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5253 - val_loss: 0.2497 - val_acc: 0.4996\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5198 - val_loss: 0.2504 - val_acc: 0.5287\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5315 - val_loss: 0.2502 - val_acc: 0.5200\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5220 - val_loss: 0.2499 - val_acc: 0.5128\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5145 - val_loss: 0.2505 - val_acc: 0.5591\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5264 - val_loss: 0.2507 - val_acc: 0.5188\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5172 - val_loss: 0.2506 - val_acc: 0.5162\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5179 - val_loss: 0.2498 - val_acc: 0.5084\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5157 - val_loss: 0.2500 - val_acc: 0.5240\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.5157 - val_loss: 0.2507 - val_acc: 0.5054\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2497 - acc: 0.5295\n",
      ". theta fit =  [0.9643043 1.4528594]\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2509 - acc: 0.5145 - val_loss: 0.2499 - val_acc: 0.5107\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5166 - val_loss: 0.2498 - val_acc: 0.5306\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5257 - val_loss: 0.2497 - val_acc: 0.5251\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5321 - val_loss: 0.2500 - val_acc: 0.5125\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5094 - val_loss: 0.2498 - val_acc: 0.5281\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5291 - val_loss: 0.2509 - val_acc: 0.5004\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.5192 - val_loss: 0.2498 - val_acc: 0.5399\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5364 - val_loss: 0.2497 - val_acc: 0.5061\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5170 - val_loss: 0.2499 - val_acc: 0.5105\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.5223 - val_loss: 0.2497 - val_acc: 0.5320\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5289 - val_loss: 0.2506 - val_acc: 0.5107\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5185 - val_loss: 0.2499 - val_acc: 0.5362\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5322 - val_loss: 0.2506 - val_acc: 0.5056\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2498 - acc: 0.5254\n",
      ". theta fit =  [0.96427155 1.4528259 ]\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2508 - acc: 0.5182 - val_loss: 0.2505 - val_acc: 0.5067\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5138 - val_loss: 0.2497 - val_acc: 0.5192\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5313 - val_loss: 0.2497 - val_acc: 0.4939\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5238 - val_loss: 0.2498 - val_acc: 0.4955\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5231 - val_loss: 0.2510 - val_acc: 0.5048\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5190 - val_loss: 0.2497 - val_acc: 0.5210\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5343 - val_loss: 0.2506 - val_acc: 0.5064\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5245 - val_loss: 0.2507 - val_acc: 0.4973\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5137 - val_loss: 0.2502 - val_acc: 0.5119\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5272 - val_loss: 0.2498 - val_acc: 0.4922\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5134 - val_loss: 0.2497 - val_acc: 0.5634\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5380 - val_loss: 0.2498 - val_acc: 0.4928\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2498 - acc: 0.5194\n",
      ". theta fit =  [0.9642504 1.452792 ]\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2510 - acc: 0.5127 - val_loss: 0.2499 - val_acc: 0.4842\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5026 - val_loss: 0.2499 - val_acc: 0.5291\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5262 - val_loss: 0.2503 - val_acc: 0.5077\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5139 - val_loss: 0.2498 - val_acc: 0.4978\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5264 - val_loss: 0.2498 - val_acc: 0.4988\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5139 - val_loss: 0.2510 - val_acc: 0.5390\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5281 - val_loss: 0.2500 - val_acc: 0.5057\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5134 - val_loss: 0.2499 - val_acc: 0.4901\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5148 - val_loss: 0.2499 - val_acc: 0.4905\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5085 - val_loss: 0.2501 - val_acc: 0.5410\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5268 - val_loss: 0.2500 - val_acc: 0.5403\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5264 - val_loss: 0.2497 - val_acc: 0.5137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5121 - val_loss: 0.2502 - val_acc: 0.5025\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5265 - val_loss: 0.2508 - val_acc: 0.5133\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5063 - val_loss: 0.2497 - val_acc: 0.5367\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5231 - val_loss: 0.2500 - val_acc: 0.5035\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5140 - val_loss: 0.2497 - val_acc: 0.5296\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5211 - val_loss: 0.2498 - val_acc: 0.4987\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5180 - val_loss: 0.2498 - val_acc: 0.5087\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2497 - acc: 0.5063 - val_loss: 0.2502 - val_acc: 0.5510\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.5234 - val_loss: 0.2504 - val_acc: 0.5055\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2496 - acc: 0.5114 - val_loss: 0.2512 - val_acc: 0.5454\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.5238 - val_loss: 0.2502 - val_acc: 0.4854\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5146 - val_loss: 0.2510 - val_acc: 0.5653\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5204 - val_loss: 0.2506 - val_acc: 0.5656\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2497 - acc: 0.5371\n",
      ". theta fit =  [0.9642851 1.4528267]\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2501 - acc: 0.5070 - val_loss: 0.2505 - val_acc: 0.5056\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5111 - val_loss: 0.2501 - val_acc: 0.5489\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.5157 - val_loss: 0.2500 - val_acc: 0.5376\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.5188 - val_loss: 0.2497 - val_acc: 0.5124\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5241 - val_loss: 0.2497 - val_acc: 0.5235\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5221 - val_loss: 0.2497 - val_acc: 0.5098\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5202 - val_loss: 0.2497 - val_acc: 0.5057\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.5200 - val_loss: 0.2499 - val_acc: 0.5347\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5217 - val_loss: 0.2507 - val_acc: 0.5163\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5103 - val_loss: 0.2497 - val_acc: 0.5489\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.5279 - val_loss: 0.2508 - val_acc: 0.5235\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.5282 - val_loss: 0.2498 - val_acc: 0.4919\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.5116 - val_loss: 0.2508 - val_acc: 0.5574\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.5237 - val_loss: 0.2498 - val_acc: 0.4956\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2498 - acc: 0.5126\n",
      ". theta fit =  [0.96425027 1.4527918 ]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration)\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()\n",
    "\n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "                      loss=my_loss_wrapper_fit(myinputs_fit, 1),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train),\n",
    "                  y_train,\n",
    "                  epochs=100,\n",
    "                  batch_size=1000,\n",
    "                  validation_data=(np.array(X_test), y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = True\n",
    "\n",
    "    # special optimizer and batch size = 2*N\n",
    "    model_fit.compile(optimizer=optimizer,\n",
    "                      loss=my_loss_wrapper_fit(myinputs_fit, -1),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta),\n",
    "                  y_train_theta,\n",
    "                  epochs=1,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_mu = np.array(fit_vals)[(index_refine[-1]):, 0]\n",
    "    fit_vals_sigma = np.array(fit_vals)[(index_refine[-1]):, 1]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "    extrema_mu = np.concatenate(\n",
    "        (argrelmin(fit_vals_mu)[0], argrelmax(fit_vals_mu)[0]))\n",
    "    extrema_mu = extrema_mu[extrema_mu >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    extrema_sigma = np.concatenate(\n",
    "        (argrelmin(fit_vals_sigma)[0], argrelmax(fit_vals_sigma)[0]))\n",
    "    extrema_sigma = extrema_sigma[extrema_sigma >= iteration -\n",
    "                                  index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema_mu\", extrema_mu)\n",
    "    print(\"extrema_sigma\", extrema_sigma)\n",
    "    '''\n",
    "\n",
    "    if (len(extrema_mu) == 0) or (\n",
    "            len(extrema_sigma)\n",
    "            == 0):  # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema_mu) >= 6) and (len(extrema_sigma) >=\n",
    "                                     6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10\n",
    "\n",
    "        mean_fit = np.array([[\n",
    "            np.mean(fit_vals_mu[len(fit_vals_mu) - 4:len(fit_vals_mu)]),\n",
    "            np.mean(fit_vals_sigma[len(fit_vals_sigma) -\n",
    "                                   4:len(fit_vals_sigma)])\n",
    "        ]])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T11:46:55.718959Z",
     "start_time": "2020-06-09T11:46:55.195626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex=True, constrained_layout=True)\n",
    "fig.suptitle(\n",
    "    \"GaussianAltFit-2D (DCTR Reweight):\\n N = {:.0e}, Iterations = {:.0f}\".\n",
    "    format(N, iterations))\n",
    "axs[0].plot(fit_vals[:, 0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], 0, len(fit_vals), label='$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(fit_vals[:, 1], label='Model $\\sigma$ Fit', color='orange')\n",
    "axs[1].hlines(theta1_param[1], 0, len(fit_vals), label='$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-2D (DCTR Reweight):\\n N = {:.0e}, Iterations = {:.0f}.png\".\n",
    "#     format(N, iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T11:46:56.407084Z",
     "start_time": "2020-06-09T11:46:55.721685Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "\n",
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex=True, constrained_layout=True)\n",
    "fig.suptitle(\n",
    "    \"GaussianAltFit-2D Zoomed (DCTR Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\"\n",
    "    .format(N, index_refine[1], iterations))\n",
    "axs[0].plot(np.arange(index_refine[1], len(fit_vals[:, 0])),\n",
    "            fit_vals[index_refine[1]:, 0],\n",
    "            label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0],\n",
    "              index_refine[1],\n",
    "              len(fit_vals),\n",
    "              label='$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.arange(index_refine[1], len(fit_vals[:, 1])),\n",
    "            fit_vals[index_refine[1]:, 1],\n",
    "            label='Model $\\sigma$ Fit',\n",
    "            color='orange')\n",
    "axs[1].hlines(theta1_param[1],\n",
    "              index_refine[1],\n",
    "              len(fit_vals),\n",
    "              label='$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-2D Zoomed (DCTR Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\"\n",
    "#     .format(N, index_refine[1], iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Fitting between DCTR Reweighting and Analytical Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T11:46:56.435340Z",
     "start_time": "2020-06-09T11:46:56.411834Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_vals = [theta_fit_init]\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:36:04.666322Z",
     "start_time": "2020-06-09T11:46:56.439983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2500 - acc: 0.5456 - val_loss: 0.2495 - val_acc: 0.5881\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5591 - val_loss: 0.2488 - val_acc: 0.6072\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5808 - val_loss: 0.2489 - val_acc: 0.5609\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5636 - val_loss: 0.2489 - val_acc: 0.5893\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5682 - val_loss: 0.2495 - val_acc: 0.5654\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5830 - val_loss: 0.2492 - val_acc: 0.5686\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5729 - val_loss: 0.2493 - val_acc: 0.6032\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5752 - val_loss: 0.2493 - val_acc: 0.5659\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5798 - val_loss: 0.2491 - val_acc: 0.6011\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5702 - val_loss: 0.2486 - val_acc: 0.6018\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5782 - val_loss: 0.2487 - val_acc: 0.6223\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5848 - val_loss: 0.2484 - val_acc: 0.5906\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5706 - val_loss: 0.2495 - val_acc: 0.6320\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5925 - val_loss: 0.2488 - val_acc: 0.5875\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5826 - val_loss: 0.2495 - val_acc: 0.5624\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5682 - val_loss: 0.2492 - val_acc: 0.5682\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5721 - val_loss: 0.2493 - val_acc: 0.6065\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5864 - val_loss: 0.2488 - val_acc: 0.5219\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5723 - val_loss: 0.2493 - val_acc: 0.5635\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5840 - val_loss: 0.2492 - val_acc: 0.5420\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5779 - val_loss: 0.2492 - val_acc: 0.5597\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5633 - val_loss: 0.2484 - val_acc: 0.6039\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6002 - val_loss: 0.2484 - val_acc: 0.5505\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5741 - val_loss: 0.2495 - val_acc: 0.5934\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5792 - val_loss: 0.2486 - val_acc: 0.5657\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5820 - val_loss: 0.2486 - val_acc: 0.6134\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5860 - val_loss: 0.2486 - val_acc: 0.5115\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5766 - val_loss: 0.2485 - val_acc: 0.5647\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5730 - val_loss: 0.2485 - val_acc: 0.5948\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5809 - val_loss: 0.2485 - val_acc: 0.5876\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5839 - val_loss: 0.2488 - val_acc: 0.5899\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5923 - val_loss: 0.2494 - val_acc: 0.5390\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2485 - acc: 0.6044\n",
      ". theta fit =  [1.4584224 1.9505682]\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2462 - acc: 0.3911 - val_loss: 0.2450 - val_acc: 0.3823\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2385 - acc: 0.3786 - val_loss: 0.2378 - val_acc: 0.3764\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2376 - acc: 0.3881 - val_loss: 0.2371 - val_acc: 0.3745\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2366 - acc: 0.3925 - val_loss: 0.2367 - val_acc: 0.3866\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2359 - acc: 0.3947 - val_loss: 0.2456 - val_acc: 0.4074\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2371 - acc: 0.3976 - val_loss: 0.2388 - val_acc: 0.3981\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2374 - acc: 0.3969 - val_loss: 0.2361 - val_acc: 0.4027\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2363 - acc: 0.4010 - val_loss: 0.2361 - val_acc: 0.3977\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2367 - acc: 0.4018 - val_loss: 0.2361 - val_acc: 0.3992\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2361 - acc: 0.4050 - val_loss: 0.2367 - val_acc: 0.3993\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2357 - acc: 0.4051 - val_loss: 0.2423 - val_acc: 0.4115\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2363 - acc: 0.4061 - val_loss: 0.2370 - val_acc: 0.4149\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2361 - acc: 0.4096 - val_loss: 0.2360 - val_acc: 0.3976\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2358 - acc: 0.4100 - val_loss: 0.2360 - val_acc: 0.4056\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2365 - acc: 0.4114 - val_loss: 0.2361 - val_acc: 0.4034\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2365 - acc: 0.4103 - val_loss: 0.2379 - val_acc: 0.4105\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2366 - acc: 0.4114 - val_loss: 0.2364 - val_acc: 0.4147\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2362 - acc: 0.4119 - val_loss: 0.2360 - val_acc: 0.4212\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2358 - acc: 0.4155 - val_loss: 0.2392 - val_acc: 0.4020\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2377 - acc: 0.4137 - val_loss: 0.2359 - val_acc: 0.4105\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2357 - acc: 0.4145 - val_loss: 0.2414 - val_acc: 0.4101\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2378 - acc: 0.4163 - val_loss: 0.2440 - val_acc: 0.4181\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2372 - acc: 0.4145 - val_loss: 0.2364 - val_acc: 0.4146\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2367 - acc: 0.4115 - val_loss: 0.2403 - val_acc: 0.4194\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2368 - acc: 0.4128 - val_loss: 0.2420 - val_acc: 0.4202\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2364 - acc: 0.4136 - val_loss: 0.2432 - val_acc: 0.4308\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2363 - acc: 0.4153 - val_loss: 0.2397 - val_acc: 0.4110\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2368 - acc: 0.4132 - val_loss: 0.2361 - val_acc: 0.4142\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2360 - acc: 0.4156 - val_loss: 0.2359 - val_acc: 0.4190\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2376 - acc: 0.4160 - val_loss: 0.2358 - val_acc: 0.4165\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2355 - acc: 0.4188 - val_loss: 0.2433 - val_acc: 0.3914\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2374 - acc: 0.4128 - val_loss: 0.2359 - val_acc: 0.4220\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2358 - acc: 0.4147 - val_loss: 0.2372 - val_acc: 0.4221\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2355 - acc: 0.4169 - val_loss: 0.2446 - val_acc: 0.4278\n",
      "Epoch 35/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2372 - acc: 0.4156 - val_loss: 0.2381 - val_acc: 0.4177\n",
      "Epoch 36/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2364 - acc: 0.4145 - val_loss: 0.2362 - val_acc: 0.4141\n",
      "Epoch 37/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2357 - acc: 0.4152 - val_loss: 0.2434 - val_acc: 0.4162\n",
      "Epoch 38/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2371 - acc: 0.4156 - val_loss: 0.2359 - val_acc: 0.4177\n",
      "Epoch 39/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2364 - acc: 0.4163 - val_loss: 0.2360 - val_acc: 0.4212\n",
      "Epoch 40/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2361 - acc: 0.4131 - val_loss: 0.2360 - val_acc: 0.4262\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2359 - acc: 0.4159\n",
      ". theta fit =  [1.0864253 1.5785503]\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2518 - acc: 0.4005 - val_loss: 0.2509 - val_acc: 0.3831\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2489 - acc: 0.3935 - val_loss: 0.2497 - val_acc: 0.3840\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2480 - acc: 0.3948 - val_loss: 0.2492 - val_acc: 0.3911\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2479 - acc: 0.3888 - val_loss: 0.2500 - val_acc: 0.3749\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.3852 - val_loss: 0.2478 - val_acc: 0.3604\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2475 - acc: 0.3922 - val_loss: 0.2482 - val_acc: 0.3664\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.3945 - val_loss: 0.2483 - val_acc: 0.3880\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.3899 - val_loss: 0.2504 - val_acc: 0.3841\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2480 - acc: 0.3974 - val_loss: 0.2475 - val_acc: 0.3644\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.3855 - val_loss: 0.2506 - val_acc: 0.4415\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.3899 - val_loss: 0.2475 - val_acc: 0.3746\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2470 - acc: 0.3939 - val_loss: 0.2516 - val_acc: 0.3982\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.3900 - val_loss: 0.2504 - val_acc: 0.3906\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.3813 - val_loss: 0.2475 - val_acc: 0.4049\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2481 - acc: 0.4045 - val_loss: 0.2477 - val_acc: 0.3520\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2475 - acc: 0.3908 - val_loss: 0.2478 - val_acc: 0.3754\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.3918 - val_loss: 0.2474 - val_acc: 0.4069\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2478 - acc: 0.4049 - val_loss: 0.2476 - val_acc: 0.4130\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2475 - acc: 0.3935 - val_loss: 0.2480 - val_acc: 0.4255\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2479 - acc: 0.3965 - val_loss: 0.2476 - val_acc: 0.3733\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2476 - acc: 0.3990 - val_loss: 0.2478 - val_acc: 0.4151\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.3953 - val_loss: 0.2499 - val_acc: 0.3774\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2476 - acc: 0.3995 - val_loss: 0.2478 - val_acc: 0.4172\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2476 - acc: 0.4009 - val_loss: 0.2477 - val_acc: 0.4225\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2478 - acc: 0.4017 - val_loss: 0.2483 - val_acc: 0.4546\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2475 - acc: 0.4034 - val_loss: 0.2476 - val_acc: 0.3439\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2481 - acc: 0.3894 - val_loss: 0.2481 - val_acc: 0.4129\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2474 - acc: 0.4064\n",
      ". theta fit =  [0.76716864 1.2592243 ]\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2460 - acc: 0.6153 - val_loss: 0.2447 - val_acc: 0.6119\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2448 - acc: 0.6113 - val_loss: 0.2446 - val_acc: 0.6069\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2446 - acc: 0.6140 - val_loss: 0.2446 - val_acc: 0.6167\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.6142 - val_loss: 0.2445 - val_acc: 0.6195\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.6157 - val_loss: 0.2446 - val_acc: 0.6063\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.6172 - val_loss: 0.2445 - val_acc: 0.6140\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.6158 - val_loss: 0.2445 - val_acc: 0.6202\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2446 - acc: 0.6165 - val_loss: 0.2446 - val_acc: 0.6080\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2447 - acc: 0.6182 - val_loss: 0.2445 - val_acc: 0.6152\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.6173 - val_loss: 0.2446 - val_acc: 0.6275\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.6179 - val_loss: 0.2445 - val_acc: 0.6216\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2446 - acc: 0.6187 - val_loss: 0.2445 - val_acc: 0.6273\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.6184 - val_loss: 0.2445 - val_acc: 0.6165\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2446 - acc: 0.6170 - val_loss: 0.2446 - val_acc: 0.6117\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.6170 - val_loss: 0.2445 - val_acc: 0.6184\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2446 - acc: 0.6177 - val_loss: 0.2446 - val_acc: 0.6191\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2445 - acc: 0.6144\n",
      ". theta fit =  [1.0576555 1.5497549]\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2501 - acc: 0.4620 - val_loss: 0.2482 - val_acc: 0.4285\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2480 - acc: 0.4103 - val_loss: 0.2482 - val_acc: 0.3509\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.4203 - val_loss: 0.2480 - val_acc: 0.3731\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.4057 - val_loss: 0.2480 - val_acc: 0.4300\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.4136 - val_loss: 0.2481 - val_acc: 0.4360\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.4142 - val_loss: 0.2484 - val_acc: 0.4801\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2479 - acc: 0.4110 - val_loss: 0.2486 - val_acc: 0.4249\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.4103 - val_loss: 0.2488 - val_acc: 0.4668\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2480 - acc: 0.4090 - val_loss: 0.2489 - val_acc: 0.3547\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.3973 - val_loss: 0.2484 - val_acc: 0.4512\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.3936 - val_loss: 0.2488 - val_acc: 0.3980\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2478 - acc: 0.4086 - val_loss: 0.2502 - val_acc: 0.4219\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.3943 - val_loss: 0.2484 - val_acc: 0.4976\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2481 - acc: 0.4079 - val_loss: 0.2478 - val_acc: 0.4610\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2480 - acc: 0.4102 - val_loss: 0.2488 - val_acc: 0.4746\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.3903 - val_loss: 0.2481 - val_acc: 0.4526\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2479 - acc: 0.3957 - val_loss: 0.2480 - val_acc: 0.3642\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2480 - acc: 0.3909 - val_loss: 0.2484 - val_acc: 0.4416\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2483 - acc: 0.3934 - val_loss: 0.2492 - val_acc: 0.4597\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.4034 - val_loss: 0.2486 - val_acc: 0.3816\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2479 - acc: 0.3951 - val_loss: 0.2494 - val_acc: 0.4179\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.3901 - val_loss: 0.2492 - val_acc: 0.4392\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2481 - acc: 0.3847 - val_loss: 0.2479 - val_acc: 0.4639\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2478 - acc: 0.3941 - val_loss: 0.2504 - val_acc: 0.3441\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2479 - acc: 0.4605\n",
      ". theta fit =  [0.785138  1.2771072]\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2465 - acc: 0.6239 - val_loss: 0.2454 - val_acc: 0.6233\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2455 - acc: 0.6219 - val_loss: 0.2456 - val_acc: 0.6164\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2455 - acc: 0.6219 - val_loss: 0.2453 - val_acc: 0.6242\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6171 - val_loss: 0.2457 - val_acc: 0.6020\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6187 - val_loss: 0.2453 - val_acc: 0.6335\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6202 - val_loss: 0.2452 - val_acc: 0.6261\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2453 - acc: 0.6202 - val_loss: 0.2453 - val_acc: 0.6221\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2453 - acc: 0.6205 - val_loss: 0.2453 - val_acc: 0.6256\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6208 - val_loss: 0.2453 - val_acc: 0.6084\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6200 - val_loss: 0.2452 - val_acc: 0.6157\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6201 - val_loss: 0.2453 - val_acc: 0.6126\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6202 - val_loss: 0.2452 - val_acc: 0.6228\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6206 - val_loss: 0.2453 - val_acc: 0.6241\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6206 - val_loss: 0.2452 - val_acc: 0.6129\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6188 - val_loss: 0.2452 - val_acc: 0.6148\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2453 - acc: 0.6203 - val_loss: 0.2455 - val_acc: 0.5954\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6182 - val_loss: 0.2453 - val_acc: 0.6309\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2453 - acc: 0.6206 - val_loss: 0.2453 - val_acc: 0.6165\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6209 - val_loss: 0.2452 - val_acc: 0.6110\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2453 - acc: 0.6179 - val_loss: 0.2452 - val_acc: 0.6162\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2453 - acc: 0.6188 - val_loss: 0.2452 - val_acc: 0.6065\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6195 - val_loss: 0.2452 - val_acc: 0.6213\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6209 - val_loss: 0.2452 - val_acc: 0.6163\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6210 - val_loss: 0.2452 - val_acc: 0.6168\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6204 - val_loss: 0.2452 - val_acc: 0.6160\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2452 - acc: 0.6191 - val_loss: 0.2454 - val_acc: 0.6243\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2453 - acc: 0.6210 - val_loss: 0.2454 - val_acc: 0.6098\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6187 - val_loss: 0.2453 - val_acc: 0.6288\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2453 - acc: 0.6203 - val_loss: 0.2453 - val_acc: 0.6126\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2454 - acc: 0.6207 - val_loss: 0.2452 - val_acc: 0.6177\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2453 - acc: 0.6193 - val_loss: 0.2453 - val_acc: 0.6250\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2454 - acc: 0.6199 - val_loss: 0.2452 - val_acc: 0.6194\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2453 - acc: 0.6204 - val_loss: 0.2453 - val_acc: 0.6228\n",
      "Epoch 34/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2453 - acc: 0.6183 - val_loss: 0.2453 - val_acc: 0.6295\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2452 - acc: 0.6173\n",
      ". theta fit =  [1.0461183 1.5381337]\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2505 - acc: 0.4464 - val_loss: 0.2497 - val_acc: 0.3540\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2481 - acc: 0.3774 - val_loss: 0.2508 - val_acc: 0.3515\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.3861 - val_loss: 0.2486 - val_acc: 0.3385\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.4000 - val_loss: 0.2481 - val_acc: 0.4343\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.3960 - val_loss: 0.2481 - val_acc: 0.4802\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2483 - acc: 0.4133 - val_loss: 0.2481 - val_acc: 0.4491\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.3998 - val_loss: 0.2482 - val_acc: 0.3627\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.4152 - val_loss: 0.2480 - val_acc: 0.4559\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2483 - acc: 0.4148 - val_loss: 0.2483 - val_acc: 0.3701\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2478 - acc: 0.4032 - val_loss: 0.2481 - val_acc: 0.4199\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.4077 - val_loss: 0.2480 - val_acc: 0.4814\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.4236 - val_loss: 0.2481 - val_acc: 0.3804\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.3993 - val_loss: 0.2481 - val_acc: 0.3631\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2481 - acc: 0.4056 - val_loss: 0.2484 - val_acc: 0.4335\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2481 - acc: 0.3913 - val_loss: 0.2492 - val_acc: 0.3473\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2478 - acc: 0.3896 - val_loss: 0.2479 - val_acc: 0.4165\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.3923 - val_loss: 0.2480 - val_acc: 0.3763\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2480 - acc: 0.4126 - val_loss: 0.2489 - val_acc: 0.3333\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.3890 - val_loss: 0.2489 - val_acc: 0.3697\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3865 - val_loss: 0.2484 - val_acc: 0.3883\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.4074 - val_loss: 0.2486 - val_acc: 0.3438\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.3859 - val_loss: 0.2482 - val_acc: 0.3660\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.3984 - val_loss: 0.2484 - val_acc: 0.3467\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.3953 - val_loss: 0.2496 - val_acc: 0.4640\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2481 - acc: 0.4222 - val_loss: 0.2494 - val_acc: 0.4829\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2481 - acc: 0.4237 - val_loss: 0.2492 - val_acc: 0.3880\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2480 - acc: 0.4159\n",
      ". theta fit =  [0.79840064 1.2903491 ]\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2466 - acc: 0.6117 - val_loss: 0.2460 - val_acc: 0.6123\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2461 - acc: 0.6166 - val_loss: 0.2459 - val_acc: 0.6135\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2459 - acc: 0.6152 - val_loss: 0.2458 - val_acc: 0.6294\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6156 - val_loss: 0.2457 - val_acc: 0.6231\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6161 - val_loss: 0.2458 - val_acc: 0.6202\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2459 - acc: 0.6141 - val_loss: 0.2457 - val_acc: 0.6210\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6169 - val_loss: 0.2458 - val_acc: 0.6161\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2459 - acc: 0.6163 - val_loss: 0.2457 - val_acc: 0.6117\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6154 - val_loss: 0.2458 - val_acc: 0.6075\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6146 - val_loss: 0.2457 - val_acc: 0.6185\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2459 - acc: 0.6165 - val_loss: 0.2457 - val_acc: 0.6152\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2459 - acc: 0.6170 - val_loss: 0.2458 - val_acc: 0.5966\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6153 - val_loss: 0.2458 - val_acc: 0.6210\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6158 - val_loss: 0.2458 - val_acc: 0.6099\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6181 - val_loss: 0.2457 - val_acc: 0.6181\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6155 - val_loss: 0.2459 - val_acc: 0.6271\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2457 - acc: 0.6163 - val_loss: 0.2459 - val_acc: 0.6206\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2459 - acc: 0.6161 - val_loss: 0.2457 - val_acc: 0.6094\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2458 - acc: 0.6155 - val_loss: 0.2457 - val_acc: 0.6196\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.6141 - val_loss: 0.2457 - val_acc: 0.6131\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2458 - acc: 0.6146 - val_loss: 0.2458 - val_acc: 0.6110\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2457 - acc: 0.6157\n",
      ". theta fit =  [1.0461992 1.5381924]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.5910 - val_loss: 0.2494 - val_acc: 0.5912\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5880 - val_loss: 0.2484 - val_acc: 0.5746\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5947 - val_loss: 0.2491 - val_acc: 0.6160\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6012 - val_loss: 0.2483 - val_acc: 0.5870\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.6051 - val_loss: 0.2488 - val_acc: 0.6048\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6109 - val_loss: 0.2492 - val_acc: 0.5797\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.6029 - val_loss: 0.2488 - val_acc: 0.6296\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.6062 - val_loss: 0.2493 - val_acc: 0.6096\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.6073 - val_loss: 0.2492 - val_acc: 0.6083\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6059 - val_loss: 0.2482 - val_acc: 0.6058\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.6107 - val_loss: 0.2484 - val_acc: 0.6086\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.6105 - val_loss: 0.2484 - val_acc: 0.6062\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6062 - val_loss: 0.2484 - val_acc: 0.5641\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.6049 - val_loss: 0.2483 - val_acc: 0.6182\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.6106 - val_loss: 0.2482 - val_acc: 0.6081\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.6042 - val_loss: 0.2486 - val_acc: 0.6194\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6043 - val_loss: 0.2482 - val_acc: 0.6166\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.6075 - val_loss: 0.2493 - val_acc: 0.6015\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6036 - val_loss: 0.2494 - val_acc: 0.5999\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6064 - val_loss: 0.2489 - val_acc: 0.5890\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2482 - acc: 0.6063\n",
      ". theta fit =  [0.943353  1.4353704]\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2500 - acc: 0.6032 - val_loss: 0.2499 - val_acc: 0.6075\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5974 - val_loss: 0.2498 - val_acc: 0.6122\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6005 - val_loss: 0.2496 - val_acc: 0.5968\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5927 - val_loss: 0.2493 - val_acc: 0.6052\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6036 - val_loss: 0.2492 - val_acc: 0.5918\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6042 - val_loss: 0.2489 - val_acc: 0.5764\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6002 - val_loss: 0.2485 - val_acc: 0.5428\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6004 - val_loss: 0.2490 - val_acc: 0.6460\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.6034 - val_loss: 0.2487 - val_acc: 0.5876\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5994 - val_loss: 0.2493 - val_acc: 0.6371\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6041 - val_loss: 0.2487 - val_acc: 0.6136\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5996 - val_loss: 0.2492 - val_acc: 0.6026\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5939 - val_loss: 0.2486 - val_acc: 0.5141\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6019 - val_loss: 0.2487 - val_acc: 0.5820\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6055 - val_loss: 0.2487 - val_acc: 0.6035\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6040 - val_loss: 0.2488 - val_acc: 0.5805\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6041 - val_loss: 0.2487 - val_acc: 0.6237\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2485 - acc: 0.5432\n",
      ". theta fit =  [0.91919994 1.4595532 ]\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2492 - acc: 0.6343 - val_loss: 0.2500 - val_acc: 0.6465\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6291 - val_loss: 0.2496 - val_acc: 0.6675\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2483 - acc: 0.6318 - val_loss: 0.2499 - val_acc: 0.6712\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6439 - val_loss: 0.2490 - val_acc: 0.6322\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.6371 - val_loss: 0.2487 - val_acc: 0.6133\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.6345 - val_loss: 0.2493 - val_acc: 0.6480\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.6359 - val_loss: 0.2485 - val_acc: 0.6658\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6422 - val_loss: 0.2496 - val_acc: 0.6513\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6404 - val_loss: 0.2496 - val_acc: 0.6663\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6403 - val_loss: 0.2499 - val_acc: 0.6579\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6362 - val_loss: 0.2485 - val_acc: 0.5713\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6377 - val_loss: 0.2491 - val_acc: 0.6534\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6356 - val_loss: 0.2487 - val_acc: 0.6512\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6364 - val_loss: 0.2486 - val_acc: 0.6006\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6244 - val_loss: 0.2496 - val_acc: 0.6463\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6406 - val_loss: 0.2492 - val_acc: 0.6457\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6436 - val_loss: 0.2484 - val_acc: 0.6177\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6366 - val_loss: 0.2487 - val_acc: 0.6346\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6364 - val_loss: 0.2492 - val_acc: 0.6615\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6367 - val_loss: 0.2485 - val_acc: 0.6380\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6421 - val_loss: 0.2485 - val_acc: 0.6670\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6422 - val_loss: 0.2485 - val_acc: 0.6270\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.6390 - val_loss: 0.2484 - val_acc: 0.6613\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.6396 - val_loss: 0.2484 - val_acc: 0.6076\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6318 - val_loss: 0.2493 - val_acc: 0.6295\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6337 - val_loss: 0.2491 - val_acc: 0.5956\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6374 - val_loss: 0.2491 - val_acc: 0.6449\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2484 - acc: 0.6183\n",
      ". theta fit =  [0.9432705 1.4835242]\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2499 - acc: 0.6307 - val_loss: 0.2502 - val_acc: 0.6256\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6155 - val_loss: 0.2504 - val_acc: 0.6504\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6180 - val_loss: 0.2486 - val_acc: 0.6640\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6242 - val_loss: 0.2485 - val_acc: 0.6072\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6238 - val_loss: 0.2484 - val_acc: 0.6321\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6353 - val_loss: 0.2487 - val_acc: 0.6448\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.6413 - val_loss: 0.2484 - val_acc: 0.6316\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.6295 - val_loss: 0.2489 - val_acc: 0.6716\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6252 - val_loss: 0.2490 - val_acc: 0.6666\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.6375 - val_loss: 0.2484 - val_acc: 0.6707\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.6298 - val_loss: 0.2499 - val_acc: 0.6640\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6357 - val_loss: 0.2489 - val_acc: 0.6324\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6318 - val_loss: 0.2492 - val_acc: 0.6719\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6403 - val_loss: 0.2497 - val_acc: 0.6566\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6431 - val_loss: 0.2497 - val_acc: 0.6495\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6336 - val_loss: 0.2487 - val_acc: 0.6719\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6388 - val_loss: 0.2496 - val_acc: 0.6704\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6422 - val_loss: 0.2496 - val_acc: 0.6670\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6300 - val_loss: 0.2483 - val_acc: 0.6718\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6508 - val_loss: 0.2497 - val_acc: 0.6619\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6463 - val_loss: 0.2487 - val_acc: 0.6443\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6414 - val_loss: 0.2490 - val_acc: 0.5715\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6358 - val_loss: 0.2484 - val_acc: 0.6330\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6351 - val_loss: 0.2490 - val_acc: 0.6189\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.6214 - val_loss: 0.2496 - val_acc: 0.5644\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.6262 - val_loss: 0.2485 - val_acc: 0.6093\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.6301 - val_loss: 0.2500 - val_acc: 0.6568\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6361 - val_loss: 0.2489 - val_acc: 0.6539\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6174 - val_loss: 0.2498 - val_acc: 0.6720\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2484 - acc: 0.6717\n",
      ". theta fit =  [0.96730024 1.4594768 ]\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2496 - acc: 0.5896 - val_loss: 0.2491 - val_acc: 0.5982\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5983 - val_loss: 0.2492 - val_acc: 0.4838\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5836 - val_loss: 0.2485 - val_acc: 0.4998\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5819 - val_loss: 0.2484 - val_acc: 0.5871\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5888 - val_loss: 0.2485 - val_acc: 0.5658\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5823 - val_loss: 0.2487 - val_acc: 0.5992\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5900 - val_loss: 0.2485 - val_acc: 0.5687\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5808 - val_loss: 0.2503 - val_acc: 0.6118\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5895 - val_loss: 0.2485 - val_acc: 0.6138\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5970 - val_loss: 0.2500 - val_acc: 0.4939\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5673 - val_loss: 0.2493 - val_acc: 0.4950\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5776 - val_loss: 0.2492 - val_acc: 0.6095\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5815 - val_loss: 0.2488 - val_acc: 0.5825\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5794 - val_loss: 0.2486 - val_acc: 0.6194\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2484 - acc: 0.5876\n",
      ". theta fit =  [0.9434349 1.4355587]\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2504 - acc: 0.6016 - val_loss: 0.2500 - val_acc: 0.5815\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.6058 - val_loss: 0.2497 - val_acc: 0.6405\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6117 - val_loss: 0.2494 - val_acc: 0.6442\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6093 - val_loss: 0.2483 - val_acc: 0.6105\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6082 - val_loss: 0.2489 - val_acc: 0.6079\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5976 - val_loss: 0.2485 - val_acc: 0.5721\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5970 - val_loss: 0.2483 - val_acc: 0.6400\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6102 - val_loss: 0.2484 - val_acc: 0.5983\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6009 - val_loss: 0.2494 - val_acc: 0.6110\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.6072 - val_loss: 0.2493 - val_acc: 0.6055\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6075 - val_loss: 0.2487 - val_acc: 0.5854\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.6057 - val_loss: 0.2493 - val_acc: 0.5962\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.5952 - val_loss: 0.2487 - val_acc: 0.6332\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5989 - val_loss: 0.2484 - val_acc: 0.6187\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2484 - acc: 0.6110\n",
      ". theta fit =  [0.9675644 1.4597033]\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2518 - acc: 0.5890 - val_loss: 0.2504 - val_acc: 0.5190\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5627 - val_loss: 0.2483 - val_acc: 0.6183\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5911 - val_loss: 0.2483 - val_acc: 0.5914\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.5765 - val_loss: 0.2497 - val_acc: 0.6504\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.5774 - val_loss: 0.2497 - val_acc: 0.6233\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.5723 - val_loss: 0.2487 - val_acc: 0.5053\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5682 - val_loss: 0.2486 - val_acc: 0.5877\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5833 - val_loss: 0.2488 - val_acc: 0.6401\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5890 - val_loss: 0.2485 - val_acc: 0.5001\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5815 - val_loss: 0.2486 - val_acc: 0.5774\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5778 - val_loss: 0.2496 - val_acc: 0.5847\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2489 - acc: 0.5914 - val_loss: 0.2487 - val_acc: 0.5712\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2484 - acc: 0.6187\n",
      ". theta fit =  [0.9917097 1.4362577]\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2512 - acc: 0.5165 - val_loss: 0.2506 - val_acc: 0.5372\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.5121 - val_loss: 0.2485 - val_acc: 0.4710\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.5016 - val_loss: 0.2496 - val_acc: 0.5684\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5166 - val_loss: 0.2486 - val_acc: 0.4884\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.4905 - val_loss: 0.2490 - val_acc: 0.5149\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.5109 - val_loss: 0.2486 - val_acc: 0.5267\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2482 - acc: 0.4965 - val_loss: 0.2502 - val_acc: 0.5978\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5009 - val_loss: 0.2486 - val_acc: 0.4543\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5009 - val_loss: 0.2485 - val_acc: 0.5158\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5103 - val_loss: 0.2489 - val_acc: 0.5807\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5251 - val_loss: 0.2492 - val_acc: 0.5360\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5004 - val_loss: 0.2484 - val_acc: 0.5526\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5135 - val_loss: 0.2488 - val_acc: 0.5297\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5036 - val_loss: 0.2484 - val_acc: 0.5169\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.4937 - val_loss: 0.2485 - val_acc: 0.5111\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5216 - val_loss: 0.2487 - val_acc: 0.4709\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.4949 - val_loss: 0.2485 - val_acc: 0.5134\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5084 - val_loss: 0.2486 - val_acc: 0.4530\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.5046 - val_loss: 0.2498 - val_acc: 0.4916\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5059 - val_loss: 0.2488 - val_acc: 0.4410\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.4977 - val_loss: 0.2495 - val_acc: 0.4971\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5045 - val_loss: 0.2487 - val_acc: 0.5280\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2485 - acc: 0.5528\n",
      ". theta fit =  [0.96738386 1.460682  ]\n",
      "Iteration:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2491 - acc: 0.5762 - val_loss: 0.2492 - val_acc: 0.6021\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5815 - val_loss: 0.2487 - val_acc: 0.4938\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5698 - val_loss: 0.2489 - val_acc: 0.6184\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5742 - val_loss: 0.2494 - val_acc: 0.5889\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5821 - val_loss: 0.2492 - val_acc: 0.5890\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5631 - val_loss: 0.2485 - val_acc: 0.6532\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5910 - val_loss: 0.2499 - val_acc: 0.5950\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5978 - val_loss: 0.2503 - val_acc: 0.4995\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5697 - val_loss: 0.2484 - val_acc: 0.6280\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6011 - val_loss: 0.2487 - val_acc: 0.5475\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.5769 - val_loss: 0.2499 - val_acc: 0.6188\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2489 - acc: 0.5957 - val_loss: 0.2488 - val_acc: 0.6226\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5896 - val_loss: 0.2505 - val_acc: 0.6335\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5748 - val_loss: 0.2486 - val_acc: 0.6570\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5976 - val_loss: 0.2493 - val_acc: 0.6238\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5916 - val_loss: 0.2502 - val_acc: 0.5876\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5915 - val_loss: 0.2484 - val_acc: 0.5168\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5718 - val_loss: 0.2485 - val_acc: 0.4977\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5952 - val_loss: 0.2490 - val_acc: 0.5564\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2490 - acc: 0.5796 - val_loss: 0.2487 - val_acc: 0.5562\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5869 - val_loss: 0.2485 - val_acc: 0.5413\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5880 - val_loss: 0.2486 - val_acc: 0.6038\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5907 - val_loss: 0.2492 - val_acc: 0.5786\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5772 - val_loss: 0.2485 - val_acc: 0.6406\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5758 - val_loss: 0.2485 - val_acc: 0.6494\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5877 - val_loss: 0.2485 - val_acc: 0.5781\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5838 - val_loss: 0.2494 - val_acc: 0.6326\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2485 - acc: 0.5171\n",
      ". theta fit =  [0.94278467 1.4361436 ]\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2493 - acc: 0.5867 - val_loss: 0.2504 - val_acc: 0.5773\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.5945 - val_loss: 0.2499 - val_acc: 0.6251\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.6110 - val_loss: 0.2486 - val_acc: 0.6302\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6153 - val_loss: 0.2486 - val_acc: 0.5878\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6069 - val_loss: 0.2484 - val_acc: 0.6265\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.6122 - val_loss: 0.2485 - val_acc: 0.5712\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6060 - val_loss: 0.2485 - val_acc: 0.5624\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6103 - val_loss: 0.2484 - val_acc: 0.6239\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6113 - val_loss: 0.2487 - val_acc: 0.6171\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6133 - val_loss: 0.2485 - val_acc: 0.5823\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6046 - val_loss: 0.2484 - val_acc: 0.6418\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.6108 - val_loss: 0.2485 - val_acc: 0.6009\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6031 - val_loss: 0.2486 - val_acc: 0.6002\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5919 - val_loss: 0.2484 - val_acc: 0.5992\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6016 - val_loss: 0.2495 - val_acc: 0.6332\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6129 - val_loss: 0.2495 - val_acc: 0.5943\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6018 - val_loss: 0.2487 - val_acc: 0.5783\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.6035 - val_loss: 0.2489 - val_acc: 0.5847\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2484 - acc: 0.6243\n",
      ". theta fit =  [0.96759486 1.4609733 ]\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2496 - acc: 0.5700 - val_loss: 0.2500 - val_acc: 0.6234\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5895 - val_loss: 0.2495 - val_acc: 0.5869\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5856 - val_loss: 0.2492 - val_acc: 0.6109\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5930 - val_loss: 0.2485 - val_acc: 0.5612\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5931 - val_loss: 0.2486 - val_acc: 0.6062\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6056 - val_loss: 0.2487 - val_acc: 0.4885\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5893 - val_loss: 0.2485 - val_acc: 0.4891\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5800 - val_loss: 0.2485 - val_acc: 0.5524\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5903 - val_loss: 0.2485 - val_acc: 0.5104\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2491 - acc: 0.5909 - val_loss: 0.2492 - val_acc: 0.5540\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5874 - val_loss: 0.2498 - val_acc: 0.6453\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.5983 - val_loss: 0.2491 - val_acc: 0.6466\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5955 - val_loss: 0.2485 - val_acc: 0.6257\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2484 - acc: 0.5992 - val_loss: 0.2495 - val_acc: 0.5613\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2485 - acc: 0.5615\n",
      ". theta fit =  [0.942594  1.4380469]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2507 - acc: 0.5900 - val_loss: 0.2501 - val_acc: 0.5741\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2494 - acc: 0.5934 - val_loss: 0.2496 - val_acc: 0.5945\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5976 - val_loss: 0.2485 - val_acc: 0.5029\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5878 - val_loss: 0.2493 - val_acc: 0.6242\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6084 - val_loss: 0.2486 - val_acc: 0.5397\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5966 - val_loss: 0.2484 - val_acc: 0.5965\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.5987 - val_loss: 0.2486 - val_acc: 0.6308\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5971 - val_loss: 0.2487 - val_acc: 0.4931\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5900 - val_loss: 0.2492 - val_acc: 0.5902\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5977 - val_loss: 0.2501 - val_acc: 0.5990\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6030 - val_loss: 0.2487 - val_acc: 0.5374\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5812 - val_loss: 0.2491 - val_acc: 0.5997\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6075 - val_loss: 0.2485 - val_acc: 0.5811\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6024 - val_loss: 0.2492 - val_acc: 0.5919\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5989 - val_loss: 0.2485 - val_acc: 0.5665\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6045 - val_loss: 0.2486 - val_acc: 0.5529\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2485 - acc: 0.5970\n",
      ". theta fit =  [0.9526418 1.4514848]\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2487 - acc: 0.5974 - val_loss: 0.2496 - val_acc: 0.5771\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5963 - val_loss: 0.2498 - val_acc: 0.6171\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.5877 - val_loss: 0.2486 - val_acc: 0.6385\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5974 - val_loss: 0.2492 - val_acc: 0.6322\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6066 - val_loss: 0.2489 - val_acc: 0.5877\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6071 - val_loss: 0.2486 - val_acc: 0.5841\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5978 - val_loss: 0.2485 - val_acc: 0.6000\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6030 - val_loss: 0.2489 - val_acc: 0.6369\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5992 - val_loss: 0.2487 - val_acc: 0.6134\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6137 - val_loss: 0.2490 - val_acc: 0.5685\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6099 - val_loss: 0.2491 - val_acc: 0.5519\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5990 - val_loss: 0.2498 - val_acc: 0.6214\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5930 - val_loss: 0.2487 - val_acc: 0.6252\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6037 - val_loss: 0.2488 - val_acc: 0.6465\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6087 - val_loss: 0.2488 - val_acc: 0.6269\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6106 - val_loss: 0.2486 - val_acc: 0.6245\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.6113 - val_loss: 0.2491 - val_acc: 0.6022\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2485 - acc: 0.6004\n",
      ". theta fit =  [0.9551816 1.4540393]\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2492 - acc: 0.5994 - val_loss: 0.2490 - val_acc: 0.6025\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5927 - val_loss: 0.2486 - val_acc: 0.6376\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5987 - val_loss: 0.2499 - val_acc: 0.6376\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6130 - val_loss: 0.2487 - val_acc: 0.6255\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6037 - val_loss: 0.2496 - val_acc: 0.5854\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5860 - val_loss: 0.2485 - val_acc: 0.5868\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6025 - val_loss: 0.2484 - val_acc: 0.6270\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6104 - val_loss: 0.2485 - val_acc: 0.5543\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6074 - val_loss: 0.2492 - val_acc: 0.5570\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6017 - val_loss: 0.2487 - val_acc: 0.5904\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5984 - val_loss: 0.2486 - val_acc: 0.6117\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6093 - val_loss: 0.2491 - val_acc: 0.5877\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5942 - val_loss: 0.2484 - val_acc: 0.6032\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6059 - val_loss: 0.2498 - val_acc: 0.5988\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5737 - val_loss: 0.2487 - val_acc: 0.6196\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5897 - val_loss: 0.2486 - val_acc: 0.5695\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5973 - val_loss: 0.2492 - val_acc: 0.5800\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2485 - acc: 0.6273\n",
      ". theta fit =  [0.9577476 1.4565885]\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2495 - acc: 0.5969 - val_loss: 0.2499 - val_acc: 0.5857\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6001 - val_loss: 0.2495 - val_acc: 0.5008\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5939 - val_loss: 0.2490 - val_acc: 0.4914\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5825 - val_loss: 0.2491 - val_acc: 0.5930\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.5980 - val_loss: 0.2486 - val_acc: 0.6298\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6074 - val_loss: 0.2486 - val_acc: 0.6000\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5990 - val_loss: 0.2488 - val_acc: 0.6285\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6036 - val_loss: 0.2484 - val_acc: 0.6350\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5954 - val_loss: 0.2487 - val_acc: 0.5594\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5926 - val_loss: 0.2485 - val_acc: 0.5970\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5949 - val_loss: 0.2485 - val_acc: 0.5991\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6098 - val_loss: 0.2492 - val_acc: 0.5205\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5944 - val_loss: 0.2484 - val_acc: 0.5706\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6042 - val_loss: 0.2484 - val_acc: 0.6114\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2487 - acc: 0.6023 - val_loss: 0.2485 - val_acc: 0.6326\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5992 - val_loss: 0.2484 - val_acc: 0.6109\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5945 - val_loss: 0.2489 - val_acc: 0.5944\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5950 - val_loss: 0.2496 - val_acc: 0.6038\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5970 - val_loss: 0.2489 - val_acc: 0.5241\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5991 - val_loss: 0.2495 - val_acc: 0.5410\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5827 - val_loss: 0.2502 - val_acc: 0.6174\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6035 - val_loss: 0.2486 - val_acc: 0.5007\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5919 - val_loss: 0.2491 - val_acc: 0.5324\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2485 - acc: 0.5892 - val_loss: 0.2484 - val_acc: 0.6353\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2484 - acc: 0.6119\n",
      ". theta fit =  [0.96034837 1.4591879 ]\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2499 - acc: 0.5838 - val_loss: 0.2501 - val_acc: 0.5829\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5934 - val_loss: 0.2496 - val_acc: 0.6021\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6006 - val_loss: 0.2491 - val_acc: 0.5911\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5980 - val_loss: 0.2495 - val_acc: 0.6369\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6003 - val_loss: 0.2496 - val_acc: 0.6434\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6105 - val_loss: 0.2484 - val_acc: 0.6053\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5974 - val_loss: 0.2484 - val_acc: 0.6312\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.6006 - val_loss: 0.2502 - val_acc: 0.6322\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6016 - val_loss: 0.2494 - val_acc: 0.6535\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6007 - val_loss: 0.2484 - val_acc: 0.6145\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6038 - val_loss: 0.2486 - val_acc: 0.6039\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5902 - val_loss: 0.2485 - val_acc: 0.6350\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5846 - val_loss: 0.2489 - val_acc: 0.6528\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5978 - val_loss: 0.2493 - val_acc: 0.6427\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5763 - val_loss: 0.2486 - val_acc: 0.5841\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6118 - val_loss: 0.2501 - val_acc: 0.6317\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2484 - acc: 0.6058\n",
      ". theta fit =  [0.9590685 1.4618126]\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2498 - acc: 0.5846 - val_loss: 0.2504 - val_acc: 0.6594\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6013 - val_loss: 0.2502 - val_acc: 0.6005\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5960 - val_loss: 0.2488 - val_acc: 0.6538\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5803 - val_loss: 0.2491 - val_acc: 0.5452\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5649 - val_loss: 0.2490 - val_acc: 0.6024\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6112 - val_loss: 0.2486 - val_acc: 0.5536\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5936 - val_loss: 0.2484 - val_acc: 0.6590\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6122 - val_loss: 0.2488 - val_acc: 0.6478\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6050 - val_loss: 0.2492 - val_acc: 0.6111\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.6093 - val_loss: 0.2487 - val_acc: 0.5174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5873 - val_loss: 0.2495 - val_acc: 0.5583\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5753 - val_loss: 0.2484 - val_acc: 0.6481\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6026 - val_loss: 0.2485 - val_acc: 0.5254\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5945 - val_loss: 0.2488 - val_acc: 0.5286\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5884 - val_loss: 0.2485 - val_acc: 0.5901\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5974 - val_loss: 0.2486 - val_acc: 0.4990\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5873 - val_loss: 0.2499 - val_acc: 0.6485\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6166 - val_loss: 0.2489 - val_acc: 0.5926\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5950 - val_loss: 0.2491 - val_acc: 0.6652\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6044 - val_loss: 0.2485 - val_acc: 0.6604\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6118 - val_loss: 0.2485 - val_acc: 0.6200\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6080 - val_loss: 0.2484 - val_acc: 0.5912\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5946 - val_loss: 0.2489 - val_acc: 0.6388\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6158 - val_loss: 0.2485 - val_acc: 0.6329\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5967 - val_loss: 0.2488 - val_acc: 0.5198\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5854 - val_loss: 0.2485 - val_acc: 0.5857\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6020 - val_loss: 0.2488 - val_acc: 0.6272\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6088 - val_loss: 0.2487 - val_acc: 0.6511\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6085 - val_loss: 0.2485 - val_acc: 0.6528\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6005 - val_loss: 0.2487 - val_acc: 0.5357\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5908 - val_loss: 0.2486 - val_acc: 0.6525\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2486 - acc: 0.5933 - val_loss: 0.2489 - val_acc: 0.6575\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2484 - acc: 0.5917\n",
      ". theta fit =  [0.96158284 1.4644722 ]\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2494 - acc: 0.5878 - val_loss: 0.2500 - val_acc: 0.5942\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5885 - val_loss: 0.2486 - val_acc: 0.6267\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5839 - val_loss: 0.2490 - val_acc: 0.5953\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5975 - val_loss: 0.2486 - val_acc: 0.4931\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5929 - val_loss: 0.2488 - val_acc: 0.6625\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6064 - val_loss: 0.2485 - val_acc: 0.5061\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5963 - val_loss: 0.2499 - val_acc: 0.6584\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5972 - val_loss: 0.2496 - val_acc: 0.6505\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5982 - val_loss: 0.2486 - val_acc: 0.6118\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5990 - val_loss: 0.2498 - val_acc: 0.6519\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5917 - val_loss: 0.2486 - val_acc: 0.6412\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5978 - val_loss: 0.2485 - val_acc: 0.6586\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6054 - val_loss: 0.2495 - val_acc: 0.6484\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6161 - val_loss: 0.2484 - val_acc: 0.6003\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6004 - val_loss: 0.2484 - val_acc: 0.6448\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5879 - val_loss: 0.2498 - val_acc: 0.6600\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2488 - acc: 0.6044 - val_loss: 0.2487 - val_acc: 0.6467\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5883 - val_loss: 0.2499 - val_acc: 0.6495\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6044 - val_loss: 0.2486 - val_acc: 0.6153\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6144 - val_loss: 0.2486 - val_acc: 0.4935\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5876 - val_loss: 0.2486 - val_acc: 0.6420\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6091 - val_loss: 0.2484 - val_acc: 0.6392\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6052 - val_loss: 0.2489 - val_acc: 0.6329\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6065 - val_loss: 0.2494 - val_acc: 0.5883\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5926 - val_loss: 0.2487 - val_acc: 0.5282\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2484 - acc: 0.6453\n",
      ". theta fit =  [0.9642772 1.4617774]\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2510 - acc: 0.5926 - val_loss: 0.2504 - val_acc: 0.6232\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2494 - acc: 0.5786 - val_loss: 0.2500 - val_acc: 0.5273\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5748 - val_loss: 0.2495 - val_acc: 0.5578\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5778 - val_loss: 0.2499 - val_acc: 0.6501\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5914 - val_loss: 0.2484 - val_acc: 0.6356\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5933 - val_loss: 0.2492 - val_acc: 0.6042\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5847 - val_loss: 0.2486 - val_acc: 0.5376\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5791 - val_loss: 0.2494 - val_acc: 0.6500\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5988 - val_loss: 0.2492 - val_acc: 0.6553\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6006 - val_loss: 0.2503 - val_acc: 0.6334\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.5923 - val_loss: 0.2485 - val_acc: 0.6521\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5952 - val_loss: 0.2483 - val_acc: 0.6149\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5836 - val_loss: 0.2506 - val_acc: 0.6520\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5934 - val_loss: 0.2500 - val_acc: 0.6174\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.6024 - val_loss: 0.2486 - val_acc: 0.5955\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.6024 - val_loss: 0.2501 - val_acc: 0.4936\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5945 - val_loss: 0.2486 - val_acc: 0.4939\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5899 - val_loss: 0.2493 - val_acc: 0.5850\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5783 - val_loss: 0.2499 - val_acc: 0.6543\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6025 - val_loss: 0.2500 - val_acc: 0.6372\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5979 - val_loss: 0.2499 - val_acc: 0.5755\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5827 - val_loss: 0.2495 - val_acc: 0.6189\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2484 - acc: 0.6153\n",
      ". theta fit =  [0.9615907 1.4590604]\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2503 - acc: 0.5796 - val_loss: 0.2502 - val_acc: 0.6028\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5831 - val_loss: 0.2488 - val_acc: 0.5859\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6032 - val_loss: 0.2484 - val_acc: 0.6167\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5907 - val_loss: 0.2489 - val_acc: 0.6525\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6014 - val_loss: 0.2497 - val_acc: 0.5712\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5856 - val_loss: 0.2498 - val_acc: 0.6381\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5996 - val_loss: 0.2492 - val_acc: 0.6331\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5987 - val_loss: 0.2484 - val_acc: 0.5864\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5953 - val_loss: 0.2485 - val_acc: 0.5765\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5950 - val_loss: 0.2485 - val_acc: 0.6570\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.6041 - val_loss: 0.2490 - val_acc: 0.5780\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5961 - val_loss: 0.2489 - val_acc: 0.6451\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6087 - val_loss: 0.2485 - val_acc: 0.5177\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2485 - acc: 0.6171\n",
      ". theta fit =  [0.96434593 1.4564201 ]\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2502 - acc: 0.5845 - val_loss: 0.2506 - val_acc: 0.5281\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5621 - val_loss: 0.2494 - val_acc: 0.6289\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5937 - val_loss: 0.2485 - val_acc: 0.5021\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5952 - val_loss: 0.2497 - val_acc: 0.5061\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5987 - val_loss: 0.2491 - val_acc: 0.5525\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5800 - val_loss: 0.2491 - val_acc: 0.6537\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6058 - val_loss: 0.2488 - val_acc: 0.6439\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5961 - val_loss: 0.2498 - val_acc: 0.5969\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5980 - val_loss: 0.2486 - val_acc: 0.4931\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5788 - val_loss: 0.2485 - val_acc: 0.6241\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5970 - val_loss: 0.2500 - val_acc: 0.6229\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5959 - val_loss: 0.2499 - val_acc: 0.6106\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5981 - val_loss: 0.2485 - val_acc: 0.6278\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6036 - val_loss: 0.2490 - val_acc: 0.5476\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5890 - val_loss: 0.2489 - val_acc: 0.5458\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5884 - val_loss: 0.2486 - val_acc: 0.5001\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5828 - val_loss: 0.2486 - val_acc: 0.5879\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5976 - val_loss: 0.2487 - val_acc: 0.6064\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5891 - val_loss: 0.2486 - val_acc: 0.5479\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5902 - val_loss: 0.2487 - val_acc: 0.6478\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5868 - val_loss: 0.2492 - val_acc: 0.5563\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5735 - val_loss: 0.2498 - val_acc: 0.5903\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5766 - val_loss: 0.2485 - val_acc: 0.5451\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2486 - acc: 0.6282\n",
      ". theta fit =  [0.96712   1.4536704]\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2487 - acc: 0.5648 - val_loss: 0.2497 - val_acc: 0.6093\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5893 - val_loss: 0.2493 - val_acc: 0.6014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5843 - val_loss: 0.2484 - val_acc: 0.6347\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6001 - val_loss: 0.2486 - val_acc: 0.5611\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5769 - val_loss: 0.2485 - val_acc: 0.6519\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5976 - val_loss: 0.2485 - val_acc: 0.6070\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5893 - val_loss: 0.2485 - val_acc: 0.5884\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5913 - val_loss: 0.2493 - val_acc: 0.6207\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5747 - val_loss: 0.2492 - val_acc: 0.6202\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5807 - val_loss: 0.2485 - val_acc: 0.6239\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5993 - val_loss: 0.2488 - val_acc: 0.5359\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5764 - val_loss: 0.2493 - val_acc: 0.6281\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5958 - val_loss: 0.2484 - val_acc: 0.5611\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.5871 - val_loss: 0.2503 - val_acc: 0.5375\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5737 - val_loss: 0.2485 - val_acc: 0.6149\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5857 - val_loss: 0.2485 - val_acc: 0.5590\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5606 - val_loss: 0.2497 - val_acc: 0.6305\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5916 - val_loss: 0.2488 - val_acc: 0.6010\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5896 - val_loss: 0.2494 - val_acc: 0.5977\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5714 - val_loss: 0.2487 - val_acc: 0.5045\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5747 - val_loss: 0.2485 - val_acc: 0.5272\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5877 - val_loss: 0.2498 - val_acc: 0.6413\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6044 - val_loss: 0.2487 - val_acc: 0.5045\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2485 - acc: 0.5614\n",
      ". theta fit =  [0.9642949 1.456432 ]\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2505 - acc: 0.5805 - val_loss: 0.2496 - val_acc: 0.4941\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2493 - acc: 0.5641 - val_loss: 0.2494 - val_acc: 0.6140\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5800 - val_loss: 0.2488 - val_acc: 0.6301\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5812 - val_loss: 0.2501 - val_acc: 0.6547\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5969 - val_loss: 0.2492 - val_acc: 0.5853\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6016 - val_loss: 0.2496 - val_acc: 0.6304\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5916 - val_loss: 0.2502 - val_acc: 0.6539\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5867 - val_loss: 0.2491 - val_acc: 0.5762\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5978 - val_loss: 0.2487 - val_acc: 0.5054\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5880 - val_loss: 0.2495 - val_acc: 0.6227\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5543 - val_loss: 0.2492 - val_acc: 0.5797\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5605 - val_loss: 0.2487 - val_acc: 0.5011\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5754 - val_loss: 0.2492 - val_acc: 0.5072\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5856 - val_loss: 0.2489 - val_acc: 0.6513\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5923 - val_loss: 0.2489 - val_acc: 0.6549\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5942 - val_loss: 0.2486 - val_acc: 0.5776\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6015 - val_loss: 0.2486 - val_acc: 0.5266\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5799 - val_loss: 0.2486 - val_acc: 0.6311\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5952 - val_loss: 0.2492 - val_acc: 0.6460\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6028 - val_loss: 0.2485 - val_acc: 0.5674\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5988 - val_loss: 0.2486 - val_acc: 0.6366\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5945 - val_loss: 0.2495 - val_acc: 0.6345\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5931 - val_loss: 0.2485 - val_acc: 0.6422\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.6055 - val_loss: 0.2498 - val_acc: 0.5537\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5908 - val_loss: 0.2492 - val_acc: 0.5680\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5788 - val_loss: 0.2501 - val_acc: 0.6187\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5866 - val_loss: 0.2485 - val_acc: 0.6395\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5932 - val_loss: 0.2485 - val_acc: 0.6498\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5920 - val_loss: 0.2495 - val_acc: 0.6525\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6020 - val_loss: 0.2492 - val_acc: 0.6133\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5982 - val_loss: 0.2490 - val_acc: 0.5958\n",
      "Epoch 32/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5962 - val_loss: 0.2490 - val_acc: 0.6289\n",
      "Epoch 33/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6047 - val_loss: 0.2488 - val_acc: 0.5909\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2484 - acc: 0.6425\n",
      ". theta fit =  [0.96716005 1.459285  ]\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2497 - acc: 0.5887 - val_loss: 0.2501 - val_acc: 0.6179\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5726 - val_loss: 0.2487 - val_acc: 0.6173\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5861 - val_loss: 0.2485 - val_acc: 0.5002\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5861 - val_loss: 0.2490 - val_acc: 0.5844\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5845 - val_loss: 0.2484 - val_acc: 0.6351\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5886 - val_loss: 0.2501 - val_acc: 0.6256\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5926 - val_loss: 0.2499 - val_acc: 0.6524\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5851 - val_loss: 0.2485 - val_acc: 0.6061\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5937 - val_loss: 0.2486 - val_acc: 0.5821\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5920 - val_loss: 0.2485 - val_acc: 0.6064\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5940 - val_loss: 0.2488 - val_acc: 0.5662\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5678 - val_loss: 0.2485 - val_acc: 0.5944\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5913 - val_loss: 0.2495 - val_acc: 0.6004\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5993 - val_loss: 0.2487 - val_acc: 0.5963\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5874 - val_loss: 0.2487 - val_acc: 0.6449\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2484 - acc: 0.6357\n",
      ". theta fit =  [0.9700345 1.4621108]\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2505 - acc: 0.5829 - val_loss: 0.2503 - val_acc: 0.5171\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5653 - val_loss: 0.2501 - val_acc: 0.5894\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5750 - val_loss: 0.2493 - val_acc: 0.5120\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5527 - val_loss: 0.2494 - val_acc: 0.5259\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5692 - val_loss: 0.2485 - val_acc: 0.5096\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5683 - val_loss: 0.2501 - val_acc: 0.5970\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5768 - val_loss: 0.2488 - val_acc: 0.6086\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5704 - val_loss: 0.2487 - val_acc: 0.6463\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5894 - val_loss: 0.2487 - val_acc: 0.6481\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5985 - val_loss: 0.2487 - val_acc: 0.4959\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5654 - val_loss: 0.2496 - val_acc: 0.6449\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5864 - val_loss: 0.2489 - val_acc: 0.5896\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5609 - val_loss: 0.2485 - val_acc: 0.5217\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5900 - val_loss: 0.2490 - val_acc: 0.5875\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5897 - val_loss: 0.2487 - val_acc: 0.6263\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2485 - acc: 0.5099\n",
      ". theta fit =  [0.9671007 1.459235 ]\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2503 - acc: 0.5604 - val_loss: 0.2495 - val_acc: 0.5002\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2493 - acc: 0.5646 - val_loss: 0.2503 - val_acc: 0.6341\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5830 - val_loss: 0.2504 - val_acc: 0.6083\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5627 - val_loss: 0.2488 - val_acc: 0.5087\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5818 - val_loss: 0.2493 - val_acc: 0.6169\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5719 - val_loss: 0.2502 - val_acc: 0.5173\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5786 - val_loss: 0.2495 - val_acc: 0.6526\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5780 - val_loss: 0.2486 - val_acc: 0.6543\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5841 - val_loss: 0.2485 - val_acc: 0.5858\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5764 - val_loss: 0.2486 - val_acc: 0.5929\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5714 - val_loss: 0.2486 - val_acc: 0.6375\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5866 - val_loss: 0.2494 - val_acc: 0.6399\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5822 - val_loss: 0.2484 - val_acc: 0.6185\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5975 - val_loss: 0.2493 - val_acc: 0.6116\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5870 - val_loss: 0.2484 - val_acc: 0.5821\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5990 - val_loss: 0.2485 - val_acc: 0.4899\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5839 - val_loss: 0.2489 - val_acc: 0.5041\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5754 - val_loss: 0.2486 - val_acc: 0.4949\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5674 - val_loss: 0.2486 - val_acc: 0.6123\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5821 - val_loss: 0.2489 - val_acc: 0.6381\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6054 - val_loss: 0.2486 - val_acc: 0.5215\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5933 - val_loss: 0.2501 - val_acc: 0.5586\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5842 - val_loss: 0.2492 - val_acc: 0.5777\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2485 - acc: 0.6189\n",
      ". theta fit =  [0.964256  1.4562958]\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2508 - acc: 0.5755 - val_loss: 0.2503 - val_acc: 0.5443\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5721 - val_loss: 0.2503 - val_acc: 0.6244\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5817 - val_loss: 0.2494 - val_acc: 0.5819\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5732 - val_loss: 0.2488 - val_acc: 0.6288\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5984 - val_loss: 0.2485 - val_acc: 0.5314\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5942 - val_loss: 0.2497 - val_acc: 0.5812\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5801 - val_loss: 0.2484 - val_acc: 0.6521\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.6047 - val_loss: 0.2487 - val_acc: 0.4941\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5846 - val_loss: 0.2490 - val_acc: 0.5997\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5928 - val_loss: 0.2488 - val_acc: 0.6419\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5995 - val_loss: 0.2485 - val_acc: 0.6320\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5885 - val_loss: 0.2485 - val_acc: 0.5564\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.5978 - val_loss: 0.2486 - val_acc: 0.5031\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5734 - val_loss: 0.2486 - val_acc: 0.5687\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5812 - val_loss: 0.2489 - val_acc: 0.5695\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5639 - val_loss: 0.2486 - val_acc: 0.4970\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5993 - val_loss: 0.2487 - val_acc: 0.4935\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2485 - acc: 0.6522\n",
      ". theta fit =  [0.9672494 1.4533437]\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2492 - acc: 0.5661 - val_loss: 0.2497 - val_acc: 0.6406\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5684 - val_loss: 0.2488 - val_acc: 0.5660\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5748 - val_loss: 0.2501 - val_acc: 0.6276\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5675 - val_loss: 0.2486 - val_acc: 0.6104\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5865 - val_loss: 0.2486 - val_acc: 0.4959\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5823 - val_loss: 0.2498 - val_acc: 0.5294\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.5731 - val_loss: 0.2491 - val_acc: 0.6298\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5940 - val_loss: 0.2496 - val_acc: 0.5967\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5845 - val_loss: 0.2487 - val_acc: 0.6224\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5887 - val_loss: 0.2484 - val_acc: 0.5980\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5845 - val_loss: 0.2485 - val_acc: 0.6369\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5968 - val_loss: 0.2487 - val_acc: 0.5757\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5487 - val_loss: 0.2486 - val_acc: 0.5221\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5732 - val_loss: 0.2486 - val_acc: 0.4989\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5689 - val_loss: 0.2485 - val_acc: 0.5155\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5886 - val_loss: 0.2484 - val_acc: 0.6131\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5850 - val_loss: 0.2485 - val_acc: 0.6460\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5976 - val_loss: 0.2500 - val_acc: 0.5806\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5942 - val_loss: 0.2493 - val_acc: 0.6438\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5867 - val_loss: 0.2485 - val_acc: 0.6264\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2484 - acc: 0.5986\n",
      ". theta fit =  [0.9642285 1.4563687]\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2515 - acc: 0.5824 - val_loss: 0.2508 - val_acc: 0.6167\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.5655 - val_loss: 0.2501 - val_acc: 0.5560\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5727 - val_loss: 0.2489 - val_acc: 0.6242\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5822 - val_loss: 0.2490 - val_acc: 0.5264\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5662 - val_loss: 0.2487 - val_acc: 0.6148\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5913 - val_loss: 0.2486 - val_acc: 0.5066\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5740 - val_loss: 0.2484 - val_acc: 0.5694\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5931 - val_loss: 0.2487 - val_acc: 0.6201\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5849 - val_loss: 0.2497 - val_acc: 0.6090\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5788 - val_loss: 0.2485 - val_acc: 0.6160\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5788 - val_loss: 0.2485 - val_acc: 0.6319\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5951 - val_loss: 0.2499 - val_acc: 0.6551\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5778 - val_loss: 0.2485 - val_acc: 0.6384\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6014 - val_loss: 0.2484 - val_acc: 0.6428\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6012 - val_loss: 0.2498 - val_acc: 0.5793\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5939 - val_loss: 0.2485 - val_acc: 0.6137\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6027 - val_loss: 0.2500 - val_acc: 0.5691\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5776 - val_loss: 0.2487 - val_acc: 0.4960\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5849 - val_loss: 0.2484 - val_acc: 0.4966\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5922 - val_loss: 0.2498 - val_acc: 0.6139\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5860 - val_loss: 0.2489 - val_acc: 0.6407\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6041 - val_loss: 0.2499 - val_acc: 0.5822\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5928 - val_loss: 0.2495 - val_acc: 0.5743\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5885 - val_loss: 0.2490 - val_acc: 0.5812\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2484 - acc: 0.6431\n",
      ". theta fit =  [0.9672893 1.453338 ]\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2487 - acc: 0.5805 - val_loss: 0.2495 - val_acc: 0.6347\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5600 - val_loss: 0.2485 - val_acc: 0.5933\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5793 - val_loss: 0.2485 - val_acc: 0.6378\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6048 - val_loss: 0.2498 - val_acc: 0.6102\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5905 - val_loss: 0.2487 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5617 - val_loss: 0.2487 - val_acc: 0.5670\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5650 - val_loss: 0.2496 - val_acc: 0.5977\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5780 - val_loss: 0.2496 - val_acc: 0.6477\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5884 - val_loss: 0.2501 - val_acc: 0.6043\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5925 - val_loss: 0.2485 - val_acc: 0.5958\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5956 - val_loss: 0.2485 - val_acc: 0.5948\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5891 - val_loss: 0.2484 - val_acc: 0.6230\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6101 - val_loss: 0.2485 - val_acc: 0.5261\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5914 - val_loss: 0.2487 - val_acc: 0.5480\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5899 - val_loss: 0.2484 - val_acc: 0.6537\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5883 - val_loss: 0.2498 - val_acc: 0.5728\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5830 - val_loss: 0.2497 - val_acc: 0.6300\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5897 - val_loss: 0.2489 - val_acc: 0.5093\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5742 - val_loss: 0.2484 - val_acc: 0.5417\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5948 - val_loss: 0.2492 - val_acc: 0.5247\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5740 - val_loss: 0.2486 - val_acc: 0.4996\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5738 - val_loss: 0.2487 - val_acc: 0.4951\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5486 - val_loss: 0.2485 - val_acc: 0.5583\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5717 - val_loss: 0.2484 - val_acc: 0.5599\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5910 - val_loss: 0.2485 - val_acc: 0.5583\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5882 - val_loss: 0.2486 - val_acc: 0.6281\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5844 - val_loss: 0.2484 - val_acc: 0.5149\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5843 - val_loss: 0.2494 - val_acc: 0.5844\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5706 - val_loss: 0.2495 - val_acc: 0.6245\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2485 - acc: 0.5421\n",
      ". theta fit =  [0.9641885 1.4563956]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2501 - acc: 0.5740 - val_loss: 0.2513 - val_acc: 0.6002\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5823 - val_loss: 0.2493 - val_acc: 0.5275\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5734 - val_loss: 0.2487 - val_acc: 0.6470\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5892 - val_loss: 0.2488 - val_acc: 0.6028\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5966 - val_loss: 0.2498 - val_acc: 0.5431\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5862 - val_loss: 0.2503 - val_acc: 0.5669\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5821 - val_loss: 0.2490 - val_acc: 0.5763\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5848 - val_loss: 0.2486 - val_acc: 0.4921\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5847 - val_loss: 0.2486 - val_acc: 0.6359\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5995 - val_loss: 0.2485 - val_acc: 0.5543\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5873 - val_loss: 0.2491 - val_acc: 0.6298\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5933 - val_loss: 0.2484 - val_acc: 0.6382\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5979 - val_loss: 0.2485 - val_acc: 0.5971\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5989 - val_loss: 0.2492 - val_acc: 0.5424\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5791 - val_loss: 0.2487 - val_acc: 0.5963\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5983 - val_loss: 0.2484 - val_acc: 0.5487\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5897 - val_loss: 0.2486 - val_acc: 0.5156\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5820 - val_loss: 0.2488 - val_acc: 0.5807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5871 - val_loss: 0.2495 - val_acc: 0.5883\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5936 - val_loss: 0.2489 - val_acc: 0.5795\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5865 - val_loss: 0.2485 - val_acc: 0.5580\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6010 - val_loss: 0.2487 - val_acc: 0.5822\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2485 - acc: 0.6386\n",
      ". theta fit =  [0.96543485 1.4546496 ]\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2499 - acc: 0.5845 - val_loss: 0.2504 - val_acc: 0.5474\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.5598 - val_loss: 0.2487 - val_acc: 0.5430\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5828 - val_loss: 0.2488 - val_acc: 0.6249\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5926 - val_loss: 0.2487 - val_acc: 0.5306\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5555 - val_loss: 0.2494 - val_acc: 0.6061\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5705 - val_loss: 0.2502 - val_acc: 0.5842\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5843 - val_loss: 0.2485 - val_acc: 0.5419\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5845 - val_loss: 0.2485 - val_acc: 0.5743\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5835 - val_loss: 0.2484 - val_acc: 0.5837\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5838 - val_loss: 0.2493 - val_acc: 0.6159\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5693 - val_loss: 0.2497 - val_acc: 0.6396\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5656 - val_loss: 0.2508 - val_acc: 0.6038\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.5708 - val_loss: 0.2486 - val_acc: 0.5449\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5854 - val_loss: 0.2486 - val_acc: 0.5023\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5713 - val_loss: 0.2487 - val_acc: 0.5951\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.5846 - val_loss: 0.2488 - val_acc: 0.5858\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5923 - val_loss: 0.2494 - val_acc: 0.5016\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5751 - val_loss: 0.2487 - val_acc: 0.6023\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5859 - val_loss: 0.2508 - val_acc: 0.6205\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2484 - acc: 0.5842\n",
      ". theta fit =  [0.9651183 1.4549665]\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2492 - acc: 0.5886 - val_loss: 0.2502 - val_acc: 0.5958\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.5780 - val_loss: 0.2503 - val_acc: 0.6048\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.5800 - val_loss: 0.2503 - val_acc: 0.6129\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5864 - val_loss: 0.2485 - val_acc: 0.6248\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5998 - val_loss: 0.2489 - val_acc: 0.5512\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5752 - val_loss: 0.2488 - val_acc: 0.6078\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5900 - val_loss: 0.2492 - val_acc: 0.6037\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5910 - val_loss: 0.2487 - val_acc: 0.5367\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5859 - val_loss: 0.2489 - val_acc: 0.5698\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5893 - val_loss: 0.2501 - val_acc: 0.5811\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5825 - val_loss: 0.2484 - val_acc: 0.6443\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5823 - val_loss: 0.2496 - val_acc: 0.5808\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5749 - val_loss: 0.2485 - val_acc: 0.5140\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5731 - val_loss: 0.2484 - val_acc: 0.5841\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5714 - val_loss: 0.2498 - val_acc: 0.6477\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5953 - val_loss: 0.2491 - val_acc: 0.5796\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.5690 - val_loss: 0.2486 - val_acc: 0.6113\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5841 - val_loss: 0.2488 - val_acc: 0.5036\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5642 - val_loss: 0.2499 - val_acc: 0.6321\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5825 - val_loss: 0.2493 - val_acc: 0.5935\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5844 - val_loss: 0.2485 - val_acc: 0.6397\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6051 - val_loss: 0.2484 - val_acc: 0.6054\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6001 - val_loss: 0.2490 - val_acc: 0.5448\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6035 - val_loss: 0.2487 - val_acc: 0.5031\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5746 - val_loss: 0.2488 - val_acc: 0.6087\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5659 - val_loss: 0.2501 - val_acc: 0.6548\n",
      "Epoch 27/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5943 - val_loss: 0.2485 - val_acc: 0.6139\n",
      "Epoch 28/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6034 - val_loss: 0.2500 - val_acc: 0.5825\n",
      "Epoch 29/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5872 - val_loss: 0.2495 - val_acc: 0.6279\n",
      "Epoch 30/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5902 - val_loss: 0.2486 - val_acc: 0.6043\n",
      "Epoch 31/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6020 - val_loss: 0.2487 - val_acc: 0.5046\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5805 - val_loss: 0.2484 - val_acc: 0.5050\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2484 - acc: 0.6059\n",
      ". theta fit =  [0.96481943 1.4552857 ]\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2499 - acc: 0.5627 - val_loss: 0.2504 - val_acc: 0.6341\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5843 - val_loss: 0.2487 - val_acc: 0.5027\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5630 - val_loss: 0.2500 - val_acc: 0.5801\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5635 - val_loss: 0.2501 - val_acc: 0.5712\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5590 - val_loss: 0.2499 - val_acc: 0.6408\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5914 - val_loss: 0.2484 - val_acc: 0.6135\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5960 - val_loss: 0.2501 - val_acc: 0.5444\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2482 - acc: 0.5809 - val_loss: 0.2485 - val_acc: 0.4968\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5948 - val_loss: 0.2496 - val_acc: 0.4963\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2483 - acc: 0.5703 - val_loss: 0.2487 - val_acc: 0.5974\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6053 - val_loss: 0.2484 - val_acc: 0.5717\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.6039 - val_loss: 0.2497 - val_acc: 0.5021\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5770 - val_loss: 0.2485 - val_acc: 0.6473\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6052 - val_loss: 0.2485 - val_acc: 0.5140\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5791 - val_loss: 0.2489 - val_acc: 0.5934\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5814 - val_loss: 0.2487 - val_acc: 0.5686\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5924 - val_loss: 0.2494 - val_acc: 0.5652\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5986 - val_loss: 0.2486 - val_acc: 0.5132\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5741 - val_loss: 0.2494 - val_acc: 0.6209\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5822 - val_loss: 0.2488 - val_acc: 0.6499\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6097 - val_loss: 0.2484 - val_acc: 0.5239\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2485 - acc: 0.5722\n",
      ". theta fit =  [0.96449643 1.4549654 ]\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2514 - acc: 0.5861 - val_loss: 0.2502 - val_acc: 0.5432\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5659 - val_loss: 0.2494 - val_acc: 0.5944\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5868 - val_loss: 0.2493 - val_acc: 0.6435\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5959 - val_loss: 0.2493 - val_acc: 0.5904\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5904 - val_loss: 0.2495 - val_acc: 0.5823\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5964 - val_loss: 0.2492 - val_acc: 0.5709\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5752 - val_loss: 0.2485 - val_acc: 0.6404\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5827 - val_loss: 0.2487 - val_acc: 0.6393\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5826 - val_loss: 0.2498 - val_acc: 0.6408\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5934 - val_loss: 0.2486 - val_acc: 0.5748\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.5942 - val_loss: 0.2489 - val_acc: 0.5725\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5902 - val_loss: 0.2493 - val_acc: 0.6365\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2484 - acc: 0.5832 - val_loss: 0.2486 - val_acc: 0.6379\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5965 - val_loss: 0.2490 - val_acc: 0.6304\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.5859 - val_loss: 0.2496 - val_acc: 0.5822\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.5620 - val_loss: 0.2484 - val_acc: 0.6136\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2485 - acc: 0.5815 - val_loss: 0.2498 - val_acc: 0.6326\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5713 - val_loss: 0.2495 - val_acc: 0.5655\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5797 - val_loss: 0.2485 - val_acc: 0.5092\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.5803 - val_loss: 0.2485 - val_acc: 0.6393\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.5910 - val_loss: 0.2493 - val_acc: 0.6074\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5869 - val_loss: 0.2494 - val_acc: 0.5268\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5527 - val_loss: 0.2487 - val_acc: 0.5000\n",
      "Epoch 24/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5543 - val_loss: 0.2488 - val_acc: 0.6273\n",
      "Epoch 25/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5917 - val_loss: 0.2488 - val_acc: 0.5218\n",
      "Epoch 26/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5707 - val_loss: 0.2503 - val_acc: 0.6283\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2484 - acc: 0.6141\n",
      ". theta fit =  [0.9648214 1.4552919]\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2511 - acc: 0.5689 - val_loss: 0.2507 - val_acc: 0.6134\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.5610 - val_loss: 0.2505 - val_acc: 0.6365\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.5839 - val_loss: 0.2498 - val_acc: 0.5901\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5796 - val_loss: 0.2484 - val_acc: 0.5061\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5680 - val_loss: 0.2486 - val_acc: 0.5976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5841 - val_loss: 0.2500 - val_acc: 0.6111\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5705 - val_loss: 0.2485 - val_acc: 0.5423\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5916 - val_loss: 0.2491 - val_acc: 0.5404\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5741 - val_loss: 0.2497 - val_acc: 0.6394\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5941 - val_loss: 0.2490 - val_acc: 0.5256\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5940 - val_loss: 0.2491 - val_acc: 0.5077\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5749 - val_loss: 0.2497 - val_acc: 0.5937\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5827 - val_loss: 0.2498 - val_acc: 0.6200\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5854 - val_loss: 0.2499 - val_acc: 0.5978\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2485 - acc: 0.5065\n",
      ". theta fit =  [0.96449095 1.4556203 ]\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2516 - acc: 0.5827 - val_loss: 0.2506 - val_acc: 0.6301\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.5649 - val_loss: 0.2505 - val_acc: 0.5569\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2494 - acc: 0.5651 - val_loss: 0.2498 - val_acc: 0.6218\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.5821 - val_loss: 0.2498 - val_acc: 0.5102\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2493 - acc: 0.5596 - val_loss: 0.2500 - val_acc: 0.5130\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5576 - val_loss: 0.2488 - val_acc: 0.5911\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5790 - val_loss: 0.2484 - val_acc: 0.6521\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5879 - val_loss: 0.2485 - val_acc: 0.6482\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5961 - val_loss: 0.2484 - val_acc: 0.5736\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5875 - val_loss: 0.2500 - val_acc: 0.5646\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5644 - val_loss: 0.2485 - val_acc: 0.6183\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.5939 - val_loss: 0.2486 - val_acc: 0.5673\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5880 - val_loss: 0.2489 - val_acc: 0.5836\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5906 - val_loss: 0.2485 - val_acc: 0.5222\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5864 - val_loss: 0.2493 - val_acc: 0.6225\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5941 - val_loss: 0.2488 - val_acc: 0.6127\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5909 - val_loss: 0.2494 - val_acc: 0.6506\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2485 - acc: 0.6518\n",
      ". theta fit =  [0.96482474 1.4559435 ]\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2498 - acc: 0.5807 - val_loss: 0.2501 - val_acc: 0.5696\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5631 - val_loss: 0.2500 - val_acc: 0.5468\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5799 - val_loss: 0.2495 - val_acc: 0.5067\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5324 - val_loss: 0.2486 - val_acc: 0.5360\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5545 - val_loss: 0.2491 - val_acc: 0.5229\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5780 - val_loss: 0.2487 - val_acc: 0.5086\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5603 - val_loss: 0.2492 - val_acc: 0.6546\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.5925 - val_loss: 0.2487 - val_acc: 0.5275\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.5957 - val_loss: 0.2489 - val_acc: 0.5128\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5531 - val_loss: 0.2496 - val_acc: 0.6386\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2484 - acc: 0.5611 - val_loss: 0.2497 - val_acc: 0.6519\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.5941 - val_loss: 0.2488 - val_acc: 0.6058\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5941 - val_loss: 0.2484 - val_acc: 0.5834\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6022 - val_loss: 0.2487 - val_acc: 0.5634\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5781 - val_loss: 0.2486 - val_acc: 0.5146\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5814 - val_loss: 0.2489 - val_acc: 0.6215\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5834 - val_loss: 0.2486 - val_acc: 0.6522\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5928 - val_loss: 0.2492 - val_acc: 0.6423\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5971 - val_loss: 0.2486 - val_acc: 0.6097\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5867 - val_loss: 0.2491 - val_acc: 0.5532\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5666 - val_loss: 0.2485 - val_acc: 0.5397\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5884 - val_loss: 0.2486 - val_acc: 0.6073\n",
      "Epoch 23/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5955 - val_loss: 0.2495 - val_acc: 0.5584\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2485 - acc: 0.5840\n",
      ". theta fit =  [0.9644881 1.4562707]\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2487 - acc: 0.5705 - val_loss: 0.2493 - val_acc: 0.6239\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5802 - val_loss: 0.2486 - val_acc: 0.6005\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.6036 - val_loss: 0.2494 - val_acc: 0.6018\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5956 - val_loss: 0.2485 - val_acc: 0.5416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5938 - val_loss: 0.2485 - val_acc: 0.5568\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6019 - val_loss: 0.2485 - val_acc: 0.6211\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6026 - val_loss: 0.2487 - val_acc: 0.5690\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5864 - val_loss: 0.2498 - val_acc: 0.6548\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5942 - val_loss: 0.2486 - val_acc: 0.4914\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5862 - val_loss: 0.2495 - val_acc: 0.6274\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.5898 - val_loss: 0.2485 - val_acc: 0.5328\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5723 - val_loss: 0.2497 - val_acc: 0.5316\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5773 - val_loss: 0.2485 - val_acc: 0.5728\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5984 - val_loss: 0.2489 - val_acc: 0.5913\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2486 - acc: 0.5420\n",
      ". theta fit =  [0.9641446 1.4559302]\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2510 - acc: 0.5633 - val_loss: 0.2494 - val_acc: 0.6289\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6029 - val_loss: 0.2490 - val_acc: 0.5080\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5824 - val_loss: 0.2488 - val_acc: 0.4921\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5774 - val_loss: 0.2487 - val_acc: 0.5818\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6109 - val_loss: 0.2486 - val_acc: 0.5022\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5824 - val_loss: 0.2484 - val_acc: 0.6482\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.6008 - val_loss: 0.2499 - val_acc: 0.6041\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.6062 - val_loss: 0.2486 - val_acc: 0.5615\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5974 - val_loss: 0.2485 - val_acc: 0.5776\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.5965 - val_loss: 0.2490 - val_acc: 0.6279\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5896 - val_loss: 0.2495 - val_acc: 0.6338\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6027 - val_loss: 0.2489 - val_acc: 0.5318\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5865 - val_loss: 0.2486 - val_acc: 0.5322\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5820 - val_loss: 0.2489 - val_acc: 0.6421\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5886 - val_loss: 0.2489 - val_acc: 0.6411\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5872 - val_loss: 0.2492 - val_acc: 0.6074\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2485 - acc: 0.6484\n",
      ". theta fit =  [0.96448755 1.4555913 ]\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2490 - acc: 0.5832 - val_loss: 0.2487 - val_acc: 0.4918\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5822 - val_loss: 0.2488 - val_acc: 0.6254\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5984 - val_loss: 0.2487 - val_acc: 0.6239\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.6005 - val_loss: 0.2488 - val_acc: 0.6347\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5908 - val_loss: 0.2492 - val_acc: 0.6138\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5930 - val_loss: 0.2489 - val_acc: 0.5573\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5880 - val_loss: 0.2490 - val_acc: 0.4964\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5875 - val_loss: 0.2493 - val_acc: 0.5313\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.5924 - val_loss: 0.2485 - val_acc: 0.5564\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5826 - val_loss: 0.2484 - val_acc: 0.5647\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2484 - acc: 0.5909 - val_loss: 0.2487 - val_acc: 0.5875\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5904 - val_loss: 0.2483 - val_acc: 0.6058\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.6042 - val_loss: 0.2486 - val_acc: 0.5301\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.5847 - val_loss: 0.2495 - val_acc: 0.5349\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5741 - val_loss: 0.2484 - val_acc: 0.6236\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.5802 - val_loss: 0.2487 - val_acc: 0.5703\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2486 - acc: 0.5834 - val_loss: 0.2485 - val_acc: 0.6354\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5934 - val_loss: 0.2485 - val_acc: 0.5393\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2485 - acc: 0.5902 - val_loss: 0.2495 - val_acc: 0.5066\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.5475 - val_loss: 0.2485 - val_acc: 0.5652\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5888 - val_loss: 0.2494 - val_acc: 0.6516\n",
      "Epoch 22/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6062 - val_loss: 0.2486 - val_acc: 0.5005\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2484 - acc: 0.6064\n",
      ". theta fit =  [0.96414256 1.4559354 ]\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2504 - acc: 0.5794 - val_loss: 0.2500 - val_acc: 0.5511\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.5847 - val_loss: 0.2500 - val_acc: 0.5967\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.5878 - val_loss: 0.2484 - val_acc: 0.6147\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.6013 - val_loss: 0.2485 - val_acc: 0.6041\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5842 - val_loss: 0.2485 - val_acc: 0.6530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5820 - val_loss: 0.2487 - val_acc: 0.5711\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5726 - val_loss: 0.2488 - val_acc: 0.6094\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5796 - val_loss: 0.2486 - val_acc: 0.6427\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2484 - acc: 0.5915 - val_loss: 0.2501 - val_acc: 0.6171\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.5960 - val_loss: 0.2490 - val_acc: 0.6086\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.6148 - val_loss: 0.2486 - val_acc: 0.5943\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.5954 - val_loss: 0.2484 - val_acc: 0.5556\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.5901 - val_loss: 0.2497 - val_acc: 0.6347\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2484 - acc: 0.6152\n",
      ". theta fit =  [0.96381146 1.4562784 ]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration)\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()\n",
    "\n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "                      loss=my_loss_wrapper_fit(myinputs_fit,\n",
    "                                               1,\n",
    "                                               reweight_analytically=True),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train),\n",
    "                  y_train,\n",
    "                  epochs=100,\n",
    "                  batch_size=1000,\n",
    "                  validation_data=(np.array(X_test), y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = True\n",
    "\n",
    "    # special optimizer and batch size = 2*N\n",
    "    model_fit.compile(optimizer=optimizer,\n",
    "                      loss=my_loss_wrapper_fit(myinputs_fit,\n",
    "                                               -1,\n",
    "                                               reweight_analytically=True),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta),\n",
    "                  y_train_theta,\n",
    "                  epochs=1,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_mu = np.array(fit_vals)[(index_refine[-1]):, 0]\n",
    "    fit_vals_sigma = np.array(fit_vals)[(index_refine[-1]):, 1]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "    extrema_mu = np.concatenate(\n",
    "        (argrelmin(fit_vals_mu)[0], argrelmax(fit_vals_mu)[0]))\n",
    "    extrema_mu = extrema_mu[extrema_mu >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    extrema_sigma = np.concatenate(\n",
    "        (argrelmin(fit_vals_sigma)[0], argrelmax(fit_vals_sigma)[0]))\n",
    "    extrema_sigma = extrema_sigma[extrema_sigma >= iteration -\n",
    "                                  index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema_mu\", extrema_mu)\n",
    "    print(\"extrema_sigma\", extrema_sigma)\n",
    "    '''\n",
    "\n",
    "    if (len(extrema_mu) == 0) or (\n",
    "            len(extrema_sigma)\n",
    "            == 0):  # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema_mu) >= 6) and (len(extrema_sigma) >=\n",
    "                                     6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10\n",
    "\n",
    "        mean_fit = np.array([[\n",
    "            np.mean(fit_vals_mu[len(fit_vals_mu) - 4:len(fit_vals_mu)]),\n",
    "            np.mean(fit_vals_sigma[len(fit_vals_sigma) -\n",
    "                                   4:len(fit_vals_sigma)])\n",
    "        ]])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:36:05.072251Z",
     "start_time": "2020-06-09T14:36:04.672173Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex=True, constrained_layout=True)\n",
    "fig.suptitle(\n",
    "    \"GaussianAltFit-2D (Analytical Reweight):\\n N = {:.0e}, Iterations = {:.0f}\"\n",
    "    .format(N, iterations))\n",
    "axs[0].plot(fit_vals[:, 0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], 0, len(fit_vals), label='$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(fit_vals[:, 1], label='Model $\\sigma$ Fit', color='orange')\n",
    "axs[1].hlines(theta1_param[1], 0, len(fit_vals), label='$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-2D (Analytical Reweight):\\n N = {:.0e}, Iterations = {:.0f}.png\"\n",
    "#     .format(N, iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T14:36:05.429595Z",
     "start_time": "2020-06-09T14:36:05.076357Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "\n",
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex=True, constrained_layout=True)\n",
    "fig.suptitle(\n",
    "    \"GaussianAltFit-2D Zoomed (Analytical Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\"\n",
    "    .format(N, index_refine[1], iterations))\n",
    "axs[0].plot(np.arange(index_refine[1], len(fit_vals[:, 0])),\n",
    "            fit_vals[index_refine[1]:, 0],\n",
    "            label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0],\n",
    "              index_refine[1],\n",
    "              len(fit_vals),\n",
    "              label='$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.arange(index_refine[1], len(fit_vals[:, 1])),\n",
    "            fit_vals[index_refine[1]:, 1],\n",
    "            label='Model $\\sigma$ Fit',\n",
    "            color='orange')\n",
    "axs[1].hlines(theta1_param[1],\n",
    "              index_refine[1],\n",
    "              len(fit_vals),\n",
    "              label='$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-2D Zoomed (Analytical Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\"\n",
    "#     .format(N, index_refine[1], iterations))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "682px",
    "left": "237px",
    "top": "140px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
