{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:08.392373Z",
     "start_time": "2020-06-09T07:59:05.916425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy.signal import argrelmin, argrelmax\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "import inspect\n",
    "\n",
    "fontP = FontProperties()\n",
    "fontP.set_size('small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:08.403492Z",
     "start_time": "2020-06-09T07:59:08.396850Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__)  #1.15.0\n",
    "print(keras.__version__)  #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{(1-f(x_{T,i},\\theta'))}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it $\\mu$ and $\\sigma$ values in addition to $X_i\\sim\\mathcal{N}(\\mu, \\sigma)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ and $\\sigma$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:08.412870Z",
     "start_time": "2020-06-09T07:59:08.408099Z"
    }
   },
   "outputs": [],
   "source": [
    "mu_min = -2\n",
    "mu_max = 2\n",
    "\n",
    "sigma_min = 0.5\n",
    "sigma_max = 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with these $\\mu$ and $\\sigma$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0, \\sigma = 1$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu, \\sigma$ values uniformly sampled in the specified range [$\\mu_{min}, \\mu_{max}$] and [$\\sigma_{min}, \\sigma_{max}$].\n",
    "\n",
    "For every value of $\\mu$ in mu_values and every value of $\\sigma$ in sigma_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ and $\\sigma$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:08.468615Z",
     "start_time": "2020-06-09T07:59:08.416888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Either load or train DCTR\n",
    "\n",
    "def get_dctr(load=False, n_data_points=10**7):\n",
    "    if load:\n",
    "        json_file = open('2d_gaussian_dctr_model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        dctr_model.load_weights(\"2d_gaussian_dctr_model.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "    else:\n",
    "        # Generate training & validation data\n",
    "        mu_values = np.random.uniform(mu_min, mu_max, n_data_points)\n",
    "        sigma_values = np.random.uniform(sigma_min, sigma_max, n_data_points)\n",
    "\n",
    "        X0 = [(np.random.normal(0, 1), mu_values[i], sigma_values[i])\n",
    "              for i in range(n_data_points)]  # Note the zero in normal(0, 1)\n",
    "        X1 = [(np.random.normal(mu_values[i], sigma_values[i]), mu_values[i],\n",
    "               sigma_values[i]) for i in range(n_data_points)]\n",
    "\n",
    "        Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "        Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "        X = np.concatenate((X0, X1))\n",
    "        Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
    "                                                            Y,\n",
    "                                                            test_size=0.5)\n",
    "\n",
    "        # Build Model\n",
    "        inputs = Input((3, ))\n",
    "        hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "        hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "        hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "        outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "        dctr_model = Model(inputs=inputs, outputs=outputs)\n",
    "        dctr_model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='Adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        # Train Model\n",
    "        earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        dctr_model.fit(X_train,\n",
    "                       Y_train,\n",
    "                       epochs=200,\n",
    "                       batch_size=10000,\n",
    "                       validation_data=(X_test, Y_test),\n",
    "                       callbacks=[earlystopping])\n",
    "\n",
    "    return dctr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:09.759294Z",
     "start_time": "2020-06-09T07:59:08.473605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "dctr_model = get_dctr(load=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining reweighting functions\n",
    "\n",
    "For a fully trained DCTR $f(x, \\theta)$, the reweighting function is: $w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$.\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$.\n",
    "\n",
    "The expression for the analytical reweighting function is : $w(x_T, \\mu, \\sigma) = \\frac{\\sigma_0}{\\sigma}\\exp{(-((\\frac{x_T-\\mu}{\\sigma})^2-(\\frac{x_T-\\mu_0}{\\sigma_0})^2)/2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:09.777648Z",
     "start_time": "2020-06-09T07:59:09.763791Z"
    }
   },
   "outputs": [],
   "source": [
    "theta0_param = (0, 1)\n",
    "\n",
    "\n",
    "# from NN (DCTR)\n",
    "def reweight(events, param):\n",
    "\n",
    "    # creating tensor with same length as inputs, with theta_prime in every entry\n",
    "    concat_input_and_params = K.ones(shape=(events.shape[0], 2)) * param\n",
    "    # combining and reshaping into correct format:\n",
    "    model_inputs = K.concatenate((events, concat_input_and_params), axis=-1)\n",
    "    f = dctr_model(model_inputs)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "# from analytical formula for normal distributions\n",
    "def analytical_reweight(events, param, param0=theta0_param):\n",
    "    weights = (param0[1] / param[1]) * K.exp(-0.5 * (\n",
    "        ((events - param[0]) / param[1])**2 -\n",
    "        ((events - param0[0]) / param0[1])**2))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate DCTR for any $\\mu$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate for Truth Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:09.931557Z",
     "start_time": "2020-06-09T07:59:09.782371Z"
    }
   },
   "outputs": [],
   "source": [
    "mu1 = 1.5\n",
    "sigma1 = 1.5\n",
    "assert mu1 >= mu_min and mu1 <= mu_max  # choose mu1 in valid range\n",
    "assert sigma1 >= sigma_min and sigma1 <= sigma_max  # choose mu1 in valid range\n",
    "X0_val_T = np.random.normal(0, 1, 10**6)\n",
    "X1_val_T = np.random.normal(mu1, sigma1, 10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:10.628854Z",
     "start_time": "2020-06-09T07:59:09.936360Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = reweight(\n",
    "    K.expand_dims(tf.convert_to_tensor(X0_val_T, dtype=tf.float32)),\n",
    "    np.array([mu1, sigma1]))\n",
    "analytical_weights = analytical_reweight(X0_val_T, np.array([mu1, sigma1]))\n",
    "weights = K.eval(weights)\n",
    "analytical_weights = K.eval(analytical_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:12.107959Z",
     "start_time": "2020-06-09T07:59:10.633589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-6, 6, 31)\n",
    "plt.hist(X0_val_T, bins=bins, alpha=0.5, label=r'$\\mu=0$, $\\sigma=1$')\n",
    "plt.hist(X0_val_T,\n",
    "         bins=bins,\n",
    "         label=r'$\\mu=0$, $\\sigma=1$ NN wgt.',\n",
    "         weights=weights,\n",
    "         histtype='step',\n",
    "         color='k')\n",
    "plt.hist(X0_val_T,\n",
    "         bins=bins,\n",
    "         label=r'$\\mu=0$, $\\sigma=1$ analytical wgt.',\n",
    "         weights=analytical_weights,\n",
    "         histtype='step',\n",
    "         linestyle='--',\n",
    "         color='k')\n",
    "plt.hist(X1_val_T,\n",
    "         bins=bins,\n",
    "         alpha=0.5,\n",
    "         label=r'$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "plt.legend()\n",
    "plt.title(\"Truth Level: Reweighting\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate for Detector Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:19.474207Z",
     "start_time": "2020-06-09T07:59:12.112751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate detector level by smearing truth\n",
    "\n",
    "epsilon_val = 0.5  #Smearing width\n",
    "\n",
    "X0_val_D = np.array([(x + np.random.normal(0, epsilon_val))\n",
    "                     for x in X0_val_T])  #Detector smearing\n",
    "X1_val_D = np.array([(x + np.random.normal(0, epsilon_val))\n",
    "                     for x in X1_val_T])  #Detector smearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:21.130283Z",
     "start_time": "2020-06-09T07:59:19.479123Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/figure.py:459: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True, constrained_layout=True)\n",
    "fig.suptitle('Detector Smearing')\n",
    "axs[0].set_title('$\\mu=0$, $\\sigma=1$')\n",
    "axs[0].hist(X0_val_T, bins=bins, alpha=0.5, label='TRUTH')\n",
    "axs[0].hist(X0_val_D, bins=bins, alpha=0.5, label='DETECTOR')\n",
    "axs[0].legend(prop=fontP)\n",
    "axs[1].set_title('$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "axs[1].hist(X1_val_T, bins=bins, alpha=0.5, label='TRUTH:')\n",
    "axs[1].hist(X1_val_D, bins=bins, alpha=0.5, label='DETECTOR')\n",
    "axs[1].legend(prop=fontP)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:22.313331Z",
     "start_time": "2020-06-09T07:59:21.135003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-7, 7, 31)\n",
    "plt.hist(X0_val_D, bins=bins, alpha=0.5, label=r'$\\mu=0$, $\\sigma=1$')\n",
    "plt.hist(X0_val_D,\n",
    "         bins=bins,\n",
    "         label=r'$\\mu=0$, $\\sigma=1$ NN wgt.',\n",
    "         weights=weights,\n",
    "         histtype='step',\n",
    "         color='k')\n",
    "plt.hist(X0_val_D,\n",
    "         bins=bins,\n",
    "         label=r'$\\mu=0$, $\\sigma=1$ analytical wgt.',\n",
    "         weights=analytical_weights,\n",
    "         histtype='step',\n",
    "         linestyle='--',\n",
    "         color='k')\n",
    "plt.hist(X1_val_D,\n",
    "         bins=bins,\n",
    "         alpha=0.5,\n",
    "         label=r'$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "plt.legend()\n",
    "plt.title(\"Detector Level: Reweighting\")\n",
    "#plt.savefig(\"Detector Level: Reweighting.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:22.325226Z",
     "start_time": "2020-06-09T07:59:22.317580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"2d_gaussian_dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"2d_gaussian_dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"2d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"2d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply detector effects, each event $x_{T,i}$in the sample is smeared by shifting by $Z_{i}$ from $Z = \\mathcal{N}(0,\\epsilon)$,where $\\epsilon$ represents the smearing. Thus: $x_{D,i} = x_{T,i} + Z_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:30.134830Z",
     "start_time": "2020-06-09T07:59:22.329690Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "#param = (mu, sigma)\n",
    "theta0_param = (\n",
    "    0, 1\n",
    ")  #this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = (1, 1.5)  #this is the data (the target)\n",
    "\n",
    "epsilon = theta0_param[1] / 2  #Smearing width\n",
    "\n",
    "theta0_T = np.random.normal(theta0_param[0], theta0_param[1], N)\n",
    "theta0_D = np.array([(x + np.random.normal(0, epsilon))\n",
    "                     for x in theta0_T])  #Detector smearing\n",
    "theta0 = np.stack([theta0_T, theta0_D], axis=1)\n",
    "\n",
    "theta1_T = np.random.normal(theta1_param[0], theta1_param[1], N)\n",
    "theta1_D = np.array([(x + np.random.normal(0, epsilon))\n",
    "                     for x in theta1_T])  #Detector smearing\n",
    "theta1 = np.stack([theta1_T, theta1_D], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:32.056825Z",
     "start_time": "2020-06-09T07:59:30.140150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAElCAYAAACWMvcuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuYFNWd//H3RxBBRbmIRkEEI3gXNKOS24bEVZDNLmZXEy+JYDTEje6aixsx7m81xmvWLJvExIQEBEwiGmOUNRglKms2URSNd1EmSMLgBQTFKyj4/f1RZ7AZu2d6enqmZ2o+r+epZ7pOnao6VdOnv12nTtdRRGBmZpY3W9W6AGZmZu3BAc7MzHLJAc7MzHLJAc7MzHLJAc7MzHLJAc7MzHLJAc66PEmvSdqzg/f5uKSxrVznUklfbqcidSmS+kj6H0nrJP0ypV0k6UVJz3dQGXaR9KSkbTpif9bxHOByStI2kmZI+oukVyU9JOnoguVjJb2TgsNrkhokXS/p0Ba220vSf0h6StLrklZKulXSUe1/VMVFxPYRsaza25U0S9JbBefoNUmfSfvcPyIWpnwXSPpZC9saBJwM/DjNV3T+i5TvoooPcMtthaS9qrGttL1haZuvFTt/wLHALsDAiDhO0lDga8B+EfG+Nux3rKSGcvJGxAvAXcCUSvdnnZsDXH71BFYAHwN2BP4duF7SsII8z0bE9kBfYAywBPi9pCOa2e4NwESyD+v+wHDgu8DfVbn8ncW3UwBtnK6rcDuTgfkR8WZBWiXnv9OR1LOZxf1KnL89gKcjYmOaHwqsiYhV7VrY9/o58MUO3qd1lIjw1E0m4BHgn9LrsUBDkTxXAotLrP+3wJvAkBb2MxX4M/Aq8ATwqYJlFwA/K5gfBgTQM81PBpaldZ8BTkrpewH/C6wDXgSuK9hGAHul138H/Al4hSzAX1BkX5OAv6btnNfMccwCLiqxbHk6H+OBt4C3gdeAh0vkvxP4bMF8Wecf2AdYAKwFngI+ndKnpH2+lfb7Pyl9N+BXwOp0/v61YFs9gG8U/G8eAHYH7k7n5fW0rc+k/F8A6tO+5wG7NTnnZwBLgWeKHMcW/9cmy77Z5Jx9Mb2v3knzs1K+McAfgZeBh4GxBdsYAFwNPAu8BNwEbNdkO6+l83EYsDi9J14A/qtgOz2BN4A9al0/PVV/qnkBPHXQPzprDloP7JPmS33AfiJ9QGxXZNllwMIy9nVc+mDZCvhM+uDcNS27gBIBLn1AvQLsnZbtCuyfXl8LnJe22Rv4SME2CgPcWODAlO+g9IF2TJN9/QToA4wCNgD7ljiOWbQQ4IodU4n8q4FDC+ZbPP9pWgGcks7PwWRBeb9i5UvH/ADwH0AvYE+yLwvj0vJ/Ax4F9gaUjn9g03NYUI4XgUOAbYDvA3c3OecLyAJNnyLHsfn/WuJ8NH0fbHE+gMHAGmBCOq4j0/ygtPw3wHVkrQhbAx8rdV6Be4DPpdfbA2OaLH8E+Ida11FP1Z/cRNkNSNqarClmdkQsaSH7s2Qffv2KLNsJ2NwBQNIASS+njgLrG9Mj4pcR8WxEvBNZk9RSsm/R5XgHOEBSn4h4LiIeT+lvkzVr7RYR6yPi/4qtHBELI+LRtO9HyALjx5pk+2ZEvBkRD5NdGYxqpjxnp2N8WdKLZR5DMf3IrppaUnj+Pwksj4irI2JjRPyJ7OrsuBLrHkoWAC6MiLciuy/5E+D4tPw04N8j4qnIPBwRa0ps6yRgZkQ8GBEbgHOBDzZp4r40ItbGls2uTb1YcP5elrRvy6cAgM+SNenOT//LBWRXYRMk7QocDZweES9FxNsR8b/NbOttYC9JO0XEaxFxb5Plr1L8/W5dnANczknaCriGrEnozDJWGUz2zfvlIsvWkF1VAZA+3PoBHyD7lt+4z5NTp5aXJb0MHEAWHJsVEa+TXfGdDjwn6TeS9kmLv072wX9f6sH4+WLbkHS4pLskrZa0Lm2r6b4Le+m9QfatvpQrIqJfmlo8hma8RHavrSWF538P4PDCAEEWeEp1wtgD2K1J/m+QXb1D1hz55zLLuxvwl8aZiHiN7P8/uCDPijK2s1PB+esXEU+Wuf89gOOaHMtHyN5/uwNrI+KlMrd1KjASWCLpfkmfbLK8L8Xf79bFOcDlmCQBM8g+4P4pIt4uY7VPAQ+mYNPUHcChkoY0s889yK4aziRr/uoHPEYWnCBrrty2YJUtPqwj4raIOJLsg2xJ2hYR8XxEfCEidiO7Z/PDEr3+fkF2v2j3iNgR+FHBvttLOUNyPEL2IduSwvO/AvjfJgFi+4j45xL7XUF2P6wwf9+ImFCw/P1llAGyK8k9GmckbQcMBFYW5GnPoUhWANc0OZbtIuKytGyApGJXXe8pU0QsjYgTgJ2By4Eb0vE0dpDZi+xK3nLGAS7frgL2Bf6+uWYkZQZLOp+sGesbxfJFxO1k3apvSldKvVLz55iCbNuRfcisTts+hewKrtFDwN9IGippR7Kmr8Zy7CJpYvrw2UDWSeCdtOy4gsD6UtrHO0WK2Zfs2/16SYcBJ5Y67ip6ARiWrpZLmc97m0qBZs//LcBISZ+TtHWaDi1o5nuB7D5bo/uAVyWdo+x3Zj0kHVDw04OfAt+SNCLt8yBJA0ts61rgFEmjlf1O7BJgUUQsL+eEVMHPgL+XNC4dR+/0E4AhEfEccCvZl5z+6bz8TcFxDEzvLQAkfVbSoIh4h3ev1BrfO4eRNQNvvlq1/HCAy6l0JfVFYDTwfMHvkE4qyLabpMbeZveTdc4YmwJZKZ8i++D9GdmHxTNkzWbjACLiCeA7ZDf2X0jb/EPjyuleynVkVzQPpG012gr4KtnVw1qygNB4tXIosCiVdx5wVhT/7duXgAslvUrW2eL6Zo6lWn6Z/q6R9GCJPHPI7h/1KUhr9vxHxKvAUWT30J4la1q9nHebg2cA+6UmvJsiYhPZfbvRZP+XF8mCWuOH/X+RnY/byTrzzCDrbANZp4/ZaVufjojfAf+P7J7fc2RXfo338lrjZW35O7ivlrNSRKwg+znKN8i+LK0g6yTT+Jn1ObJ7a0uAVcCX03pLyILzsnQsu5H1dH08nevvAscXfOE7iewq33JIER7w1KwjSLoEWBUR/13rshhI2pnspycHR8T6lvJb1+MAZ2ZmueQmSjMzyyUHODMzyyUHODMzyyUHODMzyyUHODMzyyUHuJySdJKk5n7P1pZtt2kcMtVggFIz634c4Lo4SR+R9EdlDzxeK+kPkg6NiJ9HRM0GIS0o30JJpxWmRTsNUGpWLZKWS3pT2WDBL6c6dnoLT6tpXLdxsNfmxsmzDuAA14VJ2oHsSSDfJxu2ZDDZWFsbalkus5z4+4joS/ZMzsuAc8ie/mJdhANc1zYSICKujYhNaQiY2yPiEUmTJW0eUiZ9o/ySpKXpW+m3JL0/fTN9RdL1knqlvFusW7D+ex5unJ4FeIuyp/e/lF4PScsuBj4KXJmaJa9sui1JO0qak9b/i6R/b/yW3FgOSVekbT8j6ej2OZVmxUXEuoiYRzbSxaT0fM+/k/SnVHdWSLqgYJW709/Gx5R9MNW1OyWtkfSipJ+r+MOirYoc4Lq2p4FNkmZLOlpS/xbyjyMb2mYM2fAz08nG3dqd7IHIJ1RQhq3IRlbeAxhKNqLylQARcR7we+DM1CxZbLie75M9K3FPsmdPnkw2wGejw8lGst4J+DYwQ1J7jw5g9h4RcR/QQPal7XWy92o/slHk/1nSMSlr44OfG0d/uIdsRItLyYYh2peszl3QcaXvnhzgurCIeIVsjKzGUapXS5onaZcSq3w7Il5Jg4g+BtweEcsiYh3Z09kPrqAMayLiVxHxRno48MWUeGp+U5J6kD3A99yIeDU9qf47ZA/SbfSXiPhJepDwbLJhdEodn1l7exYYUObAuptFRH1ELIiIDRGxmuzB12XVE6ucA1wXFxFPRsTkiBhCdhW2G1DqYb4vFLx+s8h8cwN/FiVpW0k/Ts2Lr5A1z/RLwaslOwFbUzCwZnpdOKjm5sFJI+KN9LLV5TSrksHAWpU3sO5myoaCmitpZaonP2suv1WHA1yOpKFCZrHl+GuV2GJQUkmlRpAG+BqwN3B4ROzAu80zjc2IzT3N+0WyIU/2KEgbypaDapp1CsrG1RsM/B/ND6xb7D1/SUo/MNWTzxbkt3biANeFSdpH0tcKOnXsTnYf7d42bvphYH9lg132pvl7BX3Jrv5eljQAOL/J8qYDaW6Wmh2vBy6W1FfZGHZfJft2a9YpSNpB0ieBucDPIuJRmh9YdzXZgKqF7/u+ZOP+rZM0mGxsO2tnDnBd26tknTAWSXqdLLA9RnZVVbGIeBq4EPgdsJTsG2sp/002aOaLaf+/bbL8u8CxqRfk94qs/y9kV4zLePeb8cy2lN+sSv5H2cC5K4DzyO6bNXaAKjmwbmpKvxj4Q/oN3Riyn+8cAqwDfgPc2GFH0Y15PDgzM8slX8GZmVkuOcCZmVkuOcCZmVkuOcCZmVkuOcCZmVkuOcCZmVkuOcCZmVkuOcBVmaRLJX251uWwtpN0n6T9a12OPHC9yI+uVC8c4KpI0iCyITR+3MH7HSDp15JeTw89PrHSvK3ZVi1IOlPSYkkbJM0qI3/J4ynjWK8ge6KLtUEN60XZ75VuWC8WSlqfxqt7TdJTBctyUy88pHp1TQbmR8SbHbzfHwBvkQ0jMxr4jaSH07A4rc3bmm3VwrPARWRj2/UpI39zx9PSsc4DfiTpfRHxfJFtW3kmU5t60Zr3SnerF5CN0/jTIun5qRcR4alKE3An8NmC+fOAHxXM9yd7en7vKu5zO7I348iCtGuAy1qbtzXbKlGWrcmewbc8HWek6ZF2ONcXAbMqPTflHiuwAJhU6/dWV55qUS9a817pbvUi5VsInNbac1GQ1iXqhZsoq+tAstGnC+cfKpgfDTwVEeuLrSzplvRw1mLTLSX2ORLYGNkDkhs9DBRrI28pb2u2VcxFwBFkIx73A+4Afg0c0zRjhcfaWs0dT7nH+iQwqkrl6a5qUS9ao7vVi0aXSnpR0h8kjU1puaoXbqKsrn5kT/hvdCBbDj46muzNUlREfLKCfW4PvNIkbR3Z8BytzduabW1BUl/gX4GDImJFSvsV8JmIWNY0f4XH2lrNHU+5x/oq2SjiVrla1IvW6G71AuAc4Amyq7XjyUZOGE3O6oWv4KrrJdIbQVIv4P3AIwXLR7HlN9dqeA3YoUnaDmz5gVJu3tZsq6m/AZZFxNKCtP4UjMhdA80dT7nH2hd4uV1K133Uol60RnerF0TEooh4NSI2RMRs4A/ABHJWLxzgqusRskt8gH2BlZGNDYUkAWNp5puqpFsLejU1nW4tsdrTQE9JIwrSRgHFbn63lLc122pqENkHWeOxCPgUULRZpcJjba3mjqfcY92XZv5nVpZa1IvW6G71opggG2E8X/Wi1jcB8zSRjUY9Pb3+HNm3nveT9Wq6iOxNNLId9jsXuJbsBvGHyZoU9q8kb3PLgVmUuIEN1AFvkDU39SHryHEfsHWVj7Un0Bu4lOzmd2+gZyXnpoxz0RtYC+xW6/dWV55qWC/Kfq90p3pB1mQ8rjEPcBLZoMMjyzwXXaZe1LwAeZqAnYCG9Eb+NnAD2WjAK8navFcAs9thvwOAm9Kb9K/AiQXLbgW+UU7eMrZ1B/CFZspxHll35edSpd+pHY71At7thdY4XVDJ8ZZxLo4Dbqz1+6qrTzWsFyXfK925XpBdVd5P9kXjZeBe4MhWnIsuUy88oneVSboEWEX2DemnEfGrGhepKtK9k4fJbpa/XevydARJi4BTI+KxWpelq3O9yI+uVC8c4NqJpAbgqIh4otZlMessXC+sIznAtQNJ/YEXgO26y7c6s5a4XlhHc4AzM7Nc8s8EzMwslxzgzMwslxzgzMwsl3L3LMqddtophg0bVutimFXFAw888GJEDGrLNlwnLG/KrRe5C3DDhg1j8eLFtS6GWVVI+ktbt+E6YXlTbr1osYlS0kxJqyQ91iT9XyQtkfS4pG8XpJ8rqV7SU5LGFaSPT2n1kqYWpA+XtCilX5d+OImkbdJ8fVo+rJwDMjMzg/Luwc0CxhcmSPo4MBEYFRH7kw1hjqT9yIZe2D+t80NJPST1IBsl9mhgP+CElBfgcmBaROxF9lDSU1P6qcBLKX1aymdmZlaWFgNcRNxN9mDNQv9MNsLrhpRnVUqfCMyNbAiGZ4B64LA01UfEsoh4i+xhnhPTk7U/QfZsOoDZvDsI4MQ0T1p+RMpvZmbWokrvwY0EPirpYmA9cHZE3A8MJntwZ6OGlAbZA1UL0w8HBgIvR8TGIvkHN64TERslrUv5X2xaGElTgCkAQ4cOrfCQrBrefvttGhoaWL++6ODMVkLv3r0ZMmQIW2+9dVW25zrRebhOVK6t9aLSANeT7InTY4BDgesl7VnhttosIqYD0wHq6ur8aJYaamhooG/fvgwbNgxfcJcnIlizZg0NDQ0MHz68Wtt0negkXCcqU416Uenv4BrIhkuIiLgPeIdsSIyVwO4F+YaktFLpa4B+kno2SadwnbR8x5TfOrH169czcOBAV+RWkMTAgQP9DT+nXCcqU416UWmAuwn4eCrESKAXWdPhPOD41ANyODCCbHC/+4ERqcdkL7KOKPMiexDmXcCxabuTgJvT63lpnrT8zvCDM7sEV+TW8znLN/9/K9PW81bOzwSuBe4B9pbUIOlUYCawZ/rpwFxgUrqaexy4HngC+C1wRkRsSvfYzgRuA54Erk95IRvw8KuS6snusc1I6TOAgSn9q8DmnxaYlbJmzRpGjx7N6NGjed/73sfgwYM3z7/11ltlbePGG29kyZIlm+c/8pGP8NBDD7VXkc3aVXeuEy3eg4uIE0os+myJ/BcDFxdJnw/ML5K+jKyXZdP09WQjx1qNTVvw9HvSvnLkyIrXbYuW9jtw4MDNFe+CCy5g++235+yzz94iT+Nov1ttVfz73Y033shWW23FPvvsU51CmxVwneg4fhalVWTagqeLTp1VfX09++23HyeddBL7778/K1asoF+/fpuXz507l9NOO43f//73zJ8/n6985SuMHj2a5cuXb15+2GGHsffee/PHP/6xRkdhVj3doU7k7lFdZqUsWbKEOXPmUFdXx8aNG4vm+ehHP8qECRM49thjOeaYYzanRwT33Xcf8+bN48ILL+S3v/1tRxXbrN3kvU74Cs66jfe///3U1dVVtO4//uM/AvCBD3xg8zdYs64u73XCAc66je22227z66222orCTrktdUXeZpttAOjRo0fJb7pmXU3e64QDnHVLW221Ff3792fp0qW88847/PrXv968rG/fvrz66qs1LJ1Zx8tjnXCAs27r8ssvZ9y4cXzoQx9iyJAhm9NPOOEELrnkki1uqJt1B3mrE8rbb6fr6urCY19VV2t6R44fsol99923HUuTX08++eR7zp2kByKispskietEbRX7v1r52lIvfAVnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnudOjRw9Gjx7N/vvvz6hRo/jOd77DO++80+w6y5cv5xe/+EXF+5w1axbPPvtsxeubtafuWif8sGVrX3ddWt3tffzcFrP06dNn8/Agq1at4sQTT+SVV17hm9/8Zsl1GivziSeeWFGxZs2axQEHHMBuu+1W9jqbNm2iR48eFe3PujDXiZKqXSd8BWe5tvPOOzN9+nSuvPJKIoJNmzbxb//2bxx66KEcdNBB/PjHPwZg6tSp/P73v2f06NFMmzatZD7InvZw4IEHMmrUKKZOncoNN9zA4sWLOemkkxg9ejRvvvkmd9xxBwcffDAHHnggn//859mwYQMAw4YN45xzzuGQQw7hl7/8ZU3OiXVv3alOtHgFJ2km8ElgVUQc0GTZ14ArgEER8aKy8cW/C0wA3gAmR8SDKe8k4N/TqhdFxOyU/gFgFtCHbEDUsyIiJA0ArgOGAcuBT0fES206WuuW9txzTzZt2sSqVau4+eab2XHHHbn//vvZsGEDH/7whznqqKO47LLLuOKKK7jlllsAmD59etF8S5Ys4eabb2bRokVsu+22rF27lgEDBnDllVdyxRVXUFdXx/r165k8eTJ33HEHI0eO5OSTT+aqq67iy1/+MpANQPnggw/W8pRYN9dd6kQ5V3CzgPFNEyXtDhwF/LUg+WhgRJqmAFelvAOA84HDyUbvPl9S/7TOVcAXCtZr3NdU4I6IGAHckebN2uT2229nzpw5jB49msMPP5w1a9awdOnSsvP97ne/45RTTmHbbbcFYMCAAe9Z96mnnmL48OGMHJmNtDxp0iTuvvvuzcs/85nPtNPRmbVenutEi1dwEXG3pGFFFk0Dvg7cXJA2EZgT2QMu75XUT9KuwFhgQUSsBZC0ABgvaSGwQ0Tcm9LnAMcAt6ZtjU3bnQ0sBM5p1dGZAcuWLaNHjx7svPPORATf//73GTdu3BZ5Fi5cuMV8qXy33XZbm8tTOESJWS10lzpR0T04SROBlRHxcJNFg4EVBfMNKa259IYi6QC7RMRz6fXzwC6VlNW6t9WrV3P66adz5plnIolx48Zx1VVX8fbbbwPw9NNP8/rrr79nOJBS+Y488kiuvvpq3njjDQDWrl0LbDmcyN57783y5cupr68H4JprruFjH/tYhx2zWXO6U51odS9KSdsC3yBrnuwQ6Z5cyWEPJE0haxJl6NChHVWsbmPMX6eXn3nIqe1XkDK9+eabjB49mrfffpuePXvyuc99jq9+9asAnHbaaSxfvpxDDjmEiGDQoEHcdNNNHHTQQfTo0YNRo0YxefJkzjrrrKL5xo8fz0MPPURdXR29evViwoQJXHLJJUyePJnTTz+dPn36cM8993D11Vdz3HHHsXHjRg499FBOP/30Dj0HrhNWqLvWibKGy0lNlLdExAGSDiS7J/ZGWjwEeJbs3to3gYURcW1a7ymyZsaxwNiI+GJK/zFZk+NC4K6I2Celn9CYr3HdiHguNXMujIi9WyqrhwapvntmnF123n4fOtVDg1TIw+Xkk4fLaZsOHS4nIh6NiJ0jYlhEDCNrVjwkIp4H5gEnKzMGWJeaGW8DjpLUP3UuOQq4LS17RdKY1APzZN69pzcPmJReT2LLe31mZmbNajHASboWuAfYW1KDpObaoOYDy4B64CfAlwBS55JvAfen6cLGDicpz0/TOn8m62ACcBlwpKSlwN+meTMzs7KU04vyhBaWDyt4HcAZJfLNBGYWSV8MHFAkfQ1wREvlMzMzK8ZPMrGqK+e+rm3J5yzf/P+tTFvPmwOcVVXv3r1Zs2aNK3QrRARr1qyhd+/etS6KtQPXicpUo174YctWVUOGDKGhoYHVq1fXuihdSu/evRkyZEiti2HtwHWicm2tFw5wVlVbb701w4cPr3UxzDoN14nacYCzzaYteLpo+pg2buMrR46ssERmZpXzPTgzM8slBzgzM8slBzgzM8slBzgzM8slBzgzM8slBzgzM8slBzgzM8slBzgzM8slBzgzM8slBzgzM8slBzgzM8slP4vSNhvz1+nttI0r2rxdM7PWavEKTtJMSaskPVaQ9p+Slkh6RNKvJfUrWHaupHpJT0kaV5A+PqXVS5pakD5c0qKUfp2kXil9mzRfn5YPq9ZBm5lZ/pXTRDkLGN8kbQFwQEQcBDwNnAsgaT/geGD/tM4PJfWQ1AP4AXA0sB9wQsoLcDkwLSL2Al4CTk3ppwIvpfRpKZ+ZmVlZWgxwEXE3sLZJ2u0RsTHN3gs0jkg3EZgbERsi4hmgHjgsTfURsSwi3gLmAhMlCfgEcENafzZwTMG2ZqfXNwBHpPxmZmYtqkYnk88Dt6bXg4EVBcsaUlqp9IHAywXBsjF9i22l5etS/veQNEXSYkmLPWqumeuEGbQxwEk6D9gI/Lw6xalMREyPiLqIqBs0aFAti2LWKbhOmLWhF6WkycAngSMiIlLySmD3gmxDUhol0tcA/ST1TFdphfkbt9UgqSewY8pvZmbWooqu4CSNB74O/ENEvFGwaB5wfOoBORwYAdwH3A+MSD0me5F1RJmXAuNdwLFp/UnAzQXbmpReHwvcWRBIzczMmtXiFZyka4GxwE6SGoDzyXpNbgMsSP0+7o2I0yPicUnXA0+QNV2eERGb0nbOBG4DegAzI+LxtItzgLmSLgL+BMxI6TOAayTVk3VyOb4Kx2tmZt1EiwEuIk4okjyjSFpj/ouBi4ukzwfmF0lfRtbLsmn6euC4lspnZmZWjB/VZWZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmueQAZ2ZmuVTxgKdmZla5aQuefk/aV44cWYOS5Jev4MzMLJcc4MzMLJdaDHCSZkpaJemxgrQBkhZIWpr+9k/pkvQ9SfWSHpF0SME6k1L+pZImFaR/QNKjaZ3vKQ0RXmofZmZm5SjnHtws4EpgTkHaVOCOiLhM0tQ0fw5wNDAiTYcDVwGHSxoAnA/UAQE8IGleRLyU8nwBWEQ24vd44NZm9mFm1mUUu9fWmry+L1e5FgNcRNwtaViT5InA2PR6NrCQLPhMBOZERAD3SuonadeUd0FErAWQtAAYL2khsENE3JvS5wDHkAW4UvswM+vyxvx1+nvS7h06pQYlya9K78HtEhHPpdfPA7uk14OBFQX5GlJac+kNRdKb24eZmVmL2tzJJF2tRRXKUvE+JE2RtFjS4tWrV7dnUcy6BNcJs8oD3Aup6ZH0d1VKXwnsXpBvSEprLn1IkfTm9vEeETE9Iuoiom7QoEEVHpJZfrhOmFUe4OYBjT0hJwE3F6SfnHpTjgHWpWbG24CjJPVPvSGPAm5Ly16RNCb1njy5ybaK7cPMzKxFLXYykXQtWWePnSQ1kPWGvAy4XtKpwF+AT6fs84EJQD3wBnAKQESslfQt4P6U78LGDifAl8h6avYh61xya0ovtQ8zsy6jWGeS1uW9onqF6WbK6UV5QolFRxTJG8AZJbYzE5hZJH0xcECR9DXF9mHVUaw78pgO3Je7PptZe/NvtiwHAAAPpElEQVSTTMzMLJf8sOVuqjXNJu2zLze7mFn78hWcmZnlkgOcmZnlkpsozcyqpD06b5V6lqU7arXMV3BmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZL7kVpZlYl7fEAhdLb9MMSWuIrODMzyyUHODMzyyUHODMzyyUHODMzyyUHODMzyyUHODMzy6U2BThJX5H0uKTHJF0rqbek4ZIWSaqXdJ2kXinvNmm+Pi0fVrCdc1P6U5LGFaSPT2n1kqa2paxmZta9VBzgJA0G/hWoi4gDgB7A8cDlwLSI2At4CTg1rXIq8FJKn5byIWm/tN7+wHjgh5J6SOoB/AA4GtgPOCHlNTMza1Fbmyh7An0k9QS2BZ4DPgHckJbPBo5JryemedLyIyQppc+NiA0R8QxQDxyWpvqIWBYRbwFzU14zM7MWVRzgImIl2U/p/0oW2NYBDwAvR8TGlK0BGJxeDwZWpHU3pvwDC9ObrFMq/T0kTZG0WNLi1atXV3pIZrnhOmHWtibK/mRXVMOB3YDtyJoYO1xETI+IuoioGzRoUC2KYNapuE6Yta2J8m+BZyJidUS8DdwIfBjol5osAYYAK9PrlcDuAGn5jsCawvQm65RKNzMza1FbAtxfgTGStk330o4AngDuAo5NeSYBN6fX89I8afmdEREp/fjUy3I4MAK4D7gfGJF6ZfYi64gyrw3lNTOzbqTi0QQiYpGkG4AHgY3An4DpwG+AuZIuSmkz0iozgGsk1QNryQIWEfG4pOvJguNG4IyI2AQg6UzgNrIemjMj4vFKy2tmVi3TFjxdNH1MjcvwlSNHdmAJOr82DZcTEecD5zdJXkbWA7Jp3vXAcSW2czFwcZH0+cD8tpTRzMy6J48HZ2bWSu0x7lt1yuAx4gr5UV1mZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLDnBmZpZLHk0g7+66tNYlKK5UuT5+bseWw8xyy1dwZmaWSw5wZmaWS20KcJL6SbpB0hJJT0r6oKQBkhZIWpr+9k95Jel7kuolPSLpkILtTEr5l0qaVJD+AUmPpnW+J0ltKa+ZmXUfbb2C+y7w24jYBxgFPAlMBe6IiBHAHWke4GhgRJqmAFcBSBoAnA8cDhwGnN8YFFOeLxSsN76N5TUzs26i4k4mknYE/gaYDBARbwFvSZoIjE3ZZgMLgXOAicCciAjg3nT1t2vKuyAi1qbtLgDGS1oI7BAR96b0OcAxwK2VltnMrNU6a0etIu6ZcXbR9A+eekUHl6RzaEsvyuHAauBqSaOAB4CzgF0i4rmU53lgl/R6MLCiYP2GlNZcekOR9PeQNIXsqpChQ4dWfkQ5dM+yNbUuQlGlyvXBj3dwQXLKdaJ6Omsdspa1pYmyJ3AIcFVEHAy8zrvNkQCkq7Vowz7KEhHTI6IuIuoGDRrU3rsz6/RcJ8zaFuAagIaIWJTmbyALeC+kpkfS31Vp+Upg94L1h6S05tKHFEk3MzNrUcUBLiKeB1ZI2jslHQE8AcwDGntCTgJuTq/nASen3pRjgHWpKfM24ChJ/VPnkqOA29KyVySNSb0nTy7YlpmZWbPa+iSTfwF+LqkXsAw4hSxoXi/pVOAvwKdT3vnABKAeeCPlJSLWSvoWcH/Kd2FjhxPgS8AsoA9Z5xJ3MDEzs7K0KcBFxENAXZFFRxTJG8AZJbYzE5hZJH0xcEBbymhmZt2Tn2RiZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma55ABnZma51NbhcszM8uGuS2tdgvZT7Ng+fm7Hl6OD+QrOzMxyyVdwZmbAPcvW1LoI7abYsX3w4zUoSAdr8xWcpB6S/iTpljQ/XNIiSfWSrkujfSNpmzRfn5YPK9jGuSn9KUnjCtLHp7R6SVPbWlYzM+s+qtFEeRbwZMH85cC0iNgLeAk4NaWfCryU0qelfEjaDzge2B8YD/wwBc0ewA+Ao4H9gBNSXjMzsxa1KcBJGgL8HfDTNC/gE8ANKcts4Jj0emKaJy0/IuWfCMyNiA0R8QxQDxyWpvqIWBYRbwFzU14zM7MWtfUK7r+BrwPvpPmBwMsRsTHNNwCD0+vBwAqAtHxdyr85vck6pdLNzMxaVHEnE0mfBFZFxAOSxlavSBWVZQowBWDo0KG1LIq1VTftzlxtrhNmbetF+WHgHyRNAHoDOwDfBfpJ6pmu0oYAK1P+lcDuQIOknsCOwJqC9EaF65RK30JETAemA9TV1UUbjqlry/PveKxVXCfM2tBEGRHnRsSQiBhG1knkzog4CbgLODZlmwTcnF7PS/Ok5XdGRKT041Mvy+HACOA+4H5gROqV2SvtY16l5TUzs+6lPX4Hdw4wV9JFwJ+AGSl9BnCNpHpgLVnAIiIel3Q98ASwETgjIjYBSDoTuA3oAcyMiMfbobxmZpZDVQlwEbEQWJheLyPrAdk0z3rguBLrXwxcXCR9PjC/GmU0M7PuxY/qMjOzXHKAMzOzXHKAMzOzXHKAMzOzXPJoAtapdNennptZ9TnA5Uieh/swqyo/FKH0OcjRk4PcRGlmZrnkAGdmZrnkJkoz63bcnF/6HOTpnrev4MzMLJcc4MzMLJcc4MzMLJcc4MzMLJcc4MzMLJcc4MzMLJcc4MzMLJcc4MzMLJcqDnCSdpd0l6QnJD0u6ayUPkDSAklL09/+KV2SviepXtIjkg4p2NaklH+ppEkF6R+Q9Gha53uS1JaDNTOz7qMtV3Abga9FxH7AGOAMSfsBU4E7ImIEcEeaBzgaGJGmKcBVkAVE4HzgcOAw4PzGoJjyfKFgvfFtKK+ZmXUjFT+qKyKeA55Lr1+V9CQwGJgIjE3ZZgMLgXNS+pyICOBeSf0k7ZryLoiItQCSFgDjJS0EdoiIe1P6HOAY4NZKy5wb3e1J6N3gqefWTrpbXamGYuesi9a1qtyDkzQMOBhYBOySgh/A88Au6fVgYEXBag0prbn0hiLpxfY/RdJiSYtXr17dpmMxywPXCbMqBDhJ2wO/Ar4cEa8ULktXa9HWfbQkIqZHRF1E1A0aNKi9d2fW6blOmLVxNAFJW5MFt59HxI0p+QVJu0bEc6kJclVKXwnsXrD6kJS2knebNBvTF6b0IUXym5mVxaMGtF6xc9ZVRxhoSy9KATOAJyPivwoWzQMae0JOAm4uSD859aYcA6xLTZm3AUdJ6p86lxwF3JaWvSJpTNrXyQXbMjMza1ZbruA+DHwOeFTSQyntG8BlwPWSTgX+Anw6LZsPTADqgTeAUwAiYq2kbwH3p3wXNnY4Ab4EzAL6kHUucQcTMzMrS1t6Uf4fUOp3aUcUyR/AGSW2NROYWSR9MXBApWU0M7Puy08yMTOzXGpTJxOrje5247zU8XbVG99m1jF8BWdmZrnkKzgzywc/taT9dNGnCfkKzszMcskBzszMcslNlGaWC92t81VH6qodvRzgOjvfVygtR089N7PqcxOlmZnlkgOcmZnlkpsozaxrcbN959HJbxP4Cs7MzHLJV3Bm1qW4t2Tn0dnHjnOA6+RcmUvr7JXLzGrLAa6z8H0FM8uDTvRYLwc4y5dOVLmsCvzFz9qg0wc4SeOB7wI9gJ9GxGU1LpKZdRA30Xc9nempJ506wEnqAfwAOBJoAO6XNC8inqhtyarPFbk6OlPlslbwlVr+1eAnBZ06wAGHAfURsQxA0lxgItC1A5wrc8fr5L/X6e78BS//inYKo31vKXT2ADcYWFEw3wAcXqOytF6JQObK3PFqUbkM7plxdq2LYJ1Ye7e4dPYAVxZJU4ApafY1SU+1w252Al5sh+22lsuxpXYoxzc6STkA2KOSlVwnaqazlKVrl+O077SUo6x6oYho9b47iqQPAhdExLg0fy5ARHR4G5+kxRFR19H7dTlcjs6qsxxzZykHdJ6yuByZzv6orvuBEZKGS+oFHA/Mq3GZzMysC+jUTZQRsVHSmcBtZD8TmBkRj9e4WGZm1gV06gAHEBHzgfm1LgcwvdYFSFyOLbkctdNZjrmzlAM6T1lcDjr5PTgzM7NKdfZ7cGZmZhVxgCtB0gWSVkp6KE0TSuQbL+kpSfWSprZDOf5T0hJJj0j6taR+JfItl/RoKuviKu6/2eOTtI2k69LyRZKGVWvfBfvYXdJdkp6Q9Liks4rkGStpXcH/6z+qXY60n2bPszLfS+fjEUmHtEc5asF1YvN2XSfeu6/OWS8iwlORCbgAOLuFPD2APwN7Ar2Ah4H9qlyOo4Ce6fXlwOUl8i0Hdqryvls8PuBLwI/S6+OB69rhf7ErcEh63Rd4ukg5xgK3dMD7otnzDEwAbgUEjAEWtXeZOmpynXCdaKY8nbJe+AqubTY/Siwi3gIaHyVWNRFxe0RsTLP3AkOquf0WlHN8E4HZ6fUNwBGSVM1CRMRzEfFgev0q8CTZU246o4nAnMjcC/STtGutC9WBXCdcJ4qpSb1wgGvemelyeqak/kWWF3uUWHu+yT5P9i2omABul/RAeopFNZRzfJvzpA+ddcDAKu3/PVJzz8HAoiKLPyjpYUm3Stq/nYrQ0nnu6PdER3OdcJ0oplPWi07/M4H2JOl3wPuKLDoPuAr4Ftk/7lvAd8gqU4eWIyJuTnnOAzYCPy+xmY9ExEpJOwMLJC2JiLvbo7y1Iml74FfAlyPilSaLHwT2iIjX0r2hm4AR7VCMXJ9n14mupZPUCeik57pbB7iI+Nty8kn6CXBLkUUrgd0L5oektKqWQ9Jk4JPAEZEatItsY2X6u0rSr8maUtr6Bivn+BrzNEjqCewIVP1p0pK2JqvIP4+IG5suL6zcETFf0g8l7RQRVX0eXxnnuSrviVpxnWiR60QRnbVeuImyhCbtw58CHiuSrd0fJaZswNevA/8QEW+UyLOdpL6Nr8luwhcrb2uVc3zzgEnp9bHAnaU+cCqV7l/MAJ6MiP8qked9jfc5JB1G9t6u6odKmed5HnBy6jU2BlgXEc9Vsxy14joBuE4U20/nrRcd0ZOlK07ANcCjwCPpn7NrSt8NmF+QbwJZD6Y/kzWfVLsc9WRt1w+l6UdNy0HWo+vhND1ezXIUOz7gQrIPF4DewC9TOe8D9myHc/ARsmaxRwrOwwTgdOD0lOfMdOwPk3U8+FA7lKPoeW5SDpEN0vvn9P6pq/V7uYrH7zpR4vi6a51o7lx3hnrhJ5mYmVkuuYnSzMxyyQHOzMxyyQHOzMxyyQHOzMxyyQHOzMxyyQHOzMxyyQHOzMxyyQHOzMxy6f8Dj/vh1l9HSscAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-6, 6, 31)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, constrained_layout=True)\n",
    "fig.suptitle('2D Gaussian Fit (Detector Effects)')\n",
    "\n",
    "axs[0].set_title(\"Simulation\\n($\\mu$ = {:.2f}, $\\sigma$ = {:.2f})\".format(\n",
    "    theta0_param[0], theta0_param[1]))\n",
    "axs[0].hist(theta0_T, bins=bins, alpha=0.5, label='Truth')\n",
    "axs[0].hist(theta0_D, bins=bins, alpha=0.5, label='Detector')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title(\"Data\\n($\\mu$ = {:.2f}, $\\sigma$ = {:.2f})\".format(\n",
    "    theta1_param[0], theta1_param[1]))\n",
    "axs[1].hist(theta1_T, bins=bins, alpha=0.5, label='Truth')\n",
    "axs[1].hist(theta1_D, bins=bins, alpha=0.5, label='Detector')\n",
    "axs[1].legend()\n",
    "\n",
    "#fig.savefig(\"2D Gaussian: Data ($\\mu$ = {:.2f}, $\\sigma$ = {:.2f}) w detector effects.png\".format(theta1_param[0], theta1_param[1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:32.076559Z",
     "start_time": "2020-06-09T07:59:32.061685Z"
    }
   },
   "outputs": [],
   "source": [
    "#'Erasing' Truth level for data, we can't actually observe this\n",
    "theta1 = np.stack([np.zeros_like(theta0_D), theta0_D], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:32.561723Z",
     "start_time": "2020-06-09T07:59:32.080157Z"
    }
   },
   "outputs": [],
   "source": [
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0_D, theta1_D])\n",
    "y_true = np.concatenate([labels0, labels1])\n",
    "# 'hiding' truth level for simulation in model output (used in reweighting)\n",
    "truth_level = np.concatenate([theta0_T, theta1_T])\n",
    "yvals = np.stack([y_true, truth_level], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals,\n",
    "                                                    yvals,\n",
    "                                                    test_size=0.5)\n",
    "X_train_theta, y_train_theta = shuffle(xvals, yvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:32.577174Z",
     "start_time": "2020-06-09T07:59:32.567008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njson_file = open(\\'2d_gaussian_dctr_model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\ndctr_model = keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\ndctr_model.load_weights(\"2d_gaussian_dctr_model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('2d_gaussian_dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"2d_gaussian_dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Section for $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T07:59:32.676573Z",
     "start_time": "2020-06-09T07:59:32.582205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1, ), dtype=tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "def my_loss_wrapper(val=0., reweight_analytically=False, MSE_loss=True):\n",
    "    def my_loss(y_true, y_pred):\n",
    "        y_true = tf.gather(y_true, np.arange(batch_size))\n",
    "        y_labels = tf.gather(y_true, [0], axis=1)  #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis=1)  # sim truth for reweighting\n",
    "\n",
    "        theta_prime = [val, theta1_param[1]]  # fixed theta_sigma = sigma_truth\n",
    "\n",
    "        if reweight_analytically:\n",
    "            # analytical reweight\n",
    "            weights = analytical_reweight(events=x_T, param=theta_prime)\n",
    "        else:\n",
    "            # NN (DCTR) reweight\n",
    "            weights = reweight(events=x_T, param=theta_prime)\n",
    "\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_labels * (y_labels - y_pred)**2 + weights * (\n",
    "                1. - y_labels) * (y_labels - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_labels) * K.log(y_pred) + weights *\n",
    "                       (1 - y_labels) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:19:33.913387Z",
     "start_time": "2020-06-09T07:59:32.681860Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : -2.0\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1228 - acc: 0.2903 - val_loss: 0.1219 - val_acc: 0.2893\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1219 - acc: 0.2896 - val_loss: 0.1219 - val_acc: 0.2898\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1219 - acc: 0.2895 - val_loss: 0.1219 - val_acc: 0.2888\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1219 - acc: 0.2896 - val_loss: 0.1219 - val_acc: 0.2878\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1219 - acc: 0.2896 - val_loss: 0.1219 - val_acc: 0.2907\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1219 - acc: 0.2896 - val_loss: 0.1219 - val_acc: 0.2878\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1219 - acc: 0.2895 - val_loss: 0.1219 - val_acc: 0.2903\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1219 - acc: 0.2896 - val_loss: 0.1219 - val_acc: 0.2886\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1219 - acc: 0.2898 - val_loss: 0.1219 - val_acc: 0.2891\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1219 - acc: 0.2898 - val_loss: 0.1219 - val_acc: 0.2887\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1219 - acc: 0.2896 - val_loss: 0.1219 - val_acc: 0.2906\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1219 - acc: 0.2898 - val_loss: 0.1219 - val_acc: 0.2904\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1219 - acc: 0.2898 - val_loss: 0.1219 - val_acc: 0.2886\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1219 - acc: 0.2897 - val_loss: 0.1219 - val_acc: 0.2895\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1219 - acc: 0.2896 - val_loss: 0.1219 - val_acc: 0.2897\n",
      "[0.12189492417871953]\n",
      "testing theta = : -1.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1347 - acc: 0.2946 - val_loss: 0.1347 - val_acc: 0.2953\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1347 - acc: 0.2948 - val_loss: 0.1348 - val_acc: 0.2937\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1347 - acc: 0.2943 - val_loss: 0.1347 - val_acc: 0.2925\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1347 - acc: 0.2945 - val_loss: 0.1347 - val_acc: 0.2949\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1347 - acc: 0.2943 - val_loss: 0.1347 - val_acc: 0.2938\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1347 - acc: 0.2947 - val_loss: 0.1348 - val_acc: 0.2913\n",
      "[0.12189492417871953, 0.13467893448472024]\n",
      "testing theta = : -1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2995 - val_loss: 0.1481 - val_acc: 0.3008\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2995 - val_loss: 0.1481 - val_acc: 0.2989\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2994 - val_loss: 0.1481 - val_acc: 0.2992\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1481 - acc: 0.2994 - val_loss: 0.1481 - val_acc: 0.2972\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2989 - val_loss: 0.1481 - val_acc: 0.2998\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1481 - acc: 0.2994 - val_loss: 0.1481 - val_acc: 0.2988\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2992 - val_loss: 0.1481 - val_acc: 0.2968\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2993 - val_loss: 0.1481 - val_acc: 0.2973\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2992 - val_loss: 0.1481 - val_acc: 0.2984\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2992 - val_loss: 0.1481 - val_acc: 0.2988\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1481 - acc: 0.2993 - val_loss: 0.1481 - val_acc: 0.2984\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034]\n",
      "testing theta = : -1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1623 - acc: 0.3048 - val_loss: 0.1623 - val_acc: 0.3054\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1623 - acc: 0.3049 - val_loss: 0.1623 - val_acc: 0.3051\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1623 - acc: 0.3049 - val_loss: 0.1623 - val_acc: 0.3042\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1623 - acc: 0.3049 - val_loss: 0.1624 - val_acc: 0.3039\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1623 - acc: 0.3046 - val_loss: 0.1625 - val_acc: 0.3050\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1623 - acc: 0.3050 - val_loss: 0.1623 - val_acc: 0.3037\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1623 - acc: 0.3049 - val_loss: 0.1623 - val_acc: 0.3054\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1623 - acc: 0.3052 - val_loss: 0.1623 - val_acc: 0.3043\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073]\n",
      "testing theta = : -1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1766 - acc: 0.3109 - val_loss: 0.1767 - val_acc: 0.3108\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1766 - acc: 0.3110 - val_loss: 0.1767 - val_acc: 0.3124\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1766 - acc: 0.3113 - val_loss: 0.1767 - val_acc: 0.3114\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1766 - acc: 0.3113 - val_loss: 0.1767 - val_acc: 0.3098\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1766 - acc: 0.3110 - val_loss: 0.1767 - val_acc: 0.3121\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1766 - acc: 0.3112 - val_loss: 0.1767 - val_acc: 0.3112\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285]\n",
      "testing theta = : -0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3160 - val_loss: 0.1910 - val_acc: 0.3162\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3159 - val_loss: 0.1910 - val_acc: 0.3156\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3158 - val_loss: 0.1910 - val_acc: 0.3147\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1908 - acc: 0.3158 - val_loss: 0.1910 - val_acc: 0.3171\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3159 - val_loss: 0.1910 - val_acc: 0.3146\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3158 - val_loss: 0.1910 - val_acc: 0.3151\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3156 - val_loss: 0.1910 - val_acc: 0.3146\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3157 - val_loss: 0.1909 - val_acc: 0.3153\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3158 - val_loss: 0.1910 - val_acc: 0.3158\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3159 - val_loss: 0.1910 - val_acc: 0.3139\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1908 - acc: 0.3158 - val_loss: 0.1910 - val_acc: 0.3164\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1908 - acc: 0.3160 - val_loss: 0.1910 - val_acc: 0.3151\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1908 - acc: 0.3158 - val_loss: 0.1910 - val_acc: 0.3170\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631]\n",
      "testing theta = : -0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2043 - acc: 0.3189 - val_loss: 0.2046 - val_acc: 0.3189\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3189 - val_loss: 0.2046 - val_acc: 0.3176\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3188 - val_loss: 0.2046 - val_acc: 0.3189\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3189 - val_loss: 0.2046 - val_acc: 0.3190\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3189 - val_loss: 0.2048 - val_acc: 0.3190\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3189 - val_loss: 0.2046 - val_acc: 0.3194\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3192 - val_loss: 0.2045 - val_acc: 0.3189\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2043 - acc: 0.3190 - val_loss: 0.2046 - val_acc: 0.3181\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3192 - val_loss: 0.2046 - val_acc: 0.3178\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3189 - val_loss: 0.2046 - val_acc: 0.3176\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3189 - val_loss: 0.2046 - val_acc: 0.3191\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.3189 - val_loss: 0.2045 - val_acc: 0.3181\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713]\n",
      "testing theta = : -0.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2165 - acc: 0.3222 - val_loss: 0.2168 - val_acc: 0.3217\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2165 - acc: 0.3222 - val_loss: 0.2168 - val_acc: 0.3224\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2165 - acc: 0.3221 - val_loss: 0.2168 - val_acc: 0.3213\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2165 - acc: 0.3220 - val_loss: 0.2168 - val_acc: 0.3211\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2165 - acc: 0.3218 - val_loss: 0.2168 - val_acc: 0.3206\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2165 - acc: 0.3219 - val_loss: 0.2168 - val_acc: 0.3215\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2165 - acc: 0.3220 - val_loss: 0.2168 - val_acc: 0.3200\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2165 - acc: 0.3220 - val_loss: 0.2168 - val_acc: 0.3204\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286]\n",
      "testing theta = : 0.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2280 - acc: 0.3249 - val_loss: 0.2283 - val_acc: 0.3252\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2280 - acc: 0.3250 - val_loss: 0.2282 - val_acc: 0.3248\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2280 - acc: 0.3249 - val_loss: 0.2283 - val_acc: 0.3244\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2280 - acc: 0.3250 - val_loss: 0.2282 - val_acc: 0.3249\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2280 - acc: 0.3249 - val_loss: 0.2283 - val_acc: 0.3263\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2280 - acc: 0.3248 - val_loss: 0.2284 - val_acc: 0.3234\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2280 - acc: 0.3249 - val_loss: 0.2283 - val_acc: 0.3255\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2280 - acc: 0.3250 - val_loss: 0.2283 - val_acc: 0.3255\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2280 - acc: 0.3251 - val_loss: 0.2282 - val_acc: 0.3260\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035]\n",
      "testing theta = : 0.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2373 - acc: 0.3270 - val_loss: 0.2377 - val_acc: 0.3273\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2373 - acc: 0.3270 - val_loss: 0.2376 - val_acc: 0.3269\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2373 - acc: 0.3272 - val_loss: 0.2377 - val_acc: 0.3264\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2373 - acc: 0.3270 - val_loss: 0.2376 - val_acc: 0.3274\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2373 - acc: 0.3271 - val_loss: 0.2375 - val_acc: 0.3264\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2373 - acc: 0.3270 - val_loss: 0.2375 - val_acc: 0.3276\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2373 - acc: 0.3272 - val_loss: 0.2375 - val_acc: 0.3267\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2372 - acc: 0.3273 - val_loss: 0.2376 - val_acc: 0.3272\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2373 - acc: 0.3273 - val_loss: 0.2376 - val_acc: 0.3281\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2373 - acc: 0.3272 - val_loss: 0.2376 - val_acc: 0.3264\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2373 - acc: 0.3272 - val_loss: 0.2376 - val_acc: 0.3261\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2372 - acc: 0.3273 - val_loss: 0.2379 - val_acc: 0.3262\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503]\n",
      "testing theta = : 0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2442 - acc: 0.3284 - val_loss: 0.2444 - val_acc: 0.3287\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.3281 - val_loss: 0.2444 - val_acc: 0.3266\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.3277 - val_loss: 0.2445 - val_acc: 0.3261\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3279 - val_loss: 0.2444 - val_acc: 0.3279\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3277 - val_loss: 0.2445 - val_acc: 0.3272\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.3279 - val_loss: 0.2445 - val_acc: 0.3272\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503, 0.2441234815120697]\n",
      "testing theta = : 0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.3254 - val_loss: 0.2486 - val_acc: 0.3284\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3262 - val_loss: 0.2493 - val_acc: 0.3014\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3248 - val_loss: 0.2487 - val_acc: 0.3287\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.3259 - val_loss: 0.2490 - val_acc: 0.3283\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2486 - acc: 0.3266 - val_loss: 0.2488 - val_acc: 0.3263\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.3276 - val_loss: 0.2487 - val_acc: 0.3280\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503, 0.2441234815120697, 0.24851155441999434]\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2509 - acc: 0.2348 - val_loss: 0.2499 - val_acc: 0.2559\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2509 - acc: 0.2131 - val_loss: 0.2502 - val_acc: 0.2516\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2499 - acc: 0.2470 - val_loss: 0.2501 - val_acc: 0.2097\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.2401 - val_loss: 0.2504 - val_acc: 0.3038\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2504 - acc: 0.2631 - val_loss: 0.2499 - val_acc: 0.2585\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2512 - acc: 0.2473 - val_loss: 0.2501 - val_acc: 0.2498\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503, 0.2441234815120697, 0.24851155441999434, 0.249922381952405]\n",
      "testing theta = : 1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.1787 - val_loss: 0.2488 - val_acc: 0.1748\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.1737 - val_loss: 0.2488 - val_acc: 0.1755\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.1739 - val_loss: 0.2488 - val_acc: 0.1733\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.1741 - val_loss: 0.2490 - val_acc: 0.1787\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.1743 - val_loss: 0.2488 - val_acc: 0.1750\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2487 - acc: 0.1739 - val_loss: 0.2487 - val_acc: 0.1752\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.1761 - val_loss: 0.2487 - val_acc: 0.1784\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.1749 - val_loss: 0.2487 - val_acc: 0.1778\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.1754 - val_loss: 0.2488 - val_acc: 0.1735\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.1736 - val_loss: 0.2489 - val_acc: 0.1792\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.1762 - val_loss: 0.2488 - val_acc: 0.1756\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.1758 - val_loss: 0.2486 - val_acc: 0.1764\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.1790 - val_loss: 0.2484 - val_acc: 0.1788\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2494 - acc: 0.1804 - val_loss: 0.2490 - val_acc: 0.1724\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.1774 - val_loss: 0.2488 - val_acc: 0.1773\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.1735 - val_loss: 0.2488 - val_acc: 0.1749\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.1770 - val_loss: 0.2487 - val_acc: 0.1740\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.1731 - val_loss: 0.2488 - val_acc: 0.1757\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503, 0.2441234815120697, 0.24851155441999434, 0.249922381952405, 0.24865917272865773]\n",
      "testing theta = : 1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2467 - acc: 0.1792 - val_loss: 0.2438 - val_acc: 0.1776\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2464 - acc: 0.1755 - val_loss: 0.2440 - val_acc: 0.1762\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.1750 - val_loss: 0.2442 - val_acc: 0.1754\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.1744 - val_loss: 0.2442 - val_acc: 0.1737\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2448 - acc: 0.1746 - val_loss: 0.2445 - val_acc: 0.1735\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2450 - acc: 0.1751 - val_loss: 0.2445 - val_acc: 0.1812\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503, 0.2441234815120697, 0.24851155441999434, 0.249922381952405, 0.24865917272865773, 0.24465334551036358]\n",
      "testing theta = : 1.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2458 - acc: 0.1791 - val_loss: 0.2367 - val_acc: 0.1749\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2381 - acc: 0.1771 - val_loss: 0.2373 - val_acc: 0.1741\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2371 - acc: 0.1766 - val_loss: 0.2372 - val_acc: 0.1735\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2372 - acc: 0.1762 - val_loss: 0.2372 - val_acc: 0.1771\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2373 - acc: 0.1764 - val_loss: 0.2372 - val_acc: 0.1768\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2374 - acc: 0.1766 - val_loss: 0.2372 - val_acc: 0.1755\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503, 0.2441234815120697, 0.24851155441999434, 0.249922381952405, 0.24865917272865773, 0.24465334551036358, 0.2370636328756809]\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2396 - acc: 0.1786 - val_loss: 0.2276 - val_acc: 0.1774\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2289 - acc: 0.1782 - val_loss: 0.2282 - val_acc: 0.1819\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2281 - acc: 0.1784 - val_loss: 0.2279 - val_acc: 0.1776\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2280 - acc: 0.1785 - val_loss: 0.2279 - val_acc: 0.1785\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2285 - acc: 0.1786 - val_loss: 0.2283 - val_acc: 0.1820\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2280 - acc: 0.1784 - val_loss: 0.2280 - val_acc: 0.1766\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503, 0.2441234815120697, 0.24851155441999434, 0.249922381952405, 0.24865917272865773, 0.24465334551036358, 0.2370636328756809, 0.22796498994529246]\n",
      "[0.12189492417871953, 0.13467893448472024, 0.14806123162806034, 0.1622769716978073, 0.17656221951544285, 0.1907737924903631, 0.20428884841501713, 0.21648369516432286, 0.22800474075973035, 0.23723593820631503, 0.2441234815120697, 0.24851155441999434, 0.249922381952405, 0.24865917272865773, 0.24465334551036358, 0.2370636328756809, 0.22796498994529246]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2, 2, 17)\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "earlystopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=my_loss_wrapper(theta),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train),\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              batch_size=1000,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1,\n",
    "              callbacks=[earlystopping])\n",
    "    lvals += [np.min(model.history.history['loss'])]\n",
    "    vlvals += [np.min(model.history.history['val_loss'])]\n",
    "    print(lvals)\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:19:34.296888Z",
     "start_time": "2020-06-09T08:19:33.918455Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.title(\"$\\mu$ Cross Section\\nFixed $\\sigma = \\sigma_{Truth}$\")\n",
    "plt.xlabel(r'$\\theta_{\\mu}$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param[0],\n",
    "           ymin=np.min(lvals),\n",
    "           ymax=np.max(lvals),\n",
    "           label='Truth')\n",
    "plt.legend()\n",
    "#plt.savefig(\"GaussianAltFit-2D-DetectorEffects-\\mu cross section.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Section for $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:19:34.390761Z",
     "start_time": "2020-06-09T08:19:34.301768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1, ), dtype=tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "def my_loss_wrapper(val=0., reweight_analytically=False, MSE_loss=True):\n",
    "    def my_loss(y_true, y_pred):\n",
    "        y_true = tf.gather(y_true, np.arange(batch_size))\n",
    "        y_labels = tf.gather(y_true, [0], axis=1)  #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis=1)  # sim truth for reweighting\n",
    "\n",
    "        theta_prime = [theta1_param[0], val]  # fixed theta_sigma = sigma_truth\n",
    "\n",
    "        if reweight_analytically:\n",
    "            # analytical reweight\n",
    "            weights = analytical_reweight(events=x_T, param=theta_prime)\n",
    "        else:\n",
    "            # NN (DCTR) reweight\n",
    "            weights = reweight(events=x_T, param=theta_prime)\n",
    "\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = y_labels * (y_labels - y_pred)**2 + weights * (\n",
    "                1. - y_labels) * (y_labels - y_pred)**2\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            # Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -((y_labels) * K.log(y_pred) + weights *\n",
    "                       (1 - y_labels) * K.log(1 - y_pred))\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:39:00.881481Z",
     "start_time": "2020-06-09T08:19:34.395576Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : 0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2015 - acc: 0.2491 - val_loss: 0.2000 - val_acc: 0.2482\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1998 - acc: 0.2471 - val_loss: 0.2000 - val_acc: 0.2486\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1998 - acc: 0.2473 - val_loss: 0.2001 - val_acc: 0.2461\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1998 - acc: 0.2472 - val_loss: 0.2007 - val_acc: 0.2421\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1997 - acc: 0.2471 - val_loss: 0.2002 - val_acc: 0.2487\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1998 - acc: 0.2470 - val_loss: 0.2001 - val_acc: 0.2490\n",
      "testing theta = : 0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2231 - acc: 0.2461 - val_loss: 0.2233 - val_acc: 0.2460\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2230 - acc: 0.2460 - val_loss: 0.2231 - val_acc: 0.2460\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2230 - acc: 0.2465 - val_loss: 0.2230 - val_acc: 0.2473\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2230 - acc: 0.2462 - val_loss: 0.2232 - val_acc: 0.2431\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2230 - acc: 0.2462 - val_loss: 0.2233 - val_acc: 0.2487\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2230 - acc: 0.2465 - val_loss: 0.2231 - val_acc: 0.2454\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2229 - acc: 0.2461 - val_loss: 0.2231 - val_acc: 0.2469\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2230 - acc: 0.2463 - val_loss: 0.2231 - val_acc: 0.2483\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2385 - acc: 0.2454 - val_loss: 0.2387 - val_acc: 0.2439\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2384 - acc: 0.2455 - val_loss: 0.2387 - val_acc: 0.2433\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2384 - acc: 0.2451 - val_loss: 0.2390 - val_acc: 0.2500\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2384 - acc: 0.2449 - val_loss: 0.2387 - val_acc: 0.2428\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2456 - val_loss: 0.2387 - val_acc: 0.2423\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2450 - val_loss: 0.2387 - val_acc: 0.2431\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2440 - val_loss: 0.2387 - val_acc: 0.2469\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2451 - val_loss: 0.2387 - val_acc: 0.2437\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2384 - acc: 0.2448 - val_loss: 0.2387 - val_acc: 0.2461\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2384 - acc: 0.2448 - val_loss: 0.2386 - val_acc: 0.2436\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2451 - val_loss: 0.2388 - val_acc: 0.2475\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2452 - val_loss: 0.2387 - val_acc: 0.2455\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2452 - val_loss: 0.2388 - val_acc: 0.2482\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2451 - val_loss: 0.2387 - val_acc: 0.2445\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2383 - acc: 0.2456 - val_loss: 0.2387 - val_acc: 0.2410\n",
      "testing theta = : 1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2475 - acc: 0.2430 - val_loss: 0.2480 - val_acc: 0.2540\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2401 - val_loss: 0.2477 - val_acc: 0.2421\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.2416 - val_loss: 0.2479 - val_acc: 0.2325\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2475 - acc: 0.2410 - val_loss: 0.2477 - val_acc: 0.2466\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.2416 - val_loss: 0.2483 - val_acc: 0.2274\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2413 - val_loss: 0.2477 - val_acc: 0.2470\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2415 - val_loss: 0.2477 - val_acc: 0.2453\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2424 - val_loss: 0.2477 - val_acc: 0.2413\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2406 - val_loss: 0.2477 - val_acc: 0.2456\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.2430 - val_loss: 0.2477 - val_acc: 0.2440\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.2406 - val_loss: 0.2477 - val_acc: 0.2382\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.2402 - val_loss: 0.2476 - val_acc: 0.2428\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2412 - val_loss: 0.2476 - val_acc: 0.2411\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2474 - acc: 0.2399 - val_loss: 0.2477 - val_acc: 0.2387\n",
      "Epoch 15/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2412 - val_loss: 0.2478 - val_acc: 0.2318\n",
      "Epoch 16/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2407 - val_loss: 0.2476 - val_acc: 0.2430\n",
      "Epoch 17/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2404 - val_loss: 0.2477 - val_acc: 0.2465\n",
      "Epoch 18/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2474 - acc: 0.2409 - val_loss: 0.2479 - val_acc: 0.2516\n",
      "Epoch 19/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.2456 - val_loss: 0.2477 - val_acc: 0.2437\n",
      "Epoch 20/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.2417 - val_loss: 0.2478 - val_acc: 0.2358\n",
      "Epoch 21/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2473 - acc: 0.2430 - val_loss: 0.2478 - val_acc: 0.2492\n",
      "testing theta = : 1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2510 - acc: 0.2472 - val_loss: 0.2500 - val_acc: 0.2329\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2503 - acc: 0.2538 - val_loss: 0.2503 - val_acc: 0.2921\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2508 - acc: 0.2553 - val_loss: 0.2500 - val_acc: 0.2436\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2592 - val_loss: 0.2500 - val_acc: 0.2523\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.2525 - val_loss: 0.2501 - val_acc: 0.2709\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2503 - acc: 0.2422 - val_loss: 0.2501 - val_acc: 0.2239\n",
      "testing theta = : 1.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2483 - acc: 0.2657 - val_loss: 0.2470 - val_acc: 0.2594\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.2523 - val_loss: 0.2469 - val_acc: 0.2365\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2471 - acc: 0.2467 - val_loss: 0.2470 - val_acc: 0.2638\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2481 - acc: 0.2612 - val_loss: 0.2468 - val_acc: 0.2370\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2475 - acc: 0.2562 - val_loss: 0.2468 - val_acc: 0.2414\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.2511 - val_loss: 0.2466 - val_acc: 0.2503\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.2566 - val_loss: 0.2469 - val_acc: 0.2311\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.2441 - val_loss: 0.2474 - val_acc: 0.2863\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2485 - acc: 0.2457 - val_loss: 0.2468 - val_acc: 0.2379\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.2512 - val_loss: 0.2473 - val_acc: 0.2230\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2490 - acc: 0.2462 - val_loss: 0.2468 - val_acc: 0.2553\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2493 - acc: 0.2508 - val_loss: 0.2413 - val_acc: 0.2396\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2437 - acc: 0.2429 - val_loss: 0.2415 - val_acc: 0.2437\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2426 - acc: 0.2480 - val_loss: 0.2415 - val_acc: 0.2363\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2426 - acc: 0.2428 - val_loss: 0.2416 - val_acc: 0.2387\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2424 - acc: 0.2406 - val_loss: 0.2416 - val_acc: 0.2419\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2423 - acc: 0.2462 - val_loss: 0.2417 - val_acc: 0.2431\n",
      "testing theta = : 2.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2404 - acc: 0.2496 - val_loss: 0.2341 - val_acc: 0.2524\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2367 - acc: 0.2506 - val_loss: 0.2342 - val_acc: 0.2384\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2356 - acc: 0.2433 - val_loss: 0.2343 - val_acc: 0.2488\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2363 - acc: 0.2464 - val_loss: 0.2344 - val_acc: 0.2341\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2368 - acc: 0.2455 - val_loss: 0.2344 - val_acc: 0.2380\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2351 - acc: 0.2452 - val_loss: 0.2364 - val_acc: 0.2756\n",
      "testing theta = : 2.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2333 - acc: 0.2531 - val_loss: 0.2264 - val_acc: 0.2497\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2290 - acc: 0.2462 - val_loss: 0.2263 - val_acc: 0.2353\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2281 - acc: 0.2434 - val_loss: 0.2262 - val_acc: 0.2431\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2279 - acc: 0.2436 - val_loss: 0.2264 - val_acc: 0.2476\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2275 - acc: 0.2466 - val_loss: 0.2270 - val_acc: 0.2517\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2287 - acc: 0.2474 - val_loss: 0.2262 - val_acc: 0.2418\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2302 - acc: 0.2453 - val_loss: 0.2265 - val_acc: 0.2495\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2277 - acc: 0.2463 - val_loss: 0.2264 - val_acc: 0.2489\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2308 - acc: 0.2476 - val_loss: 0.2262 - val_acc: 0.2409\n",
      "Epoch 10/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2283 - acc: 0.2449 - val_loss: 0.2264 - val_acc: 0.2501\n",
      "Epoch 11/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2275 - acc: 0.2448 - val_loss: 0.2265 - val_acc: 0.2516\n",
      "Epoch 12/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2275 - acc: 0.2494 - val_loss: 0.2265 - val_acc: 0.2494\n",
      "Epoch 13/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2272 - acc: 0.2500 - val_loss: 0.2266 - val_acc: 0.2549\n",
      "Epoch 14/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2325 - acc: 0.2457 - val_loss: 0.2269 - val_acc: 0.2517\n",
      "testing theta = : 2.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2307 - acc: 0.2499 - val_loss: 0.2182 - val_acc: 0.2444\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2203 - acc: 0.2474 - val_loss: 0.2182 - val_acc: 0.2483\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2198 - acc: 0.2450 - val_loss: 0.2181 - val_acc: 0.2409\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2195 - acc: 0.2503 - val_loss: 0.2184 - val_acc: 0.2402\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2196 - acc: 0.2460 - val_loss: 0.2186 - val_acc: 0.2504\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2196 - acc: 0.2468 - val_loss: 0.2182 - val_acc: 0.2437\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2194 - acc: 0.2437 - val_loss: 0.2181 - val_acc: 0.2471\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2199 - acc: 0.2470 - val_loss: 0.2183 - val_acc: 0.2429\n",
      "testing theta = : 3.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2290 - acc: 0.2507 - val_loss: 0.2105 - val_acc: 0.2370\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2135 - acc: 0.2458 - val_loss: 0.2108 - val_acc: 0.2495\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2130 - acc: 0.2490 - val_loss: 0.2107 - val_acc: 0.2424\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2128 - acc: 0.2443 - val_loss: 0.2106 - val_acc: 0.2447\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.2463 - val_loss: 0.2108 - val_acc: 0.2491\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2125 - acc: 0.2471 - val_loss: 0.2107 - val_acc: 0.2462\n",
      "testing theta = : 3.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2206 - acc: 0.2481 - val_loss: 0.2035 - val_acc: 0.2373\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2098 - acc: 0.2470 - val_loss: 0.2037 - val_acc: 0.2485\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2057 - acc: 0.2488 - val_loss: 0.2041 - val_acc: 0.2386\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2057 - acc: 0.2455 - val_loss: 0.2039 - val_acc: 0.2511\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2061 - acc: 0.2471 - val_loss: 0.2036 - val_acc: 0.2431\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2056 - acc: 0.2470 - val_loss: 0.2038 - val_acc: 0.2498\n",
      "testing theta = : 3.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2084 - acc: 0.2511 - val_loss: 0.1967 - val_acc: 0.2465\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1991 - acc: 0.2479 - val_loss: 0.1967 - val_acc: 0.2477\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1985 - acc: 0.2482 - val_loss: 0.1974 - val_acc: 0.2571\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1989 - acc: 0.2511 - val_loss: 0.1975 - val_acc: 0.2386\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1987 - acc: 0.2472 - val_loss: 0.1971 - val_acc: 0.2550\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1985 - acc: 0.2470 - val_loss: 0.1970 - val_acc: 0.2506\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1989 - acc: 0.2477 - val_loss: 0.1968 - val_acc: 0.2502\n",
      "testing theta = : 3.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2106 - acc: 0.2518 - val_loss: 0.1902 - val_acc: 0.2487\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1933 - acc: 0.2504 - val_loss: 0.1907 - val_acc: 0.2558\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1923 - acc: 0.2498 - val_loss: 0.1903 - val_acc: 0.2471\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1943 - acc: 0.2510 - val_loss: 0.1901 - val_acc: 0.2395\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1973 - acc: 0.2483 - val_loss: 0.1908 - val_acc: 0.2543\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1922 - acc: 0.2502 - val_loss: 0.1913 - val_acc: 0.2379\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1916 - acc: 0.2465 - val_loss: 0.1906 - val_acc: 0.2502\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1913 - acc: 0.2486 - val_loss: 0.1908 - val_acc: 0.2512\n",
      "Epoch 9/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1919 - acc: 0.2478 - val_loss: 0.1904 - val_acc: 0.2463\n",
      "testing theta = : 4.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.1911 - acc: 0.2528 - val_loss: 0.1853 - val_acc: 0.2560\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1864 - acc: 0.2499 - val_loss: 0.1845 - val_acc: 0.2469\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1867 - acc: 0.2469 - val_loss: 0.1845 - val_acc: 0.2449\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1857 - acc: 0.2481 - val_loss: 0.1848 - val_acc: 0.2506\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1856 - acc: 0.2492 - val_loss: 0.1846 - val_acc: 0.2469\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1854 - acc: 0.2491 - val_loss: 0.1850 - val_acc: 0.2531\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1865 - acc: 0.2507 - val_loss: 0.1851 - val_acc: 0.2393\n",
      "testing theta = : 4.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.1951 - acc: 0.2533 - val_loss: 0.1788 - val_acc: 0.2479\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1795 - acc: 0.2488 - val_loss: 0.1791 - val_acc: 0.2502\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1810 - acc: 0.2495 - val_loss: 0.1791 - val_acc: 0.2446\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1802 - acc: 0.2475 - val_loss: 0.1802 - val_acc: 0.2379\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1794 - acc: 0.2486 - val_loss: 0.1800 - val_acc: 0.2432\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1798 - acc: 0.2487 - val_loss: 0.1802 - val_acc: 0.2389\n",
      "testing theta = : 4.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.1751 - acc: 0.2517 - val_loss: 0.1734 - val_acc: 0.2515\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1872 - acc: 0.2519 - val_loss: 0.1738 - val_acc: 0.2489\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1746 - acc: 0.2507 - val_loss: 0.1735 - val_acc: 0.2489\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1742 - acc: 0.2486 - val_loss: 0.1737 - val_acc: 0.2514\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1742 - acc: 0.2495 - val_loss: 0.1737 - val_acc: 0.2438\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1736 - acc: 0.2485 - val_loss: 0.1734 - val_acc: 0.2504\n",
      "[0.19974756653606893, 0.22293790447711945, 0.23830069687962532, 0.24727838741242886, 0.2502615033239126, 0.24714878958463668, 0.24231680795550348, 0.2351053201109171, 0.22718603298068046, 0.21943474316596984, 0.212517861276865, 0.20557716916501523, 0.1985436647683382, 0.19130294790863991, 0.18538538125157356, 0.17936726856231688, 0.17363087618350984]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.5, 4.5, 17)\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "earlystopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=my_loss_wrapper(theta),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train),\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1,\n",
    "              callbacks=[earlystopping])\n",
    "    lvals += [np.min(model.history.history['loss'])]\n",
    "    vlvals += [np.min(model.history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:39:01.251594Z",
     "start_time": "2020-06-09T08:39:00.886294Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas, lvals, label='lvals')\n",
    "plt.plot(thetas, vlvals, label='vlvals')\n",
    "plt.title(\"$\\sigma$ Cross Section\\nFixed $\\mu = \\mu{Truth}$\")\n",
    "plt.xlabel(r'$\\theta_{\\sigma}$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param[1],\n",
    "           ymin=np.min(lvals),\n",
    "           ymax=np.max(lvals),\n",
    "           label='Truth')\n",
    "plt.legend()\n",
    "#plt.savefig(\"GaussianAltFit-2D-DetectorEffects-\\sigma cross section.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T07:42:17.487848Z",
     "start_time": "2020-06-08T07:40:35.979Z"
    }
   },
   "source": [
    "We've shown for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:39:01.268957Z",
     "start_time": "2020-06-09T08:39:01.256475Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\n",
    "    \". theta fit = \", model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = [0., 1.]\n",
    "\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(\n",
    "    on_epoch_end=lambda batch, logs: fit_vals.append(model_fit.layers[-1].\n",
    "                                                     get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T08:39:01.429440Z",
     "start_time": "2020-06-09T08:39:01.273682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 16,899\n",
      "Trainable params: 16,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1, ))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers) - 1].add_weight(\n",
    "    name=\"thetaX\",\n",
    "    shape=(2, ),\n",
    "    initializer=keras.initializers.Constant(value=theta_fit_init),\n",
    "    trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "batch_size = 2 * N\n",
    "iterations = 50\n",
    "\n",
    "# optimizer will be refined as fit progresses for better precision\n",
    "lr_initial = 5e-1\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])\n",
    "\n",
    "\n",
    "def my_loss_wrapper_fit(\n",
    "        mysign=1,  # -1 for training theta, +1 for training g\n",
    "        reweight_analytically=False,\n",
    "        MSE_loss=True):\n",
    "    theta = 0.  #starting value\n",
    "\n",
    "    #Getting theta_prime:\n",
    "\n",
    "    def my_loss(y_true, y_pred):\n",
    "\n",
    "        # Getting theta_prime:\n",
    "        if mysign == 1:\n",
    "            # regular batch size\n",
    "            y_true = tf.gather(y_true, np.arange(1000))\n",
    "            # when not training theta, fetch as np array\n",
    "            theta_prime = model_fit.layers[-1].get_weights()[0]\n",
    "        else:\n",
    "            # special theta batch size\n",
    "            y_true = tf.gather(y_true, np.arange(batch_size))\n",
    "            # when training theta, fetch as tf.Variable\n",
    "            theta_prime = model_fit.trainable_weights[-1]\n",
    "\n",
    "        y_labels = tf.gather(y_true, [0], axis=1)  #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis=1)  # sim truth for reweighting\n",
    "\n",
    "        if reweight_analytically:\n",
    "            # analytical reweight\n",
    "            weights = analytical_reweight(x_T, theta_prime)\n",
    "        else:\n",
    "            # NN reweight\n",
    "            weights = reweight(x_T, theta_prime)\n",
    "\n",
    "        if MSE_loss:\n",
    "            # Mean Squared Loss\n",
    "            t_loss = mysign * (y_labels * (y_labels - y_pred)**2 + weights *\n",
    "                               (1. - y_labels) * (y_labels - y_pred)**2)\n",
    "        else:\n",
    "            # Categorical Cross-Entropy Loss\n",
    "            #Clip the prediction value to prevent NaN's and Inf's\n",
    "            epsilon = K.epsilon()\n",
    "            y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "            t_loss = -mysign * ((y_labels) * K.log(y_pred) + weights *\n",
    "                                (1 - y_labels) * K.log(1 - y_pred))\n",
    "\n",
    "        return K.mean(t_loss)\n",
    "\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T10:37:50.942347Z",
     "start_time": "2020-06-09T08:39:01.434321Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2174 - acc: 0.3248 - val_loss: 0.2139 - val_acc: 0.3287\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2136 - acc: 0.3291 - val_loss: 0.2136 - val_acc: 0.3290\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2135 - acc: 0.3292 - val_loss: 0.2135 - val_acc: 0.3292\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2134 - acc: 0.3293 - val_loss: 0.2135 - val_acc: 0.3292\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2134 - acc: 0.3292 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2134 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3292\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2134 - acc: 0.3292 - val_loss: 0.2134 - val_acc: 0.3292\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2134 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3292\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2134 - acc: 0.3292 - val_loss: 0.2134 - val_acc: 0.3292\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2134 - acc: 0.3292 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2134 - acc: 0.3292 - val_loss: 0.2134 - val_acc: 0.3292\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3292 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3291\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3292 - val_loss: 0.2134 - val_acc: 0.3292\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2133 - acc: 0.3293 - val_loss: 0.2134 - val_acc: 0.3292\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.2134 - acc: 0.3292\n",
      ". theta fit =  [0.49996558 1.4999661 ]\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2454 - acc: 0.3270 - val_loss: 0.2446 - val_acc: 0.3268\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.3279 - val_loss: 0.2444 - val_acc: 0.3287\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.3269 - val_loss: 0.2444 - val_acc: 0.3274\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.3277 - val_loss: 0.2444 - val_acc: 0.3274\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3276 - val_loss: 0.2444 - val_acc: 0.3280\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3276 - val_loss: 0.2444 - val_acc: 0.3282\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3275 - val_loss: 0.2444 - val_acc: 0.3274\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3274 - val_loss: 0.2444 - val_acc: 0.3278\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.3281 - val_loss: 0.2450 - val_acc: 0.3282\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2442 - acc: 0.3278 - val_loss: 0.2444 - val_acc: 0.3274\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3280 - val_loss: 0.2444 - val_acc: 0.3279\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3279 - val_loss: 0.2446 - val_acc: 0.3271\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2441 - acc: 0.3280 - val_loss: 0.2444 - val_acc: 0.3265\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: -0.2442 - acc: 0.3276\n",
      ". theta fit =  [0.87197995 1.8673968 ]\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2465 - acc: 0.2666 - val_loss: 0.2445 - val_acc: 0.2850\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2461 - acc: 0.2545 - val_loss: 0.2445 - val_acc: 0.2604\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2460 - acc: 0.2524 - val_loss: 0.2445 - val_acc: 0.2731\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2461 - acc: 0.2588 - val_loss: 0.2445 - val_acc: 0.2566\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2459 - acc: 0.2555 - val_loss: 0.2446 - val_acc: 0.2562\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2455 - acc: 0.2560 - val_loss: 0.2450 - val_acc: 0.2826\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2453 - acc: 0.2693 - val_loss: 0.2452 - val_acc: 0.2334\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2457 - acc: 0.2573 - val_loss: 0.2456 - val_acc: 0.2306\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2453 - acc: 0.2632 - val_loss: 0.2464 - val_acc: 0.2326\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2458 - acc: 0.2570 - val_loss: 0.2479 - val_acc: 0.2231\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2453 - acc: 0.2539 - val_loss: 0.2447 - val_acc: 0.2697\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2454 - acc: 0.2570 - val_loss: 0.2449 - val_acc: 0.2786\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: -0.2448 - acc: 0.2608\n",
      ". theta fit =  [1.1911956 1.5480466]\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2505 - acc: 0.1896 - val_loss: 0.2490 - val_acc: 0.1864\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.1866 - val_loss: 0.2504 - val_acc: 0.1715\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2506 - acc: 0.1812 - val_loss: 0.2493 - val_acc: 0.1811\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.1830 - val_loss: 0.2492 - val_acc: 0.1814\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.1811 - val_loss: 0.2492 - val_acc: 0.1830\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.1835 - val_loss: 0.2491 - val_acc: 0.1757\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.1813 - val_loss: 0.2519 - val_acc: 0.1721\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.1825 - val_loss: 0.2491 - val_acc: 0.1760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.1846 - val_loss: 0.2492 - val_acc: 0.1801\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.1828 - val_loss: 0.2491 - val_acc: 0.1774\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.1828 - val_loss: 0.2499 - val_acc: 0.1725\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 5us/step - loss: -0.2492 - acc: 0.1866\n",
      ". theta fit =  [0.90074944 1.2576199 ]\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2476 - acc: 0.2655 - val_loss: 0.2476 - val_acc: 0.2642\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2679 - val_loss: 0.2477 - val_acc: 0.2727\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2675 - val_loss: 0.2477 - val_acc: 0.2691\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2691 - val_loss: 0.2476 - val_acc: 0.2655\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2683 - val_loss: 0.2476 - val_acc: 0.2754\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2681 - val_loss: 0.2477 - val_acc: 0.2645\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2701 - val_loss: 0.2476 - val_acc: 0.2647\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2688 - val_loss: 0.2477 - val_acc: 0.2727\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2689 - val_loss: 0.2476 - val_acc: 0.2733\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.2691 - val_loss: 0.2477 - val_acc: 0.2752\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2697 - val_loss: 0.2476 - val_acc: 0.2732\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2683 - val_loss: 0.2480 - val_acc: 0.2878\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2698 - val_loss: 0.2476 - val_acc: 0.2612\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.2692 - val_loss: 0.2478 - val_acc: 0.2529\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2704 - val_loss: 0.2476 - val_acc: 0.2677\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.2691 - val_loss: 0.2478 - val_acc: 0.2782\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.2693 - val_loss: 0.2477 - val_acc: 0.2767\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2693 - val_loss: 0.2476 - val_acc: 0.2648\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2686 - val_loss: 0.2476 - val_acc: 0.2682\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2472 - acc: 0.2696 - val_loss: 0.2477 - val_acc: 0.2804\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2475 - acc: 0.2802\n",
      ". theta fit =  [1.1733774 1.5303313]\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2512 - acc: 0.1913 - val_loss: 0.2491 - val_acc: 0.1769\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2504 - acc: 0.1792 - val_loss: 0.2492 - val_acc: 0.1837\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1826 - val_loss: 0.2492 - val_acc: 0.1775\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.1818 - val_loss: 0.2495 - val_acc: 0.1741\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.1792 - val_loss: 0.2493 - val_acc: 0.1786\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1860 - val_loss: 0.2511 - val_acc: 0.1727\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1789 - val_loss: 0.2493 - val_acc: 0.1774\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1809 - val_loss: 0.2500 - val_acc: 0.1718\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1800 - val_loss: 0.2494 - val_acc: 0.1753\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1782 - val_loss: 0.2496 - val_acc: 0.1774\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.1813 - val_loss: 0.2494 - val_acc: 0.1792\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2495 - acc: 0.1769\n",
      ". theta fit =  [0.9124286 1.2697463]\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2477 - acc: 0.2606 - val_loss: 0.2480 - val_acc: 0.2535\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2659 - val_loss: 0.2480 - val_acc: 0.2731\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2671 - val_loss: 0.2479 - val_acc: 0.2690\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2667 - val_loss: 0.2480 - val_acc: 0.2767\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2476 - acc: 0.2665 - val_loss: 0.2480 - val_acc: 0.2824\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2679 - val_loss: 0.2479 - val_acc: 0.2594\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2669 - val_loss: 0.2479 - val_acc: 0.2670\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2667 - val_loss: 0.2479 - val_acc: 0.2681\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2675 - val_loss: 0.2481 - val_acc: 0.2753\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2659 - val_loss: 0.2479 - val_acc: 0.2681\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2682 - val_loss: 0.2479 - val_acc: 0.2591\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2654 - val_loss: 0.2479 - val_acc: 0.2727\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2683 - val_loss: 0.2479 - val_acc: 0.2681\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2673 - val_loss: 0.2481 - val_acc: 0.2739\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2671 - val_loss: 0.2481 - val_acc: 0.2739\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2675 - val_loss: 0.2479 - val_acc: 0.2643\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2475 - acc: 0.2655 - val_loss: 0.2480 - val_acc: 0.2788\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2475 - acc: 0.2680 - val_loss: 0.2479 - val_acc: 0.2720\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2476 - acc: 0.2679 - val_loss: 0.2480 - val_acc: 0.2719\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2475 - acc: 0.2672 - val_loss: 0.2479 - val_acc: 0.2644\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2477 - acc: 0.2641\n",
      ". theta fit =  [1.1653397 1.5228986]\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2512 - acc: 0.1925 - val_loss: 0.2492 - val_acc: 0.1790\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.1813 - val_loss: 0.2510 - val_acc: 0.1755\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.1795 - val_loss: 0.2494 - val_acc: 0.1823\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.1791 - val_loss: 0.2494 - val_acc: 0.1852\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.1799 - val_loss: 0.2494 - val_acc: 0.1751\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.1794 - val_loss: 0.2495 - val_acc: 0.1920\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1787 - val_loss: 0.2496 - val_acc: 0.1945\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.1820 - val_loss: 0.2494 - val_acc: 0.1773\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.1799 - val_loss: 0.2499 - val_acc: 0.1714\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1783 - val_loss: 0.2495 - val_acc: 0.1743\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.1798 - val_loss: 0.2495 - val_acc: 0.1742\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2495 - acc: 0.1790\n",
      ". theta fit =  [0.9175997 1.7701093]\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2489 - acc: 0.2497 - val_loss: 0.2459 - val_acc: 0.2552\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2478 - acc: 0.2664 - val_loss: 0.2475 - val_acc: 0.2230\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2473 - acc: 0.2486 - val_loss: 0.2465 - val_acc: 0.2376\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2467 - acc: 0.2515 - val_loss: 0.2463 - val_acc: 0.2772\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2470 - acc: 0.2660 - val_loss: 0.2465 - val_acc: 0.2413\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2469 - acc: 0.2535 - val_loss: 0.2464 - val_acc: 0.2605\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2467 - acc: 0.2571 - val_loss: 0.2464 - val_acc: 0.2732\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2466 - acc: 0.2572 - val_loss: 0.2466 - val_acc: 0.2804\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.2643 - val_loss: 0.2464 - val_acc: 0.2661\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2467 - acc: 0.2662 - val_loss: 0.2468 - val_acc: 0.2331\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2468 - acc: 0.2491 - val_loss: 0.2463 - val_acc: 0.2638\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2467 - acc: 0.2555\n",
      ". theta fit =  [1.1617827 1.525793 ]\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2507 - acc: 0.1885 - val_loss: 0.2491 - val_acc: 0.1796\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2507 - acc: 0.1808 - val_loss: 0.2492 - val_acc: 0.1798\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1825 - val_loss: 0.2493 - val_acc: 0.1842\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1810 - val_loss: 0.2494 - val_acc: 0.1812\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.1826 - val_loss: 0.2493 - val_acc: 0.1771\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.1785 - val_loss: 0.2495 - val_acc: 0.1877\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.1783 - val_loss: 0.2494 - val_acc: 0.1825\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.1807 - val_loss: 0.2494 - val_acc: 0.1811\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.1770 - val_loss: 0.2494 - val_acc: 0.1791\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.1806 - val_loss: 0.2494 - val_acc: 0.1739\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.1826 - val_loss: 0.2517 - val_acc: 0.1789\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 10s 5us/step - loss: -0.2496 - acc: 0.1797\n",
      ". theta fit =  [0.9196925 1.7671577]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2504 - acc: 0.2201 - val_loss: 0.2483 - val_acc: 0.2131\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2235 - val_loss: 0.2486 - val_acc: 0.2284\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2353 - val_loss: 0.2495 - val_acc: 0.2025\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2207 - val_loss: 0.2487 - val_acc: 0.2322\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2289 - val_loss: 0.2487 - val_acc: 0.2347\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2286 - val_loss: 0.2488 - val_acc: 0.2136\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2291 - val_loss: 0.2488 - val_acc: 0.2177\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2260 - val_loss: 0.2488 - val_acc: 0.2530\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2333 - val_loss: 0.2488 - val_acc: 0.2314\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2295 - val_loss: 0.2488 - val_acc: 0.2251\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2239 - val_loss: 0.2488 - val_acc: 0.2346\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.2489 - acc: 0.2134\n",
      ". theta fit =  [1.0171052 1.6223884]\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2507 - acc: 0.2370 - val_loss: 0.2489 - val_acc: 0.2078\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2277 - val_loss: 0.2489 - val_acc: 0.2629\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2377 - val_loss: 0.2490 - val_acc: 0.2381\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2381 - val_loss: 0.2491 - val_acc: 0.2429\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2514 - val_loss: 0.2494 - val_acc: 0.2208\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.2313 - val_loss: 0.2495 - val_acc: 0.2105\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2285 - val_loss: 0.2491 - val_acc: 0.2493\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2348 - val_loss: 0.2491 - val_acc: 0.2307\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2388 - val_loss: 0.2494 - val_acc: 0.2075\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2262 - val_loss: 0.2491 - val_acc: 0.2661\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2352 - val_loss: 0.2491 - val_acc: 0.2431\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2409 - val_loss: 0.2490 - val_acc: 0.2177\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.2492 - acc: 0.2632\n",
      ". theta fit =  [1.0411518 1.5983223]\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2515 - acc: 0.2235 - val_loss: 0.2490 - val_acc: 0.2028\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2509 - acc: 0.2143 - val_loss: 0.2490 - val_acc: 0.2156\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2211 - val_loss: 0.2493 - val_acc: 0.1960\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.2163 - val_loss: 0.2493 - val_acc: 0.2528\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2225 - val_loss: 0.2493 - val_acc: 0.2317\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2205 - val_loss: 0.2493 - val_acc: 0.2257\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2258 - val_loss: 0.2494 - val_acc: 0.1941\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.2177 - val_loss: 0.2495 - val_acc: 0.1938\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2136 - val_loss: 0.2494 - val_acc: 0.2428\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2280 - val_loss: 0.2493 - val_acc: 0.2175\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.2170 - val_loss: 0.2494 - val_acc: 0.2037\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.2497 - acc: 0.2029\n",
      ". theta fit =  [1.0651026 1.5742321]\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2511 - acc: 0.1976 - val_loss: 0.2492 - val_acc: 0.1972\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2509 - acc: 0.1977 - val_loss: 0.2493 - val_acc: 0.1846\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.1950 - val_loss: 0.2496 - val_acc: 0.1865\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.1978 - val_loss: 0.2496 - val_acc: 0.1883\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1969 - val_loss: 0.2496 - val_acc: 0.2109\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2020 - val_loss: 0.2496 - val_acc: 0.2097\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2114 - val_loss: 0.2497 - val_acc: 0.1800\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.1917 - val_loss: 0.2496 - val_acc: 0.2097\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.1988 - val_loss: 0.2496 - val_acc: 0.2029\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.1984 - val_loss: 0.2495 - val_acc: 0.1905\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.1994 - val_loss: 0.2497 - val_acc: 0.2521\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.2498 - acc: 0.1974\n",
      ". theta fit =  [1.040993  1.5500631]\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2511 - acc: 0.1994 - val_loss: 0.2495 - val_acc: 0.2051\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2036 - val_loss: 0.2497 - val_acc: 0.2362\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.2045 - val_loss: 0.2497 - val_acc: 0.2194\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2504 - acc: 0.2087 - val_loss: 0.2497 - val_acc: 0.2032\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.2038 - val_loss: 0.2497 - val_acc: 0.2071\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2110 - val_loss: 0.2498 - val_acc: 0.1867\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2025 - val_loss: 0.2498 - val_acc: 0.1807\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2034 - val_loss: 0.2498 - val_acc: 0.1893\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.2106 - val_loss: 0.2499 - val_acc: 0.1772\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.2004 - val_loss: 0.2498 - val_acc: 0.1940\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2111 - val_loss: 0.2503 - val_acc: 0.1744\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.2499 - acc: 0.2054\n",
      ". theta fit =  [1.0651811 1.5257779]\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2507 - acc: 0.1930 - val_loss: 0.2511 - val_acc: 0.1914\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.1864 - val_loss: 0.2499 - val_acc: 0.2225\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.1972 - val_loss: 0.2502 - val_acc: 0.1722\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.1891 - val_loss: 0.2499 - val_acc: 0.2027\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1998 - val_loss: 0.2508 - val_acc: 0.1941\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1892 - val_loss: 0.2506 - val_acc: 0.1752\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1847 - val_loss: 0.2499 - val_acc: 0.1909\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1876 - val_loss: 0.2499 - val_acc: 0.1963\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.1905 - val_loss: 0.2498 - val_acc: 0.1913\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1910 - val_loss: 0.2499 - val_acc: 0.1890\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1959 - val_loss: 0.2500 - val_acc: 0.1725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1844 - val_loss: 0.2499 - val_acc: 0.1854\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1992 - val_loss: 0.2500 - val_acc: 0.1723\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.1850 - val_loss: 0.2500 - val_acc: 0.2164\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.1937 - val_loss: 0.2499 - val_acc: 0.1932\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1928 - val_loss: 0.2499 - val_acc: 0.1924\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.1957 - val_loss: 0.2499 - val_acc: 0.1773\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1857 - val_loss: 0.2499 - val_acc: 0.1794\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.1924 - val_loss: 0.2510 - val_acc: 0.2029\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.2499 - acc: 0.1914\n",
      ". theta fit =  [1.040763  1.5013404]\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2498 - acc: 0.1899 - val_loss: 0.2499 - val_acc: 0.1908\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.1933 - val_loss: 0.2500 - val_acc: 0.2046\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.1975 - val_loss: 0.2499 - val_acc: 0.1872\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1925 - val_loss: 0.2499 - val_acc: 0.1884\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.1899 - val_loss: 0.2499 - val_acc: 0.1923\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.1999 - val_loss: 0.2503 - val_acc: 0.2378\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.1941 - val_loss: 0.2500 - val_acc: 0.1836\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.1917 - val_loss: 0.2500 - val_acc: 0.2015\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2504 - acc: 0.2003 - val_loss: 0.2500 - val_acc: 0.1938\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1944 - val_loss: 0.2500 - val_acc: 0.2020\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2068 - val_loss: 0.2500 - val_acc: 0.2316\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 5us/step - loss: -0.2501 - acc: 0.1908\n",
      ". theta fit =  [1.0652745 1.5251133]\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2508 - acc: 0.1883 - val_loss: 0.2497 - val_acc: 0.1942\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2507 - acc: 0.1922 - val_loss: 0.2500 - val_acc: 0.1741\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1878 - val_loss: 0.2498 - val_acc: 0.2141\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.1918 - val_loss: 0.2498 - val_acc: 0.1976\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1913 - val_loss: 0.2499 - val_acc: 0.1773\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1880 - val_loss: 0.2498 - val_acc: 0.1828\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.1881 - val_loss: 0.2499 - val_acc: 0.1975\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1867 - val_loss: 0.2500 - val_acc: 0.1998\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1935 - val_loss: 0.2499 - val_acc: 0.1744\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1849 - val_loss: 0.2499 - val_acc: 0.1979\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1969 - val_loss: 0.2499 - val_acc: 0.1755\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2500 - acc: 0.1944\n",
      ". theta fit =  [1.0404667 1.5002832]\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2509 - acc: 0.1963 - val_loss: 0.2502 - val_acc: 0.1983\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2507 - acc: 0.1902 - val_loss: 0.2499 - val_acc: 0.1895\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2037 - val_loss: 0.2503 - val_acc: 0.1750\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.1909 - val_loss: 0.2500 - val_acc: 0.1973\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1991 - val_loss: 0.2501 - val_acc: 0.2304\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2504 - acc: 0.2128 - val_loss: 0.2503 - val_acc: 0.1972\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1925 - val_loss: 0.2501 - val_acc: 0.2241\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1984 - val_loss: 0.2500 - val_acc: 0.1836\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1918 - val_loss: 0.2502 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1982 - val_loss: 0.2502 - val_acc: 0.2498\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.2060 - val_loss: 0.2501 - val_acc: 0.1984\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1993 - val_loss: 0.2501 - val_acc: 0.1959\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2500 - acc: 0.1896\n",
      ". theta fit =  [1.0154258 1.475219 ]\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2507 - acc: 0.2188 - val_loss: 0.2500 - val_acc: 0.1948\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.2128 - val_loss: 0.2501 - val_acc: 0.1983\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2180 - val_loss: 0.2502 - val_acc: 0.2847\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2340 - val_loss: 0.2501 - val_acc: 0.1924\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2102 - val_loss: 0.2502 - val_acc: 0.2248\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2180 - val_loss: 0.2502 - val_acc: 0.2568\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2149 - val_loss: 0.2502 - val_acc: 0.2480\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2241 - val_loss: 0.2501 - val_acc: 0.2107\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.2087 - val_loss: 0.2502 - val_acc: 0.1994\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2114 - val_loss: 0.2502 - val_acc: 0.1968\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2105 - val_loss: 0.2501 - val_acc: 0.2454\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2501 - acc: 0.1948\n",
      ". theta fit =  [0.9901511 1.4499362]\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2505 - acc: 0.2308 - val_loss: 0.2501 - val_acc: 0.2566\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2491 - val_loss: 0.2500 - val_acc: 0.2531\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2502 - acc: 0.2516 - val_loss: 0.2508 - val_acc: 0.2055\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2440 - val_loss: 0.2508 - val_acc: 0.2263\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2348 - val_loss: 0.2502 - val_acc: 0.3070\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2510 - val_loss: 0.2502 - val_acc: 0.2625\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2448 - val_loss: 0.2502 - val_acc: 0.2637\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.2461 - val_loss: 0.2502 - val_acc: 0.2635\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2448 - val_loss: 0.2501 - val_acc: 0.2578\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2491 - val_loss: 0.2508 - val_acc: 0.2332\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2271 - val_loss: 0.2501 - val_acc: 0.2455\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2476 - val_loss: 0.2501 - val_acc: 0.2402\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2501 - acc: 0.2528\n",
      ". theta fit =  [1.0157125 1.4754941]\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 15s 15us/step - loss: 0.2508 - acc: 0.2221 - val_loss: 0.2499 - val_acc: 0.1981\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2085 - val_loss: 0.2500 - val_acc: 0.2488\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2269 - val_loss: 0.2501 - val_acc: 0.1820\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2048 - val_loss: 0.2501 - val_acc: 0.2408\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2129 - val_loss: 0.2502 - val_acc: 0.2568\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2503 - acc: 0.2340 - val_loss: 0.2502 - val_acc: 0.1826\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2235 - val_loss: 0.2504 - val_acc: 0.1994\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2044 - val_loss: 0.2501 - val_acc: 0.2401\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2277 - val_loss: 0.2502 - val_acc: 0.2111\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2127 - val_loss: 0.2502 - val_acc: 0.1947\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2215 - val_loss: 0.2503 - val_acc: 0.1797\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2502 - acc: 0.1980\n",
      ". theta fit =  [1.0415026 1.5004003]\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2509 - acc: 0.1965 - val_loss: 0.2497 - val_acc: 0.1794\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.1921 - val_loss: 0.2499 - val_acc: 0.1841\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1943 - val_loss: 0.2502 - val_acc: 0.1731\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1900 - val_loss: 0.2501 - val_acc: 0.1865\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.1989 - val_loss: 0.2501 - val_acc: 0.1853\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1996 - val_loss: 0.2501 - val_acc: 0.1794\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1986 - val_loss: 0.2503 - val_acc: 0.2278\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.1972 - val_loss: 0.2502 - val_acc: 0.1753\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.1898 - val_loss: 0.2500 - val_acc: 0.1849\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2004 - val_loss: 0.2501 - val_acc: 0.1782\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.1893 - val_loss: 0.2500 - val_acc: 0.1890\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2501 - acc: 0.1793\n",
      ". theta fit =  [1.0153729 1.4743195]\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2506 - acc: 0.2072 - val_loss: 0.2501 - val_acc: 0.1989\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2137 - val_loss: 0.2502 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2505 - acc: 0.2346 - val_loss: 0.2501 - val_acc: 0.2062\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2114 - val_loss: 0.2502 - val_acc: 0.2582\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2208 - val_loss: 0.2502 - val_acc: 0.2235\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2209 - val_loss: 0.2502 - val_acc: 0.2141\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2243 - val_loss: 0.2502 - val_acc: 0.2118\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2047 - val_loss: 0.2501 - val_acc: 0.2165\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2166 - val_loss: 0.2502 - val_acc: 0.2103\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2273 - val_loss: 0.2501 - val_acc: 0.1850\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2071 - val_loss: 0.2501 - val_acc: 0.2012\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2082 - val_loss: 0.2501 - val_acc: 0.2422\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2182 - val_loss: 0.2500 - val_acc: 0.2389\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2234 - val_loss: 0.2508 - val_acc: 0.1994\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2101 - val_loss: 0.2501 - val_acc: 0.2074\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2122 - val_loss: 0.2501 - val_acc: 0.2171\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2220 - val_loss: 0.2501 - val_acc: 0.2114\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2173 - val_loss: 0.2501 - val_acc: 0.2030\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2084 - val_loss: 0.2502 - val_acc: 0.2079\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2153 - val_loss: 0.2501 - val_acc: 0.2053\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2501 - acc: 0.2051\n",
      ". theta fit =  [0.9889713 1.4478992]\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2505 - acc: 0.2333 - val_loss: 0.2499 - val_acc: 0.2503\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2386 - val_loss: 0.2499 - val_acc: 0.2415\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2401 - val_loss: 0.2500 - val_acc: 0.2518\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2443 - val_loss: 0.2501 - val_acc: 0.2437\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2493 - val_loss: 0.2503 - val_acc: 0.2366\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2419 - val_loss: 0.2502 - val_acc: 0.2523\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2480 - val_loss: 0.2502 - val_acc: 0.2525\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.2468 - val_loss: 0.2505 - val_acc: 0.2357\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2298 - val_loss: 0.2502 - val_acc: 0.2495\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2460 - val_loss: 0.2502 - val_acc: 0.2529\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2480 - val_loss: 0.2501 - val_acc: 0.2544\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2501 - acc: 0.2499\n",
      ". theta fit =  [1.0156972 1.4746437]\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2509 - acc: 0.2140 - val_loss: 0.2499 - val_acc: 0.1931\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2508 - acc: 0.2007 - val_loss: 0.2499 - val_acc: 0.2350\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.2061 - val_loss: 0.2500 - val_acc: 0.2143\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.2201 - val_loss: 0.2499 - val_acc: 0.2169\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2145 - val_loss: 0.2500 - val_acc: 0.2012\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2099 - val_loss: 0.2501 - val_acc: 0.2056\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2208 - val_loss: 0.2505 - val_acc: 0.2317\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2040 - val_loss: 0.2502 - val_acc: 0.1938\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2111 - val_loss: 0.2502 - val_acc: 0.2151\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2254 - val_loss: 0.2504 - val_acc: 0.2391\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2079 - val_loss: 0.2502 - val_acc: 0.2069\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2202 - val_loss: 0.2501 - val_acc: 0.1863\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2501 - acc: 0.2349\n",
      ". theta fit =  [1.0426837 1.4485176]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2504 - acc: 0.2183 - val_loss: 0.2499 - val_acc: 0.2111\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.2139 - val_loss: 0.2500 - val_acc: 0.2289\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2211 - val_loss: 0.2500 - val_acc: 0.2206\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2250 - val_loss: 0.2501 - val_acc: 0.2380\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2160 - val_loss: 0.2502 - val_acc: 0.2255\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2238 - val_loss: 0.2502 - val_acc: 0.2255\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2291 - val_loss: 0.2502 - val_acc: 0.2054\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2254 - val_loss: 0.2501 - val_acc: 0.2366\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2060 - val_loss: 0.2502 - val_acc: 0.2294\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2145 - val_loss: 0.2502 - val_acc: 0.2153\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2238 - val_loss: 0.2502 - val_acc: 0.1909\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2501 - acc: 0.2109\n",
      ". theta fit =  [1.0129477 1.4640819]\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2507 - acc: 0.2158 - val_loss: 0.2499 - val_acc: 0.1999\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2234 - val_loss: 0.2503 - val_acc: 0.2027\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.1974 - val_loss: 0.2501 - val_acc: 0.2178\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2227 - val_loss: 0.2501 - val_acc: 0.2107\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2269 - val_loss: 0.2501 - val_acc: 0.2178\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2212 - val_loss: 0.2501 - val_acc: 0.2507\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2267 - val_loss: 0.2502 - val_acc: 0.2157\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2283 - val_loss: 0.2502 - val_acc: 0.2095\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2128 - val_loss: 0.2502 - val_acc: 0.2254\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2218 - val_loss: 0.2502 - val_acc: 0.2453\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2196 - val_loss: 0.2502 - val_acc: 0.2314\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2501 - acc: 0.1997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  [1.0101898 1.4665805]\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2505 - acc: 0.2183 - val_loss: 0.2500 - val_acc: 0.2133\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2217 - val_loss: 0.2501 - val_acc: 0.1899\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2184 - val_loss: 0.2501 - val_acc: 0.2131\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2223 - val_loss: 0.2501 - val_acc: 0.2421\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2504 - acc: 0.2316 - val_loss: 0.2502 - val_acc: 0.2261\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2232 - val_loss: 0.2502 - val_acc: 0.2547\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2226 - val_loss: 0.2502 - val_acc: 0.2299\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2305 - val_loss: 0.2502 - val_acc: 0.1954\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2259 - val_loss: 0.2501 - val_acc: 0.2117\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2173 - val_loss: 0.2502 - val_acc: 0.2616\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2364 - val_loss: 0.2502 - val_acc: 0.2369\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2502 - acc: 0.2133\n",
      ". theta fit =  [1.012985  1.4693805]\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2505 - acc: 0.2189 - val_loss: 0.2502 - val_acc: 0.2104\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2505 - acc: 0.2193 - val_loss: 0.2503 - val_acc: 0.2191\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2120 - val_loss: 0.2502 - val_acc: 0.2215\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2300 - val_loss: 0.2502 - val_acc: 0.2094\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2246 - val_loss: 0.2502 - val_acc: 0.2114\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2194 - val_loss: 0.2502 - val_acc: 0.2159\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2224 - val_loss: 0.2502 - val_acc: 0.1926\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2151 - val_loss: 0.2502 - val_acc: 0.1945\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2267 - val_loss: 0.2502 - val_acc: 0.2007\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2222 - val_loss: 0.2503 - val_acc: 0.2322\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2050 - val_loss: 0.2502 - val_acc: 0.2050\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2223 - val_loss: 0.2502 - val_acc: 0.1921\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2178 - val_loss: 0.2501 - val_acc: 0.2135\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2119 - val_loss: 0.2501 - val_acc: 0.2179\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2229 - val_loss: 0.2503 - val_acc: 0.2378\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2144 - val_loss: 0.2501 - val_acc: 0.2122\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2128 - val_loss: 0.2502 - val_acc: 0.2070\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2161 - val_loss: 0.2501 - val_acc: 0.2514\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2272 - val_loss: 0.2502 - val_acc: 0.1828\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2504 - acc: 0.2062 - val_loss: 0.2501 - val_acc: 0.2048\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2501 - acc: 0.2046\n",
      ". theta fit =  [1.0101508 1.4665467]\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2509 - acc: 0.2152 - val_loss: 0.2500 - val_acc: 0.2202\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.2190 - val_loss: 0.2500 - val_acc: 0.1866\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2149 - val_loss: 0.2501 - val_acc: 0.1907\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2177 - val_loss: 0.2501 - val_acc: 0.2167\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2345 - val_loss: 0.2502 - val_acc: 0.2108\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2202 - val_loss: 0.2502 - val_acc: 0.2093\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2268 - val_loss: 0.2502 - val_acc: 0.2078\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2162 - val_loss: 0.2502 - val_acc: 0.2202\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2280 - val_loss: 0.2501 - val_acc: 0.2286\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2194 - val_loss: 0.2502 - val_acc: 0.2304\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2372 - val_loss: 0.2503 - val_acc: 0.2006\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2503 - acc: 0.2200\n",
      ". theta fit =  [1.0130191 1.4694164]\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2509 - acc: 0.2193 - val_loss: 0.2499 - val_acc: 0.2070\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2507 - acc: 0.2142 - val_loss: 0.2500 - val_acc: 0.1806\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2032 - val_loss: 0.2500 - val_acc: 0.2463\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2260 - val_loss: 0.2501 - val_acc: 0.2278\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2274 - val_loss: 0.2501 - val_acc: 0.2075\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2161 - val_loss: 0.2502 - val_acc: 0.2290\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2320 - val_loss: 0.2503 - val_acc: 0.2209\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2227 - val_loss: 0.2504 - val_acc: 0.2280\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2204 - val_loss: 0.2502 - val_acc: 0.1977\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2117 - val_loss: 0.2503 - val_acc: 0.2132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2312 - val_loss: 0.2502 - val_acc: 0.2130\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2502 - acc: 0.2069\n",
      ". theta fit =  [1.0101194 1.4723028]\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2507 - acc: 0.2033 - val_loss: 0.2499 - val_acc: 0.2129\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2508 - acc: 0.2088 - val_loss: 0.2500 - val_acc: 0.2127\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.2185 - val_loss: 0.2501 - val_acc: 0.2276\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2239 - val_loss: 0.2500 - val_acc: 0.1990\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2183 - val_loss: 0.2501 - val_acc: 0.2032\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2278 - val_loss: 0.2502 - val_acc: 0.1980\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2235 - val_loss: 0.2503 - val_acc: 0.2405\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2270 - val_loss: 0.2505 - val_acc: 0.2373\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2105 - val_loss: 0.2502 - val_acc: 0.2470\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2275 - val_loss: 0.2502 - val_acc: 0.2211\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.2220 - val_loss: 0.2502 - val_acc: 0.2279\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2503 - acc: 0.2129\n",
      ". theta fit =  [1.0130532 1.4752398]\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2508 - acc: 0.2132 - val_loss: 0.2499 - val_acc: 0.2483\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2507 - acc: 0.2196 - val_loss: 0.2499 - val_acc: 0.1853\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2508 - acc: 0.2033 - val_loss: 0.2499 - val_acc: 0.1911\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2506 - acc: 0.2148 - val_loss: 0.2501 - val_acc: 0.2389\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2505 - acc: 0.2274 - val_loss: 0.2501 - val_acc: 0.1966\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2504 - acc: 0.2170 - val_loss: 0.2501 - val_acc: 0.2360\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2175 - val_loss: 0.2502 - val_acc: 0.2006\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2239 - val_loss: 0.2503 - val_acc: 0.2332\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2225 - val_loss: 0.2506 - val_acc: 0.2347\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2218 - val_loss: 0.2503 - val_acc: 0.2093\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2275 - val_loss: 0.2503 - val_acc: 0.2117\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2503 - acc: 0.2479\n",
      ". theta fit =  [1.0160233 1.4782104]\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2508 - acc: 0.2082 - val_loss: 0.2498 - val_acc: 0.1864\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2508 - acc: 0.2052 - val_loss: 0.2499 - val_acc: 0.1990\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2507 - acc: 0.2076 - val_loss: 0.2500 - val_acc: 0.1933\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2131 - val_loss: 0.2501 - val_acc: 0.1805\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2504 - acc: 0.2106 - val_loss: 0.2501 - val_acc: 0.1948\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2201 - val_loss: 0.2501 - val_acc: 0.2375\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2115 - val_loss: 0.2502 - val_acc: 0.2201\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2201 - val_loss: 0.2503 - val_acc: 0.2438\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2193 - val_loss: 0.2503 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2362 - val_loss: 0.2502 - val_acc: 0.2169\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2085 - val_loss: 0.2501 - val_acc: 0.2080\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2502 - acc: 0.1863\n",
      ". theta fit =  [1.0188227 1.4811978]\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2509 - acc: 0.2110 - val_loss: 0.2501 - val_acc: 0.2179\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2507 - acc: 0.2052 - val_loss: 0.2501 - val_acc: 0.1757\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.1974 - val_loss: 0.2501 - val_acc: 0.2005\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2202 - val_loss: 0.2503 - val_acc: 0.2307\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.1985 - val_loss: 0.2502 - val_acc: 0.2094\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2138 - val_loss: 0.2502 - val_acc: 0.2050\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2078 - val_loss: 0.2502 - val_acc: 0.2108\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2203 - val_loss: 0.2502 - val_acc: 0.2033\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2090 - val_loss: 0.2502 - val_acc: 0.2209\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2219 - val_loss: 0.2502 - val_acc: 0.2005\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.1968 - val_loss: 0.2501 - val_acc: 0.2017\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2180 - val_loss: 0.2501 - val_acc: 0.1885\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.1970 - val_loss: 0.2502 - val_acc: 0.2038\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2504 - acc: 0.2212 - val_loss: 0.2504 - val_acc: 0.2327\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2056 - val_loss: 0.2501 - val_acc: 0.2447\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2034 - val_loss: 0.2501 - val_acc: 0.1957\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2035 - val_loss: 0.2502 - val_acc: 0.2191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2072 - val_loss: 0.2501 - val_acc: 0.2421\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2115 - val_loss: 0.2500 - val_acc: 0.1995\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2047 - val_loss: 0.2501 - val_acc: 0.1868\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2501 - acc: 0.1866\n",
      ". theta fit =  [1.0157869 1.4781609]\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2511 - acc: 0.2068 - val_loss: 0.2498 - val_acc: 0.1876\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2510 - acc: 0.1989 - val_loss: 0.2499 - val_acc: 0.1964\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2507 - acc: 0.2065 - val_loss: 0.2500 - val_acc: 0.1938\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2037 - val_loss: 0.2501 - val_acc: 0.2446\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2181 - val_loss: 0.2501 - val_acc: 0.2061\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2154 - val_loss: 0.2502 - val_acc: 0.2052\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2187 - val_loss: 0.2502 - val_acc: 0.1945\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2043 - val_loss: 0.2502 - val_acc: 0.2070\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2242 - val_loss: 0.2502 - val_acc: 0.1976\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2124 - val_loss: 0.2502 - val_acc: 0.2165\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2155 - val_loss: 0.2501 - val_acc: 0.2354\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2502 - acc: 0.1875\n",
      ". theta fit =  [1.018848  1.4812218]\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2510 - acc: 0.1961 - val_loss: 0.2497 - val_acc: 0.2105\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2510 - acc: 0.2042 - val_loss: 0.2498 - val_acc: 0.1898\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2509 - acc: 0.2052 - val_loss: 0.2499 - val_acc: 0.1901\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2507 - acc: 0.1996 - val_loss: 0.2501 - val_acc: 0.1975\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2092 - val_loss: 0.2502 - val_acc: 0.1997\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2127 - val_loss: 0.2502 - val_acc: 0.1966\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2209 - val_loss: 0.2501 - val_acc: 0.2223\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2035 - val_loss: 0.2502 - val_acc: 0.1978\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2167 - val_loss: 0.2502 - val_acc: 0.2065\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2046 - val_loss: 0.2502 - val_acc: 0.1982\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2504 - acc: 0.2133 - val_loss: 0.2502 - val_acc: 0.2099\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2503 - acc: 0.2103\n",
      ". theta fit =  [1.021942  1.4843276]\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2510 - acc: 0.2050 - val_loss: 0.2499 - val_acc: 0.1797\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2509 - acc: 0.1949 - val_loss: 0.2499 - val_acc: 0.1898\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.2042 - val_loss: 0.2502 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2162 - val_loss: 0.2501 - val_acc: 0.2216\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2090 - val_loss: 0.2502 - val_acc: 0.2138\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2157 - val_loss: 0.2501 - val_acc: 0.2072\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2139 - val_loss: 0.2502 - val_acc: 0.1906\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2057 - val_loss: 0.2501 - val_acc: 0.2309\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2125 - val_loss: 0.2501 - val_acc: 0.1925\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2148 - val_loss: 0.2503 - val_acc: 0.2233\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.1956 - val_loss: 0.2502 - val_acc: 0.2118\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2502 - acc: 0.1797\n",
      ". theta fit =  [1.0188034 1.4811883]\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2511 - acc: 0.2075 - val_loss: 0.2499 - val_acc: 0.2306\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2509 - acc: 0.2040 - val_loss: 0.2499 - val_acc: 0.1841\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.2139 - val_loss: 0.2503 - val_acc: 0.1921\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2036 - val_loss: 0.2502 - val_acc: 0.2388\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2163 - val_loss: 0.2502 - val_acc: 0.2084\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2123 - val_loss: 0.2502 - val_acc: 0.2121\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2213 - val_loss: 0.2502 - val_acc: 0.2149\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2193 - val_loss: 0.2503 - val_acc: 0.2294\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2043 - val_loss: 0.2501 - val_acc: 0.2183\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2093 - val_loss: 0.2501 - val_acc: 0.2401\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2097 - val_loss: 0.2503 - val_acc: 0.2180\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2503 - acc: 0.2304\n",
      ". theta fit =  [1.0156293 1.4843407]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2510 - acc: 0.2110 - val_loss: 0.2500 - val_acc: 0.1786\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2507 - acc: 0.1966 - val_loss: 0.2501 - val_acc: 0.2012\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2183 - val_loss: 0.2503 - val_acc: 0.2284\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2033 - val_loss: 0.2502 - val_acc: 0.1955\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2105 - val_loss: 0.2502 - val_acc: 0.2352\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2235 - val_loss: 0.2502 - val_acc: 0.2069\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2052 - val_loss: 0.2502 - val_acc: 0.2031\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2188 - val_loss: 0.2502 - val_acc: 0.2055\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2501 - acc: 0.2135 - val_loss: 0.2502 - val_acc: 0.2029\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2197 - val_loss: 0.2501 - val_acc: 0.2185\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2199 - val_loss: 0.2502 - val_acc: 0.2387\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2502 - acc: 0.1785\n",
      ". theta fit =  [1.0184848 1.4824489]\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2507 - acc: 0.2042 - val_loss: 0.2501 - val_acc: 0.2001\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.2064 - val_loss: 0.2501 - val_acc: 0.1997\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2177 - val_loss: 0.2502 - val_acc: 0.2266\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2284 - val_loss: 0.2502 - val_acc: 0.1990\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2104 - val_loss: 0.2502 - val_acc: 0.2045\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2206 - val_loss: 0.2505 - val_acc: 0.2296\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2091 - val_loss: 0.2501 - val_acc: 0.2425\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2153 - val_loss: 0.2502 - val_acc: 0.2067\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2136 - val_loss: 0.2501 - val_acc: 0.2195\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2168 - val_loss: 0.2501 - val_acc: 0.2041\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2034 - val_loss: 0.2501 - val_acc: 0.1923\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2065 - val_loss: 0.2501 - val_acc: 0.1898\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2053 - val_loss: 0.2502 - val_acc: 0.2221\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2061 - val_loss: 0.2500 - val_acc: 0.2058\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2092 - val_loss: 0.2501 - val_acc: 0.1856\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.1973 - val_loss: 0.2501 - val_acc: 0.2075\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2102 - val_loss: 0.2501 - val_acc: 0.1893\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2094 - val_loss: 0.2501 - val_acc: 0.2400\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2070 - val_loss: 0.2500 - val_acc: 0.2273\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2019 - val_loss: 0.2501 - val_acc: 0.1956\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2501 - acc: 0.1956\n",
      ". theta fit =  [1.0188075 1.4821254]\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2513 - acc: 0.2159 - val_loss: 0.2497 - val_acc: 0.2021\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2510 - acc: 0.1946 - val_loss: 0.2498 - val_acc: 0.2031\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2510 - acc: 0.2099 - val_loss: 0.2500 - val_acc: 0.2077\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2507 - acc: 0.2113 - val_loss: 0.2500 - val_acc: 0.2085\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2102 - val_loss: 0.2502 - val_acc: 0.2226\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2229 - val_loss: 0.2503 - val_acc: 0.2308\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2127 - val_loss: 0.2502 - val_acc: 0.1975\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2127 - val_loss: 0.2502 - val_acc: 0.2071\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2035 - val_loss: 0.2501 - val_acc: 0.2096\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2021 - val_loss: 0.2502 - val_acc: 0.1998\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2097 - val_loss: 0.2501 - val_acc: 0.2140\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2503 - acc: 0.2020\n",
      ". theta fit =  [1.0184808 1.4824522]\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2512 - acc: 0.2124 - val_loss: 0.2501 - val_acc: 0.2262\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2511 - acc: 0.2046 - val_loss: 0.2499 - val_acc: 0.2034\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2508 - acc: 0.2004 - val_loss: 0.2500 - val_acc: 0.1916\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2157 - val_loss: 0.2499 - val_acc: 0.2205\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2070 - val_loss: 0.2502 - val_acc: 0.2032\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2058 - val_loss: 0.2501 - val_acc: 0.2001\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2158 - val_loss: 0.2502 - val_acc: 0.2053\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2000 - val_loss: 0.2501 - val_acc: 0.2122\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2089 - val_loss: 0.2501 - val_acc: 0.2331\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2155 - val_loss: 0.2501 - val_acc: 0.1926\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.1997 - val_loss: 0.2500 - val_acc: 0.2282\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2145 - val_loss: 0.2501 - val_acc: 0.2007\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.1973 - val_loss: 0.2501 - val_acc: 0.1948\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2006 - val_loss: 0.2501 - val_acc: 0.2127\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2501 - acc: 0.2202\n",
      ". theta fit =  [1.0181572 1.4821236]\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2510 - acc: 0.2147 - val_loss: 0.2498 - val_acc: 0.1999\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2509 - acc: 0.2140 - val_loss: 0.2499 - val_acc: 0.2398\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.2076 - val_loss: 0.2501 - val_acc: 0.1986\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2219 - val_loss: 0.2502 - val_acc: 0.2302\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2048 - val_loss: 0.2502 - val_acc: 0.1990\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2233 - val_loss: 0.2502 - val_acc: 0.2034\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2019 - val_loss: 0.2501 - val_acc: 0.2445\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2115 - val_loss: 0.2501 - val_acc: 0.2112\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2100 - val_loss: 0.2502 - val_acc: 0.2159\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2504 - acc: 0.2143 - val_loss: 0.2502 - val_acc: 0.2328\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2503 - acc: 0.2046 - val_loss: 0.2501 - val_acc: 0.1850\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2503 - acc: 0.2000\n",
      ". theta fit =  [1.0184946 1.4824609]\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2511 - acc: 0.2093 - val_loss: 0.2498 - val_acc: 0.2405\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2508 - acc: 0.2164 - val_loss: 0.2501 - val_acc: 0.1932\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2505 - acc: 0.1931 - val_loss: 0.2501 - val_acc: 0.2090\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2504 - acc: 0.2107 - val_loss: 0.2501 - val_acc: 0.2415\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2227 - val_loss: 0.2502 - val_acc: 0.2064\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2090 - val_loss: 0.2502 - val_acc: 0.2086\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2141 - val_loss: 0.2502 - val_acc: 0.2059\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2501 - acc: 0.2050 - val_loss: 0.2502 - val_acc: 0.2057\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2505 - acc: 0.2134 - val_loss: 0.2501 - val_acc: 0.1864\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.1986 - val_loss: 0.2502 - val_acc: 0.2069\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2074 - val_loss: 0.2501 - val_acc: 0.2508\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2502 - acc: 0.2402\n",
      ". theta fit =  [1.0188304 1.4827883]\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2510 - acc: 0.2131 - val_loss: 0.2499 - val_acc: 0.1897\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2506 - acc: 0.1990 - val_loss: 0.2500 - val_acc: 0.2075\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2505 - acc: 0.2144 - val_loss: 0.2501 - val_acc: 0.2025\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2503 - acc: 0.2201 - val_loss: 0.2502 - val_acc: 0.2225\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2036 - val_loss: 0.2501 - val_acc: 0.2411\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2176 - val_loss: 0.2502 - val_acc: 0.2092\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2501 - acc: 0.2052 - val_loss: 0.2502 - val_acc: 0.2162\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2501 - acc: 0.2170 - val_loss: 0.2501 - val_acc: 0.2380\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2073 - val_loss: 0.2501 - val_acc: 0.1973\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2159 - val_loss: 0.2502 - val_acc: 0.2029\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2031 - val_loss: 0.2502 - val_acc: 0.2013\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2502 - acc: 0.1896\n",
      ". theta fit =  [1.0184908 1.4824476]\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2511 - acc: 0.2050 - val_loss: 0.2499 - val_acc: 0.1888\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2509 - acc: 0.2052 - val_loss: 0.2501 - val_acc: 0.1950\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2505 - acc: 0.2132 - val_loss: 0.2502 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2503 - acc: 0.2196 - val_loss: 0.2502 - val_acc: 0.2059\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2132 - val_loss: 0.2502 - val_acc: 0.2012\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2068 - val_loss: 0.2502 - val_acc: 0.2153\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2171 - val_loss: 0.2502 - val_acc: 0.1919\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2501 - acc: 0.2059 - val_loss: 0.2501 - val_acc: 0.2109\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2055 - val_loss: 0.2502 - val_acc: 0.2112\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2173 - val_loss: 0.2501 - val_acc: 0.1975\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2505 - acc: 0.2084 - val_loss: 0.2501 - val_acc: 0.1923\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2502 - acc: 0.1887\n",
      ". theta fit =  [1.0181481 1.4821041]\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2513 - acc: 0.2059 - val_loss: 0.2498 - val_acc: 0.1942\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2511 - acc: 0.2042 - val_loss: 0.2499 - val_acc: 0.2062\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2508 - acc: 0.2000 - val_loss: 0.2501 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2507 - acc: 0.2220 - val_loss: 0.2501 - val_acc: 0.2003\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2505 - acc: 0.2208 - val_loss: 0.2502 - val_acc: 0.1876\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2180 - val_loss: 0.2501 - val_acc: 0.2094\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2110 - val_loss: 0.2502 - val_acc: 0.2140\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2502 - acc: 0.2131 - val_loss: 0.2502 - val_acc: 0.2129\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2501 - acc: 0.2044 - val_loss: 0.2502 - val_acc: 0.2246\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2168 - val_loss: 0.2502 - val_acc: 0.2426\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2501 - acc: 0.2048 - val_loss: 0.2502 - val_acc: 0.2074\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2503 - acc: 0.1942\n",
      ". theta fit =  [1.0184896 1.4817611]\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2512 - acc: 0.1965 - val_loss: 0.2498 - val_acc: 0.1975\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2509 - acc: 0.2079 - val_loss: 0.2500 - val_acc: 0.1915\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2506 - acc: 0.2000 - val_loss: 0.2501 - val_acc: 0.2072\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2504 - acc: 0.2189 - val_loss: 0.2501 - val_acc: 0.2056\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2502 - acc: 0.2081 - val_loss: 0.2502 - val_acc: 0.1975\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2502 - acc: 0.2059 - val_loss: 0.2502 - val_acc: 0.2085\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2502 - acc: 0.2166 - val_loss: 0.2503 - val_acc: 0.2482\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2501 - acc: 0.2145 - val_loss: 0.2502 - val_acc: 0.2037\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2110 - val_loss: 0.2501 - val_acc: 0.2503\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2503 - acc: 0.2177 - val_loss: 0.2501 - val_acc: 0.1938\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2502 - acc: 0.2010 - val_loss: 0.2502 - val_acc: 0.1974\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2504 - acc: 0.1974\n",
      ". theta fit =  [1.01884   1.4821118]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration)\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()\n",
    "\n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "                      loss=my_loss_wrapper_fit(1),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train),\n",
    "                  y_train,\n",
    "                  epochs=20,\n",
    "                  batch_size=1000,\n",
    "                  validation_data=(np.array(X_test), y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer,\n",
    "                      loss=my_loss_wrapper_fit(-1),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta),\n",
    "                  y_train_theta,\n",
    "                  epochs=1,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_mu = np.array(fit_vals)[(index_refine[-1]):, 0]\n",
    "    fit_vals_sigma = np.array(fit_vals)[(index_refine[-1]):, 1]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "    extrema_mu = np.concatenate(\n",
    "        (argrelmin(fit_vals_mu)[0], argrelmax(fit_vals_mu)[0]))\n",
    "    extrema_mu = extrema_mu[extrema_mu >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    extrema_sigma = np.concatenate(\n",
    "        (argrelmin(fit_vals_sigma)[0], argrelmax(fit_vals_sigma)[0]))\n",
    "    extrema_sigma = extrema_sigma[extrema_sigma >= iteration -\n",
    "                                  index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema_mu\", extrema_mu)\n",
    "    print(\"extrema_sigma\", extrema_sigma)\n",
    "    '''\n",
    "\n",
    "    if (len(extrema_mu) == 0) or (\n",
    "            len(extrema_sigma)\n",
    "            == 0):  # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema_mu) >= 6) and (len(extrema_sigma) >=\n",
    "                                     6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10\n",
    "\n",
    "        mean_fit = np.array([[\n",
    "            np.mean(fit_vals_mu[len(fit_vals_mu) - 4:len(fit_vals_mu)]),\n",
    "            np.mean(fit_vals_sigma[len(fit_vals_sigma) -\n",
    "                                   4:len(fit_vals_sigma)])\n",
    "        ]])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T10:37:51.510994Z",
     "start_time": "2020-06-09T10:37:50.947107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex=True, constrained_layout=True)\n",
    "fig.suptitle(\n",
    "    \"GaussianAltFit-2D-DetectorEffects (DCTR Reweight):\\n N = {:.0e}, Iterations = {:.0f}\"\n",
    "    .format(N, iterations))\n",
    "axs[0].plot(fit_vals[:, 0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], 0, len(fit_vals), label='$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(fit_vals[:, 1], label='Model $\\sigma$ Fit', color='orange')\n",
    "axs[1].hlines(theta1_param[1], 0, len(fit_vals), label='$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-2D-DetectorEffects (DCTR Reweight):\\n N = {:.0e}, Iterations = {:.0f}.png\"\n",
    "#     .format(N, iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T10:37:52.173863Z",
     "start_time": "2020-06-09T10:37:51.516131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "\n",
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex=True, constrained_layout=True)\n",
    "fig.suptitle(\n",
    "    \"GaussianAltFit-2D-DetectorEffects (DCTR Reweight) Zoomed:\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\"\n",
    "    .format(N, index_refine[1], iterations))\n",
    "axs[0].plot(np.arange(index_refine[1], len(fit_vals[:, 0])),\n",
    "            fit_vals[index_refine[1]:, 0],\n",
    "            label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0],\n",
    "              index_refine[1],\n",
    "              len(fit_vals),\n",
    "              label='$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.arange(index_refine[1], len(fit_vals[:, 1])),\n",
    "            fit_vals[index_refine[1]:, 1],\n",
    "            label='Model $\\sigma$ Fit',\n",
    "            color='orange')\n",
    "axs[1].hlines(theta1_param[1],\n",
    "              index_refine[1],\n",
    "              len(fit_vals),\n",
    "              label='$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-2D-DetectorEffects (DCTR Reweight) Zoomed:\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\"\n",
    "#     .format(N, index_refine[1], iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Fitting between DCTR Reweighting and Analytical Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T10:37:52.204190Z",
     "start_time": "2020-06-09T10:37:52.179118Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_vals = [theta_fit_init]\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "index_refine = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T12:36:55.932489Z",
     "start_time": "2020-06-09T10:37:52.209143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2502 - acc: 0.2223 - val_loss: 0.2493 - val_acc: 0.2393\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2497 - acc: 0.2042 - val_loss: 0.2488 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2425 - val_loss: 0.2489 - val_acc: 0.2527\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2492 - acc: 0.2490 - val_loss: 0.2489 - val_acc: 0.2365\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2259 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2432 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2437 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2503 - val_loss: 0.2490 - val_acc: 0.2156\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2349 - val_loss: 0.2489 - val_acc: 0.2162\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2365 - val_loss: 0.2489 - val_acc: 0.2315\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2321 - val_loss: 0.2488 - val_acc: 0.2520\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2495 - acc: 0.2344 - val_loss: 0.2488 - val_acc: 0.2106\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2489 - acc: 0.2500\n",
      ". theta fit =  [0.52035904 0.98297185]\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2326 - acc: 0.3060 - val_loss: 0.2308 - val_acc: 0.3089\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3096 - val_loss: 0.2308 - val_acc: 0.3091\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2306 - acc: 0.3102 - val_loss: 0.2307 - val_acc: 0.3090\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3101 - val_loss: 0.2307 - val_acc: 0.3085\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3099 - val_loss: 0.2307 - val_acc: 0.3097\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2306 - acc: 0.3101 - val_loss: 0.2308 - val_acc: 0.3088\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3100 - val_loss: 0.2307 - val_acc: 0.3094\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2306 - acc: 0.3100 - val_loss: 0.2308 - val_acc: 0.3110\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2306 - acc: 0.3101 - val_loss: 0.2308 - val_acc: 0.3086\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2306 - acc: 0.3101 - val_loss: 0.2308 - val_acc: 0.3094\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3100 - val_loss: 0.2308 - val_acc: 0.3104\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3100 - val_loss: 0.2308 - val_acc: 0.3105\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3100 - val_loss: 0.2308 - val_acc: 0.3113\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3101 - val_loss: 0.2307 - val_acc: 0.3089\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2306 - acc: 0.3097 - val_loss: 0.2307 - val_acc: 0.3105\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2306 - acc: 0.3099 - val_loss: 0.2308 - val_acc: 0.3107\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2306 - acc: 0.3097 - val_loss: 0.2308 - val_acc: 0.3125\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2306 - acc: 0.3099 - val_loss: 0.2307 - val_acc: 0.3103\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3098 - val_loss: 0.2308 - val_acc: 0.3105\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2306 - acc: 0.3099 - val_loss: 0.2308 - val_acc: 0.3071\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2307 - acc: 0.3071\n",
      ". theta fit =  [0.8923709 1.355016 ]\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2494 - acc: 0.2877 - val_loss: 0.2480 - val_acc: 0.2919\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2482 - acc: 0.2895 - val_loss: 0.2481 - val_acc: 0.3061\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2481 - acc: 0.2927 - val_loss: 0.2480 - val_acc: 0.2841\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2480 - acc: 0.2934 - val_loss: 0.2480 - val_acc: 0.2844\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2480 - acc: 0.2948 - val_loss: 0.2480 - val_acc: 0.2927\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2480 - acc: 0.2941 - val_loss: 0.2481 - val_acc: 0.2980\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2480 - acc: 0.2927 - val_loss: 0.2481 - val_acc: 0.2966\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2480 - acc: 0.2957 - val_loss: 0.2481 - val_acc: 0.2962\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2479 - acc: 0.2933 - val_loss: 0.2481 - val_acc: 0.2893\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2479 - acc: 0.2944 - val_loss: 0.2481 - val_acc: 0.3046\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2479 - acc: 0.2958 - val_loss: 0.2480 - val_acc: 0.2971\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2480 - acc: 0.2917\n",
      ". theta fit =  [1.211616  1.6743431]\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2505 - acc: 0.2081 - val_loss: 0.2461 - val_acc: 0.1845\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2481 - acc: 0.1895 - val_loss: 0.2462 - val_acc: 0.1865\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2469 - acc: 0.1965 - val_loss: 0.2469 - val_acc: 0.1770\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2466 - acc: 0.1896 - val_loss: 0.2464 - val_acc: 0.1978\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2466 - acc: 0.1972 - val_loss: 0.2466 - val_acc: 0.1852\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2464 - acc: 0.1952 - val_loss: 0.2465 - val_acc: 0.1937\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2464 - acc: 0.1950 - val_loss: 0.2465 - val_acc: 0.2003\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2464 - acc: 0.1974 - val_loss: 0.2466 - val_acc: 0.2085\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2467 - acc: 0.1977 - val_loss: 0.2465 - val_acc: 0.1921\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2465 - acc: 0.1982 - val_loss: 0.2466 - val_acc: 0.1831\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2465 - acc: 0.1943 - val_loss: 0.2465 - val_acc: 0.1961\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2470 - acc: 0.1847\n",
      ". theta fit =  [0.92118734 1.3838897 ]\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2493 - acc: 0.2652 - val_loss: 0.2483 - val_acc: 0.2917\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2884 - val_loss: 0.2483 - val_acc: 0.2795\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.2893 - val_loss: 0.2486 - val_acc: 0.3077\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2487 - acc: 0.2880 - val_loss: 0.2484 - val_acc: 0.2962\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.2907 - val_loss: 0.2484 - val_acc: 0.2840\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.2915 - val_loss: 0.2484 - val_acc: 0.2996\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2485 - acc: 0.2915 - val_loss: 0.2485 - val_acc: 0.2995\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2484 - acc: 0.2912 - val_loss: 0.2484 - val_acc: 0.2886\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2484 - acc: 0.2894 - val_loss: 0.2485 - val_acc: 0.2873\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2484 - acc: 0.2904 - val_loss: 0.2485 - val_acc: 0.2925\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2484 - acc: 0.2922 - val_loss: 0.2484 - val_acc: 0.2947\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2485 - acc: 0.2918 - val_loss: 0.2484 - val_acc: 0.2802\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2484 - acc: 0.2793\n",
      ". theta fit =  [1.1936252 1.6565325]\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2501 - acc: 0.2029 - val_loss: 0.2465 - val_acc: 0.1784\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2477 - acc: 0.1886 - val_loss: 0.2467 - val_acc: 0.1978\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2473 - acc: 0.1935 - val_loss: 0.2467 - val_acc: 0.1933\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2468 - acc: 0.1956 - val_loss: 0.2469 - val_acc: 0.2116\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2470 - acc: 0.1983 - val_loss: 0.2468 - val_acc: 0.1920\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2469 - acc: 0.1954 - val_loss: 0.2469 - val_acc: 0.1961\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2469 - acc: 0.1955 - val_loss: 0.2469 - val_acc: 0.2038\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2467 - acc: 0.1980 - val_loss: 0.2470 - val_acc: 0.2139\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2474 - acc: 0.1994 - val_loss: 0.2469 - val_acc: 0.1912\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2469 - acc: 0.1970 - val_loss: 0.2469 - val_acc: 0.1938\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2468 - acc: 0.1963 - val_loss: 0.2470 - val_acc: 0.2077\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2470 - acc: 0.1786\n",
      ". theta fit =  [0.9326712 1.3955704]\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2493 - acc: 0.2585 - val_loss: 0.2484 - val_acc: 0.2920\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2490 - acc: 0.2864 - val_loss: 0.2484 - val_acc: 0.2895\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2886 - val_loss: 0.2485 - val_acc: 0.2948\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2859 - val_loss: 0.2486 - val_acc: 0.2797\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2488 - acc: 0.2896 - val_loss: 0.2486 - val_acc: 0.2910\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2487 - acc: 0.2914 - val_loss: 0.2485 - val_acc: 0.2903\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.2905 - val_loss: 0.2485 - val_acc: 0.2885\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2486 - acc: 0.2882 - val_loss: 0.2486 - val_acc: 0.2835\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2486 - acc: 0.2873 - val_loss: 0.2486 - val_acc: 0.2902\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2486 - acc: 0.2889 - val_loss: 0.2486 - val_acc: 0.2909\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.2886 - val_loss: 0.2486 - val_acc: 0.2985\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2486 - acc: 0.2917\n",
      ". theta fit =  [1.1856657 1.648668 ]\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2493 - acc: 0.2012 - val_loss: 0.2467 - val_acc: 0.1880\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2477 - acc: 0.1953 - val_loss: 0.2468 - val_acc: 0.1895\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2472 - acc: 0.1939 - val_loss: 0.2470 - val_acc: 0.2099\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2473 - acc: 0.2000 - val_loss: 0.2470 - val_acc: 0.1892\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2470 - acc: 0.1963 - val_loss: 0.2470 - val_acc: 0.1952\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2470 - acc: 0.1976 - val_loss: 0.2470 - val_acc: 0.2010\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2471 - acc: 0.1989 - val_loss: 0.2470 - val_acc: 0.1930\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2475 - acc: 0.1980 - val_loss: 0.2470 - val_acc: 0.1878\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2470 - acc: 0.1950 - val_loss: 0.2469 - val_acc: 0.1973\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2468 - acc: 0.1967 - val_loss: 0.2470 - val_acc: 0.2199\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2481 - acc: 0.1944 - val_loss: 0.2468 - val_acc: 0.1955\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2470 - acc: 0.1882\n",
      ". theta fit =  [0.9378796 1.4008657]\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2496 - acc: 0.2658 - val_loss: 0.2484 - val_acc: 0.2970\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2492 - acc: 0.2850 - val_loss: 0.2484 - val_acc: 0.2899\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2864 - val_loss: 0.2491 - val_acc: 0.2479\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2853 - val_loss: 0.2487 - val_acc: 0.2729\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2487 - acc: 0.2901 - val_loss: 0.2487 - val_acc: 0.3058\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2488 - acc: 0.2893 - val_loss: 0.2486 - val_acc: 0.2954\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2487 - acc: 0.2877 - val_loss: 0.2486 - val_acc: 0.2949\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.2885 - val_loss: 0.2487 - val_acc: 0.3063\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2487 - acc: 0.2856 - val_loss: 0.2486 - val_acc: 0.2943\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.2893 - val_loss: 0.2487 - val_acc: 0.2995\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2487 - acc: 0.2873 - val_loss: 0.2487 - val_acc: 0.3076\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2486 - acc: 0.2896 - val_loss: 0.2487 - val_acc: 0.2762\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2486 - acc: 0.2897\n",
      ". theta fit =  [1.1819762 1.645136 ]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2502 - acc: 0.2181 - val_loss: 0.2484 - val_acc: 0.1872\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.2043 - val_loss: 0.2486 - val_acc: 0.1776\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2492 - acc: 0.2048 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2155 - val_loss: 0.2487 - val_acc: 0.1860\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2134 - val_loss: 0.2488 - val_acc: 0.1722\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2103 - val_loss: 0.2488 - val_acc: 0.1796\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2488 - acc: 0.2048 - val_loss: 0.2488 - val_acc: 0.1978\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2487 - acc: 0.2061 - val_loss: 0.2487 - val_acc: 0.2272\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2094 - val_loss: 0.2488 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2488 - acc: 0.2149 - val_loss: 0.2487 - val_acc: 0.1950\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2033 - val_loss: 0.2487 - val_acc: 0.2188\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2488 - acc: 0.1873\n",
      ". theta fit =  [1.0353453 1.4983562]\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2493 - acc: 0.2112 - val_loss: 0.2485 - val_acc: 0.2408\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2187 - val_loss: 0.2486 - val_acc: 0.2013\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.2113 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.2309 - val_loss: 0.2489 - val_acc: 0.2113\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2491 - acc: 0.2169 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2360 - val_loss: 0.2490 - val_acc: 0.1868\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2186 - val_loss: 0.2490 - val_acc: 0.1893\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2171 - val_loss: 0.2489 - val_acc: 0.2386\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2256 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.2254 - val_loss: 0.2490 - val_acc: 0.1823\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2026 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2490 - acc: 0.2410\n",
      ". theta fit =  [1.0113125 1.47609  ]\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2496 - acc: 0.2567 - val_loss: 0.2485 - val_acc: 0.2198\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2355 - val_loss: 0.2488 - val_acc: 0.2039\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2372 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2617 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2546 - val_loss: 0.2492 - val_acc: 0.2110\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2459 - val_loss: 0.2490 - val_acc: 0.2087\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2393 - val_loss: 0.2489 - val_acc: 0.2679\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2532 - val_loss: 0.2490 - val_acc: 0.2518\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2398 - val_loss: 0.2489 - val_acc: 0.2281\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2408 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2518 - val_loss: 0.2490 - val_acc: 0.2100\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2490 - acc: 0.2197\n",
      ". theta fit =  [0.9873235 1.5001228]\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2502 - acc: 0.2693 - val_loss: 0.2485 - val_acc: 0.3206\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2777 - val_loss: 0.2487 - val_acc: 0.2502\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2742 - val_loss: 0.2488 - val_acc: 0.3256\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2920 - val_loss: 0.2490 - val_acc: 0.2540\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2838 - val_loss: 0.2490 - val_acc: 0.2785\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2830 - val_loss: 0.2489 - val_acc: 0.2958\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.3003 - val_loss: 0.2490 - val_acc: 0.2691\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2946 - val_loss: 0.2490 - val_acc: 0.2709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2827 - val_loss: 0.2489 - val_acc: 0.2828\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2926 - val_loss: 0.2490 - val_acc: 0.2036\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2683 - val_loss: 0.2489 - val_acc: 0.3241\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2491 - acc: 0.3208\n",
      ". theta fit =  [1.0113872 1.5240694]\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2502 - acc: 0.2580 - val_loss: 0.2483 - val_acc: 0.2736\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2341 - val_loss: 0.2485 - val_acc: 0.2819\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2638 - val_loss: 0.2489 - val_acc: 0.1730\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2364 - val_loss: 0.2488 - val_acc: 0.2509\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2641 - val_loss: 0.2488 - val_acc: 0.2573\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2585 - val_loss: 0.2489 - val_acc: 0.1919\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2348 - val_loss: 0.2490 - val_acc: 0.2546\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2529 - val_loss: 0.2489 - val_acc: 0.2928\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2700 - val_loss: 0.2489 - val_acc: 0.1893\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2529 - val_loss: 0.2488 - val_acc: 0.3171\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2624 - val_loss: 0.2489 - val_acc: 0.2623\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2491 - acc: 0.2735\n",
      ". theta fit =  [1.0354965 1.4999799]\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2501 - acc: 0.2177 - val_loss: 0.2485 - val_acc: 0.2094\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2165 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2227 - val_loss: 0.2488 - val_acc: 0.1936\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2341 - val_loss: 0.2493 - val_acc: 0.2358\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2140 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2194 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2277 - val_loss: 0.2489 - val_acc: 0.1865\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2180 - val_loss: 0.2489 - val_acc: 0.2139\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2362 - val_loss: 0.2491 - val_acc: 0.2006\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2228 - val_loss: 0.2490 - val_acc: 0.1746\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2096 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2490 - acc: 0.2092\n",
      ". theta fit =  [1.0112283 1.4757745]\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2501 - acc: 0.2331 - val_loss: 0.2486 - val_acc: 0.2498\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2447 - val_loss: 0.2488 - val_acc: 0.1948\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2404 - val_loss: 0.2488 - val_acc: 0.2513\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2425 - val_loss: 0.2489 - val_acc: 0.2680\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2529 - val_loss: 0.2489 - val_acc: 0.2348\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2489 - val_loss: 0.2489 - val_acc: 0.2199\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2520 - val_loss: 0.2490 - val_acc: 0.2079\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2364 - val_loss: 0.2489 - val_acc: 0.2607\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2473 - val_loss: 0.2490 - val_acc: 0.2646\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2454 - val_loss: 0.2489 - val_acc: 0.2244\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2307 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2491 - acc: 0.2500\n",
      ". theta fit =  [1.0356139 1.5001976]\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2502 - acc: 0.2245 - val_loss: 0.2486 - val_acc: 0.1809\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2079 - val_loss: 0.2488 - val_acc: 0.1750\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2258 - val_loss: 0.2495 - val_acc: 0.2380\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2232 - val_loss: 0.2491 - val_acc: 0.1835\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2219 - val_loss: 0.2492 - val_acc: 0.1946\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2094 - val_loss: 0.2489 - val_acc: 0.2093\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2260 - val_loss: 0.2489 - val_acc: 0.1978\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2225 - val_loss: 0.2489 - val_acc: 0.2154\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2160 - val_loss: 0.2489 - val_acc: 0.1990\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2215 - val_loss: 0.2489 - val_acc: 0.1850\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2137 - val_loss: 0.2489 - val_acc: 0.2057\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2490 - acc: 0.1809\n",
      ". theta fit =  [1.0109984 1.4756027]\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2501 - acc: 0.2178 - val_loss: 0.2485 - val_acc: 0.2552\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.2381 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2432 - val_loss: 0.2489 - val_acc: 0.2687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2540 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2551 - val_loss: 0.2489 - val_acc: 0.2658\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2535 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2463 - val_loss: 0.2490 - val_acc: 0.2495\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2409 - val_loss: 0.2490 - val_acc: 0.2848\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2568 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2440 - val_loss: 0.2489 - val_acc: 0.2336\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2515 - val_loss: 0.2489 - val_acc: 0.2990\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2491 - acc: 0.2555\n",
      ". theta fit =  [1.0339725 1.5004119]\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2504 - acc: 0.2153 - val_loss: 0.2483 - val_acc: 0.1957\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2155 - val_loss: 0.2486 - val_acc: 0.1774\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2181 - val_loss: 0.2491 - val_acc: 0.2260\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2039 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2270 - val_loss: 0.2489 - val_acc: 0.2305\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2263 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2362 - val_loss: 0.2490 - val_acc: 0.2304\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2190 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2277 - val_loss: 0.2489 - val_acc: 0.2496\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2202 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2226 - val_loss: 0.2489 - val_acc: 0.3155\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.1958\n",
      ". theta fit =  [1.0089555 1.4802163]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2501 - acc: 0.2164 - val_loss: 0.2484 - val_acc: 0.2193\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2224 - val_loss: 0.2485 - val_acc: 0.2157\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.2353 - val_loss: 0.2486 - val_acc: 0.2507\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2370 - val_loss: 0.2489 - val_acc: 0.1951\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2100 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2434 - val_loss: 0.2489 - val_acc: 0.2632\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2471 - val_loss: 0.2490 - val_acc: 0.2191\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2283 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2494 - val_loss: 0.2490 - val_acc: 0.2343\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2206 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2392 - val_loss: 0.2490 - val_acc: 0.2110\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2492 - acc: 0.2192\n",
      ". theta fit =  [1.0249088 1.4916377]\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2505 - acc: 0.2051 - val_loss: 0.2484 - val_acc: 0.1876\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2280 - val_loss: 0.2485 - val_acc: 0.2256\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2220 - val_loss: 0.2486 - val_acc: 0.1916\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2241 - val_loss: 0.2488 - val_acc: 0.2075\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2494 - acc: 0.2282 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2440 - val_loss: 0.2490 - val_acc: 0.1959\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2205 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2294 - val_loss: 0.2490 - val_acc: 0.2535\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2372 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2354 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2348 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.1877\n",
      ". theta fit =  [1.0223607 1.4941779]\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2502 - acc: 0.2200 - val_loss: 0.2484 - val_acc: 0.1971\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.2217 - val_loss: 0.2488 - val_acc: 0.1980\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2203 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2302 - val_loss: 0.2489 - val_acc: 0.2550\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2459 - val_loss: 0.2489 - val_acc: 0.2519\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2437 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2361 - val_loss: 0.2489 - val_acc: 0.2458\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2444 - val_loss: 0.2489 - val_acc: 0.1848\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2232 - val_loss: 0.2489 - val_acc: 0.2547\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2462 - val_loss: 0.2489 - val_acc: 0.2356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2339 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.1972\n",
      ". theta fit =  [1.0197518 1.4967718]\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2503 - acc: 0.2335 - val_loss: 0.2488 - val_acc: 0.2404\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2287 - val_loss: 0.2486 - val_acc: 0.1933\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2224 - val_loss: 0.2488 - val_acc: 0.2499\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2386 - val_loss: 0.2488 - val_acc: 0.2662\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2441 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2409 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2457 - val_loss: 0.2489 - val_acc: 0.3055\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2488 - val_loss: 0.2490 - val_acc: 0.1922\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2310 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2397 - val_loss: 0.2490 - val_acc: 0.1874\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2220 - val_loss: 0.2490 - val_acc: 0.2539\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2392 - val_loss: 0.2489 - val_acc: 0.2806\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2490 - acc: 0.1934\n",
      ". theta fit =  [1.0171391 1.4941592]\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2506 - acc: 0.2242 - val_loss: 0.2485 - val_acc: 0.2498\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2378 - val_loss: 0.2485 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2270 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2498 - acc: 0.2398 - val_loss: 0.2488 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2517 - val_loss: 0.2490 - val_acc: 0.2024\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2272 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2385 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2773 - val_loss: 0.2497 - val_acc: 0.2263\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2182 - val_loss: 0.2490 - val_acc: 0.2143\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2311 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2498 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2406 - val_loss: 0.2487 - val_acc: 0.2637\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.2500\n",
      ". theta fit =  [1.0194529 1.4915321]\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2500 - acc: 0.2384 - val_loss: 0.2485 - val_acc: 0.2179\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2503 - acc: 0.2297 - val_loss: 0.2484 - val_acc: 0.2219\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2204 - val_loss: 0.2486 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2499 - acc: 0.2402 - val_loss: 0.2488 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2388 - val_loss: 0.2490 - val_acc: 0.2362\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2498 - val_loss: 0.2490 - val_acc: 0.2031\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2418 - val_loss: 0.2491 - val_acc: 0.2021\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2386 - val_loss: 0.2491 - val_acc: 0.1886\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2184 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2446 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2329 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2417 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.2216\n",
      ". theta fit =  [1.0168445 1.4942011]\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2504 - acc: 0.2246 - val_loss: 0.2486 - val_acc: 0.2498\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2500 - acc: 0.2345 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2320 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2291 - val_loss: 0.2490 - val_acc: 0.2726\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2599 - val_loss: 0.2489 - val_acc: 0.2464\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2390 - val_loss: 0.2490 - val_acc: 0.2497\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2381 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2395 - val_loss: 0.2489 - val_acc: 0.2610\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2487 - val_loss: 0.2489 - val_acc: 0.1959\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2418 - val_loss: 0.2491 - val_acc: 0.2422\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2261 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.2500\n",
      ". theta fit =  [1.0195415 1.4915091]\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2501 - acc: 0.2207 - val_loss: 0.2486 - val_acc: 0.2471\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2497 - acc: 0.2385 - val_loss: 0.2488 - val_acc: 0.2598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2494 - acc: 0.2376 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2361 - val_loss: 0.2490 - val_acc: 0.2537\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2463 - val_loss: 0.2490 - val_acc: 0.2116\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2380 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2381 - val_loss: 0.2490 - val_acc: 0.2652\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2317 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2491 - acc: 0.2366 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2491 - acc: 0.2449 - val_loss: 0.2490 - val_acc: 0.1836\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2213 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2490 - acc: 0.2470\n",
      ". theta fit =  [1.0168087 1.4887737]\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2503 - acc: 0.2483 - val_loss: 0.2485 - val_acc: 0.1811\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2249 - val_loss: 0.2486 - val_acc: 0.2338\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.2450 - val_loss: 0.2491 - val_acc: 0.2378\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.2326 - val_loss: 0.2489 - val_acc: 0.1996\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2300 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2370 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2562 - val_loss: 0.2490 - val_acc: 0.2339\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2473 - val_loss: 0.2490 - val_acc: 0.2370\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2351 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2468 - val_loss: 0.2490 - val_acc: 0.2331\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2402 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2491 - acc: 0.1811\n",
      ". theta fit =  [1.0140395 1.486142 ]\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2502 - acc: 0.2296 - val_loss: 0.2486 - val_acc: 0.1770\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2338 - val_loss: 0.2489 - val_acc: 0.1917\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2308 - val_loss: 0.2490 - val_acc: 0.2027\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2464 - val_loss: 0.2491 - val_acc: 0.1949\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2289 - val_loss: 0.2490 - val_acc: 0.3100\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2591 - val_loss: 0.2490 - val_acc: 0.2433\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2462 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2432 - val_loss: 0.2490 - val_acc: 0.2521\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2417 - val_loss: 0.2491 - val_acc: 0.2610\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.2653 - val_loss: 0.2489 - val_acc: 0.2079\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2277 - val_loss: 0.2489 - val_acc: 0.1926\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.1771\n",
      ". theta fit =  [1.0112371 1.4833426]\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2502 - acc: 0.2329 - val_loss: 0.2487 - val_acc: 0.1955\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2188 - val_loss: 0.2488 - val_acc: 0.2565\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.2588 - val_loss: 0.2489 - val_acc: 0.2832\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2652 - val_loss: 0.2491 - val_acc: 0.1947\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2317 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2535 - val_loss: 0.2490 - val_acc: 0.2999\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2577 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2613 - val_loss: 0.2491 - val_acc: 0.2386\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2309 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2531 - val_loss: 0.2490 - val_acc: 0.2635\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2656 - val_loss: 0.2491 - val_acc: 0.2305\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.1954\n",
      ". theta fit =  [1.008401  1.4805124]\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2507 - acc: 0.2320 - val_loss: 0.2485 - val_acc: 0.2498\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2560 - val_loss: 0.2487 - val_acc: 0.2565\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2495 - acc: 0.2470 - val_loss: 0.2490 - val_acc: 0.2430\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2492 - acc: 0.2608 - val_loss: 0.2490 - val_acc: 0.2650\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2580 - val_loss: 0.2490 - val_acc: 0.2434\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2497 - val_loss: 0.2490 - val_acc: 0.2838\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2580 - val_loss: 0.2490 - val_acc: 0.2768\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2581 - val_loss: 0.2490 - val_acc: 0.2427\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2492 - val_loss: 0.2490 - val_acc: 0.2417\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2481 - val_loss: 0.2490 - val_acc: 0.2669\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2494 - acc: 0.2671 - val_loss: 0.2498 - val_acc: 0.2451\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2491 - acc: 0.2500\n",
      ". theta fit =  [1.0055518 1.4833527]\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2501 - acc: 0.2511 - val_loss: 0.2485 - val_acc: 0.2582\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2558 - val_loss: 0.2486 - val_acc: 0.2628\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.2657 - val_loss: 0.2492 - val_acc: 0.2274\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.2464 - val_loss: 0.2490 - val_acc: 0.2623\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2816 - val_loss: 0.2493 - val_acc: 0.2420\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2527 - val_loss: 0.2492 - val_acc: 0.2256\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2503 - val_loss: 0.2491 - val_acc: 0.2213\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2544 - val_loss: 0.2490 - val_acc: 0.2559\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2593 - val_loss: 0.2490 - val_acc: 0.2990\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2625 - val_loss: 0.2490 - val_acc: 0.2629\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.2442 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2491 - acc: 0.2580\n",
      ". theta fit =  [1.0083709 1.4862523]\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2504 - acc: 0.2489 - val_loss: 0.2485 - val_acc: 0.2395\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.2311 - val_loss: 0.2486 - val_acc: 0.2356\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2499 - acc: 0.2466 - val_loss: 0.2488 - val_acc: 0.3088\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.2548 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2587 - val_loss: 0.2490 - val_acc: 0.3034\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2630 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2532 - val_loss: 0.2490 - val_acc: 0.2873\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2667 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2693 - val_loss: 0.2490 - val_acc: 0.2403\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.2412 - val_loss: 0.2490 - val_acc: 0.2591\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2467 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2493 - acc: 0.2391\n",
      ". theta fit =  [1.0112962 1.4891901]\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2506 - acc: 0.2413 - val_loss: 0.2484 - val_acc: 0.2450\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2451 - val_loss: 0.2490 - val_acc: 0.2333\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2325 - val_loss: 0.2488 - val_acc: 0.2738\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2587 - val_loss: 0.2489 - val_acc: 0.2572\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2717 - val_loss: 0.2493 - val_acc: 0.2362\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2295 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2477 - val_loss: 0.2490 - val_acc: 0.2806\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2565 - val_loss: 0.2490 - val_acc: 0.2587\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2423 - val_loss: 0.2490 - val_acc: 0.2976\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2479 - val_loss: 0.2490 - val_acc: 0.2843\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2495 - acc: 0.2545 - val_loss: 0.2488 - val_acc: 0.2177\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2492 - acc: 0.2447\n",
      ". theta fit =  [1.0142571 1.4921606]\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2503 - acc: 0.2310 - val_loss: 0.2485 - val_acc: 0.2498\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.2381 - val_loss: 0.2486 - val_acc: 0.2589\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.2577 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2496 - acc: 0.2453 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.2528 - val_loss: 0.2490 - val_acc: 0.1783\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2283 - val_loss: 0.2490 - val_acc: 0.2615\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2548 - val_loss: 0.2490 - val_acc: 0.2014\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2414 - val_loss: 0.2490 - val_acc: 0.2574\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2587 - val_loss: 0.2492 - val_acc: 0.2310\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2418 - val_loss: 0.2490 - val_acc: 0.2068\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2295 - val_loss: 0.2490 - val_acc: 0.2245\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2491 - acc: 0.2500\n",
      ". theta fit =  [1.0112936 1.4951042]\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2502 - acc: 0.2313 - val_loss: 0.2485 - val_acc: 0.1924\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2500 - acc: 0.2385 - val_loss: 0.2486 - val_acc: 0.2640\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2497 - acc: 0.2350 - val_loss: 0.2489 - val_acc: 0.2702\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.2637 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2450 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2566 - val_loss: 0.2490 - val_acc: 0.2369\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2474 - val_loss: 0.2490 - val_acc: 0.2681\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2434 - val_loss: 0.2490 - val_acc: 0.3118\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2514 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2639 - val_loss: 0.2490 - val_acc: 0.2418\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2276 - val_loss: 0.2489 - val_acc: 0.3282\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2491 - acc: 0.1926\n",
      ". theta fit =  [1.0082629 1.4920701]\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2506 - acc: 0.2399 - val_loss: 0.2487 - val_acc: 0.2319\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.2367 - val_loss: 0.2486 - val_acc: 0.2353\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.2524 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2493 - acc: 0.2418 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2671 - val_loss: 0.2490 - val_acc: 0.2648\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2525 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2581 - val_loss: 0.2490 - val_acc: 0.2852\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2536 - val_loss: 0.2489 - val_acc: 0.2490\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2501 - acc: 0.2538 - val_loss: 0.2488 - val_acc: 0.2286\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2494 - acc: 0.2325 - val_loss: 0.2489 - val_acc: 0.2674\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.2568 - val_loss: 0.2489 - val_acc: 0.2150\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2451 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2491 - acc: 0.2350\n",
      ". theta fit =  [1.0051929 1.4890018]\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2502 - acc: 0.2547 - val_loss: 0.2486 - val_acc: 0.2498\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2615 - val_loss: 0.2488 - val_acc: 0.2177\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2495 - acc: 0.2554 - val_loss: 0.2489 - val_acc: 0.3274\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2492 - acc: 0.2591 - val_loss: 0.2490 - val_acc: 0.2725\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2643 - val_loss: 0.2490 - val_acc: 0.2895\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2804 - val_loss: 0.2495 - val_acc: 0.2261\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2349 - val_loss: 0.2491 - val_acc: 0.2600\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2747 - val_loss: 0.2491 - val_acc: 0.2243\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2336 - val_loss: 0.2491 - val_acc: 0.2708\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2701 - val_loss: 0.2490 - val_acc: 0.2553\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2528 - val_loss: 0.2490 - val_acc: 0.2888\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2491 - acc: 0.2500\n",
      ". theta fit =  [1.0020986 1.4859027]\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2503 - acc: 0.2567 - val_loss: 0.2486 - val_acc: 0.2498\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2615 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.2612 - val_loss: 0.2488 - val_acc: 0.2822\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2673 - val_loss: 0.2489 - val_acc: 0.3010\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2688 - val_loss: 0.2490 - val_acc: 0.2720\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2798 - val_loss: 0.2493 - val_acc: 0.1973\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2425 - val_loss: 0.2490 - val_acc: 0.2748\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2693 - val_loss: 0.2490 - val_acc: 0.2665\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2592 - val_loss: 0.2490 - val_acc: 0.2963\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2588 - val_loss: 0.2490 - val_acc: 0.2790\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2632 - val_loss: 0.2490 - val_acc: 0.2543\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2492 - acc: 0.2500\n",
      ". theta fit =  [1.0052357 1.4890255]\n",
      "==============================\n",
      "====Refining Learning Rate====\n",
      "==============================\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2503 - acc: 0.2607 - val_loss: 0.2486 - val_acc: 0.2438\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.2465 - val_loss: 0.2488 - val_acc: 0.2955\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2495 - acc: 0.2581 - val_loss: 0.2489 - val_acc: 0.2769\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2491 - acc: 0.2646 - val_loss: 0.2490 - val_acc: 0.1984\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2362 - val_loss: 0.2492 - val_acc: 0.2755\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2611 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.2637 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2613 - val_loss: 0.2490 - val_acc: 0.2057\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2356 - val_loss: 0.2491 - val_acc: 0.2588\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2794 - val_loss: 0.2493 - val_acc: 0.2360\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2344 - val_loss: 0.2491 - val_acc: 0.2537\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.2435\n",
      ". theta fit =  [1.0048815 1.4886836]\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2502 - acc: 0.2483 - val_loss: 0.2487 - val_acc: 0.2705\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2552 - val_loss: 0.2490 - val_acc: 0.2562\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2646 - val_loss: 0.2490 - val_acc: 0.2213\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2555 - val_loss: 0.2490 - val_acc: 0.2703\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2536 - val_loss: 0.2490 - val_acc: 0.2681\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2694 - val_loss: 0.2490 - val_acc: 0.2586\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2608 - val_loss: 0.2490 - val_acc: 0.2171\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2463 - val_loss: 0.2491 - val_acc: 0.2720\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2491 - acc: 0.2704 - val_loss: 0.2494 - val_acc: 0.2536\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2538 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2536 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.2708\n",
      ". theta fit =  [1.0045655 1.4883628]\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2499 - acc: 0.2485 - val_loss: 0.2486 - val_acc: 0.2734\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2579 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2540 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2658 - val_loss: 0.2490 - val_acc: 0.2266\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2520 - val_loss: 0.2490 - val_acc: 0.2690\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2542 - val_loss: 0.2490 - val_acc: 0.2606\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2532 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2660 - val_loss: 0.2490 - val_acc: 0.2878\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2768 - val_loss: 0.2491 - val_acc: 0.1940\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2378 - val_loss: 0.2490 - val_acc: 0.2696\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2584 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.2736\n",
      ". theta fit =  [1.0048885 1.488067 ]\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2501 - acc: 0.2449 - val_loss: 0.2488 - val_acc: 0.1909\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2498 - acc: 0.2348 - val_loss: 0.2489 - val_acc: 0.2518\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2667 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2563 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2452 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2601 - val_loss: 0.2490 - val_acc: 0.3105\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2618 - val_loss: 0.2490 - val_acc: 0.3133\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2692 - val_loss: 0.2490 - val_acc: 0.2741\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2540 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2523 - val_loss: 0.2490 - val_acc: 0.2766\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2616 - val_loss: 0.2490 - val_acc: 0.2590\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.1908\n",
      ". theta fit =  [1.004561  1.4877396]\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2506 - acc: 0.2367 - val_loss: 0.2485 - val_acc: 0.2329\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2436 - val_loss: 0.2486 - val_acc: 0.2568\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2726 - val_loss: 0.2490 - val_acc: 0.1773\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2440 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2598 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2853 - val_loss: 0.2494 - val_acc: 0.2361\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2411 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2563 - val_loss: 0.2490 - val_acc: 0.2778\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2570 - val_loss: 0.2490 - val_acc: 0.2576\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2601 - val_loss: 0.2490 - val_acc: 0.2552\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2578 - val_loss: 0.2489 - val_acc: 0.2363\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2492 - acc: 0.2328\n",
      ". theta fit =  [1.0042305 1.487413 ]\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2504 - acc: 0.2522 - val_loss: 0.2484 - val_acc: 0.2042\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2443 - val_loss: 0.2487 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2610 - val_loss: 0.2489 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2505 - val_loss: 0.2490 - val_acc: 0.2794\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2691 - val_loss: 0.2490 - val_acc: 0.2899\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2545 - val_loss: 0.2491 - val_acc: 0.2659\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2709 - val_loss: 0.2490 - val_acc: 0.3124\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2652 - val_loss: 0.2490 - val_acc: 0.2294\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2458 - val_loss: 0.2490 - val_acc: 0.2704\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2489 - acc: 0.2593 - val_loss: 0.2490 - val_acc: 0.2542\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2678 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2492 - acc: 0.2041\n",
      ". theta fit =  [1.0039002 1.487747 ]\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2504 - acc: 0.2527 - val_loss: 0.2485 - val_acc: 0.2575\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2501 - acc: 0.2571 - val_loss: 0.2487 - val_acc: 0.2641\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2499 - acc: 0.2617 - val_loss: 0.2488 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2595 - val_loss: 0.2489 - val_acc: 0.2553\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2628 - val_loss: 0.2489 - val_acc: 0.2545\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2689 - val_loss: 0.2494 - val_acc: 0.2274\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2431 - val_loss: 0.2490 - val_acc: 0.2603\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2467 - val_loss: 0.2490 - val_acc: 0.3113\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2711 - val_loss: 0.2491 - val_acc: 0.2191\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2511 - val_loss: 0.2490 - val_acc: 0.2881\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2697 - val_loss: 0.2492 - val_acc: 0.2384\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2492 - acc: 0.2578\n",
      ". theta fit =  [1.0042375 1.4880842]\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2503 - acc: 0.2536 - val_loss: 0.2485 - val_acc: 0.2308\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2583 - val_loss: 0.2489 - val_acc: 0.2057\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2497 - acc: 0.2444 - val_loss: 0.2489 - val_acc: 0.2070\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2492 - acc: 0.2513 - val_loss: 0.2490 - val_acc: 0.3160\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2660 - val_loss: 0.2490 - val_acc: 0.2912\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2599 - val_loss: 0.2490 - val_acc: 0.2543\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2604 - val_loss: 0.2490 - val_acc: 0.2473\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2539 - val_loss: 0.2490 - val_acc: 0.2759\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2620 - val_loss: 0.2489 - val_acc: 0.2958\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2639 - val_loss: 0.2490 - val_acc: 0.2339\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2614 - val_loss: 0.2490 - val_acc: 0.2220\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.2306\n",
      ". theta fit =  [1.0038977 1.4884243]\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2500 - acc: 0.2625 - val_loss: 0.2497 - val_acc: 0.2426\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2496 - acc: 0.2394 - val_loss: 0.2489 - val_acc: 0.2778\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2552 - val_loss: 0.2489 - val_acc: 0.2998\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2735 - val_loss: 0.2490 - val_acc: 0.2448\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2516 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2571 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2517 - val_loss: 0.2490 - val_acc: 0.2663\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2530 - val_loss: 0.2490 - val_acc: 0.3050\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2602 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2499 - val_loss: 0.2490 - val_acc: 0.2817\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2632 - val_loss: 0.2490 - val_acc: 0.2504\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2629 - val_loss: 0.2490 - val_acc: 0.2391\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2489 - acc: 0.2781\n",
      ". theta fit =  [1.0035549 1.4880803]\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2502 - acc: 0.2631 - val_loss: 0.2487 - val_acc: 0.2568\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2498 - acc: 0.2557 - val_loss: 0.2488 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2603 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2764 - val_loss: 0.2489 - val_acc: 0.2548\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2442 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2539 - val_loss: 0.2490 - val_acc: 0.3164\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2668 - val_loss: 0.2490 - val_acc: 0.3241\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2773 - val_loss: 0.2490 - val_acc: 0.2291\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2428 - val_loss: 0.2489 - val_acc: 0.2864\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2648 - val_loss: 0.2489 - val_acc: 0.2685\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2519 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2490 - acc: 0.2572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  [1.0032098 1.4877332]\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2504 - acc: 0.2505 - val_loss: 0.2485 - val_acc: 0.2498\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2502 - acc: 0.2576 - val_loss: 0.2486 - val_acc: 0.2413\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2500 - acc: 0.2477 - val_loss: 0.2488 - val_acc: 0.2642\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2494 - acc: 0.2572 - val_loss: 0.2490 - val_acc: 0.3136\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2660 - val_loss: 0.2490 - val_acc: 0.2700\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2770 - val_loss: 0.2493 - val_acc: 0.2321\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2566 - val_loss: 0.2490 - val_acc: 0.2672\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2605 - val_loss: 0.2490 - val_acc: 0.2556\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2579 - val_loss: 0.2490 - val_acc: 0.2610\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2759 - val_loss: 0.2492 - val_acc: 0.2268\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2493 - acc: 0.2561 - val_loss: 0.2492 - val_acc: 0.2440\n",
      "Training theta\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2493 - acc: 0.2500\n",
      ". theta fit =  [1.0035603 1.4880836]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    print(\"Iteration: \", iteration)\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()\n",
    "\n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "                      loss=my_loss_wrapper_fit(1, reweight_analytically=True),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train),\n",
    "                  y_train,\n",
    "                  epochs=20,\n",
    "                  batch_size=1000,\n",
    "                  validation_data=(np.array(X_test), y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers) - 1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass\n",
    "\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer,\n",
    "                      loss=my_loss_wrapper_fit(-1, reweight_analytically=True),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta),\n",
    "                  y_train_theta,\n",
    "                  epochs=1,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "\n",
    "    fit_vals_mu = np.array(fit_vals)[(index_refine[-1]):, 0]\n",
    "    fit_vals_sigma = np.array(fit_vals)[(index_refine[-1]):, 1]\n",
    "\n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "    extrema_mu = np.concatenate(\n",
    "        (argrelmin(fit_vals_mu)[0], argrelmax(fit_vals_mu)[0]))\n",
    "    extrema_mu = extrema_mu[extrema_mu >= iteration - index_refine[-1] - 20]\n",
    "\n",
    "    extrema_sigma = np.concatenate(\n",
    "        (argrelmin(fit_vals_sigma)[0], argrelmax(fit_vals_sigma)[0]))\n",
    "    extrema_sigma = extrema_sigma[extrema_sigma >= iteration -\n",
    "                                  index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema_mu\", extrema_mu)\n",
    "    print(\"extrema_sigma\", extrema_sigma)\n",
    "    '''\n",
    "\n",
    "    if (len(extrema_mu) == 0) or (\n",
    "            len(extrema_sigma)\n",
    "            == 0):  # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema_mu) >= 6) and (len(extrema_sigma) >=\n",
    "                                     6):  #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print('==============================\\n' +\n",
    "              '====Refining Learning Rate====\\n' +\n",
    "              '==============================')\n",
    "        optimizer.lr = optimizer.lr / 10\n",
    "\n",
    "        mean_fit = np.array([[\n",
    "            np.mean(fit_vals_mu[len(fit_vals_mu) - 4:len(fit_vals_mu)]),\n",
    "            np.mean(fit_vals_sigma[len(fit_vals_sigma) -\n",
    "                                   4:len(fit_vals_sigma)])\n",
    "        ]])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T12:36:56.535492Z",
     "start_time": "2020-06-09T12:36:55.940004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex=True, constrained_layout=True)\n",
    "fig.suptitle(\n",
    "    \"GaussianAltFit-2D-DetectorEffects (Analytical Reweight):\\n N = {:.0e}, Iterations = {:.0f}\"\n",
    "    .format(N, iterations))\n",
    "axs[0].plot(fit_vals[:, 0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], 0, len(fit_vals), label='$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(fit_vals[:, 1], label='Model $\\sigma$ Fit', color='orange')\n",
    "axs[1].hlines(theta1_param[1], 0, len(fit_vals), label='$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-2D-DetectorEffects (Analytical Reweight):\\n N = {:.0e}, Iterations = {:.0f}.png\"\n",
    "#     .format(N, iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T12:36:57.093814Z",
     "start_time": "2020-06-09T12:36:56.539813Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "\n",
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex=True, constrained_layout=True)\n",
    "fig.suptitle(\n",
    "    \"GaussianAltFit-2D-DetectorEffects Zoomed:\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\"\n",
    "    .format(N, index_refine[1], iterations))\n",
    "axs[0].plot(np.arange(index_refine[1], len(fit_vals[:, 0])),\n",
    "            fit_vals[index_refine[1]:, 0],\n",
    "            label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0],\n",
    "              index_refine[1],\n",
    "              len(fit_vals),\n",
    "              label='$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.arange(index_refine[1], len(fit_vals[:, 1])),\n",
    "            fit_vals[index_refine[1]:, 1],\n",
    "            label='Model $\\sigma$ Fit',\n",
    "            color='orange')\n",
    "axs[1].hlines(theta1_param[1],\n",
    "              index_refine[1],\n",
    "              len(fit_vals),\n",
    "              label='$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "# plt.savefig(\n",
    "#     \"GaussianAltFit-2D-DetectorEffects (Analytical Reweight) Zoomed:\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\"\n",
    "#     .format(N, index_refine[1], iterations))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
