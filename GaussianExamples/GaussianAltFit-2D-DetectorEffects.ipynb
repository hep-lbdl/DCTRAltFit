{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:39:47.084058Z",
     "start_time": "2020-05-31T20:39:45.477415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy.signal import argrelmin, argrelmax\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "import inspect\n",
    "\n",
    "fontP = FontProperties()\n",
    "fontP.set_size('small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:39:47.092560Z",
     "start_time": "2020-05-31T20:39:47.087868Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "#Check Versions\n",
    "print(tf.__version__)  #1.15.0\n",
    "print(keras.__version__)  #2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Alternative DCTR fitting algorithm\n",
    "\n",
    "The DCTR paper (https://arxiv.org/abs/1907.08209) shows how a continuously parameterized NN used for reweighting:\n",
    "\n",
    "$f(x,\\theta)=\\text{argmax}_{f'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f'(x_i,\\theta)+\\sum_{i\\in\\bf{\\theta}}\\log (1-f'(x_i,\\theta)))$\n",
    "\n",
    "can also be used for fitting:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}(\\sum_{i\\in\\bf{\\theta}_0}\\log f(x_i,\\theta')+\\sum_{i\\in\\bf{\\theta}}\\log (1-f(x_i,\\theta')))$\n",
    "\n",
    "This works well when the reweighting and fitting happen on the same 'level'.  However, if the reweighting happens at truth level (before detector simulation) while the fit happens in data (after the effects of the detector), this procedure will not work.  It works only if the reweighting and fitting both happen at detector-level or both happen at truth-level.  This notebook illustrates an alternative procedure:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_i)-\\sum_{i\\in\\bf{\\theta}}(f(x_i,\\theta')/(1-f(x_i,\\theta')))\\log (1-g(x_i)))$\n",
    "\n",
    "where the $f(x,\\theta')/(1-f(x,\\theta'))$ is the reweighting function.  The intuition of the above equation is that the classifier $g$ is trying to distinguish the two samples and we try to find a $\\theta$ that makes $g$'s task maximally hard.  If $g$ can't tell apart the two samples, then the reweighting has worked!  This is similar to the minimax graining of a GAN, only now the analog of the generator network is the reweighting network which is fixed and thus the only trainable parameters are the $\\theta'$.  The advantage of this second approach is that it readily generalizes to the case where the reweighting happens on a different level:\n",
    "\n",
    "$\\theta^*=\\text{argmax}_{\\theta'}\\text{min}_{g}(-\\sum_{i\\in\\bf{\\theta}_0}\\log g(x_{D,i})-\\sum_{i\\in\\bf{\\theta}}\\frac{f(x_{T,i},\\theta')}{(1-f(x_{T,i},\\theta'))}\\log (1-g(x_{D,i})))$\n",
    "\n",
    "where $x_T$ is the truth value and $x_D$ is the detector-level value.  In simulation (the second sum), these come in pairs and so one can apply the reweighting on one level and the classification on the other.  Asympotitically, both this method and the one in the body of the DCTR paper learn the same result: $\\theta^*=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parametrize our network by giving it $\\mu$ and $\\sigma$ values in addition to $X_i\\sim\\mathcal{N}(\\mu, \\sigma)$.\n",
    "\n",
    "First we uniformly sample $\\mu$ and $\\sigma$ values in some range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:39:47.234850Z",
     "start_time": "2020-05-31T20:39:47.096238Z"
    }
   },
   "outputs": [],
   "source": [
    "n_data_points = 10**7\n",
    "mu_min = -2\n",
    "mu_max = 2\n",
    "mu_values = np.random.uniform(mu_min, mu_max, n_data_points)\n",
    "\n",
    "sigma_min = 0.5\n",
    "sigma_max = 4.5\n",
    "sigma_values = np.random.uniform(sigma_min, sigma_max, n_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then sample from normal distributions with these $\\mu$ and $\\sigma$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:40:08.120736Z",
     "start_time": "2020-05-31T20:39:47.858753Z"
    }
   },
   "outputs": [],
   "source": [
    "X0 = [(np.random.normal(0, 1), mu_values[i], sigma_values[i])\n",
    "      for i in range(n_data_points)]  # Note the zero in normal(0, 1)\n",
    "X1 = [(np.random.normal(mu_values[i],\n",
    "                        sigma_values[i]), mu_values[i], sigma_values[i])\n",
    "      for i in range(n_data_points)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the samples in X0 are not paired with $\\mu=0, \\sigma = 1$ as this would make the task trivial. \n",
    "\n",
    "Instead it is paired with the $\\mu, \\sigma$ values uniformly sampled in the specified range [$\\mu_{min}, \\mu_{max}$] and [$\\sigma_{min}, \\sigma_{max}$].\n",
    "\n",
    "For every value of $\\mu$ in mu_values and every value of $\\sigma$ in sigma_values, the network sees one event drawn from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and it learns to classify them. \n",
    "\n",
    "I.e. we have one network that's parametrized by $\\mu$ and $\\sigma$ that classifies between events from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(\\mu,\\sigma)$, and a trained network will give us the likelihood ratio to reweight from one to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:40:12.722128Z",
     "start_time": "2020-05-31T20:40:08.124567Z"
    }
   },
   "outputs": [],
   "source": [
    "Y0 = to_categorical(np.zeros(n_data_points), num_classes=2)\n",
    "Y1 = to_categorical(np.ones(n_data_points), num_classes=2)\n",
    "\n",
    "X = np.concatenate((X0, X1))\n",
    "Y = np.concatenate((Y0, Y1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:40:12.802489Z",
     "start_time": "2020-05-31T20:40:12.726110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((3, ))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs=inputs, outputs=outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:02.958750Z",
     "start_time": "2020-05-31T20:40:12.806561Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 8000000 samples, validate on 2000000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "8000000/8000000 [==============================] - 13s 2us/step - loss: 0.5122 - val_loss: 0.5051\n",
      "Epoch 2/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5050 - val_loss: 0.5049\n",
      "Epoch 3/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - val_loss: 0.5050\n",
      "Epoch 4/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5049 - val_loss: 0.5051\n",
      "Epoch 5/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5048 - val_loss: 0.5049\n",
      "Epoch 6/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5048 - val_loss: 0.5051\n",
      "Epoch 7/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5048 - val_loss: 0.5052\n",
      "Epoch 8/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5048 - val_loss: 0.5049\n",
      "Epoch 9/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5047 - val_loss: 0.5047\n",
      "Epoch 10/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5047 - val_loss: 0.5050\n",
      "Epoch 11/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5047 - val_loss: 0.5050\n",
      "Epoch 12/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5047 - val_loss: 0.5048\n",
      "Epoch 13/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5047 - val_loss: 0.5048\n",
      "Epoch 14/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5047 - val_loss: 0.5055\n",
      "Epoch 15/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5047 - val_loss: 0.5048\n",
      "Epoch 16/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5047 - val_loss: 0.5050\n",
      "Epoch 17/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5046 - val_loss: 0.5049\n",
      "Epoch 18/200\n",
      "8000000/8000000 [==============================] - 12s 2us/step - loss: 0.5046 - val_loss: 0.5049\n",
      "Epoch 19/200\n",
      "8000000/8000000 [==============================] - 12s 1us/step - loss: 0.5046 - val_loss: 0.5048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f876c4f0f90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "dctr_model.fit(X_train,\n",
    "               Y_train,\n",
    "               epochs=200,\n",
    "               batch_size=10000,\n",
    "               validation_data=(X_test, Y_test),\n",
    "               callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:02.974343Z",
     "start_time": "2020-05-31T20:44:02.961988Z"
    }
   },
   "outputs": [],
   "source": [
    "theta0_param = (0, 1)\n",
    "\n",
    "\n",
    "def reweight(d):  #from NN (DCTR)\n",
    "    f = dctr_model(d)\n",
    "    weights = (f[:, 1]) / (f[:, 0])\n",
    "    weights = K.expand_dims(weights, axis=1)\n",
    "    return weights\n",
    "\n",
    "\n",
    "# from analytical formula for normal distributions\n",
    "def analytical_reweight(events,\n",
    "                        mu1,\n",
    "                        sigma1,\n",
    "                        mu0=theta0_param[0],\n",
    "                        sigma0=theta0_param[1]):\n",
    "    weights = (sigma0 / sigma1) * K.exp(-0.5 * (((events - mu1) / sigma1)**2 -\n",
    "                                                ((events - mu0) / sigma0)**2))\n",
    "    #weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate DCTR for any $\\mu$ and $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate for Truth Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:03.560517Z",
     "start_time": "2020-05-31T20:44:02.978348Z"
    }
   },
   "outputs": [],
   "source": [
    "mu1 = 1.5\n",
    "sigma1 = 1.5\n",
    "assert mu1 >= mu_min and mu1 <= mu_max  # choose mu1 in valid range\n",
    "assert sigma1 >= sigma_min and sigma1 <= sigma_max  # choose mu1 in valid range\n",
    "X0_val_T = np.random.normal(0, 1, n_data_points)\n",
    "X1_val_T = np.random.normal(mu1, sigma1, n_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:06.611879Z",
     "start_time": "2020-05-31T20:44:03.564626Z"
    }
   },
   "outputs": [],
   "source": [
    "X_input = np.array([(x, mu1, sigma1) for x in X0_val_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:08.779354Z",
     "start_time": "2020-05-31T20:44:06.615750Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = reweight(tf.convert_to_tensor(X_input, dtype=tf.float32))\n",
    "analytical_weights = analytical_reweight(X0_val_T, mu1, sigma1)\n",
    "weights = K.eval(weights)\n",
    "analytical_weights = K.eval(analytical_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:11.019148Z",
     "start_time": "2020-05-31T20:44:08.783482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmYVdWV9/HvEoTSgCKTAUoobAlhEJAqESQMQgREEG2NiokSUct0NK9EOwnRNhRKErtDRHlNTCpKK3kV4hiBFhRFbBNFpRBQcaCiEAoQFZFBQUXW+8fdVd6az63p1PD7PM99uHedfc5eh4Ja90x7m7sjIiISxWFxJyAiIg2HioaIiESmoiEiIpGpaIiISGQqGiIiEpmKhoiIRKaiIfWemZ1gZvX23vD6nl9JZtbMzPaZWdcIbZubmZtZRgrbv8vMrq9OjlJ/qWhItYRfPoWvQ2a2P+nzd6u4zQIzG1mNnP6fmeVUdf3aFvav8O/pPTObZ2Zfq6v+3f1Ld2/l7v+s7rbM7HIzW1li+5e7+6+qu22pn1Q0pFrCL59W7t4K+CcwMSl2X8n2Zta87rOsl84If2cDgUHAT2PORyQSFQ2pVWY2y8z+YmYLzGwv8L2SRwJm9m0z2xTeLwA6A0vDN/Frk9pdEr6lf2Bm06uYT28ze8rMPjKzN83s3BAfamZbzeywpLbfMbM14f1hZna9mf3DzD40s4VmdkxVckjm7tuAJ4EBSf2mmdmtZrbFzHaY2e/NLC0s+7uZTQrvR4RTR2PD57FmtjppO5eHfdxlZkvN7LgQL3bKycw6mNn/mNkeM3vJzH5V8ugBGGtm+WFbc8N6JwJ3AMPCz+rDEC/6+Rb+bM3sp+Hnts3MLknKMUrfUo+oaEhdOAe4Hzga+EtFDd19MrCN8E3c3W9NWnwqcAIwFphpZj1SScLMWgHLgflAR+C7QK6Z9QSeB74ARiStclHIG+DHwJnAcCAd2AfMLaefG8zsrxFzOg4YB+QnhX8DdAf6AT2ADOCGsOxZYGR4PwJ4J+RU+PnZsN1zgZ8Ak4AOwItJ+1LSncDHwLHAVGBKGW3GA5nASSQK/7fd/VXgauC58LNqX87204EjSHwZ+AFwp5kdlULfUo+oaEhd+Ju7L3b3Q+6+vxrbyXH3A+6+Bngd6J/i+pOAt919vrsfdPc84K/AeZ4YhG0hMBnAzNqQKE4Lw7o/AK53963ufgCYCXwn+cikkLv/0t3PriSXJeHI659AAXBT6Pcw4Apgmrvvcvc9wK+BC8N6z/JVYRselhV+LioaId9fuftb7n4QmAUMMrMuyUmY2eHA2cAv3H2/u78G/LmMfH/t7rvdfROwkqQjowgOALPc/Qt3XwR8Bnwjhb6lHlHRkLqwpSY24u7vJX38FGiV4ia6AUPN7OPCF3AB0Cksvx84N/wyOxd40d0LwrKuwOKk9V4N8Y5V2Rdggru3BkYDvYG2If51oCWwLqmvJUn9/B3oY2YdgL7AvcDxZtaOxJHAc0n7+rukbXwIHCLxrT/ZsUAziv+Myvp5Vefv/kN3/7KM9aP2LfWIiobUhZK3o34CHJn0+euVtK8pW4Cn3b1N0quVu18N4O7rSfxyHEvxU1OQOBo4vcS6aSUKWcrcfQVwH4lTUgA7gM+Bnkn9HO3uR4f2+4C1JE6XrXX3L0iceroOeNPddyXt62Ul8j3C3V8skcIOSheT41LZhRTallTdviUGKhoSh7XAmWZ2jJl1Av5PieU7gOOr2UfzcEG58NUCWETiW/pFZnZ4eA0K1zQK3U/iF/IQ4KGk+B+AX1l4tsHMOprZWdXMsdAcYLyZ9Q3fyO8CbgsXic3M0s1sTFL7Z0lcSyg8FbWyxOfCfG8ws14h3zZmdl7JjkPR+SuJa0RHmFkf4Hsp5L4DSA9HZympgb4lBioaEod7gDeAzcAyvrpuUOhXJH6RfGxm06rYxw3A/qTXk+6+m8RRxPeA7SSOKn5N4nRQofuBUcDypG/tALeGXJ8O1yKeB04uq2Mzu9HMFkdNNByt3AfcGELXkfi7eQnYTeLuquSL/s8CrYH/Lecz7v5gyPlBM9sDrA/7XpZ/A9qRKAD/DSwgcd0hiuXARmCHmVXlqKs6fUsMTJMwiUgyM/st0MbdL2tKfUs0OtIQaeIs8ezKieFU2GDgUuDRxt63VI2ezhWRo0icHutE4jTRLe6+pAn0LVWg01MiIhKZTk+JiEhkje70VPv27T0jIyPuNEREGpS8vLwP3b1DZe0aXdHIyMhg9erVlTcUEZEiZrY5SrtIp6fM7Mdm9rqZvWaJ0UrTzKy7mb0YRr78S3h4CjNrGT7nh+UZSdv5eYi/ZWFkzhAfF2L5ljR6aXl9iIhIPCotGmGAs/8DZLl7XxJjxVwI/Ccwx91PAHYBhfdVXwbsCvE5oR1m1jus14fEqJ6/t8QMYs2A3wFnkBiDZ3JoSwV9iIhIDKJeCG8OHGGJCXSOJPE07Si+GmbhXhKjVUJiJNF7w/uHgNFmZiG+0N0/c/d3SQwFPSi88t39HXf/nMTTwZPCOuX1ISIiMaj0moa7bzWz2SSGcN5PYkiDPODjMOQyJAZzKxxyuQthpEp3P2hmu0kME9AFWJW06eR1tpSInxLWKa+PYswsG8gG6Nq19LTHX3zxBQUFBRw4cKCy3RWJRVpaGunp6Rx+eMpDOInUqUqLhiVmJ5tEYlKYj4EHSZxeqjfcPRfIBcjKyir14ElBQQGtW7cmIyODxAGMSP3h7uzcuZOCggK6d+8edzoiFYpyeurbwLvu/kEYlfIRYCjQxr6a7zkd2BrebyUMbxyWHw3sTI6XWKe8+M4K+kjJgQMHaNeunQqG1EtmRrt27XQkLA1ClKLxT2CwmR0ZrjOMBjYAzwCFQy1PAR4L7xfx1ZSN5wErwqxoi4ALw91V3UmM2vkS8DLQI9wp1YLExfJFYZ3y+kiZCobUZ/r3KQ1FpUUjTNryELCGxGxlh5E4FfQz4Fozyydx/eHusMrdQLsQvxaYHrbzOvAAiYKzDLjK3b8M1yyuBp4gMVz2A6EtFfQhIiIxaHRjT2VlZXnJh/veeOMNevXqFVNGItHo36nEyczy3D2rsnaN7olwkbq2Y0/NXIvYs/8L5ix/mx+f/o0a2Z5IbWiSRWPO8rdrdHt1/Z982bJlXHPNNXz55ZdcfvnlTJ8+vfKVatDUqVNZsmQJHTt25LXXXqvTvkUkXhrltoH58ssvueqqq1i6dCkbNmxgwYIFbNiwoU5z+P73v8+yZcvqtE8RqR9UNOrQyJEjefPNNwHYuXMnffv2TXkbL730EieccALHH388LVq04MILL+Sxxyq/qWzdunUMHz6c3r17c9hhh2Fm/OIXv0i5f4Dhw4fTtm3bKq0rIg1bkzw9FZf8/Hy+8Y3Eqaz169dz4oknFls+bNgw9u7dW2q92bNn8+1vfxuArVu3ctxxXz3Wkp6ezosvvlhhvwcOHOCCCy5g/vz5DBo0iBtvvJEDBw4wc+bMlPoWEVHRqCObN2+mS5cuHHZY4uBu/fr19OvXr1ib5557rlb6fuqppxg4cCCDBg0CoF+/fixbtqzYswG11beINC4qGnVk3bp1xYpEXl4eF1xwQbE2Ub7td+nShS1bvhqqq6CggC5dyhySq8hrr71W7KhmzZo1DBw4MOW+RURUNOrI2rVri4aJ2LhxI4899hizZs0q1ibKt/2TTz6ZjRs38u6779KlSxcWLlzI/fffX7R89OjRzJ8/v1ghadeuHStWrADg7bff5pFHHuH5559PuW8RkSZZNOK4D37dunWkpaXRv39/+vXrR+/evbn33nu58cYbU9pO8+bNueOOOxg7dixffvklU6dOpU+fPgAcOnSI/Pz8UhepJ0+ezKJFi+jbty/t27dnwYIFtGvXrsr7MnnyZFauXMmHH35Ieno6M2fO5LLLNNWJSFPQJItGHNavX8+aNWto3bp1tbc1fvx4xo8fXyq+YcMGzj33XI444ohi8VatWrF48eJq91towYIFNbYtEWlYdMttHdi7dy9mViMFoyJ9+/bl1ltvrdU+RKRpU9GoA61bt+btt2v2KXQRkTioaIiISGQqGiIiEpmKhoiIRKaiISIikaloiIhIZJUWDTPraWZrk157zGyambU1s+VmtjH8eUxob2Y218zyzWy9mQ1M2taU0H6jmU1Jimea2athnblhLnLK60NEROIRZY7wt9x9gLsPADKBT4FHScz9/bS79wCeDp8BzgB6hFc2cCckCgAwAzgFGATMSCoCdwJXJK03LsTL60NERGKQ6ump0cA/3H0zMAm4N8TvBc4O7ycB8z1hFdDGzDoBY4Hl7v6Ru+8ClgPjwrKj3H2VJyYsn19iW2X1ISIiMUi1aFwIFI4hcay7bw/v3wOODe+7AFuS1ikIsYriBWXEK+qjWjIyMjCzGntlZGTURFqRLVu2jJ49e3LCCSdwyy231GnfkJjutWPHjhVOImVmXHfddUWfZ8+eTU5OTuTl1fXxxx/z+9//vsa2JyIJkYuGmbUAzgIeLLksHCF4DeZVSkV9mFm2ma02s9UffPBBpdvavHkz7l5jr82bN9f07paroUz32rJlSx555BE+/PDDKi2vLhUNkdqRypHGGcAad98RPu8Ip5YIf74f4luB45LWSw+xiuLpZcQr6qMYd8919yx3z+rQoUMKu1S3mtJ0r82bNyc7O5s5c+ZUaTnAb37zG+bOnQvAj3/8Y0aNGgXAihUr+O53vwvAzTffTM+ePfnWt77F5MmTmT17NgDTp0/nH//4BwMGDOAnP/lJyvsoImVLpWhM5qtTUwCLgMI7oKYAjyXFLwl3UQ0GdodTTE8AY8zsmHABfAzwRFi2x8wGh7umLimxrbL6aJCiTPc6YMCAUq+nnnqqqE1Z071u3bqVihRO9zp79mw2bNjADTfcwL//+7+Xmu61sr5TddVVV3Hfffexe/fuKi0fNmxY0Twfq1evZt++fXzxxRc899xzDB8+nJdffpmHH36YdevWsXTpUlavXl207i233MK//Mu/sHbtWn7zm99UeR9EpLhIQ6Ob2deA04Erk8K3AA+Y2WXAZuD8EH8cGA/kk7jT6lIAd//IzG4GXg7tbnL3j8L7HwL3AEcAS8Oroj4anKY43etRRx3FJZdcwty5c0sN1x5leWZmJnl5eezZs4eWLVsycOBAVq9ezXPPPcfcuXN58sknmTRpEmlpaaSlpTFx4sQa3wcRKS5S0XD3T4B2JWI7SdxNVbKtA1eVs515wLwy4quBUudqyuujIWqq071OmzaNgQMHcumll6a8/PDDD6d79+7cc889nHrqqfTr149nnnmG/Px8evXqxZNPPlnlvESkavREeB0pa7rXkqennnvuOdauXVvqlfxLO3m6188//5yFCxdy1llnFS0fPXp0qdNV7dq1Y/369cBX071eeOGFKfddFW3btuX888/n7rvvrtLyYcOGMXv2bIYPH86wYcP4wx/+wEknnYSZMXToUBYvXsyBAwfYt28fS5YsKVqvdevWZRZBEameJlk0unXrVqO33Hbr1q3SPtetW8ehQ4fo378/N910U9F0r6lKnu61V69enH/++ZGme923bx99+/YlOzu7RqZ7HTJkCG+99Rbp6enl/sIvdN1111V4l1RFy4cNG8b27dsZMmQIxx57LGlpaQwbNgxIFNCzzjqLfv36ccYZZ3DiiSdy9NFHA4lCOXToUPr27Vt0IXz8+PFs27atKrssIoElziY1HllZWZ58QRTgjTfeoFevXjFllNCjR48am+61PK+99hrz5s1rUrP37du3j1atWvHpp58yfPhwcnNzS516q2079hyoke1syn+b53emxTKHvYiZ5bl7VmXtNEd4HdB0r7UnOzubDRs2cODAAaZMmVLnBUOkqVHRqAOa7rX23H///bW27Zo6gkjVnOXR/63oqETqWpO8piEiIlWjoiEiIpGpaIiISGQqGiIiEpmKhoiIRKaiISIikaloiIhIZCoaIiISWZMtGtnZ2cXGj9q2bRuLFy8uFsvNzQUoFiscfnvixInF4nWpIUz3WpNatWpV7rKyZug79dRTa6WvVO3++GP++09/rLHtidQHTbJoZGZmkpubW2zK1s6dOzNx4sRisezsbIBiscWLFwOwePHiYvG60lCme60rZRWN559/PqZsitu9ezf33J0bdxoiNapJFo01a9bU6PYKj0gq05Smez377LPJzMykT58+RX8/mzZtolevXlxxxRX06dOHMWPGsH///grXSfaLX/yC2267rejzDTfcwEknnVRqWtfko4X58+fTr18/+vfvz8UXXxy5r2S/u/1W7vrD7xI5/PwnnDthHAB/e3YlP7z8+wDc+l+/ZmhmP84aO4ofTL2E38+dwy9z/oPN777D6G+dwsz/+HmFfYg0FBp7qgZceeWVRUclFYky3WtlEyGVNd3riy++WGG/hdO9zp8/n0GDBnHjjTdy4MCBUtO91uQkTPPmzaNt27bs37+fk08+mXPPPRdIzCWyYMEC/vSnP3H++efz8MMP873vfa/cdZKHcJ86dSr/+q//yrRp0zh06BALFy7kqaeeYsqUKaxdu7ZUDq+//jqzZs3i+eefp3379nz00UdFyyrrK9ngU4dy5/+9nct/cBXrXlnDZ599xhdffMGqF/7O4FO/xSt5q/mfRX/l6b+/xMEvvuD04UPoN+AkbsiZxZtvbODpv1X88xFpSJpk0ejUqVOd99nUpnudO3cujz76KABbtmxh48aNfP3rX6d79+4MGDAASJwm3LRpU4XrJP8iz8jIoF27drzyyivs2LGDk046qcJ5QVasWMF3vvMd2rdvD1Ds6KiyvpL1GzCQ9WtfYe+ePbRo0ZIT+w9g3St5vPj835n1X7/l2RVPMXb8BNLS0iAtjTFnjK/C35hIwxB1jvA2wF0kpmR1YCrwFvAXIAPYBJzv7rss8ZvodhLzhH8KfN/d14TtTAH+I2x2lrvfG+KZfDVH+OPANe7uZta2rD6qs8NALBPxNKXpXleuXMlTTz3FCy+8wJFHHsnIkSOLZi1s2bJlUbtmzZoVnZ6qaJ1kl19+Offccw/vvfceU6dOTSmvKPmV5fDDD6drtwz+cv+fyTplML379OXv//u/vPvuP/hGz2/y7IqnqpSHSEMU9ZrG7cAyd/8m0B94A5gOPO3uPYCnw2eAM4Ae4ZUN3AkQCsAM4BRgEDDDzI4J69wJXJG03rgQL6+PasnJyamJzRRZtGhRpW2a0nSvu3fv5phjjuHII4/kzTffZNWqVTW2zjnnnMOyZct4+eWXGTt2bIXTuo4aNYoHH3yQnTt3AhSdnqpKfqecOpQ7/+9tDDn1Www+dSjz//tPnNivP2bGyacMYfnSxzlw4ACf7NvH8mVLAWjVuhX79mnKWWlcKi0aZnY0MBy4G8DdP3f3j4FJQOF8pfcCZ4f3k4D5nrAKaGNmnYCxwHJ3/ygcLSwHxoVlR7n7Kk/chjS/xLbK6qNaZs6cWex22by8PPLy8orFCgtL586di2KZmZlA6dt1C+MVaUrTvY4bN46DBw/Sq1cvpk+fzuDBgyvdZtR1WrRowWmnncb5559Ps2bNypzWtVCfPn244YYbGDFiBP379+faa6+tcn6Dhwxlx3vvkTnoFDp0PJaWLdM4ZchQAE7KzGLM+DMZderJXHTeJHr17sNRRx1N27btGHTKEEYMziy6EH7ReWfz3nZNOSsNV6XTvZrZACAX2EDiKCMPuAbY6u5tQhsDdrl7GzNbAtzi7n8Ly54GfgaMBNLcfVaI3wjsB1aG9t8O8WHAz9x9gpl9XFYfZeSYTeKohq5du2Zu3ry52HJN99p4HDp0iIEDB/Lggw/So0ePWu0rlUmYPtm3j6+FaWfPPuN0Zt9+B/0GnJRSf4XTvaZCkzBJTYk63WuU01PNgYHAne5+EvAJJU4ThSOEWn1YoaI+3D3X3bPcPatDhw61mUaVaLrXmrFhwwZOOOEERo8eXesFI1X/fs1VjP7WKYwZPoQJZ52dcsEQaSiiXAgvAArcvfC+wYdIFI0dZtbJ3beHU0zvh+VbgeOS1k8Psa0kjjaS4ytDPL2M9lTQR4Oi6V5rRu/evXnnnXfiTqNMd96d+qlGkYao0iMNd38P2GJmPUNoNIlTVYuAKSE2BSh8wmwRcIklDAZ2u/t24AlgjJkdEy6AjwGeCMv2mNngcArqkhLbKqsPERGJQdTnNH4E3GdmLYB3gEtJFJwHzOwyYDNwfmj7OInbbfNJ3HJ7KYC7f2RmNwMvh3Y3uXvh01Y/5KtbbpeGF8At5fQhIiIxiFQ03H0tUNYFktFltHXgqnK2Mw+YV0Z8NYlnQErGd5bVh4iIxKNJjj0lIiJVo6IhIiKRqWiIiEhkKhoiIhKZioaIFMnIyCg2RE55r4yMjLhTlZg0yaHReebXNbu90+p2gp2pU6eyZMkSOnbsyGuvvVZuu4yMDFq3bk2zZs1o3rw5q1evVo5Soc2bN0eaibKupziW+kNHGg1QKtOtPvPMM6xdu7bOfxk3hBxFJHUqGnWoJqZ7hWjTrVZVTU0LW5s5ikh8mubpqZjUxHSvqTAzxowZg5lFmpI2jmlhU81RROKlolFH4pju9W9/+xtdunTh/fff5/TTT+eb3/wmw4cPL7d9HNPCppqjVE1GRgYlpwwoS7du3eogG2nIVDTqSE1N95qKwmlgO3bsyDnnnMNLL71U4S/kup4Wtio5StVEvcAtUhkVjTpS1nSvs2bNKtamJr/Ff/LJJxw6dIjWrVvzySef8OSTTxZdmxg9ejTz588vNbd4u3btWLFiBfDVtLDPP/98LDmKSP3UNItGHd8iC4kjjbS0NPr370+/fv2Kpnu98cYbU97W5MmTWblyJR9++CHp6enMnDmTyy67DIDx48dz1113ceDAAc455xwADh48yEUXXcS4cePKnRK2cLuLFi2ib9++tG/fvlrTwlYnRxGpvyqd7rWhycrK8pK3bjaV6V6jaApTwtaUVKZ7rQm1Od1rzsivtjty5AhGjhjJb2/9LXv37gOgU6evc2X2lWQveJfc3NxKt2dmOt3VyESd7rVpHmnUsbqa7jWKxj4lrJQvZ8aMYp+vu/a6Um3+9Kc/RSoa0nSpaNQBTfcqDcWMES1rfsQEaVRUNESkmL1797Jt+zYWLFhYFJs4YQKZmZnkJD2zI02TioaIFCk8hdWzdc9Sp7OSl88cdX2d5iX1R6RhRMxsk5m9amZrzWx1iLU1s+VmtjH8eUyIm5nNNbN8M1tvZgOTtjMltN9oZlOS4plh+/lhXauoj6rQRTupz/TvUxqKVMaeOs3dByRdXZ8OPO3uPYCnw2eAM4Ae4ZUN3AmJAgDMAE4BBgEzkorAncAVSeuNq6SPlKSlpbFz5079x5R6yd3Zt3sX+w6mPnJs1KHM09Ja1kLm0hRV5/TUJGBkeH8vsBL4WYjP98Rv6FVm1sbMOoW2y939IwAzWw6MM7OVwFHuvirE5wNnA0sr6CMl6enpFBQU8MEHH6S6qjRhe/Z/UWd97TtovLHn8JTXKzk0yBVXXEFubi6ZmZmsWbMGgE6dOjH9Zz+qkTxFohYNB540Mwf+6O65wLHuvj0sfw84NrzvAmxJWrcgxCqKF5QRp4I+ijGzbBJHNXTt2rXU8sMPP5zu3btXvpciSeYsr/93vM0Y0bL0tYdnfk3e7POA82q17yhzanTr1o1NmzbVah5St6IWjW+5+1Yz6wgsN7M3kxe6u4eCUmsq6iMUsVxIPNxXm3mISIIma2qaIl3TcPet4c/3gUdJXJPYEU47Ef58PzTfChyXtHp6iFUUTy8jTgV9iEiMdH2w6aq0aJjZ18ysdeF7YAzwGrAIKLwDagrwWHi/CLgk3EU1GNgdTjE9AYwxs2PCBfAxwBNh2R4zGxzumrqkxLbK6kNEYqSnxpuuKKenjgUeDYeZzYH73X2Zmb0MPGBmlwGbgfND+8eB8UA+8ClwKYC7f2RmNwMvh3Y3FV4UB34I3AMcQeIC+NIQv6WcPkSqrCFcq0jFC+/srLTNkOOrNvBkeTRhVtNVadFw93eA/mXEdwKjy4g7cFU525oHzCsjvhooNfdpeX2IiEg8NEe4iIhEpmFERCRlb/5hCnsXXc9vk0ZMzswcyMQJE/lj7h/Zvv29GLOT2qSiISIp6/mNnkDp4dYBrsy+sui9xqhqfFQ0ROqZmy8exa4dWytvCNw06mu1nI1IcSoaIvXMrh1bufXJtyK1HfxP3foqdUsXwkUasDXPLIk7BWliVDREGrAPCjbFnYI0MSoaIiISmYqGiIhEpgvhIvXMjBEtaf3AhRRs3FAUG3nu99mz8wPWrPyfolifwSMZfPHVcaQoTZiKhkg91GfwKPoMHlUs1uHIVoxtgEVC8240LioaIlKrNO9G46JrGiJSa2aU8cS4NGwqGiJSa3JycuJOQWqYioaI1JrOnTvHnYLUMBUNEak127dvjzsFqWEqGiIiElnkomFmzczsFTNbEj53N7MXzSzfzP5iZi1CvGX4nB+WZyRt4+ch/paZjU2KjwuxfDObnhQvsw8RaRgGDhwYdwpSw1I50rgGeCPp838Cc9z9BGAXcFmIXwbsCvE5oR1m1hu4EOgDjAN+HwpRM+B3wBlAb2ByaFtRHyLSAOTl5cWdgtSwSM9pmFk6cCbwS+BaS9xUPQq4KDS5F8gB7gQmhfcADwF3hPaTgIXu/hnwrpnlA4NCu/wwFzlmthCYZGZvVNCHiDQAi687lYkTJsadhtSgqEcatwE/BQ6Fz+2Aj939YPhcAHQJ77sAWwDC8t2hfVG8xDrlxSvqoxgzyzaz1Wa2+oMPPoi4SyJS2/Ly1sSdgtSwSo80zGwC8L6755nZyNpPKXXungvkAmRlZVX++KmI1Ik2bY4mZ+ZMrrv2WrZt38aCBQuLlk2cMIHMzExyZs6MMUNJVZTTU0OBs8xsPJAGHAXcDrQxs+bhSCAdKJyfcitwHFBgZs2Bo4GdSfFCyeuUFd9ZQR8i0gBMu2Za0fuerXuWOad4zowZmku8Aan09JS7/9zd0909g8SF7BXu/l3gGeC80GwK8Fh4vyh8Jixf4YnBZxYBF4a7q7q7rpb8AAAQdUlEQVQDPYCXgJeBHuFOqRahj0VhnfL6EBGRGFRnwMKfAQvNbBbwCnB3iN8N/Dlc6P6IRBHA3V83sweADcBB4Cp3/xLAzK4GngCaAfPc/fVK+hBpcG6+eBS7dlR+sHzz6a3rIBuRqkmpaLj7SmBleP8OX939lNzmAPCdctb/JYk7sErGHwceLyNeZh8iDdGuHVu59cm3Km03+J+5dZCNSNXoiXCRemblQ/PiTkGkXCoaIvXMZ/s/jTsFkXKpaIiISGSauU+kjswY0TLS9YoX2naog2zql27dumla2AZCRUOkDj3x5zuK3g8ceSZHtevAyofvKYql9+jNkDMviCGzeF1//fVkZ2dX2k7TwsZPRUOkDo29+OpIsabmyiuvjFQ0JH66piEiIpGpaIiISGQqGiISu0WLFsWdgkSkoiEiscvMzIw7BYlIRUNEYtelS5lT5Ug9pKIhIiKRqWiIiEhkKhoiErsrrrgi7hQkIhUNEYldbq6Gg28o9ES4iMTuj5MzuDL7yrjTkAhUNEQkdvv37ydn5kxat27Fdddex8pnV7Jy5bNFy7OzdfqqvlDREJHYTbtmWrHPI0eMZOSIkfEkIxWq9JqGmaWZ2Utmts7MXjezmSHe3cxeNLN8M/uLmbUI8Zbhc35YnpG0rZ+H+FtmNjYpPi7E8s1selK8zD5ERCQeUS6EfwaMcvf+wABgnJkNBv4TmOPuJwC7gMtC+8uAXSE+J7TDzHoDFwJ9gHHA782smZk1A34HnAH0BiaHtlTQh4iIxKDSouEJ+8LHw8PLgVHAQyF+L3B2eD8pfCYsH22JQfAnAQvd/TN3fxfIBwaFV767v+PunwMLgUlhnfL6EBGRGES6phGOBvKAE0gcFfwD+NjdD4YmBUDhOABdgC0A7n7QzHYD7UJ8VdJmk9fZUiJ+SlinvD5K5pcNZAN07do1yi5JIzRn+dux9HvzxaPYtWNrpe1+fUbbOsimbC+8szNSuyHHt6vlTKShi1Q03P1LYICZtQEeBb5Zq1mlyN1zgVyArKwsjzkdaWJ27djKrU++VWm73b/sVwfZiNSulB7uc/ePgWeAIUAbMyssOulA4VetrcBxAGH50cDO5HiJdcqL76ygDxERiUGUu6c6hCMMzOwI4HTgDRLF47zQbArwWHi/KHwmLF/h7h7iF4a7q7oDPYCXgJeBHuFOqRYkLpYvCuuU14eIiMQgyumpTsC94brGYcAD7r7EzDYAC81sFvAKcHdofzfwZzPLBz4iUQRw99fN7AFgA3AQuCqc9sLMrgaeAJoB89z99bCtn5XTh0iDk96jd+WNROo5S3yhbzyysrJ89erVcachMYjrQnjJaxX/0u9kTuh/Cisfmsdn+z8F4Ki2HRhy5gVxpJeS+n4h3EZdT2P7nVVfmFmeu2dV1k5PhIvUgLEXX10qNvK8qTFkIlK7NMqtiIhEpqIhIiKRqWiIiEhkKhoiIhKZioaIiESmoiEiIpGpaIiISGQqGiLSYLRt2xYzi/TKyMiIO91GSUVDRBqMnTuLD/E+YcIE3J0JEyYUi7s7mzdvrsvUmgw9ES4iDcczv8ZX/KpUbPG1p8K1pxaLSe3QkYaIiESmoiEiIpGpaIiISGQqGiIiEpmKhoiIRKaiISIikUWZI/w4M3vGzDaY2etmdk2ItzWz5Wa2Mfx5TIibmc01s3wzW29mA5O2NSW032hmU5LimWb2alhnrplZRX2IiEg8ojyncRC4zt3XmFlrIM/MlgPfB55291vMbDowncSc3mcAPcLrFOBO4BQzawvMALIAD9tZ5O67QpsrgBeBx4FxwNKwzbL6EKl1N188il07tlbabsaIlnWQjUj9UGnRcPftwPbwfq+ZvQF0ASYBI0Oze4GVJH6hTwLme2Ii31Vm1sbMOoW2y939I4BQeMaZ2UrgKHdfFeLzgbNJFI3y+hCpdbt2bOXWJ9+qtF3JOcJFGrOUrmmYWQZwEokjgmNDQQF4Dzg2vO8CbElarSDEKooXlBGngj5E6o0+g0fGnYJInYk8jIiZtQIeBqa5+55w2QEAd3cz81rIL1IfZpYNZAN07dq1NtOQJmTGiJbFjiI6pGcw8LQJrHlmCR8UbCqKD7746hiyE4lHpKJhZoeTKBj3ufsjIbzDzDq5+/Zw+un9EN8KHJe0enqIbeWrU02F8ZUhnl5G+4r6KMbdc4FcgKysrFotXtK0jC2jIAw8bUIZLUWahih3TxlwN/CGu9+atGgRUHgH1BTgsaT4JeEuqsHA7nCK6QlgjJkdE+6CGgM8EZbtMbPBoa9LSmyrrD5ERCQGUY40hgIXA6+a2doQux64BXjAzC4DNgPnh2WPA+OBfOBT4FIAd//IzG4GXg7tbiq8KA78ELgHOILEBfClIV5eHyIiEoMod0/9DbByFo8uo70DV5WzrXnAvDLiq4G+ZcR3ltWHiIjEQ0+Ei4hIZCoaIiISmYqGiDRaeXl55OXlFZs7PCcnB4DOnTtrLvEq0HSvItJoZWZmAok5w0vatm1bsTYSjYqGiDRKM0a0jDRXeF5eXh1k03ioaEi9Nmf523Gn0KS88M7OSO2GHN+uljOpvjZtjiZn5sxK2/32rF+xd+/eOsiocVDREJFGado10yK1mznq+lrOpHHRhXAREYlMRUNERCJT0RARkchUNEREJDIVDRERiUx3T0mT8/ntQ9m/r/JbLJu3aFEH2Yg0LCoa0uTs37e3zMmVRKRyOj0lIiKRqWiIiEhkKhoiIhKZioaIiERWadEws3lm9r6ZvZYUa2tmy81sY/jzmBA3M5trZvlmtt7MBiatMyW032hmU5LimWb2alhnrplZRX2IiEh8ohxp3AOMKxGbDjzt7j2Ap8NngDOAHuGVDdwJiQIAzABOAQYBM5KKwJ3AFUnrjaukDxERiUmlt9y6+/+aWUaJ8CRgZHh/L7AS+FmIz/fEjCerzKyNmXUKbZe7+0cAZrYcGGdmK4Gj3H1ViM8HzgaWVtCHiEiNiTrvBqf9vPaTaQCq+pzGse6+Pbx/Dzg2vO8CbElqVxBiFcULyohX1EcpZpZN4siGrl27provItLEJc+7kTNjBnl5eSxesqQoNnnyhbTuuY3OnTvHkV69Uu2H+9zdzaz0XIo1qLI+3D0XyAXIysqq1VxEpPHJmTGj2OfMzMxS08Baly5lThvb1FT17qkd4bQT4c/3Q3wrcFxSu/QQqyieXka8oj5ERCQmVS0ai4DCO6CmAI8lxS8Jd1ENBnaHU0xPAGPM7JhwAXwM8ERYtsfMBoe7pi4psa2y+hARkZhUenrKzBaQuCDd3swKSNwFdQvwgJldBmwGzg/NHwfGA/nAp8ClAO7+kZndDLwc2t1UeFEc+CGJO7SOIHEBfGmIl9eHiEidu+KKK+JOoV6IcvfU5HIWjS6jrQNXlbOdecC8MuKrgb5lxHeW1YdIefb8qn+kc87hUSCRlOTm5sadQr2gUW6l0XB3jV4rtSYzM5O8vLy404idiobEYs7yt+NOQSQla9asiTuFekFFQ0RS9sI7OyO1G3J8u1rOpO5069aNzp07s23bNnJycpiZ9GzH6tWrAcjKyqJbt25s2rQppixrn4qGiDRpbdocXezhvvJ9nW3btgGQk5NDTk5OqRbu3uivmaloiEiTNu2aadEazpwZabiRGSUeFGxsNDS6iEgNKusIpDFR0RARqUGNfXwqFQ0RkRq0ffv2yhs1YLqmIfVa1Af2RKRuqGhIvaYH9qShGThwYOWNGjCdnhIRqUGN/alxHWmIiNSgxdedysQJEytv2EBnAtSRhohIDcrLa9zDjehIQ2qUxpSSZI1puJHoT443bioaEgsNYy4NTdQnx3NmzmTv3r1s276NBQsWFsUnTphAZmZmUeGZOep6JkyYwOLFi5k4cSJLkuYkd3dyc3O58sor691YVtbYbmfMysrywsHDpO5FPdLY/ct+uitKijSEI42ocmbOLDXneJkiXtMwszq57dzM8tw9q7J2OtIQEYlDhHGsABYtWlTLiaRGRUNqVIt7z+KDgk1Fn8defDUFG1/j9VUri2IDR55J475UKFJzMjMz406hmHpfNMxsHHA70Ay4y91viTmlJinqaafPPy5+4fOJP99Rqs2alf/DEa1a10he0jg0pgvmNS33ouNr9HRXddXromFmzYDfAacDBcDLZrbI3TfEm1nT89mcIRz49JNK2zVr1lzXKqRJi3qXVZs2R0cflr0eqddFAxgE5Lv7OwBmthCYBKho1JBZY47i4OefR2qrYiBxi3pEAvEdlUQtBLfdfluDvIW3Xt89ZWbnAePc/fLw+WLgFHe/ukS7bCA7fOwJvFXFLtsDH1Zx3fqmsexLY9kP0L7UV41lX6q7H93cvUNljer7kUYk7p4L5FZ3O2a2OsotZw1BY9mXxrIfoH2prxrLvtTVftT3YUS2AsclfU4PMRERiUF9LxovAz3MrLuZtQAuBOrXTcsiIk1IvT495e4Hzexq4AkSt9zOc/fXa7HLap/iqkcay740lv0A7Ut91Vj2pU72o15fCBcRkfqlvp+eEhGRekRFQ0REIlPRKIOZ/cjM3jSz183sv+LOp7rM7DozczNrH3cuVWFmvwk/j/Vm9qiZtYk7p1SZ2Tgze8vM8s1setz5VJWZHWdmz5jZhvD/45q4c6oOM2tmZq+Y2ZLKW9dfZtbGzB4K/0/eMLMhtdWXikYJZnYaiafO+7t7H2B2zClVi5kdB4wB/hl3LtWwHOjr7v2At4EGNU9m0nA4ZwC9gclm1jverKrsIHCdu/cGBgNXNeB9AbgGeCPuJGrA7cAyd/8m0J9a3CcVjdL+DbjF3T8DcPf3Y86nuuYAPwUa7B0P7v6kux8MH1eReF6nISkaDsfdPwcKh8NpcNx9u7uvCe/3kvjl1CXerKrGzNKBM4G74s6lOszsaGA4cDeAu3/u7h/XVn8qGqV9AxhmZi+a2bNmdnLcCVWVmU0Ctrr7urhzqUFTgaVxJ5GiLsCWpM8FNNBftMnMLAM4CXgx3kyq7DYSX6gOxZ1INXUHPgD+O5xqu8vMvlZbndXr5zRqi5k9BXy9jEU3kPg7aUvi0Ptk4AEzO97r6b3JlezL9SROTdV7Fe2Huz8W2txA4vTIfXWZm5RmZq2Ah4Fp7r4n7nxSZWYTgPfdPc/MRsadTzU1BwYCP3L3F83sdmA6cGNtddbkuPu3y1tmZv8GPBKKxEtmdojEQGAf1FV+qShvX8zsRBLfQNaFebbTgTVmNsjd36vDFCOp6GcCYGbfByYAo+trAa9AoxoOx8wOJ1Ew7nP3R+LOp4qGAmeZ2XggDTjKzP6fu38v5ryqogAocPfCI76HSBSNWqHTU6X9FTgNwMy+AbSgAY6A6e6vuntHd89w9wwS/7AG1seCUZkwEddPgbPc/dO486mCRjMcjiW+gdwNvOHut8adT1W5+8/dPT3837gQWNFACwbh//QWM+sZQqOpxekjmuSRRiXmAfPM7DXgc2BKA/xm29jcAbQEloejplXu/oN4U4ouhuFwatNQ4GLgVTNbG2LXu/vjMeYk8CPgvvCl5B3g0trqSMOIiIhIZDo9JSIikaloiIhIZCoaIiISmYqGiIhEpqIhIiKRqWiIiEhkKhoiIhLZ/wdzQ5l5FUUUEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-6, 6, 31)\n",
    "plt.hist(X0_val_T, bins=bins, alpha=0.5, label=r'$\\mu=0$, $\\sigma=1$')\n",
    "plt.hist(X0_val_T,\n",
    "         bins=bins,\n",
    "         label=r'$\\mu=0$, $\\sigma=1$ NN wgt.',\n",
    "         weights=weights,\n",
    "         histtype='step',\n",
    "         color='k')\n",
    "plt.hist(X0_val_T,\n",
    "         bins=bins,\n",
    "         label=r'$\\mu=0$, $\\sigma=1$ analytical wgt.',\n",
    "         weights=analytical_weights,\n",
    "         histtype='step',\n",
    "         linestyle='--',\n",
    "         color='k')\n",
    "plt.hist(X1_val_T,\n",
    "         bins=bins,\n",
    "         alpha=0.5,\n",
    "         label=r'$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "plt.legend()\n",
    "plt.title(\"Truth Level: Reweighting\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate for Detector Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:28.959766Z",
     "start_time": "2020-05-31T20:44:11.021856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate detector level by smearing truth\n",
    "\n",
    "epsilon_val =0.5  #Smearing width\n",
    "\n",
    "X0_val_D = np.array([(x + np.random.normal(0, epsilon_val)) for x in X0_val_T]) #Detector smearing\n",
    "X1_val_D = np.array([(x + np.random.normal(0, epsilon_val)) for x in X1_val_T]) #Detector smearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:30.971530Z",
     "start_time": "2020-05-31T20:44:28.963572Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/figure.py:459: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey= True,  constrained_layout=True)\n",
    "fig.suptitle('Detector Smearing')\n",
    "axs[0].set_title('$\\mu=0$, $\\sigma=1$')\n",
    "axs[0].hist(X0_val_T, bins = bins, alpha = 0.5, label = 'TRUTH')\n",
    "axs[0].hist(X0_val_D, bins = bins, alpha = 0.5, label = 'DETECTOR')\n",
    "axs[0].legend(prop = fontP)\n",
    "axs[1].set_title('$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "axs[1].hist(X1_val_T, bins = bins, alpha = 0.5, label = 'TRUTH:')\n",
    "axs[1].hist(X1_val_D, bins = bins, alpha = 0.5, label = 'DETECTOR')\n",
    "axs[1].legend(prop = fontP)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:33.388459Z",
     "start_time": "2020-05-31T20:44:30.975641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-7,7,31)\n",
    "plt.hist(X0_val_D, bins = bins, alpha = 0.5, label = r'$\\mu=0$, $\\sigma=1$')\n",
    "plt.hist(X0_val_D, bins = bins, label = r'$\\mu=0$, $\\sigma=1$ NN wgt.',\n",
    "         weights=weights, histtype='step', color='k')\n",
    "plt.hist(X0_val_D, bins = bins, label = r'$\\mu=0$, $\\sigma=1$ analytical wgt.',\n",
    "         weights=analytical_weights, histtype='step', linestyle = '--',color='k')\n",
    "plt.hist(X1_val_D, bins = bins, alpha = 0.5, label = r'$\\mu={}$, $\\sigma={}$'.format(mu1, sigma1))\n",
    "plt.legend()\n",
    "plt.title(\"Detector Level: Reweighting\")\n",
    "#plt.savefig(\"Detector Level: Reweighting.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:33.399451Z",
     "start_time": "2020-05-31T20:44:33.392692Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_json = dctr_model.to_json()\\nwith open(\"2d_gaussian_dctr_model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n# serialize weights to HDF5\\ndctr_model.save_weights(\"2d_gaussian_dctr_model.h5\")\\nprint(\"Saved model to disk\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_json = dctr_model.to_json()\n",
    "with open(\"2d_gaussian_dctr_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "dctr_model.save_weights(\"2d_gaussian_dctr_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset\n",
    "\n",
    "We'll show the new setup with a simple Gaussian example.  Let's start by setting up the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply detector effects, each event $x_{T,i}$in the sample is smeared by shifting by $Z_{i}$ from $Z = \\mathcal{N}(0,\\epsilon)$,where $\\epsilon$ represents the smearing. Thus: $x_{D,i} = x_{T,i} + Z_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:37.099201Z",
     "start_time": "2020-05-31T20:44:33.402231Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "#param = (mu, sigma)\n",
    "theta0_param = (0,1) #this is the simulation ... N.B. this notation is reversed from above!\n",
    "theta1_param = (1,1.5) #this is the data (the target)\n",
    "\n",
    "epsilon =theta0_param[1]/2 #Smearing width\n",
    "\n",
    "theta0_T = np.random.normal(theta0_param[0],theta0_param[1],N)\n",
    "theta0_D = np.array([(x + np.random.normal(0, epsilon)) for x in theta0_T]) #Detector smearing\n",
    "theta0 = np.stack([theta0_T, theta0_D], axis = 1)\n",
    "\n",
    "theta1_T = np.random.normal(theta1_param[0],theta1_param[1],N)\n",
    "theta1_D = np.array([(x + np.random.normal(0, epsilon)) for x in theta1_T]) #Detector smearing\n",
    "theta1 = np.stack([theta1_T, theta0_D], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:38.006866Z",
     "start_time": "2020-05-31T20:44:37.103669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-6,6,31)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey= True,  constrained_layout=True)\n",
    "fig.suptitle('2D Gaussian Fit (Detector Effects)')\n",
    "\n",
    "axs[0].set_title(\"Simulation\\n($\\mu$ = {:.2f}, $\\sigma$ = {:.2f})\".format(theta0_param[0], theta0_param[1]))\n",
    "axs[0].hist(theta0_T, bins = bins, alpha = 0.5, label = 'Truth')\n",
    "axs[0].hist(theta0_D, bins = bins, alpha = 0.5, label = 'Detector')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title(\"Data\\n($\\mu$ = {:.2f}, $\\sigma$ = {:.2f})\".format(theta1_param[0], theta1_param[1]))\n",
    "axs[1].hist(theta1_T, bins = bins, alpha = 0.5, label = 'Truth')\n",
    "axs[1].hist(theta1_D, bins = bins, alpha = 0.5, label = 'Detector')\n",
    "axs[1].legend()\n",
    "\n",
    "#fig.savefig(\"2D Gaussian: Data ($\\mu$ = {:.2f}, $\\sigma$ = {:.2f}) w detector effects.png\".format(theta1_param[0], theta1_param[1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:38.022752Z",
     "start_time": "2020-05-31T20:44:38.011165Z"
    }
   },
   "outputs": [],
   "source": [
    "#'Erasing' Truth level for data, we can't actually observe this\n",
    "theta1 = np.stack([np.zeros_like(theta0_D), theta0_D], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:38.269715Z",
     "start_time": "2020-05-31T20:44:38.026804Z"
    }
   },
   "outputs": [],
   "source": [
    "labels0 = np.zeros(len(theta0))\n",
    "labels1 = np.ones(len(theta1))\n",
    "\n",
    "xvals = np.concatenate([theta0_D,theta1_D])\n",
    "y_true = np.concatenate([labels0,labels1]) \n",
    "# 'hiding' truth level for simulation in model output (used in reweighting)\n",
    "truth_level = np.concatenate([theta0_T, theta1_T])\n",
    "yvals = np.stack([y_true, truth_level], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals, yvals, test_size=0.5)\n",
    "X_train_theta, X_test_theta, y_train_theta, y_test_theta = train_test_split(xvals, yvals, test_size=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model\n",
    "\n",
    "We'll start by showing that for fixed $\\theta$, the maximum loss occurs when $\\theta=\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:38.280258Z",
     "start_time": "2020-05-31T20:44:38.272797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njson_file = open(\\'2d_gaussian_dctr_model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\ndctr_model = keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\ndctr_model.load_weights(\"2d_gaussian_dctr_model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('2d_gaussian_dctr_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dctr_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "dctr_model.load_weights(\"2d_gaussian_dctr_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:38.288389Z",
     "start_time": "2020-05-31T20:44:38.283080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change to True for analytical_reweight\n",
    "\n",
    "reweight_analytically = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Section for $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:44:38.353589Z",
     "start_time": "2020-05-31T20:44:38.292831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience = 3, \n",
    "                              min_delta = 0.00005,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "def my_loss_wrapper(val=0):\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        y_true = tf.gather(y_true, np.arange(batch_size)) \n",
    "        y_labels = tf.gather(y_true, [0], axis = 1) #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis = 1) # sim truth for reweighting\n",
    "\n",
    "        theta_prime = [val, theta1_param[1]] #fixed theta_sigma = sigma_truth\n",
    "    \n",
    "        #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "        concat_input_and_params = K.ones(shape = (x_T.shape[0], 2))*theta_prime\n",
    "        #combining and reshaping into correct format:\n",
    "        data = K.concatenate((x_T, concat_input_and_params), axis=-1)\n",
    "        \n",
    "        if reweight_analytically == False: #NN reweight\n",
    "            w = reweight(data)\n",
    "        else: # analytical reweight\n",
    "            w = analytical_reweight(events = x_T, \n",
    "                                    mu1 = theta_prime[0], \n",
    "                                    sigma1 = theta_prime[1]) \n",
    "        \n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_labels*(y_labels - y_pred)**2+(w)*(1.-y_labels)*(y_labels - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \"\"\"\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_labels)*K.log(y_pred) +w*(1-y_labels)*K.log(1-y_pred))\n",
    "        \"\"\"\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:52:29.499657Z",
     "start_time": "2020-05-31T20:44:38.357460Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : -2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1202 - acc: 0.2905 - val_loss: 0.1195 - val_acc: 0.2895\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1191 - acc: 0.2896 - val_loss: 0.1196 - val_acc: 0.2917\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1191 - acc: 0.2896 - val_loss: 0.1195 - val_acc: 0.2889\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1191 - acc: 0.2897 - val_loss: 0.1196 - val_acc: 0.2911\n",
      "[0.119138102799654]\n",
      "testing theta = : -1.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1316 - acc: 0.2941 - val_loss: 0.1319 - val_acc: 0.2917\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1316 - acc: 0.2942 - val_loss: 0.1319 - val_acc: 0.2938\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1316 - acc: 0.2941 - val_loss: 0.1321 - val_acc: 0.2930\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1316 - acc: 0.2945 - val_loss: 0.1319 - val_acc: 0.2957\n",
      "[0.119138102799654, 0.1315559401512146]\n",
      "testing theta = : -1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1440 - acc: 0.2989 - val_loss: 0.1444 - val_acc: 0.3004\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1440 - acc: 0.2996 - val_loss: 0.1444 - val_acc: 0.2990\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1440 - acc: 0.2995 - val_loss: 0.1443 - val_acc: 0.2984\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1440 - acc: 0.2997 - val_loss: 0.1443 - val_acc: 0.3003\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1440 - acc: 0.2995 - val_loss: 0.1443 - val_acc: 0.2988\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1440 - acc: 0.2992 - val_loss: 0.1443 - val_acc: 0.2980\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377]\n",
      "testing theta = : -1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1587 - acc: 0.3040 - val_loss: 0.1590 - val_acc: 0.3025\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1587 - acc: 0.3035 - val_loss: 0.1590 - val_acc: 0.3032\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1587 - acc: 0.3037 - val_loss: 0.1590 - val_acc: 0.3023\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1587 - acc: 0.3036 - val_loss: 0.1590 - val_acc: 0.3027\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857]\n",
      "testing theta = : -1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1739 - acc: 0.3088 - val_loss: 0.1742 - val_acc: 0.3083\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1739 - acc: 0.3090 - val_loss: 0.1742 - val_acc: 0.3090\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1739 - acc: 0.3090 - val_loss: 0.1742 - val_acc: 0.3086\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1739 - acc: 0.3090 - val_loss: 0.1742 - val_acc: 0.3088\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403]\n",
      "testing theta = : -0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1878 - acc: 0.3125 - val_loss: 0.1880 - val_acc: 0.3115\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1878 - acc: 0.3125 - val_loss: 0.1881 - val_acc: 0.3126\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1878 - acc: 0.3124 - val_loss: 0.1880 - val_acc: 0.3124\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1878 - acc: 0.3126 - val_loss: 0.1881 - val_acc: 0.3110\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954]\n",
      "testing theta = : -0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2011 - acc: 0.3166 - val_loss: 0.2012 - val_acc: 0.3162\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2011 - acc: 0.3167 - val_loss: 0.2012 - val_acc: 0.3171\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2011 - acc: 0.3169 - val_loss: 0.2014 - val_acc: 0.3174\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2011 - acc: 0.3167 - val_loss: 0.2012 - val_acc: 0.3166\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309]\n",
      "testing theta = : -0.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2140 - acc: 0.3201 - val_loss: 0.2139 - val_acc: 0.3202\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2139 - acc: 0.3201 - val_loss: 0.2141 - val_acc: 0.3187\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2139 - acc: 0.3199 - val_loss: 0.2139 - val_acc: 0.3198\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2140 - acc: 0.3203 - val_loss: 0.2140 - val_acc: 0.3195\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375]\n",
      "testing theta = : 0.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2262 - acc: 0.3233 - val_loss: 0.2261 - val_acc: 0.3217\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2261 - acc: 0.3234 - val_loss: 0.2260 - val_acc: 0.3234\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2261 - acc: 0.3234 - val_loss: 0.2262 - val_acc: 0.3251\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2261 - acc: 0.3235 - val_loss: 0.2259 - val_acc: 0.3237\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2261 - acc: 0.3237 - val_loss: 0.2260 - val_acc: 0.3224\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766]\n",
      "testing theta = : 0.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2364 - acc: 0.3264 - val_loss: 0.2362 - val_acc: 0.3248\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2363 - acc: 0.3260 - val_loss: 0.2361 - val_acc: 0.3278\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2364 - acc: 0.3262 - val_loss: 0.2360 - val_acc: 0.3261\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2364 - acc: 0.3259 - val_loss: 0.2361 - val_acc: 0.3261\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2363 - acc: 0.3260 - val_loss: 0.2363 - val_acc: 0.3261\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072]\n",
      "testing theta = : 0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2448 - acc: 0.3286 - val_loss: 0.2443 - val_acc: 0.3283\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2447 - acc: 0.3280 - val_loss: 0.2446 - val_acc: 0.3278\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2447 - acc: 0.3275 - val_loss: 0.2446 - val_acc: 0.3283\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2447 - acc: 0.3275 - val_loss: 0.2444 - val_acc: 0.3274\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072, 0.24467911495268344]\n",
      "testing theta = : 0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.3237 - val_loss: 0.2499 - val_acc: 0.3274\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2503 - acc: 0.3269 - val_loss: 0.2498 - val_acc: 0.3269\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2503 - acc: 0.3267 - val_loss: 0.2498 - val_acc: 0.3281\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2503 - acc: 0.3275 - val_loss: 0.2499 - val_acc: 0.3279\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2503 - acc: 0.3266 - val_loss: 0.2504 - val_acc: 0.3073\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072, 0.24467911495268344, 0.2502698977738619]\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2526 - acc: 0.2687 - val_loss: 0.2520 - val_acc: 0.2605\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2526 - acc: 0.2571 - val_loss: 0.2521 - val_acc: 0.2547\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2527 - acc: 0.2541 - val_loss: 0.2520 - val_acc: 0.2574\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2524 - acc: 0.2575 - val_loss: 0.2521 - val_acc: 0.2542\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072, 0.24467911495268344, 0.2502698977738619, 0.2524392509162426]\n",
      "testing theta = : 1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2520 - acc: 0.1784 - val_loss: 0.2510 - val_acc: 0.1750\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2518 - acc: 0.1758 - val_loss: 0.2510 - val_acc: 0.1774\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2518 - acc: 0.1754 - val_loss: 0.2509 - val_acc: 0.1779\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2517 - acc: 0.1761 - val_loss: 0.2511 - val_acc: 0.1737\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2514 - acc: 0.1767 - val_loss: 0.2512 - val_acc: 0.1741\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2515 - acc: 0.1768 - val_loss: 0.2513 - val_acc: 0.1726\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072, 0.24467911495268344, 0.2502698977738619, 0.2524392509162426, 0.25142279343307017]\n",
      "testing theta = : 1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2478 - acc: 0.1754 - val_loss: 0.2469 - val_acc: 0.1772\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2477 - acc: 0.1737 - val_loss: 0.2478 - val_acc: 0.1818\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2478 - acc: 0.1737 - val_loss: 0.2467 - val_acc: 0.1724\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2474 - acc: 0.1742 - val_loss: 0.2467 - val_acc: 0.1726\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2477 - acc: 0.1733 - val_loss: 0.2468 - val_acc: 0.1761\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2475 - acc: 0.1743 - val_loss: 0.2472 - val_acc: 0.1726\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072, 0.24467911495268344, 0.2502698977738619, 0.2524392509162426, 0.25142279343307017, 0.24737636087834836]\n",
      "testing theta = : 1.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2406 - acc: 0.1744 - val_loss: 0.2394 - val_acc: 0.1730\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2404 - acc: 0.1734 - val_loss: 0.2416 - val_acc: 0.1808\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2403 - acc: 0.1743 - val_loss: 0.2395 - val_acc: 0.1727\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2406 - acc: 0.1736 - val_loss: 0.2397 - val_acc: 0.1715\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072, 0.24467911495268344, 0.2502698977738619, 0.2524392509162426, 0.25142279343307017, 0.24737636087834836, 0.24026920376718044]\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2307 - acc: 0.1763 - val_loss: 0.2308 - val_acc: 0.1786\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2310 - acc: 0.1750 - val_loss: 0.2296 - val_acc: 0.1770\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2307 - acc: 0.1745 - val_loss: 0.2297 - val_acc: 0.1748\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2304 - acc: 0.1750 - val_loss: 0.2308 - val_acc: 0.1782\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2306 - acc: 0.1755 - val_loss: 0.2299 - val_acc: 0.1732\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072, 0.24467911495268344, 0.2502698977738619, 0.2524392509162426, 0.25142279343307017, 0.24737636087834836, 0.24026920376718044, 0.2303845140337944]\n",
      "[0.119138102799654, 0.1315559401512146, 0.14397506241500377, 0.15869527372717857, 0.17390087382495403, 0.1877908307760954, 0.2010701435059309, 0.21394480450451375, 0.2261292113661766, 0.23633398194611072, 0.24467911495268344, 0.2502698977738619, 0.2524392509162426, 0.25142279343307017, 0.24737636087834836, 0.24026920376718044, 0.2303845140337944]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(-2,2,17)\n",
    "lvals = []\n",
    "vlvals = []\n",
    " \n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train, \n",
    "              epochs=100, batch_size=1000,\n",
    "              validation_data=(np.array(X_test), y_test),\n",
    "              verbose=1, callbacks = [earlystopping])\n",
    "    lvals+=[np.min(model.history.history['loss'])]\n",
    "    vlvals+=[np.min(model.history.history['val_loss'])]\n",
    "    print(lvals)\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T21:12:27.374219Z",
     "start_time": "2020-05-31T21:12:27.133208Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals, label = 'lvals')\n",
    "plt.plot(thetas, vlvals, label = 'vlvals')\n",
    "plt.title(\"$\\mu$ Cross Section\\nFixed $\\sigma = \\sigma_{Truth}$\")\n",
    "plt.xlabel(r'$\\theta_{\\mu}$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param[0], ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.legend()\n",
    "#plt.savefig(\"GaussianAltFit-2D-DetectorEffects-\\mu cross section.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Section for $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T21:12:43.730608Z",
     "start_time": "2020-05-31T21:12:43.649100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "batch_size = 1000\n",
    "\n",
    "earlystopping = EarlyStopping(patience = 3, \n",
    "                              min_delta = 0.00005,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "def my_loss_wrapper(val=0):\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        y_true = tf.gather(y_true, np.arange(batch_size)) \n",
    "        y_labels = tf.gather(y_true, [0], axis = 1) #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis = 1) # sim truth for reweighting\n",
    "\n",
    "        theta_prime = [theta1_param[0], val] #fixed theta_mu = mu_truth\n",
    "    \n",
    "        #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "        concat_input_and_params = K.ones(shape = (x_T.shape[0], 2))*theta_prime\n",
    "        #combining and reshaping into correct format:\n",
    "        data = K.concatenate((x_T, concat_input_and_params), axis=-1)\n",
    "        \n",
    "        if reweight_analytically == False: #NN reweight\n",
    "            w = reweight(data)\n",
    "        else: # analytical reweight\n",
    "            w = analytical_reweight(events = x_T, \n",
    "                                    mu1 = theta_prime[0], \n",
    "                                    sigma1 = theta_prime[1]) \n",
    "        \n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_labels*(y_labels - y_pred)**2+(w)*(1.-y_labels)*(y_labels - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_labels)*K.log(y_pred) +w*(1-y_labels)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T21:22:24.718777Z",
     "start_time": "2020-05-31T21:12:45.434095Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : 0.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2000 - acc: 0.2486 - val_loss: 0.1981 - val_acc: 0.2466\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1984 - acc: 0.2466 - val_loss: 0.1980 - val_acc: 0.2449\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1984 - acc: 0.2463 - val_loss: 0.1981 - val_acc: 0.2458\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1983 - acc: 0.2470 - val_loss: 0.1981 - val_acc: 0.2479\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1983 - acc: 0.2464 - val_loss: 0.1981 - val_acc: 0.2456\n",
      "testing theta = : 0.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2235 - acc: 0.2467 - val_loss: 0.2231 - val_acc: 0.2496\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2235 - acc: 0.2471 - val_loss: 0.2237 - val_acc: 0.2515\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2234 - acc: 0.2469 - val_loss: 0.2230 - val_acc: 0.2446\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2234 - acc: 0.2471 - val_loss: 0.2230 - val_acc: 0.2465\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2234 - acc: 0.2470 - val_loss: 0.2231 - val_acc: 0.2455\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2234 - acc: 0.2466 - val_loss: 0.2230 - val_acc: 0.2450\n",
      "testing theta = : 1.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2410 - acc: 0.2452 - val_loss: 0.2405 - val_acc: 0.2387\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2409 - acc: 0.2449 - val_loss: 0.2406 - val_acc: 0.2468\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2409 - acc: 0.2454 - val_loss: 0.2405 - val_acc: 0.2514\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2409 - acc: 0.2459 - val_loss: 0.2409 - val_acc: 0.2502\n",
      "testing theta = : 1.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2508 - acc: 0.2388 - val_loss: 0.2501 - val_acc: 0.2269\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2506 - acc: 0.2377 - val_loss: 0.2502 - val_acc: 0.2318\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2507 - acc: 0.2392 - val_loss: 0.2502 - val_acc: 0.2358\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2507 - acc: 0.2401 - val_loss: 0.2501 - val_acc: 0.2425\n",
      "testing theta = : 1.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.2508 - val_loss: 0.2527 - val_acc: 0.2647\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2525 - acc: 0.2585 - val_loss: 0.2521 - val_acc: 0.2627\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2525 - acc: 0.2499 - val_loss: 0.2520 - val_acc: 0.2660\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2526 - acc: 0.2528 - val_loss: 0.2525 - val_acc: 0.2662\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2527 - acc: 0.2536 - val_loss: 0.2523 - val_acc: 0.2537\n",
      "testing theta = : 1.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2490 - acc: 0.2412 - val_loss: 0.2482 - val_acc: 0.2443\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2489 - acc: 0.2438 - val_loss: 0.2482 - val_acc: 0.2263\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2489 - acc: 0.2417 - val_loss: 0.2481 - val_acc: 0.2442\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2482 - acc: 0.2442 - val_loss: 0.2483 - val_acc: 0.2274\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2488 - acc: 0.2409 - val_loss: 0.2480 - val_acc: 0.2338\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2488 - acc: 0.2393 - val_loss: 0.2480 - val_acc: 0.2333\n",
      "Epoch 7/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2487 - acc: 0.2418 - val_loss: 0.2481 - val_acc: 0.2302\n",
      "Epoch 8/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2486 - acc: 0.2421 - val_loss: 0.2483 - val_acc: 0.2267\n",
      "testing theta = : 2.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2408 - acc: 0.2489 - val_loss: 0.2407 - val_acc: 0.2405\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2413 - acc: 0.2472 - val_loss: 0.2402 - val_acc: 0.2423\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2409 - acc: 0.2449 - val_loss: 0.2402 - val_acc: 0.2422\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2410 - acc: 0.2460 - val_loss: 0.2404 - val_acc: 0.2500\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2409 - acc: 0.2435 - val_loss: 0.2402 - val_acc: 0.2458\n",
      "testing theta = : 2.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2319 - acc: 0.2523 - val_loss: 0.2314 - val_acc: 0.2418\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2317 - acc: 0.2472 - val_loss: 0.2314 - val_acc: 0.2409\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2316 - acc: 0.2479 - val_loss: 0.2311 - val_acc: 0.2468\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2319 - acc: 0.2483 - val_loss: 0.2314 - val_acc: 0.2521\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2318 - acc: 0.2489 - val_loss: 0.2315 - val_acc: 0.2428\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2316 - acc: 0.2473 - val_loss: 0.2347 - val_acc: 0.2758\n",
      "testing theta = : 2.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2246 - acc: 0.2503 - val_loss: 0.2236 - val_acc: 0.2386\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2247 - acc: 0.2527 - val_loss: 0.2240 - val_acc: 0.2439\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2246 - acc: 0.2475 - val_loss: 0.2238 - val_acc: 0.2446\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2245 - acc: 0.2505 - val_loss: 0.2240 - val_acc: 0.2404\n",
      "testing theta = : 2.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2183 - acc: 0.2508 - val_loss: 0.2172 - val_acc: 0.2422\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2177 - acc: 0.2485 - val_loss: 0.2169 - val_acc: 0.2490\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2177 - acc: 0.2492 - val_loss: 0.2169 - val_acc: 0.2460\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2177 - acc: 0.2482 - val_loss: 0.2176 - val_acc: 0.2333\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2175 - acc: 0.2468 - val_loss: 0.2171 - val_acc: 0.2460\n",
      "testing theta = : 3.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2117 - acc: 0.2504 - val_loss: 0.2106 - val_acc: 0.2432\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2111 - acc: 0.2480 - val_loss: 0.2102 - val_acc: 0.2465\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2109 - acc: 0.2480 - val_loss: 0.2101 - val_acc: 0.2475\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2109 - acc: 0.2491 - val_loss: 0.2103 - val_acc: 0.2429\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2107 - acc: 0.2483 - val_loss: 0.2113 - val_acc: 0.2650\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2108 - acc: 0.2515 - val_loss: 0.2102 - val_acc: 0.2513\n",
      "testing theta = : 3.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2043 - acc: 0.2512 - val_loss: 0.2033 - val_acc: 0.2498\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2039 - acc: 0.2524 - val_loss: 0.2039 - val_acc: 0.2524\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2042 - acc: 0.2507 - val_loss: 0.2034 - val_acc: 0.2458\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.2043 - acc: 0.2507 - val_loss: 0.2034 - val_acc: 0.2427\n",
      "testing theta = : 3.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1974 - acc: 0.2567 - val_loss: 0.1965 - val_acc: 0.2490\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1974 - acc: 0.2505 - val_loss: 0.1966 - val_acc: 0.2446\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1972 - acc: 0.2524 - val_loss: 0.1967 - val_acc: 0.2433\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 6s 6us/step - loss: 0.1973 - acc: 0.2515 - val_loss: 0.1977 - val_acc: 0.2391\n",
      "testing theta = : 3.75\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1914 - acc: 0.2564 - val_loss: 0.1902 - val_acc: 0.2588\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1907 - acc: 0.2529 - val_loss: 0.1899 - val_acc: 0.2581\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1906 - acc: 0.2545 - val_loss: 0.1900 - val_acc: 0.2654\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1903 - acc: 0.2562 - val_loss: 0.1904 - val_acc: 0.2509\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1905 - acc: 0.2536 - val_loss: 0.1900 - val_acc: 0.2463\n",
      "testing theta = : 4.0\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1841 - acc: 0.2558 - val_loss: 0.1836 - val_acc: 0.2619\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1841 - acc: 0.2546 - val_loss: 0.1833 - val_acc: 0.2607\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1839 - acc: 0.2612 - val_loss: 0.1833 - val_acc: 0.2610\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1835 - acc: 0.2587 - val_loss: 0.1835 - val_acc: 0.2501\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1838 - acc: 0.2553 - val_loss: 0.1856 - val_acc: 0.2614\n",
      "testing theta = : 4.25\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.1780 - acc: 0.2593 - val_loss: 0.1766 - val_acc: 0.2599\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1772 - acc: 0.2571 - val_loss: 0.1767 - val_acc: 0.2608\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1773 - acc: 0.2581 - val_loss: 0.1769 - val_acc: 0.2586\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1772 - acc: 0.2595 - val_loss: 0.1767 - val_acc: 0.2611\n",
      "testing theta = : 4.5\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/100\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.1713 - acc: 0.2583 - val_loss: 0.1712 - val_acc: 0.2593\n",
      "Epoch 2/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1706 - acc: 0.2591 - val_loss: 0.1714 - val_acc: 0.2589\n",
      "Epoch 3/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1708 - acc: 0.2559 - val_loss: 0.1702 - val_acc: 0.2588\n",
      "Epoch 4/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1708 - acc: 0.2592 - val_loss: 0.1702 - val_acc: 0.2588\n",
      "Epoch 5/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1707 - acc: 0.2592 - val_loss: 0.1726 - val_acc: 0.2597\n",
      "Epoch 6/100\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.1706 - acc: 0.2570 - val_loss: 0.1702 - val_acc: 0.2574\n",
      "[0.19832231073081494, 0.2233919989913702, 0.24092443825304508, 0.2506379389464855, 0.25250044088065626, 0.24822502638399602, 0.24082058180868626, 0.23160712264478206, 0.22449570038914682, 0.21753986164927483, 0.21067571410536767, 0.20392713652551175, 0.19715947294235228, 0.19028781639039516, 0.18349103221297264, 0.1771584448069334, 0.1705836452394724]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.5,4.5,17)\n",
    "lvals = []\n",
    "vlvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train, epochs=100, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks = [earlystopping])\n",
    "    lvals+=[np.min(model.history.history['loss'])]\n",
    "    vlvals+=[np.min(model.history.history['val_loss'])]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T21:22:24.985662Z",
     "start_time": "2020-05-31T21:22:24.723229Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals, label = 'lvals')\n",
    "plt.plot(thetas, vlvals, label = 'vlvals')\n",
    "plt.title(\"$\\sigma$ Cross Section\\nFixed $\\mu = \\mu{Truth}$\")\n",
    "plt.xlabel(r'$\\theta_{\\sigma}$')\n",
    "plt.ylabel('Loss')\n",
    "plt.vlines(theta1_param[1], ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.legend()\n",
    "#plt.savefig(\"GaussianAltFit-2D-DetectorEffects-\\sigma cross section.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T21:22:40.640741Z",
     "start_time": "2020-05-31T21:22:40.627785Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = [0., 1.]\n",
    "\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(patience = 5,\n",
    "                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T21:22:43.988962Z",
     "start_time": "2020-05-31T21:22:43.867980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 16,899\n",
      "Trainable params: 16,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=(2,),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "index_refine = np.array([0])\n",
    "batch_size = 2*N\n",
    "lr_initial = 5e-1 #smaller learning rate yields better precision\n",
    "iterations = 75 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "\n",
    "def my_loss_wrapper_fit(mysign = 1):\n",
    "    theta = 0. #starting value\n",
    "    #Getting theta_prime:\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        #Getting theta_prime:\n",
    "        if train_theta == False:\n",
    "            theta_prime = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "            y_true = tf.gather(y_true, np.arange(1000)) \n",
    "        else:\n",
    "            theta_prime = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "            y_true = tf.gather(y_true, np.arange(batch_size)) \n",
    "        y_labels = tf.gather(y_true, [0], axis = 1) #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis = 1) # sim truth for reweighting\n",
    "        \n",
    "        #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "        concat_input_and_params = K.ones(shape = (x_T.shape[0], 2), dtype=tf.float32)*theta_prime\n",
    "        data = K.concatenate((x_T, concat_input_and_params), axis=-1)\n",
    "        if reweight_analytically == False: #NN reweight\n",
    "            w = reweight(data)\n",
    "        else: # analytical reweight\n",
    "            w = analytical_reweight(events = x_T, \n",
    "                                    mu1 = theta_prime[0], \n",
    "                                    sigma1 = theta_prime[1]) \n",
    "        \n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_labels*(y_labels - y_pred)**2+(w)*(1.-y_labels)*(y_labels - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \"\"\"\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_labels)*K.log(y_pred) +w*(1-y_labels)*K.log(1-y_pred))\n",
    "        \"\"\"\n",
    "        \n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T23:34:47.048981Z",
     "start_time": "2020-05-31T21:22:44.896950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2145 - acc: 0.3226 - val_loss: 0.2101 - val_acc: 0.3287\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2098 - acc: 0.3292 - val_loss: 0.2099 - val_acc: 0.3290\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2096 - acc: 0.3294 - val_loss: 0.2099 - val_acc: 0.3290\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2096 - acc: 0.3295 - val_loss: 0.2098 - val_acc: 0.3291\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2096 - acc: 0.3294 - val_loss: 0.2098 - val_acc: 0.3291\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2096 - acc: 0.3295 - val_loss: 0.2098 - val_acc: 0.3290\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3294 - val_loss: 0.2098 - val_acc: 0.3292\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3294 - val_loss: 0.2098 - val_acc: 0.3291\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3294 - val_loss: 0.2098 - val_acc: 0.3291\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3295 - val_loss: 0.2098 - val_acc: 0.3291\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3294 - val_loss: 0.2098 - val_acc: 0.3291\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3295 - val_loss: 0.2098 - val_acc: 0.3292\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3295 - val_loss: 0.2098 - val_acc: 0.3290\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3294 - val_loss: 0.2098 - val_acc: 0.3291\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3295 - val_loss: 0.2098 - val_acc: 0.3291\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2095 - acc: 0.3295 - val_loss: 0.2098 - val_acc: 0.3292\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 2us/step - loss: -0.2096 - acc: 0.3293\n",
      ". theta fit =  [0.49996975 1.4999716 ]\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2460 - acc: 0.3262 - val_loss: 0.2450 - val_acc: 0.3227\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.3278 - val_loss: 0.2445 - val_acc: 0.3274\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.3278 - val_loss: 0.2443 - val_acc: 0.3276\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.3273 - val_loss: 0.2443 - val_acc: 0.3274\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.3277 - val_loss: 0.2444 - val_acc: 0.3273\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.3277 - val_loss: 0.2447 - val_acc: 0.3285\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2447 - acc: 0.3277 - val_loss: 0.2444 - val_acc: 0.3280\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2446 - acc: 0.3279 - val_loss: 0.2445 - val_acc: 0.3267\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 2us/step - loss: -0.2445 - acc: 0.3276\n",
      ". theta fit =  [0.8719927 1.8717374]\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2451 - acc: 0.2785 - val_loss: 0.2443 - val_acc: 0.2883\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2449 - acc: 0.2679 - val_loss: 0.2444 - val_acc: 0.2855\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2448 - acc: 0.2680 - val_loss: 0.2446 - val_acc: 0.2329\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2448 - acc: 0.2657 - val_loss: 0.2444 - val_acc: 0.2846\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2449 - acc: 0.2660 - val_loss: 0.2443 - val_acc: 0.2873\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2449 - acc: 0.2654 - val_loss: 0.2444 - val_acc: 0.2904\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 2us/step - loss: -0.2445 - acc: 0.2886\n",
      ". theta fit =  [1.1912582 1.5523635]\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2522 - acc: 0.1843 - val_loss: 0.2516 - val_acc: 0.1728\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2520 - acc: 0.1798 - val_loss: 0.2514 - val_acc: 0.1749\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2520 - acc: 0.1798 - val_loss: 0.2513 - val_acc: 0.1778\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2520 - acc: 0.1794 - val_loss: 0.2513 - val_acc: 0.1816\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2520 - acc: 0.1795 - val_loss: 0.2513 - val_acc: 0.1775\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2519 - acc: 0.1804 - val_loss: 0.2514 - val_acc: 0.1818\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2520 - acc: 0.1796 - val_loss: 0.2513 - val_acc: 0.1812\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2519 - acc: 0.1798 - val_loss: 0.2513 - val_acc: 0.1765\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2519 - acc: 0.1797 - val_loss: 0.2513 - val_acc: 0.1805\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2519 - acc: 0.1797 - val_loss: 0.2513 - val_acc: 0.1762\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2518 - acc: 0.1789 - val_loss: 0.2514 - val_acc: 0.1858\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2518 - acc: 0.1788 - val_loss: 0.2520 - val_acc: 0.1909\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2518 - acc: 0.1801 - val_loss: 0.2517 - val_acc: 0.1733\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 3us/step - loss: -0.2515 - acc: 0.1763\n",
      ". theta fit =  [0.9008106 1.2619449]\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2506 - acc: 0.2596 - val_loss: 0.2499 - val_acc: 0.2645\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.2686 - val_loss: 0.2499 - val_acc: 0.2697\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.2680 - val_loss: 0.2499 - val_acc: 0.2700\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.2686 - val_loss: 0.2501 - val_acc: 0.2602\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.2682 - val_loss: 0.2499 - val_acc: 0.2754\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.2695 - val_loss: 0.2499 - val_acc: 0.2709\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2504 - acc: 0.2672 - val_loss: 0.2499 - val_acc: 0.2692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 3us/step - loss: -0.2501 - acc: 0.2696\n",
      ". theta fit =  [1.1734337 1.5346389]\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1887 - val_loss: 0.2515 - val_acc: 0.1806\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.1804 - val_loss: 0.2517 - val_acc: 0.1864\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.1808 - val_loss: 0.2518 - val_acc: 0.1787\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.1797 - val_loss: 0.2517 - val_acc: 0.1870\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2520 - acc: 0.1802 - val_loss: 0.2515 - val_acc: 0.1762\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2520 - acc: 0.1794 - val_loss: 0.2518 - val_acc: 0.1741\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 3us/step - loss: -0.2517 - acc: 0.1804\n",
      ". theta fit =  [0.91255236 1.2737241 ]\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2508 - acc: 0.2547 - val_loss: 0.2502 - val_acc: 0.2731\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2650 - val_loss: 0.2501 - val_acc: 0.2678\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2647 - val_loss: 0.2501 - val_acc: 0.2705\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2665 - val_loss: 0.2502 - val_acc: 0.2721\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2657 - val_loss: 0.2503 - val_acc: 0.2827\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2674 - val_loss: 0.2501 - val_acc: 0.2622\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2658 - val_loss: 0.2502 - val_acc: 0.2714\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 3us/step - loss: -0.2503 - acc: 0.2675\n",
      ". theta fit =  [1.1656206 1.5268568]\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2523 - acc: 0.1892 - val_loss: 0.2515 - val_acc: 0.1782\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.1800 - val_loss: 0.2518 - val_acc: 0.1855\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2522 - acc: 0.1798 - val_loss: 0.2516 - val_acc: 0.1811\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.1801 - val_loss: 0.2518 - val_acc: 0.1849\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.1800 - val_loss: 0.2518 - val_acc: 0.1753\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.1792 - val_loss: 0.2518 - val_acc: 0.1839\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 5s 3us/step - loss: -0.2517 - acc: 0.1780\n",
      ". theta fit =  [0.9178604 1.2791965]\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2509 - acc: 0.2557 - val_loss: 0.2505 - val_acc: 0.2471\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2508 - acc: 0.2642 - val_loss: 0.2503 - val_acc: 0.2598\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2508 - acc: 0.2646 - val_loss: 0.2503 - val_acc: 0.2588\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2637 - val_loss: 0.2502 - val_acc: 0.2660\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2508 - acc: 0.2645 - val_loss: 0.2503 - val_acc: 0.2607\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2508 - acc: 0.2641 - val_loss: 0.2503 - val_acc: 0.2603\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2658 - val_loss: 0.2502 - val_acc: 0.2631\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2508 - acc: 0.2650 - val_loss: 0.2502 - val_acc: 0.2675\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2507 - acc: 0.2653 - val_loss: 0.2504 - val_acc: 0.2725\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2504 - acc: 0.2658\n",
      ". theta fit =  [1.1621019 1.5235165]\n",
      "Refining learning rate\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2522 - acc: 0.2238 - val_loss: 0.2517 - val_acc: 0.2259\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2522 - acc: 0.2207 - val_loss: 0.2516 - val_acc: 0.2109\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.2222 - val_loss: 0.2516 - val_acc: 0.2154\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.2201 - val_loss: 0.2517 - val_acc: 0.2273\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.2173 - val_loss: 0.2517 - val_acc: 0.2343\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.2217 - val_loss: 0.2518 - val_acc: 0.2363\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.2217 - val_loss: 0.2518 - val_acc: 0.2157\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2521 - acc: 0.2213 - val_loss: 0.2516 - val_acc: 0.2217\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2518 - acc: 0.2151\n",
      ". theta fit =  [1.0631745 1.4250337]\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2523 - acc: 0.2046 - val_loss: 0.2522 - val_acc: 0.2177\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2056 - val_loss: 0.2527 - val_acc: 0.1956\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2052 - val_loss: 0.2519 - val_acc: 0.2173\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2071 - val_loss: 0.2519 - val_acc: 0.1870\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2035 - val_loss: 0.2518 - val_acc: 0.2216\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2522 - acc: 0.2060 - val_loss: 0.2520 - val_acc: 0.2179\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2044 - val_loss: 0.2521 - val_acc: 0.1984\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2040 - val_loss: 0.2519 - val_acc: 0.2167\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2065 - val_loss: 0.2519 - val_acc: 0.1897\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2045 - val_loss: 0.2519 - val_acc: 0.1938\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2520 - acc: 0.2212\n",
      ". theta fit =  [1.0437809 1.4491282]\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2524 - acc: 0.2083 - val_loss: 0.2520 - val_acc: 0.1899\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.2091 - val_loss: 0.2520 - val_acc: 0.1950\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2112 - val_loss: 0.2522 - val_acc: 0.1941\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2100 - val_loss: 0.2520 - val_acc: 0.1887\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2104 - val_loss: 0.2520 - val_acc: 0.1877\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2147 - val_loss: 0.2520 - val_acc: 0.1827\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2078 - val_loss: 0.2521 - val_acc: 0.2310\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2521 - acc: 0.1949\n",
      ". theta fit =  [1.0199867 1.4731835]\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2525 - acc: 0.2283 - val_loss: 0.2520 - val_acc: 0.2414\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2342 - val_loss: 0.2521 - val_acc: 0.2342\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2350 - val_loss: 0.2521 - val_acc: 0.2208\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2348 - val_loss: 0.2520 - val_acc: 0.2408\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2363 - val_loss: 0.2520 - val_acc: 0.2330\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2304 - val_loss: 0.2523 - val_acc: 0.2114\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2521 - acc: 0.2413\n",
      ". theta fit =  [1.0440514 1.4972049]\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2525 - acc: 0.2118 - val_loss: 0.2522 - val_acc: 0.2085\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.2044 - val_loss: 0.2521 - val_acc: 0.2306\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.2045 - val_loss: 0.2521 - val_acc: 0.1816\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.2068 - val_loss: 0.2521 - val_acc: 0.2421\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2042 - val_loss: 0.2522 - val_acc: 0.1776\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2060 - val_loss: 0.2522 - val_acc: 0.1981\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.2049 - val_loss: 0.2523 - val_acc: 0.2290\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2037 - val_loss: 0.2521 - val_acc: 0.2209\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2522 - acc: 0.1814\n",
      ". theta fit =  [1.0681244 1.4730779]\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2524 - acc: 0.1878 - val_loss: 0.2522 - val_acc: 0.1964\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.1926 - val_loss: 0.2521 - val_acc: 0.1839\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.1906 - val_loss: 0.2521 - val_acc: 0.1904\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1938 - val_loss: 0.2520 - val_acc: 0.2324\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.1931 - val_loss: 0.2522 - val_acc: 0.1836\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.1901 - val_loss: 0.2521 - val_acc: 0.1906\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.1874 - val_loss: 0.2521 - val_acc: 0.1823\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.1892 - val_loss: 0.2520 - val_acc: 0.1829\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1886 - val_loss: 0.2521 - val_acc: 0.1900\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2522 - acc: 0.2321\n",
      ". theta fit =  [1.0921181 1.4972987]\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2525 - acc: 0.1862 - val_loss: 0.2521 - val_acc: 0.1818\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1810 - val_loss: 0.2520 - val_acc: 0.1826\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1820 - val_loss: 0.2520 - val_acc: 0.1855\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.1818 - val_loss: 0.2521 - val_acc: 0.1825\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1824 - val_loss: 0.2524 - val_acc: 0.1903\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1814 - val_loss: 0.2521 - val_acc: 0.1853\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1832 - val_loss: 0.2522 - val_acc: 0.1832\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1834 - val_loss: 0.2520 - val_acc: 0.1806\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1813 - val_loss: 0.2520 - val_acc: 0.1812\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1819 - val_loss: 0.2520 - val_acc: 0.1821\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1817 - val_loss: 0.2520 - val_acc: 0.1775\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1822 - val_loss: 0.2521 - val_acc: 0.1850\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1822 - val_loss: 0.2522 - val_acc: 0.1841\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.1814 - val_loss: 0.2520 - val_acc: 0.1850\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1824 - val_loss: 0.2521 - val_acc: 0.1836\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1826 - val_loss: 0.2521 - val_acc: 0.1777\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1812 - val_loss: 0.2522 - val_acc: 0.1883\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1828 - val_loss: 0.2520 - val_acc: 0.1802\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1825 - val_loss: 0.2521 - val_acc: 0.1903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1831 - val_loss: 0.2521 - val_acc: 0.1765\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2523 - acc: 0.1763\n",
      ". theta fit =  [1.067681  1.4728613]\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2524 - acc: 0.1862 - val_loss: 0.2521 - val_acc: 0.1898\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1922 - val_loss: 0.2521 - val_acc: 0.1835\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1904 - val_loss: 0.2523 - val_acc: 0.2046\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1911 - val_loss: 0.2522 - val_acc: 0.1924\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1915 - val_loss: 0.2520 - val_acc: 0.1920\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1917 - val_loss: 0.2520 - val_acc: 0.1820\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1903 - val_loss: 0.2520 - val_acc: 0.1816\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1897 - val_loss: 0.2520 - val_acc: 0.1796\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1897 - val_loss: 0.2520 - val_acc: 0.1827\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1893 - val_loss: 0.2521 - val_acc: 0.1782\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2522 - acc: 0.1919\n",
      ". theta fit =  [1.0430775 1.4974546]\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2524 - acc: 0.2016 - val_loss: 0.2521 - val_acc: 0.2400\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.2028 - val_loss: 0.2521 - val_acc: 0.2582\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2086 - val_loss: 0.2521 - val_acc: 0.1856\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1989 - val_loss: 0.2521 - val_acc: 0.2181\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2069 - val_loss: 0.2521 - val_acc: 0.1872\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1973 - val_loss: 0.2521 - val_acc: 0.1803\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2004 - val_loss: 0.2522 - val_acc: 0.2306\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2092 - val_loss: 0.2521 - val_acc: 0.1900\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2525 - acc: 0.2050 - val_loss: 0.2521 - val_acc: 0.1798\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2079 - val_loss: 0.2521 - val_acc: 0.1825\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2081 - val_loss: 0.2521 - val_acc: 0.2349\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2012 - val_loss: 0.2522 - val_acc: 0.1818\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2083 - val_loss: 0.2521 - val_acc: 0.2041\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2038 - val_loss: 0.2522 - val_acc: 0.2062\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1985 - val_loss: 0.2521 - val_acc: 0.1785\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2522 - acc: 0.1824\n",
      ". theta fit =  [1.0198339 1.4726691]\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2524 - acc: 0.2270 - val_loss: 0.2521 - val_acc: 0.2374\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2389 - val_loss: 0.2522 - val_acc: 0.2567\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2311 - val_loss: 0.2521 - val_acc: 0.2195\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2304 - val_loss: 0.2521 - val_acc: 0.2562\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2378 - val_loss: 0.2521 - val_acc: 0.2354\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2348 - val_loss: 0.2521 - val_acc: 0.2516\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2353 - val_loss: 0.2522 - val_acc: 0.2588\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2351 - val_loss: 0.2520 - val_acc: 0.2479\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2292 - val_loss: 0.2521 - val_acc: 0.2599\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2348 - val_loss: 0.2521 - val_acc: 0.2562\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2288 - val_loss: 0.2521 - val_acc: 0.2309\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2297 - val_loss: 0.2520 - val_acc: 0.2180\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2285 - val_loss: 0.2521 - val_acc: 0.2432\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2398 - val_loss: 0.2521 - val_acc: 0.1853\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2267 - val_loss: 0.2520 - val_acc: 0.2488\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2351 - val_loss: 0.2521 - val_acc: 0.2405\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2365 - val_loss: 0.2521 - val_acc: 0.2503\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 6s 3us/step - loss: -0.2522 - acc: 0.2178\n",
      ". theta fit =  [1.0448592 1.4976634]\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2525 - acc: 0.2030 - val_loss: 0.2521 - val_acc: 0.2131\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2019 - val_loss: 0.2522 - val_acc: 0.2490\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1957 - val_loss: 0.2523 - val_acc: 0.1835\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.2083 - val_loss: 0.2521 - val_acc: 0.1806\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2012 - val_loss: 0.2521 - val_acc: 0.1889\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2027 - val_loss: 0.2521 - val_acc: 0.1811\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.2522 - acc: 0.2132\n",
      ". theta fit =  [1.0667192 1.4723692]\n",
      "Iteration:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2524 - acc: 0.1909 - val_loss: 0.2521 - val_acc: 0.1862\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1871 - val_loss: 0.2523 - val_acc: 0.1963\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1924 - val_loss: 0.2520 - val_acc: 0.1883\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1892 - val_loss: 0.2522 - val_acc: 0.2314\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1911 - val_loss: 0.2521 - val_acc: 0.1870\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1872 - val_loss: 0.2521 - val_acc: 0.1948\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1888 - val_loss: 0.2523 - val_acc: 0.2272\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1914 - val_loss: 0.2521 - val_acc: 0.1843\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.2522 - acc: 0.1881\n",
      ". theta fit =  [1.0408964 1.4981775]\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2524 - acc: 0.2072 - val_loss: 0.2522 - val_acc: 0.1857\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2061 - val_loss: 0.2521 - val_acc: 0.2344\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2064 - val_loss: 0.2521 - val_acc: 0.2434\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2122 - val_loss: 0.2521 - val_acc: 0.1818\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1998 - val_loss: 0.2521 - val_acc: 0.2320\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2098 - val_loss: 0.2522 - val_acc: 0.1833\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2074 - val_loss: 0.2522 - val_acc: 0.1832\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2130 - val_loss: 0.2522 - val_acc: 0.2183\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2069 - val_loss: 0.2521 - val_acc: 0.1818\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2033 - val_loss: 0.2524 - val_acc: 0.1848\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.2522 - acc: 0.2320\n",
      ". theta fit =  [1.0666404 1.4723448]\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2524 - acc: 0.1952 - val_loss: 0.2521 - val_acc: 0.1933\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1906 - val_loss: 0.2521 - val_acc: 0.1853\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1891 - val_loss: 0.2520 - val_acc: 0.1849\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1921 - val_loss: 0.2521 - val_acc: 0.1882\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1906 - val_loss: 0.2521 - val_acc: 0.1897\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1870 - val_loss: 0.2522 - val_acc: 0.1925\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1888 - val_loss: 0.2523 - val_acc: 0.1954\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1860 - val_loss: 0.2521 - val_acc: 0.1853\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 3us/step - loss: -0.2522 - acc: 0.1848\n",
      ". theta fit =  [1.0405251 1.498443 ]\n",
      "Refining learning rate\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2524 - acc: 0.1893 - val_loss: 0.2521 - val_acc: 0.1826\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1889 - val_loss: 0.2522 - val_acc: 0.1838\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1902 - val_loss: 0.2522 - val_acc: 0.1843\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1911 - val_loss: 0.2521 - val_acc: 0.1858\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1921 - val_loss: 0.2521 - val_acc: 0.1878\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1904 - val_loss: 0.2522 - val_acc: 0.1921\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.2522 - acc: 0.1825\n",
      ". theta fit =  [1.0510554 1.4827074]\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2525 - acc: 0.1933 - val_loss: 0.2521 - val_acc: 0.1925\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1910 - val_loss: 0.2521 - val_acc: 0.2022\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1971 - val_loss: 0.2522 - val_acc: 0.2032\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1951 - val_loss: 0.2521 - val_acc: 0.1799\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1913 - val_loss: 0.2522 - val_acc: 0.2144\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2008 - val_loss: 0.2521 - val_acc: 0.1817\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1939 - val_loss: 0.2521 - val_acc: 0.1797\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2004 - val_loss: 0.2521 - val_acc: 0.1831\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1866 - val_loss: 0.2522 - val_acc: 0.2531\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1934 - val_loss: 0.2521 - val_acc: 0.1913\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1978 - val_loss: 0.2521 - val_acc: 0.1842\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.2522 - acc: 0.1816\n",
      ". theta fit =  [1.0483873 1.4800717]\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2524 - acc: 0.1888 - val_loss: 0.2523 - val_acc: 0.1843\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2024 - val_loss: 0.2521 - val_acc: 0.1887\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1937 - val_loss: 0.2521 - val_acc: 0.1927\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1963 - val_loss: 0.2521 - val_acc: 0.2219\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1989 - val_loss: 0.2521 - val_acc: 0.1867\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1984 - val_loss: 0.2521 - val_acc: 0.1830\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1974 - val_loss: 0.2521 - val_acc: 0.1840\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1971 - val_loss: 0.2521 - val_acc: 0.1794\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1941 - val_loss: 0.2522 - val_acc: 0.1826\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2058 - val_loss: 0.2522 - val_acc: 0.1890\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1930 - val_loss: 0.2521 - val_acc: 0.2182\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1998 - val_loss: 0.2521 - val_acc: 0.1877\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1911 - val_loss: 0.2521 - val_acc: 0.2094\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1970 - val_loss: 0.2521 - val_acc: 0.1842\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1960 - val_loss: 0.2522 - val_acc: 0.1881\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1937 - val_loss: 0.2521 - val_acc: 0.1844\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1921 - val_loss: 0.2521 - val_acc: 0.2178\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2005 - val_loss: 0.2521 - val_acc: 0.1926\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2005 - val_loss: 0.2521 - val_acc: 0.1865\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1913 - val_loss: 0.2521 - val_acc: 0.1947\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.2522 - acc: 0.1946\n",
      ". theta fit =  [1.0510857 1.4827747]\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2524 - acc: 0.1952 - val_loss: 0.2521 - val_acc: 0.1813\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1959 - val_loss: 0.2521 - val_acc: 0.1813\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1905 - val_loss: 0.2522 - val_acc: 0.1838\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2008 - val_loss: 0.2522 - val_acc: 0.2293\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1919 - val_loss: 0.2522 - val_acc: 0.1943\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1965 - val_loss: 0.2521 - val_acc: 0.1988\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1924 - val_loss: 0.2521 - val_acc: 0.1804\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.2522 - acc: 0.1812\n",
      ". theta fit =  [1.0483639 1.4800454]\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 12s 12us/step - loss: 0.2523 - acc: 0.1934 - val_loss: 0.2521 - val_acc: 0.1829\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1957 - val_loss: 0.2522 - val_acc: 0.1846\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1987 - val_loss: 0.2522 - val_acc: 0.1828\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2004 - val_loss: 0.2521 - val_acc: 0.2177\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1964 - val_loss: 0.2521 - val_acc: 0.1836\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1938 - val_loss: 0.2522 - val_acc: 0.1842\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.2522 - acc: 0.1828\n",
      ". theta fit =  [1.0456141 1.4827749]\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2524 - acc: 0.1982 - val_loss: 0.2521 - val_acc: 0.1823\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1937 - val_loss: 0.2521 - val_acc: 0.1867\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1971 - val_loss: 0.2521 - val_acc: 0.2101\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2011 - val_loss: 0.2521 - val_acc: 0.1823\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1976 - val_loss: 0.2521 - val_acc: 0.1810\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1955 - val_loss: 0.2522 - val_acc: 0.2567\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2044 - val_loss: 0.2522 - val_acc: 0.1914\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 7s 4us/step - loss: -0.2522 - acc: 0.1866\n",
      ". theta fit =  [1.0483536 1.4855489]\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2524 - acc: 0.2017 - val_loss: 0.2523 - val_acc: 0.1850\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1975 - val_loss: 0.2521 - val_acc: 0.2306\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1939 - val_loss: 0.2521 - val_acc: 0.1910\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1976 - val_loss: 0.2521 - val_acc: 0.1800\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1918 - val_loss: 0.2521 - val_acc: 0.2586\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1971 - val_loss: 0.2521 - val_acc: 0.1805\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1904 - val_loss: 0.2521 - val_acc: 0.1803\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1911 - val_loss: 0.2523 - val_acc: 0.1851\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1986 - val_loss: 0.2521 - val_acc: 0.1832\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1937 - val_loss: 0.2521 - val_acc: 0.1911\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1936 - val_loss: 0.2522 - val_acc: 0.1887\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1951 - val_loss: 0.2521 - val_acc: 0.1822\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1945 - val_loss: 0.2521 - val_acc: 0.2011\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1936 - val_loss: 0.2522 - val_acc: 0.1832\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1948 - val_loss: 0.2521 - val_acc: 0.1840\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1963 - val_loss: 0.2521 - val_acc: 0.1840\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1938 - val_loss: 0.2521 - val_acc: 0.1885\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1821\n",
      ". theta fit =  [1.045541  1.4827291]\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2524 - acc: 0.1954 - val_loss: 0.2521 - val_acc: 0.1841\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1951 - val_loss: 0.2521 - val_acc: 0.1822\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1929 - val_loss: 0.2523 - val_acc: 0.1895\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2001 - val_loss: 0.2521 - val_acc: 0.2023\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1917 - val_loss: 0.2522 - val_acc: 0.1857\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1954 - val_loss: 0.2521 - val_acc: 0.1795\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1840\n",
      ". theta fit =  [1.0483855 1.4855714]\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2524 - acc: 0.1902 - val_loss: 0.2521 - val_acc: 0.1867\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1978 - val_loss: 0.2522 - val_acc: 0.1927\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1923 - val_loss: 0.2521 - val_acc: 0.1882\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1914 - val_loss: 0.2521 - val_acc: 0.1809\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1905 - val_loss: 0.2521 - val_acc: 0.2351\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1984 - val_loss: 0.2521 - val_acc: 0.1893\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1866\n",
      ". theta fit =  [1.0455059 1.4826843]\n",
      "Refining learning rate\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2524 - acc: 0.1938 - val_loss: 0.2521 - val_acc: 0.1793\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1917 - val_loss: 0.2521 - val_acc: 0.2338\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2036 - val_loss: 0.2521 - val_acc: 0.1801\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1884 - val_loss: 0.2521 - val_acc: 0.2155\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2045 - val_loss: 0.2521 - val_acc: 0.1937\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1933 - val_loss: 0.2521 - val_acc: 0.1885\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1949 - val_loss: 0.2523 - val_acc: 0.2550\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1975 - val_loss: 0.2522 - val_acc: 0.1909\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1922 - val_loss: 0.2521 - val_acc: 0.1810\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.2154\n",
      ". theta fit =  [1.0472387 1.4838423]\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2523 - acc: 0.1950 - val_loss: 0.2522 - val_acc: 0.2351\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1979 - val_loss: 0.2522 - val_acc: 0.2308\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1940 - val_loss: 0.2521 - val_acc: 0.1816\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1901 - val_loss: 0.2523 - val_acc: 0.1924\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1947 - val_loss: 0.2521 - val_acc: 0.1876\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1948 - val_loss: 0.2521 - val_acc: 0.1911\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1985 - val_loss: 0.2522 - val_acc: 0.2492\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1975 - val_loss: 0.2521 - val_acc: 0.1801\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2003 - val_loss: 0.2521 - val_acc: 0.1833\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1959 - val_loss: 0.2521 - val_acc: 0.1862\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1922 - val_loss: 0.2523 - val_acc: 0.1937\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2025 - val_loss: 0.2522 - val_acc: 0.1835\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1987 - val_loss: 0.2521 - val_acc: 0.1821\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1799\n",
      ". theta fit =  [1.0469427 1.4835459]\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2523 - acc: 0.1945 - val_loss: 0.2522 - val_acc: 0.1950\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1961 - val_loss: 0.2521 - val_acc: 0.1930\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1922 - val_loss: 0.2522 - val_acc: 0.1933\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2002 - val_loss: 0.2522 - val_acc: 0.1871\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1925 - val_loss: 0.2521 - val_acc: 0.1842\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1942 - val_loss: 0.2521 - val_acc: 0.2054\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1967 - val_loss: 0.2521 - val_acc: 0.1878\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1928\n",
      ". theta fit =  [1.0466447 1.4838417]\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 13s 13us/step - loss: 0.2523 - acc: 0.1977 - val_loss: 0.2522 - val_acc: 0.2091\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1935 - val_loss: 0.2521 - val_acc: 0.1836\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2021 - val_loss: 0.2522 - val_acc: 0.1941\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1899 - val_loss: 0.2521 - val_acc: 0.1981\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1982 - val_loss: 0.2521 - val_acc: 0.2327\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1978 - val_loss: 0.2522 - val_acc: 0.1916\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1902 - val_loss: 0.2521 - val_acc: 0.1898\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1999 - val_loss: 0.2521 - val_acc: 0.2317\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1932 - val_loss: 0.2521 - val_acc: 0.1928\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1991 - val_loss: 0.2521 - val_acc: 0.2465\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1976 - val_loss: 0.2521 - val_acc: 0.1828\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1918 - val_loss: 0.2521 - val_acc: 0.1854\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1923 - val_loss: 0.2521 - val_acc: 0.2145\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2116 - val_loss: 0.2522 - val_acc: 0.1800\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1942 - val_loss: 0.2521 - val_acc: 0.1896\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1915 - val_loss: 0.2521 - val_acc: 0.1888\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1894 - val_loss: 0.2521 - val_acc: 0.1850\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1964 - val_loss: 0.2521 - val_acc: 0.2324\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.2144\n",
      ". theta fit =  [1.0469494 1.4835371]\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2524 - acc: 0.2020 - val_loss: 0.2521 - val_acc: 0.1929\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1946 - val_loss: 0.2522 - val_acc: 0.1851\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1901 - val_loss: 0.2522 - val_acc: 0.1955\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1929 - val_loss: 0.2521 - val_acc: 0.1901\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2004 - val_loss: 0.2521 - val_acc: 0.1824\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1950 - val_loss: 0.2521 - val_acc: 0.2358\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1965 - val_loss: 0.2521 - val_acc: 0.1831\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1991 - val_loss: 0.2522 - val_acc: 0.2315\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1984 - val_loss: 0.2521 - val_acc: 0.1811\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1909 - val_loss: 0.2521 - val_acc: 0.1863\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1897 - val_loss: 0.2523 - val_acc: 0.1848\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1934 - val_loss: 0.2522 - val_acc: 0.1891\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1830\n",
      ". theta fit =  [1.0466512 1.4838322]\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2523 - acc: 0.1910 - val_loss: 0.2521 - val_acc: 0.1929\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2057 - val_loss: 0.2521 - val_acc: 0.1893\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1912 - val_loss: 0.2521 - val_acc: 0.1909\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1974 - val_loss: 0.2522 - val_acc: 0.1864\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1994 - val_loss: 0.2524 - val_acc: 0.1879\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2035 - val_loss: 0.2521 - val_acc: 0.1862\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1983 - val_loss: 0.2522 - val_acc: 0.2153\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1983 - val_loss: 0.2521 - val_acc: 0.1822\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1931 - val_loss: 0.2521 - val_acc: 0.1888\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1982 - val_loss: 0.2521 - val_acc: 0.1857\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1893 - val_loss: 0.2521 - val_acc: 0.1926\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1986 - val_loss: 0.2521 - val_acc: 0.1908\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1994 - val_loss: 0.2521 - val_acc: 0.1795\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1917 - val_loss: 0.2522 - val_acc: 0.1909\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1936 - val_loss: 0.2521 - val_acc: 0.2305\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1856\n",
      ". theta fit =  [1.0464461 1.4835228]\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2523 - acc: 0.1915 - val_loss: 0.2521 - val_acc: 0.1822\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1952 - val_loss: 0.2521 - val_acc: 0.2484\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1923 - val_loss: 0.2521 - val_acc: 0.1923\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1944 - val_loss: 0.2523 - val_acc: 0.1896\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1962 - val_loss: 0.2521 - val_acc: 0.2316\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2002 - val_loss: 0.2521 - val_acc: 0.1810\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1911 - val_loss: 0.2521 - val_acc: 0.1903\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1894 - val_loss: 0.2523 - val_acc: 0.1858\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1981 - val_loss: 0.2521 - val_acc: 0.1819\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1944 - val_loss: 0.2521 - val_acc: 0.1829\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1888 - val_loss: 0.2521 - val_acc: 0.1812\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1909 - val_loss: 0.2521 - val_acc: 0.2471\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2002 - val_loss: 0.2521 - val_acc: 0.1922\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1941 - val_loss: 0.2521 - val_acc: 0.1938\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2052 - val_loss: 0.2521 - val_acc: 0.1842\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1907 - val_loss: 0.2521 - val_acc: 0.1834\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1933 - val_loss: 0.2521 - val_acc: 0.1836\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2015 - val_loss: 0.2521 - val_acc: 0.1946\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1904 - val_loss: 0.2522 - val_acc: 0.1862\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1923 - val_loss: 0.2521 - val_acc: 0.2179\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1841\n",
      ". theta fit =  [1.0461352 1.4832106]\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2523 - acc: 0.1932 - val_loss: 0.2522 - val_acc: 0.2345\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1942 - val_loss: 0.2521 - val_acc: 0.1840\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1894 - val_loss: 0.2521 - val_acc: 0.1877\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.1970 - val_loss: 0.2521 - val_acc: 0.1943\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1964 - val_loss: 0.2521 - val_acc: 0.2326\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1957 - val_loss: 0.2522 - val_acc: 0.1856\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1971 - val_loss: 0.2521 - val_acc: 0.1859\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1928 - val_loss: 0.2521 - val_acc: 0.1825\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1961 - val_loss: 0.2521 - val_acc: 0.2486\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2024 - val_loss: 0.2522 - val_acc: 0.1859\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1974 - val_loss: 0.2521 - val_acc: 0.1819\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1908 - val_loss: 0.2521 - val_acc: 0.1944\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1953 - val_loss: 0.2521 - val_acc: 0.2316\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 8s 4us/step - loss: -0.2522 - acc: 0.1824\n",
      ". theta fit =  [1.0464276 1.4828962]\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2524 - acc: 0.1939 - val_loss: 0.2521 - val_acc: 0.1825\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1921 - val_loss: 0.2521 - val_acc: 0.2225\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1933 - val_loss: 0.2521 - val_acc: 0.2486\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1965 - val_loss: 0.2523 - val_acc: 0.1924\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1898 - val_loss: 0.2523 - val_acc: 0.1931\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1926 - val_loss: 0.2522 - val_acc: 0.1857\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 9s 4us/step - loss: -0.2522 - acc: 0.1825\n",
      ". theta fit =  [1.0461826 1.4825791]\n",
      "Iteration:  41\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 14s 14us/step - loss: 0.2524 - acc: 0.1944 - val_loss: 0.2521 - val_acc: 0.2169\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1962 - val_loss: 0.2522 - val_acc: 0.1917\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1943 - val_loss: 0.2521 - val_acc: 0.1839\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2524 - acc: 0.2003 - val_loss: 0.2521 - val_acc: 0.1813\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1957 - val_loss: 0.2521 - val_acc: 0.1826\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1913 - val_loss: 0.2521 - val_acc: 0.1979\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 7s 7us/step - loss: 0.2523 - acc: 0.1905 - val_loss: 0.2521 - val_acc: 0.1923\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2015 - val_loss: 0.2521 - val_acc: 0.1795\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1933 - val_loss: 0.2522 - val_acc: 0.1956\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2522 - acc: 0.1812\n",
      ". theta fit =  [1.0458602 1.4822587]\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2523 - acc: 0.1963 - val_loss: 0.2522 - val_acc: 0.1820\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2036 - val_loss: 0.2521 - val_acc: 0.1905\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1944 - val_loss: 0.2521 - val_acc: 0.1854\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1973 - val_loss: 0.2521 - val_acc: 0.1817\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1951 - val_loss: 0.2521 - val_acc: 0.1945\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1938 - val_loss: 0.2522 - val_acc: 0.1855\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1934 - val_loss: 0.2521 - val_acc: 0.1890\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2008 - val_loss: 0.2521 - val_acc: 0.1810\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1939 - val_loss: 0.2521 - val_acc: 0.1828\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1937 - val_loss: 0.2521 - val_acc: 0.2363\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2522 - acc: 0.1944\n",
      ". theta fit =  [1.045542  1.4819393]\n",
      "Iteration:  43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2523 - acc: 0.2024 - val_loss: 0.2521 - val_acc: 0.1848\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1919 - val_loss: 0.2521 - val_acc: 0.1863\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1941 - val_loss: 0.2521 - val_acc: 0.1814\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1979 - val_loss: 0.2521 - val_acc: 0.1821\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1943 - val_loss: 0.2521 - val_acc: 0.2445\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1963 - val_loss: 0.2521 - val_acc: 0.2310\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1939 - val_loss: 0.2521 - val_acc: 0.1845\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1878 - val_loss: 0.2521 - val_acc: 0.1863\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2032 - val_loss: 0.2521 - val_acc: 0.1881\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1959 - val_loss: 0.2521 - val_acc: 0.1853\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1951 - val_loss: 0.2521 - val_acc: 0.1855\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2001 - val_loss: 0.2521 - val_acc: 0.1904\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1946 - val_loss: 0.2521 - val_acc: 0.2330\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2012 - val_loss: 0.2521 - val_acc: 0.1982\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1879\n",
      ". theta fit =  [1.0452319 1.4816101]\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2523 - acc: 0.2014 - val_loss: 0.2523 - val_acc: 0.1931\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2004 - val_loss: 0.2521 - val_acc: 0.2446\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1986 - val_loss: 0.2521 - val_acc: 0.2290\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2017 - val_loss: 0.2521 - val_acc: 0.1990\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2005 - val_loss: 0.2521 - val_acc: 0.1902\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1888 - val_loss: 0.2521 - val_acc: 0.2110\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1975 - val_loss: 0.2521 - val_acc: 0.1837\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1970 - val_loss: 0.2521 - val_acc: 0.1867\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1938 - val_loss: 0.2522 - val_acc: 0.1857\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1958 - val_loss: 0.2522 - val_acc: 0.1907\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2026 - val_loss: 0.2521 - val_acc: 0.1846\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1920 - val_loss: 0.2523 - val_acc: 0.2167\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1976 - val_loss: 0.2521 - val_acc: 0.1949\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2046 - val_loss: 0.2521 - val_acc: 0.1812\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1912 - val_loss: 0.2521 - val_acc: 0.2319\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1985 - val_loss: 0.2521 - val_acc: 0.1922\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1845\n",
      ". theta fit =  [1.0449011 1.4819306]\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 16s 16us/step - loss: 0.2524 - acc: 0.2045 - val_loss: 0.2521 - val_acc: 0.1927\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1941 - val_loss: 0.2521 - val_acc: 0.1851\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1974 - val_loss: 0.2521 - val_acc: 0.1909\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1957 - val_loss: 0.2521 - val_acc: 0.1838\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2013 - val_loss: 0.2521 - val_acc: 0.1934\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1913 - val_loss: 0.2521 - val_acc: 0.1929\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1987 - val_loss: 0.2521 - val_acc: 0.2324\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1971 - val_loss: 0.2521 - val_acc: 0.1961\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1940 - val_loss: 0.2521 - val_acc: 0.1902\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2006 - val_loss: 0.2522 - val_acc: 0.2411\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1985 - val_loss: 0.2521 - val_acc: 0.2178\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2015 - val_loss: 0.2521 - val_acc: 0.1937\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1951 - val_loss: 0.2521 - val_acc: 0.1822\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1936 - val_loss: 0.2521 - val_acc: 0.2308\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.2021 - val_loss: 0.2521 - val_acc: 0.1906\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1940 - val_loss: 0.2521 - val_acc: 0.2069\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2015 - val_loss: 0.2521 - val_acc: 0.1905\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1938 - val_loss: 0.2521 - val_acc: 0.2153\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1992 - val_loss: 0.2521 - val_acc: 0.1910\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1895 - val_loss: 0.2521 - val_acc: 0.1851\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1850\n",
      ". theta fit =  [1.045238  1.4822662]\n",
      "Refining learning rate\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2523 - acc: 0.2032 - val_loss: 0.2521 - val_acc: 0.1942\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1907 - val_loss: 0.2521 - val_acc: 0.2315\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1955 - val_loss: 0.2521 - val_acc: 0.2121\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2004 - val_loss: 0.2521 - val_acc: 0.1871\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1959 - val_loss: 0.2521 - val_acc: 0.2224\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1988 - val_loss: 0.2521 - val_acc: 0.1822\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2026 - val_loss: 0.2521 - val_acc: 0.1873\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1950 - val_loss: 0.2521 - val_acc: 0.2161\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2023 - val_loss: 0.2521 - val_acc: 0.1802\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2009 - val_loss: 0.2521 - val_acc: 0.1873\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1939 - val_loss: 0.2521 - val_acc: 0.1862\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1821\n",
      ". theta fit =  [1.0451943 1.4819034]\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2523 - acc: 0.2003 - val_loss: 0.2521 - val_acc: 0.1941\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1970 - val_loss: 0.2521 - val_acc: 0.1826\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1943 - val_loss: 0.2521 - val_acc: 0.1870\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2008 - val_loss: 0.2521 - val_acc: 0.1936\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1973 - val_loss: 0.2521 - val_acc: 0.2292\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1976 - val_loss: 0.2522 - val_acc: 0.1900\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2025 - val_loss: 0.2521 - val_acc: 0.2179\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1934 - val_loss: 0.2521 - val_acc: 0.1864\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1868\n",
      ". theta fit =  [1.0451602 1.4819374]\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2524 - acc: 0.2058 - val_loss: 0.2521 - val_acc: 0.1855\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1879 - val_loss: 0.2522 - val_acc: 0.2205\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1944 - val_loss: 0.2521 - val_acc: 0.1840\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1967 - val_loss: 0.2521 - val_acc: 0.2342\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2036 - val_loss: 0.2521 - val_acc: 0.2358\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1991 - val_loss: 0.2522 - val_acc: 0.1877\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1942 - val_loss: 0.2521 - val_acc: 0.1815\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1940 - val_loss: 0.2521 - val_acc: 0.1910\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1952 - val_loss: 0.2521 - val_acc: 0.1844\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1992 - val_loss: 0.2521 - val_acc: 0.1858\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1892 - val_loss: 0.2522 - val_acc: 0.1937\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1991 - val_loss: 0.2521 - val_acc: 0.2346\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1814\n",
      ". theta fit =  [1.0451257 1.4819033]\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2523 - acc: 0.1983 - val_loss: 0.2521 - val_acc: 0.1825\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1969 - val_loss: 0.2521 - val_acc: 0.2335\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1971 - val_loss: 0.2521 - val_acc: 0.1842\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1997 - val_loss: 0.2521 - val_acc: 0.1914\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1988 - val_loss: 0.2521 - val_acc: 0.1873\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1953 - val_loss: 0.2521 - val_acc: 0.1836\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1824\n",
      ". theta fit =  [1.0450909 1.4818684]\n",
      "Iteration:  50\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2524 - acc: 0.1955 - val_loss: 0.2521 - val_acc: 0.1823\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1976 - val_loss: 0.2521 - val_acc: 0.1883\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1955 - val_loss: 0.2521 - val_acc: 0.2405\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1984 - val_loss: 0.2521 - val_acc: 0.1875\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1937 - val_loss: 0.2521 - val_acc: 0.1821\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1982 - val_loss: 0.2521 - val_acc: 0.1864\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1939 - val_loss: 0.2521 - val_acc: 0.1907\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1979 - val_loss: 0.2521 - val_acc: 0.1813\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1928 - val_loss: 0.2521 - val_acc: 0.1818\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1968 - val_loss: 0.2521 - val_acc: 0.1909\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1966 - val_loss: 0.2521 - val_acc: 0.1812\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1976 - val_loss: 0.2521 - val_acc: 0.1850\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1926 - val_loss: 0.2521 - val_acc: 0.1848\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2045 - val_loss: 0.2521 - val_acc: 0.2137\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1818\n",
      ". theta fit =  [1.0450557 1.4818332]\n",
      "Iteration:  51\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2523 - acc: 0.1900 - val_loss: 0.2521 - val_acc: 0.1853\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1930 - val_loss: 0.2521 - val_acc: 0.1952\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1914 - val_loss: 0.2522 - val_acc: 0.1901\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2002 - val_loss: 0.2521 - val_acc: 0.1870\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1929 - val_loss: 0.2521 - val_acc: 0.1846\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1983 - val_loss: 0.2521 - val_acc: 0.2198\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1996 - val_loss: 0.2521 - val_acc: 0.1925\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1950\n",
      ". theta fit =  [1.0450206 1.4817977]\n",
      "Iteration:  52\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 17s 17us/step - loss: 0.2523 - acc: 0.1977 - val_loss: 0.2521 - val_acc: 0.1835\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2014 - val_loss: 0.2522 - val_acc: 0.1858\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2067 - val_loss: 0.2521 - val_acc: 0.2208\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1926 - val_loss: 0.2522 - val_acc: 0.1880\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1976 - val_loss: 0.2521 - val_acc: 0.2400\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1996 - val_loss: 0.2521 - val_acc: 0.1894\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1955 - val_loss: 0.2521 - val_acc: 0.1889\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2039 - val_loss: 0.2521 - val_acc: 0.1849\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2004 - val_loss: 0.2521 - val_acc: 0.1901\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1949 - val_loss: 0.2521 - val_acc: 0.1854\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2023 - val_loss: 0.2522 - val_acc: 0.1836\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2008 - val_loss: 0.2521 - val_acc: 0.1887\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1887\n",
      ". theta fit =  [1.0449858 1.4817619]\n",
      "Iteration:  53\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2523 - acc: 0.1936 - val_loss: 0.2521 - val_acc: 0.2328\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1981 - val_loss: 0.2522 - val_acc: 0.1865\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1980 - val_loss: 0.2521 - val_acc: 0.1902\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2016 - val_loss: 0.2522 - val_acc: 0.1848\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1943 - val_loss: 0.2521 - val_acc: 0.1871\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1934 - val_loss: 0.2522 - val_acc: 0.1834\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1989 - val_loss: 0.2521 - val_acc: 0.1999\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1966 - val_loss: 0.2521 - val_acc: 0.1840\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2009 - val_loss: 0.2522 - val_acc: 0.1857\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2019 - val_loss: 0.2521 - val_acc: 0.2318\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1869\n",
      ". theta fit =  [1.0449497 1.481726 ]\n",
      "Iteration:  54\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2524 - acc: 0.1907 - val_loss: 0.2521 - val_acc: 0.2027\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2026 - val_loss: 0.2521 - val_acc: 0.2309\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2006 - val_loss: 0.2521 - val_acc: 0.2292\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2001 - val_loss: 0.2521 - val_acc: 0.1916\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1969 - val_loss: 0.2521 - val_acc: 0.1797\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1973 - val_loss: 0.2521 - val_acc: 0.1840\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1925 - val_loss: 0.2521 - val_acc: 0.1998\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1978 - val_loss: 0.2521 - val_acc: 0.1832\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1998 - val_loss: 0.2521 - val_acc: 0.1976\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1988 - val_loss: 0.2521 - val_acc: 0.2311\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1984 - val_loss: 0.2521 - val_acc: 0.1904\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1956 - val_loss: 0.2521 - val_acc: 0.1848\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2009 - val_loss: 0.2521 - val_acc: 0.1894\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1903 - val_loss: 0.2521 - val_acc: 0.1882\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2015 - val_loss: 0.2521 - val_acc: 0.1813\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1979 - val_loss: 0.2521 - val_acc: 0.1842\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1980 - val_loss: 0.2521 - val_acc: 0.1974\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1983 - val_loss: 0.2521 - val_acc: 0.1945\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1961 - val_loss: 0.2521 - val_acc: 0.1830\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1921 - val_loss: 0.2521 - val_acc: 0.1926\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1924\n",
      ". theta fit =  [1.0449857 1.4817618]\n",
      "Iteration:  55\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2523 - acc: 0.1973 - val_loss: 0.2521 - val_acc: 0.1837\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1924 - val_loss: 0.2521 - val_acc: 0.2184\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1950 - val_loss: 0.2521 - val_acc: 0.1944\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1989 - val_loss: 0.2521 - val_acc: 0.1945\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1961 - val_loss: 0.2521 - val_acc: 0.1876\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1936 - val_loss: 0.2521 - val_acc: 0.2023\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1835\n",
      ". theta fit =  [1.0449502 1.481798 ]\n",
      "Iteration:  56\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2524 - acc: 0.1953 - val_loss: 0.2521 - val_acc: 0.1837\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2014 - val_loss: 0.2521 - val_acc: 0.1869\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1910 - val_loss: 0.2522 - val_acc: 0.1927\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1988 - val_loss: 0.2521 - val_acc: 0.1893\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1968 - val_loss: 0.2522 - val_acc: 0.2253\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1982 - val_loss: 0.2521 - val_acc: 0.2344\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1948 - val_loss: 0.2521 - val_acc: 0.1948\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1868\n",
      ". theta fit =  [1.0449132 1.4818333]\n",
      "Iteration:  57\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2523 - acc: 0.1922 - val_loss: 0.2523 - val_acc: 0.1876\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1975 - val_loss: 0.2521 - val_acc: 0.1818\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1983 - val_loss: 0.2522 - val_acc: 0.1949\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2077 - val_loss: 0.2521 - val_acc: 0.1837\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2069 - val_loss: 0.2521 - val_acc: 0.2237\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1970 - val_loss: 0.2521 - val_acc: 0.2202\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1954 - val_loss: 0.2521 - val_acc: 0.1950\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1933 - val_loss: 0.2521 - val_acc: 0.1863\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2003 - val_loss: 0.2521 - val_acc: 0.1880\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1957 - val_loss: 0.2521 - val_acc: 0.1892\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1973 - val_loss: 0.2521 - val_acc: 0.1916\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1929 - val_loss: 0.2522 - val_acc: 0.1881\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1996 - val_loss: 0.2521 - val_acc: 0.1890\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2034 - val_loss: 0.2521 - val_acc: 0.1812\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1955 - val_loss: 0.2521 - val_acc: 0.1837\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2012 - val_loss: 0.2521 - val_acc: 0.2176\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1939 - val_loss: 0.2521 - val_acc: 0.1838\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1978 - val_loss: 0.2521 - val_acc: 0.2350\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2027 - val_loss: 0.2522 - val_acc: 0.1890\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2522 - acc: 0.1811\n",
      ". theta fit =  [1.0448768 1.4818704]\n",
      "Iteration:  58\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2524 - acc: 0.1969 - val_loss: 0.2521 - val_acc: 0.1855\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2010 - val_loss: 0.2521 - val_acc: 0.1859\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1949 - val_loss: 0.2521 - val_acc: 0.2339\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2032 - val_loss: 0.2521 - val_acc: 0.2221\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2013 - val_loss: 0.2521 - val_acc: 0.2222\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1918 - val_loss: 0.2521 - val_acc: 0.1928\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1932 - val_loss: 0.2521 - val_acc: 0.1929\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1997 - val_loss: 0.2522 - val_acc: 0.1949\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.2337\n",
      ". theta fit =  [1.0448399 1.4818329]\n",
      "Iteration:  59\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2524 - acc: 0.1997 - val_loss: 0.2521 - val_acc: 0.1903\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1940 - val_loss: 0.2522 - val_acc: 0.1887\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1996 - val_loss: 0.2522 - val_acc: 0.1890\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1958 - val_loss: 0.2521 - val_acc: 0.1854\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1946 - val_loss: 0.2521 - val_acc: 0.1956\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1993 - val_loss: 0.2521 - val_acc: 0.1957\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1910 - val_loss: 0.2521 - val_acc: 0.1876\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1956 - val_loss: 0.2521 - val_acc: 0.1960\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2017 - val_loss: 0.2521 - val_acc: 0.1858\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1853\n",
      ". theta fit =  [1.0448022 1.4817952]\n",
      "Iteration:  60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2523 - acc: 0.1920 - val_loss: 0.2521 - val_acc: 0.1869\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1968 - val_loss: 0.2521 - val_acc: 0.1871\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2029 - val_loss: 0.2521 - val_acc: 0.1853\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1943 - val_loss: 0.2521 - val_acc: 0.2300\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2524 - acc: 0.1936 - val_loss: 0.2521 - val_acc: 0.2261\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2000 - val_loss: 0.2521 - val_acc: 0.1914\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1959 - val_loss: 0.2523 - val_acc: 0.1864\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2004 - val_loss: 0.2522 - val_acc: 0.1934\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2522 - acc: 0.1852\n",
      ". theta fit =  [1.0447642 1.4817573]\n",
      "Iteration:  61\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2523 - acc: 0.2006 - val_loss: 0.2521 - val_acc: 0.2257\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1973 - val_loss: 0.2521 - val_acc: 0.2276\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1981 - val_loss: 0.2521 - val_acc: 0.1862\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2015 - val_loss: 0.2521 - val_acc: 0.1852\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1920 - val_loss: 0.2521 - val_acc: 0.1841\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1961 - val_loss: 0.2521 - val_acc: 0.2317\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2017 - val_loss: 0.2521 - val_acc: 0.1917\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1927 - val_loss: 0.2522 - val_acc: 0.1890\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1917 - val_loss: 0.2521 - val_acc: 0.1888\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1918 - val_loss: 0.2521 - val_acc: 0.2186\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1840\n",
      ". theta fit =  [1.0448008 1.4817939]\n",
      "Iteration:  62\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2523 - acc: 0.1955 - val_loss: 0.2521 - val_acc: 0.1953\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1951 - val_loss: 0.2522 - val_acc: 0.1822\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2045 - val_loss: 0.2521 - val_acc: 0.1876\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1898 - val_loss: 0.2522 - val_acc: 0.1846\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1963 - val_loss: 0.2522 - val_acc: 0.1858\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1955 - val_loss: 0.2521 - val_acc: 0.1832\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1966 - val_loss: 0.2521 - val_acc: 0.2335\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2041 - val_loss: 0.2521 - val_acc: 0.2210\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2522 - acc: 0.1874\n",
      ". theta fit =  [1.0447618 1.4817549]\n",
      "Iteration:  63\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2523 - acc: 0.2004 - val_loss: 0.2521 - val_acc: 0.2222\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1949 - val_loss: 0.2521 - val_acc: 0.1904\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1923 - val_loss: 0.2521 - val_acc: 0.1961\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1978 - val_loss: 0.2521 - val_acc: 0.2198\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2023 - val_loss: 0.2521 - val_acc: 0.1927\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1910 - val_loss: 0.2521 - val_acc: 0.2380\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1945 - val_loss: 0.2521 - val_acc: 0.2328\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1988 - val_loss: 0.2521 - val_acc: 0.1880\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1919 - val_loss: 0.2521 - val_acc: 0.2406\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2036 - val_loss: 0.2522 - val_acc: 0.1869\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2522 - acc: 0.1925\n",
      ". theta fit =  [1.0447228 1.4817159]\n",
      "Iteration:  64\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2523 - acc: 0.1930 - val_loss: 0.2522 - val_acc: 0.1928\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1923 - val_loss: 0.2522 - val_acc: 0.2230\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1935 - val_loss: 0.2521 - val_acc: 0.2338\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1970 - val_loss: 0.2522 - val_acc: 0.1917\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1954 - val_loss: 0.2521 - val_acc: 0.1852\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1975 - val_loss: 0.2521 - val_acc: 0.1851\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1964 - val_loss: 0.2521 - val_acc: 0.1862\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2040 - val_loss: 0.2522 - val_acc: 0.1874\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1918 - val_loss: 0.2521 - val_acc: 0.1906\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1993 - val_loss: 0.2521 - val_acc: 0.1843\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1992 - val_loss: 0.2521 - val_acc: 0.1819\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1994 - val_loss: 0.2521 - val_acc: 0.1957\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2005 - val_loss: 0.2522 - val_acc: 0.1903\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1960 - val_loss: 0.2521 - val_acc: 0.2191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1992 - val_loss: 0.2521 - val_acc: 0.1941\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2522 - acc: 0.1842\n",
      ". theta fit =  [1.0446835 1.4816767]\n",
      "Iteration:  65\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2523 - acc: 0.1991 - val_loss: 0.2521 - val_acc: 0.2255\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2018 - val_loss: 0.2521 - val_acc: 0.1858\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2000 - val_loss: 0.2521 - val_acc: 0.1860\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1987 - val_loss: 0.2521 - val_acc: 0.1833\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1909 - val_loss: 0.2521 - val_acc: 0.2430\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2010 - val_loss: 0.2521 - val_acc: 0.1891\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1959 - val_loss: 0.2523 - val_acc: 0.1927\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2522 - acc: 0.1856\n",
      ". theta fit =  [1.0447228 1.4817162]\n",
      "Iteration:  66\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2523 - acc: 0.1993 - val_loss: 0.2521 - val_acc: 0.1873\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1957 - val_loss: 0.2522 - val_acc: 0.1829\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1984 - val_loss: 0.2521 - val_acc: 0.1895\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1954 - val_loss: 0.2521 - val_acc: 0.2176\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1961 - val_loss: 0.2521 - val_acc: 0.1931\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1905 - val_loss: 0.2521 - val_acc: 0.1934\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1978 - val_loss: 0.2521 - val_acc: 0.1849\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1966 - val_loss: 0.2521 - val_acc: 0.1897\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2013 - val_loss: 0.2521 - val_acc: 0.2314\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1953 - val_loss: 0.2522 - val_acc: 0.1859\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2522 - acc: 0.1930\n",
      ". theta fit =  [1.0446829 1.4816766]\n",
      "Refining learning rate\n",
      "Iteration:  67\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2524 - acc: 0.1892 - val_loss: 0.2521 - val_acc: 0.1920\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2017 - val_loss: 0.2521 - val_acc: 0.1867\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1937 - val_loss: 0.2521 - val_acc: 0.2314\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1982 - val_loss: 0.2522 - val_acc: 0.2211\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2013 - val_loss: 0.2521 - val_acc: 0.1867\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1946 - val_loss: 0.2521 - val_acc: 0.1937\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1991 - val_loss: 0.2521 - val_acc: 0.2354\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2522 - acc: 0.1865\n",
      ". theta fit =  [1.044699  1.4817002]\n",
      "Iteration:  68\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2522 - acc: 0.2064 - val_loss: 0.2522 - val_acc: 0.1878\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1949 - val_loss: 0.2521 - val_acc: 0.1800\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1919 - val_loss: 0.2521 - val_acc: 0.1929\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1991 - val_loss: 0.2521 - val_acc: 0.1883\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1943 - val_loss: 0.2522 - val_acc: 0.1867\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1936 - val_loss: 0.2522 - val_acc: 0.1851\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.2017 - val_loss: 0.2521 - val_acc: 0.1872\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1923 - val_loss: 0.2521 - val_acc: 0.1928\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1966 - val_loss: 0.2521 - val_acc: 0.1917\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1957 - val_loss: 0.2521 - val_acc: 0.2205\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1969 - val_loss: 0.2521 - val_acc: 0.2374\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1976 - val_loss: 0.2521 - val_acc: 0.1921\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2522 - acc: 0.1870\n",
      ". theta fit =  [1.0446949 1.4816961]\n",
      "Iteration:  69\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2523 - acc: 0.1998 - val_loss: 0.2521 - val_acc: 0.2178\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1947 - val_loss: 0.2521 - val_acc: 0.1906\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1947 - val_loss: 0.2521 - val_acc: 0.1898\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1943 - val_loss: 0.2521 - val_acc: 0.1818\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1951 - val_loss: 0.2521 - val_acc: 0.1856\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1955 - val_loss: 0.2521 - val_acc: 0.1915\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1970 - val_loss: 0.2521 - val_acc: 0.1855\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2008 - val_loss: 0.2521 - val_acc: 0.1879\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2524 - acc: 0.1922 - val_loss: 0.2521 - val_acc: 0.1849\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1969 - val_loss: 0.2521 - val_acc: 0.2321\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2086 - val_loss: 0.2521 - val_acc: 0.2146\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1952 - val_loss: 0.2521 - val_acc: 0.1850\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2522 - acc: 0.1940 - val_loss: 0.2522 - val_acc: 0.1892\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2014 - val_loss: 0.2521 - val_acc: 0.1833\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2522 - acc: 0.1848\n",
      ". theta fit =  [1.0446988 1.4816922]\n",
      "Iteration:  70\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2523 - acc: 0.2109 - val_loss: 0.2521 - val_acc: 0.2234\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1937 - val_loss: 0.2521 - val_acc: 0.1884\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1921 - val_loss: 0.2522 - val_acc: 0.1909\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1978 - val_loss: 0.2521 - val_acc: 0.1851\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2001 - val_loss: 0.2521 - val_acc: 0.1895\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1889 - val_loss: 0.2521 - val_acc: 0.1942\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2013 - val_loss: 0.2521 - val_acc: 0.1893\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1914 - val_loss: 0.2521 - val_acc: 0.1943\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1950 - val_loss: 0.2521 - val_acc: 0.1864\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2522 - acc: 0.1850\n",
      ". theta fit =  [1.0447029 1.4816883]\n",
      "Iteration:  71\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2524 - acc: 0.1996 - val_loss: 0.2521 - val_acc: 0.2359\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2007 - val_loss: 0.2521 - val_acc: 0.1835\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1965 - val_loss: 0.2521 - val_acc: 0.1939\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1958 - val_loss: 0.2521 - val_acc: 0.1919\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1906 - val_loss: 0.2522 - val_acc: 0.2204\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1956 - val_loss: 0.2521 - val_acc: 0.1944\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1993 - val_loss: 0.2521 - val_acc: 0.1850\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1964 - val_loss: 0.2522 - val_acc: 0.1860\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2040 - val_loss: 0.2521 - val_acc: 0.1832\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1910 - val_loss: 0.2521 - val_acc: 0.2361\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1972 - val_loss: 0.2521 - val_acc: 0.1840\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1924 - val_loss: 0.2521 - val_acc: 0.1827\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1964 - val_loss: 0.2522 - val_acc: 0.1890\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1982 - val_loss: 0.2521 - val_acc: 0.1852\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1920 - val_loss: 0.2521 - val_acc: 0.1920\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1985 - val_loss: 0.2521 - val_acc: 0.1838\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1933 - val_loss: 0.2521 - val_acc: 0.1894\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1966 - val_loss: 0.2521 - val_acc: 0.1843\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1961 - val_loss: 0.2521 - val_acc: 0.1873\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1971 - val_loss: 0.2521 - val_acc: 0.1918\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2522 - acc: 0.1916\n",
      ". theta fit =  [1.0447068 1.4816924]\n",
      "Iteration:  72\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2523 - acc: 0.1950 - val_loss: 0.2521 - val_acc: 0.1845\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1972 - val_loss: 0.2521 - val_acc: 0.1880\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1905 - val_loss: 0.2521 - val_acc: 0.2302\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2021 - val_loss: 0.2521 - val_acc: 0.1852\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1957 - val_loss: 0.2521 - val_acc: 0.2243\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1929 - val_loss: 0.2521 - val_acc: 0.2308\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1991 - val_loss: 0.2521 - val_acc: 0.1850\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1902 - val_loss: 0.2521 - val_acc: 0.1844\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1921 - val_loss: 0.2521 - val_acc: 0.1881\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 7us/step - loss: -0.2522 - acc: 0.1851\n",
      ". theta fit =  [1.0447026 1.4816966]\n",
      "Iteration:  73\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2523 - acc: 0.1899 - val_loss: 0.2522 - val_acc: 0.1910\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1985 - val_loss: 0.2521 - val_acc: 0.1933\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.2002 - val_loss: 0.2521 - val_acc: 0.1852\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1941 - val_loss: 0.2521 - val_acc: 0.1893\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1958 - val_loss: 0.2521 - val_acc: 0.1951\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1920 - val_loss: 0.2521 - val_acc: 0.1872\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1985 - val_loss: 0.2521 - val_acc: 0.1820\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1908 - val_loss: 0.2522 - val_acc: 0.1921\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2524 - acc: 0.1952 - val_loss: 0.2521 - val_acc: 0.2356\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2522 - acc: 0.1891\n",
      ". theta fit =  [1.0446985 1.4817008]\n",
      "Iteration:  74\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 20s 20us/step - loss: 0.2525 - acc: 0.1913 - val_loss: 0.2521 - val_acc: 0.1885\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2524 - acc: 0.1922 - val_loss: 0.2521 - val_acc: 0.1907\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1942 - val_loss: 0.2521 - val_acc: 0.2259\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1987 - val_loss: 0.2521 - val_acc: 0.1826\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1912 - val_loss: 0.2521 - val_acc: 0.1878\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1910 - val_loss: 0.2521 - val_acc: 0.2217\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2523 - acc: 0.1954 - val_loss: 0.2521 - val_acc: 0.2232\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1946 - val_loss: 0.2521 - val_acc: 0.1961\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1971 - val_loss: 0.2521 - val_acc: 0.1845\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2523 - acc: 0.1915 - val_loss: 0.2521 - val_acc: 0.1915\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2522 - acc: 0.1876\n",
      ". theta fit =  [1.0446943 1.4816966]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):    \n",
    "    print(\"Iteration: \", iteration )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=my_loss_wrapper_fit(1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=20, batch_size=1000,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta), y_train_theta, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test_theta), y_test_theta),verbose=1,callbacks=callbacks)\n",
    "    \n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "    \n",
    "    fit_vals_mu = np.array(fit_vals)[(index_refine[-1]):,0]\n",
    "    fit_vals_sigma = np.array(fit_vals)[(index_refine[-1]):,1]\n",
    "    \n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "    extrema_mu = np.concatenate((argrelmin(fit_vals_mu)[0], argrelmax(fit_vals_mu)[0]))\n",
    "    extrema_mu = extrema_mu[extrema_mu>= iteration - index_refine[-1] -20]\n",
    "            \n",
    "    extrema_sigma = np.concatenate((argrelmin(fit_vals_sigma)[0], argrelmax(fit_vals_sigma)[0]))\n",
    "    extrema_sigma = extrema_sigma[extrema_sigma >= iteration - index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema_mu\", extrema_mu)\n",
    "    print(\"extrema_sigma\", extrema_sigma)\n",
    "    '''\n",
    "    \n",
    "    if (len(extrema_mu) == 0) or (len(extrema_sigma) == 0): # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema_mu) >= 6) and (len(extrema_sigma) >= 6): #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print(\"Refining learning rate\")\n",
    "        optimizer.lr = optimizer.lr/10\n",
    "        \n",
    "        mean_fit = np.array([[np.mean(fit_vals_mu[len(fit_vals_mu)-4:len(fit_vals_mu)]),\n",
    "                              np.mean(fit_vals_sigma[len(fit_vals_sigma)-4:len(fit_vals_sigma)])]])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T23:34:47.711512Z",
     "start_time": "2020-05-31T23:34:47.053487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex= True,  constrained_layout=True)\n",
    "fig.suptitle(\"GaussianAltFit-2D-Detector Effects (DCTR Reweight):\\n N = {:.0e}, Iterations = {:.0f}\".format(N, len(fit_vals)))\n",
    "axs[0].plot(fit_vals[:,0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], 0, len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(fit_vals[:,1], label='Model $\\sigma$ Fit', color ='orange')\n",
    "axs[1].hlines(theta1_param[1], 0, len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.savefig(\"GaussianAltFit-2D-Detector Effects (DCTR Reweight):\\n N = {:.0e}, Iterations = {:.0f}.png\".format(N, len(fit_vals)))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T23:34:48.379437Z",
     "start_time": "2020-05-31T23:34:47.714812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "\n",
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex= True,  constrained_layout=True)\n",
    "fig.suptitle(\"GaussianAltFit-2D-Detector Effects Zoomed (DCTR Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\".format(N,index_refine[1], len(fit_vals)))\n",
    "axs[0].plot(np.arange(index_refine[1], len(fit_vals[:,0])), fit_vals[index_refine[1]:,0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], index_refine[1], len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.arange(index_refine[1], len(fit_vals[:,1])), fit_vals[index_refine[1]:,1], label='Model $\\sigma$ Fit', color = 'orange')\n",
    "axs[1].hlines(theta1_param[1], index_refine[1], len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.savefig(\"GaussianAltFit-2D-Detector Effects Zoomed (DCTR Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\".format(N,index_refine[1], len(fit_vals)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Fitting between DCTR Reweighting and Analytical Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T23:34:48.388375Z",
     "start_time": "2020-05-31T23:34:48.383786Z"
    }
   },
   "outputs": [],
   "source": [
    "reweight_analytically = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T23:34:48.402980Z",
     "start_time": "2020-05-31T23:34:48.391123Z"
    }
   },
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = [0., 1.]\n",
    "\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='loss',\n",
    "                              patience = 3,\n",
    "                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T02:02:59.194397Z",
     "start_time": "2020-06-01T02:02:59.062593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 16,899\n",
      "Trainable params: 16,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=(2,),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "index_refine = np.array([0])\n",
    "batch_size = 2*N\n",
    "lr_initial = 5e-1 #smaller learning rate yields better precision\n",
    "iterations = 75 #but requires more epochs to train\n",
    "optimizer = keras.optimizers.Adam(lr=lr_initial)\n",
    "\n",
    "def my_loss_wrapper_fit(mysign = 1):\n",
    "    theta = 0. #starting value\n",
    "    #Getting theta_prime:\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        #Getting theta_prime:\n",
    "        if train_theta == False:\n",
    "            theta_prime = model_fit.layers[-1].get_weights()[0] #when not training theta, fetch as np array\n",
    "            y_true = tf.gather(y_true, np.arange(1000)) \n",
    "        else:\n",
    "            theta_prime = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "            y_true = tf.gather(y_true, np.arange(batch_size)) \n",
    "        y_labels = tf.gather(y_true, [0], axis = 1) #actual y_true for loss\n",
    "        x_T = tf.gather(y_true, [1], axis = 1) # sim truth for reweighting\n",
    "        \n",
    "        #creating tensor with same length as inputs, with theta_prime in every entry\n",
    "        concat_input_and_params = K.ones(shape = (x_T.shape[0], 2), dtype=tf.float32)*theta_prime\n",
    "        data = K.concatenate((x_T, concat_input_and_params), axis=-1)\n",
    "        if reweight_analytically == False: #NN reweight\n",
    "            w = reweight(data)\n",
    "        else: # analytical reweight\n",
    "            w = analytical_reweight(events = x_T, \n",
    "                                    mu1 = theta_prime[0], \n",
    "                                    sigma1 = theta_prime[1]) \n",
    "        \n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_labels*(y_labels - y_pred)**2+(w)*(1.-y_labels)*(y_labels - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \"\"\"\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_labels)*K.log(y_pred) +w*(1-y_labels)*K.log(1-y_pred))\n",
    "        \"\"\"\n",
    "        \n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:30:59.486197Z",
     "start_time": "2020-06-01T02:02:59.997699Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2164 - acc: 0.3247 - val_loss: 0.2132 - val_acc: 0.3287\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2128 - acc: 0.3293 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2127 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2130 - val_acc: 0.3292\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2130 - val_acc: 0.3292\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3294 - val_loss: 0.2129 - val_acc: 0.3293\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3294 - val_loss: 0.2128 - val_acc: 0.3292\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2128 - val_acc: 0.3292\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2128 - val_acc: 0.3292\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 17/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2128 - val_acc: 0.3292\n",
      "Epoch 18/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 19/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3295 - val_loss: 0.2129 - val_acc: 0.3292\n",
      "Epoch 20/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2126 - acc: 0.3296 - val_loss: 0.2128 - val_acc: 0.3292\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2127 - acc: 0.3294\n",
      ". theta fit =  [0.49996564 1.4999661 ]\n",
      "Iteration:  1\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 19s 19us/step - loss: 0.2447 - acc: 0.3274 - val_loss: 0.2433 - val_acc: 0.3285\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2434 - acc: 0.3279 - val_loss: 0.2432 - val_acc: 0.3281\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2434 - acc: 0.3277 - val_loss: 0.2432 - val_acc: 0.3268\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2434 - acc: 0.3277 - val_loss: 0.2432 - val_acc: 0.3282\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2434 - acc: 0.3278 - val_loss: 0.2432 - val_acc: 0.3278\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2433 - acc: 0.3279 - val_loss: 0.2432 - val_acc: 0.3279\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2433 - acc: 0.3279 - val_loss: 0.2433 - val_acc: 0.3286\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2433 - acc: 0.3279 - val_loss: 0.2433 - val_acc: 0.3276\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2433 - acc: 0.3279 - val_loss: 0.2433 - val_acc: 0.3287\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2433 - acc: 0.3277 - val_loss: 0.2433 - val_acc: 0.3276\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2433 - acc: 0.3280 - val_loss: 0.2432 - val_acc: 0.3278\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2433 - acc: 0.3280 - val_loss: 0.2433 - val_acc: 0.3281\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2433 - acc: 0.3276 - val_loss: 0.2434 - val_acc: 0.3267\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2433 - acc: 0.3281 - val_loss: 0.2432 - val_acc: 0.3278\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2433 - acc: 0.3278 - val_loss: 0.2433 - val_acc: 0.3271\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2433 - acc: 0.3280 - val_loss: 0.2432 - val_acc: 0.3274\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 12s 6us/step - loss: -0.2434 - acc: 0.3267\n",
      ". theta fit =  [0.8719877 1.8716698]\n",
      "Iteration:  2\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2451 - acc: 0.2956 - val_loss: 0.2436 - val_acc: 0.2600\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2436 - acc: 0.2625 - val_loss: 0.2438 - val_acc: 0.2696\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2434 - acc: 0.2649 - val_loss: 0.2439 - val_acc: 0.2319\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2436 - acc: 0.2626 - val_loss: 0.2436 - val_acc: 0.2743\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2435 - acc: 0.2645 - val_loss: 0.2433 - val_acc: 0.2582\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2435 - acc: 0.2674 - val_loss: 0.2436 - val_acc: 0.2423\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 11s 6us/step - loss: -0.2439 - acc: 0.2321\n",
      ". theta fit =  [1.1896038 1.5523076]\n",
      "Iteration:  3\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 18s 18us/step - loss: 0.2481 - acc: 0.1934 - val_loss: 0.2478 - val_acc: 0.1944\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2479 - acc: 0.1873 - val_loss: 0.2477 - val_acc: 0.1839\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2480 - acc: 0.1855 - val_loss: 0.2477 - val_acc: 0.1876\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2481 - acc: 0.1848 - val_loss: 0.2478 - val_acc: 0.1893\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2480 - acc: 0.1873 - val_loss: 0.2483 - val_acc: 0.2179\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 13s 6us/step - loss: -0.2477 - acc: 0.1836\n",
      ". theta fit =  [0.8991468 1.2619579]\n",
      "Iteration:  4\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2472 - acc: 0.2664 - val_loss: 0.2465 - val_acc: 0.2685\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2469 - acc: 0.2753 - val_loss: 0.2465 - val_acc: 0.2743\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2469 - acc: 0.2753 - val_loss: 0.2465 - val_acc: 0.2760\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2469 - acc: 0.2765 - val_loss: 0.2465 - val_acc: 0.2762\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2469 - acc: 0.2776 - val_loss: 0.2465 - val_acc: 0.2735\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2466 - acc: 0.2743\n",
      ". theta fit =  [1.1717167 1.5346624]\n",
      "Iteration:  5\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 21s 21us/step - loss: 0.2485 - acc: 0.1950 - val_loss: 0.2482 - val_acc: 0.1731\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2483 - acc: 0.1858 - val_loss: 0.2482 - val_acc: 0.1743\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2483 - acc: 0.1869 - val_loss: 0.2480 - val_acc: 0.1775\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2482 - acc: 0.1841 - val_loss: 0.2482 - val_acc: 0.1928\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2482 - acc: 0.1867 - val_loss: 0.2481 - val_acc: 0.2079\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2482 - acc: 0.1849 - val_loss: 0.2483 - val_acc: 0.1865\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2483 - acc: 0.1854 - val_loss: 0.2482 - val_acc: 0.1755\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2481 - acc: 0.1859 - val_loss: 0.2483 - val_acc: 0.1749\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2481 - acc: 0.1862 - val_loss: 0.2484 - val_acc: 0.1729\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2483 - acc: 0.1852 - val_loss: 0.2481 - val_acc: 0.1764\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2482 - acc: 0.1852 - val_loss: 0.2480 - val_acc: 0.1740\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2481 - acc: 0.1844 - val_loss: 0.2482 - val_acc: 0.1960\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2484 - acc: 0.1728\n",
      ". theta fit =  [0.9107276 1.2737479]\n",
      "Iteration:  6\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2474 - acc: 0.2722 - val_loss: 0.2469 - val_acc: 0.2825\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2471 - acc: 0.2770 - val_loss: 0.2470 - val_acc: 0.2886\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.2768 - val_loss: 0.2468 - val_acc: 0.2798\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.2772 - val_loss: 0.2468 - val_acc: 0.2819\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.2765 - val_loss: 0.2472 - val_acc: 0.2651\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2471 - acc: 0.2887\n",
      ". theta fit =  [1.1638205 1.5268997]\n",
      "Iteration:  7\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2487 - acc: 0.1950 - val_loss: 0.2485 - val_acc: 0.1734\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2483 - acc: 0.1816 - val_loss: 0.2480 - val_acc: 0.1778\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2482 - acc: 0.1831 - val_loss: 0.2482 - val_acc: 0.1790\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2483 - acc: 0.1829 - val_loss: 0.2485 - val_acc: 0.2021\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2482 - acc: 0.1851 - val_loss: 0.2489 - val_acc: 0.1937\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2482 - acc: 0.1824 - val_loss: 0.2481 - val_acc: 0.1766\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2483 - acc: 0.1880 - val_loss: 0.2484 - val_acc: 0.1743\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2481 - acc: 0.1861 - val_loss: 0.2485 - val_acc: 0.1740\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2481 - acc: 0.1875 - val_loss: 0.2481 - val_acc: 0.1776\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2482 - acc: 0.1818 - val_loss: 0.2481 - val_acc: 0.1744\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2483 - acc: 0.1830 - val_loss: 0.2482 - val_acc: 0.2070\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 14s 7us/step - loss: -0.2485 - acc: 0.1738\n",
      ". theta fit =  [0.91600597 1.2792356 ]\n",
      "Iteration:  8\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2475 - acc: 0.2684 - val_loss: 0.2469 - val_acc: 0.2786\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2760 - val_loss: 0.2471 - val_acc: 0.2681\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2766 - val_loss: 0.2469 - val_acc: 0.2757\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2763 - val_loss: 0.2469 - val_acc: 0.2758\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2763 - val_loss: 0.2470 - val_acc: 0.2803\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2765 - val_loss: 0.2470 - val_acc: 0.2772\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2762 - val_loss: 0.2470 - val_acc: 0.2803\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.2763 - val_loss: 0.2469 - val_acc: 0.2729\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2755 - val_loss: 0.2469 - val_acc: 0.2791\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.2756 - val_loss: 0.2470 - val_acc: 0.2822\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2770 - val_loss: 0.2470 - val_acc: 0.2773\n",
      "Epoch 12/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.2746 - val_loss: 0.2469 - val_acc: 0.2777\n",
      "Epoch 13/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.2754 - val_loss: 0.2470 - val_acc: 0.2734\n",
      "Epoch 14/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2472 - acc: 0.2756 - val_loss: 0.2470 - val_acc: 0.2762\n",
      "Epoch 15/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2755 - val_loss: 0.2471 - val_acc: 0.2802\n",
      "Epoch 16/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2473 - acc: 0.2756 - val_loss: 0.2470 - val_acc: 0.2751\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2471 - acc: 0.2734\n",
      ". theta fit =  [1.160131  1.5235673]\n",
      "Refining learning rate\n",
      "Iteration:  9\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2488 - acc: 0.2298 - val_loss: 0.2487 - val_acc: 0.2225\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2216 - val_loss: 0.2486 - val_acc: 0.2524\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2306 - val_loss: 0.2486 - val_acc: 0.2433\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2286 - val_loss: 0.2487 - val_acc: 0.2440\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.2318 - val_loss: 0.2486 - val_acc: 0.2236\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2242 - val_loss: 0.2486 - val_acc: 0.2027\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2487 - acc: 0.2309 - val_loss: 0.2486 - val_acc: 0.2175\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2265 - val_loss: 0.2486 - val_acc: 0.2200\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2487 - acc: 0.2232\n",
      ". theta fit =  [1.013483  1.4250731]\n",
      "Iteration:  10\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2490 - acc: 0.2391 - val_loss: 0.2490 - val_acc: 0.2228\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2339 - val_loss: 0.2488 - val_acc: 0.2256\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2425 - val_loss: 0.2488 - val_acc: 0.2021\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 8s 8us/step - loss: 0.2488 - acc: 0.2351 - val_loss: 0.2488 - val_acc: 0.2176\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2342 - val_loss: 0.2492 - val_acc: 0.2666\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2346 - val_loss: 0.2488 - val_acc: 0.2155\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2346 - val_loss: 0.2488 - val_acc: 0.2500\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2387 - val_loss: 0.2489 - val_acc: 0.2551\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2492 - acc: 0.2666\n",
      ". theta fit =  [1.0375596 1.4491699]\n",
      "Iteration:  11\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2490 - acc: 0.2128 - val_loss: 0.2488 - val_acc: 0.2024\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2100 - val_loss: 0.2489 - val_acc: 0.2216\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2130 - val_loss: 0.2490 - val_acc: 0.2047\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2060 - val_loss: 0.2490 - val_acc: 0.2393\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2117 - val_loss: 0.2489 - val_acc: 0.1962\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2161 - val_loss: 0.2489 - val_acc: 0.2038\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2153 - val_loss: 0.2491 - val_acc: 0.2111\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2167 - val_loss: 0.2488 - val_acc: 0.2153\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2158 - val_loss: 0.2489 - val_acc: 0.1984\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2098 - val_loss: 0.2489 - val_acc: 0.2000\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 7us/step - loss: -0.2491 - acc: 0.2108\n",
      ". theta fit =  [1.0615214 1.4732194]\n",
      "Iteration:  12\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2490 - acc: 0.1927 - val_loss: 0.2491 - val_acc: 0.1973\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1939 - val_loss: 0.2490 - val_acc: 0.1952\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.1980 - val_loss: 0.2488 - val_acc: 0.1795\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1958 - val_loss: 0.2489 - val_acc: 0.1911\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1970 - val_loss: 0.2489 - val_acc: 0.1887\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1954 - val_loss: 0.2488 - val_acc: 0.1869\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2488 - acc: 0.1794\n",
      ". theta fit =  [1.0374383 1.4491496]\n",
      "Iteration:  13\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2489 - acc: 0.2107 - val_loss: 0.2491 - val_acc: 0.2439\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2241 - val_loss: 0.2490 - val_acc: 0.2131\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2060 - val_loss: 0.2489 - val_acc: 0.2188\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2144 - val_loss: 0.2489 - val_acc: 0.1983\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2131 - val_loss: 0.2489 - val_acc: 0.2231\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2048 - val_loss: 0.2490 - val_acc: 0.2133\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2159 - val_loss: 0.2489 - val_acc: 0.2055\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2027 - val_loss: 0.2493 - val_acc: 0.2257\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2070 - val_loss: 0.2492 - val_acc: 0.2263\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2188 - val_loss: 0.2490 - val_acc: 0.2033\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2095 - val_loss: 0.2491 - val_acc: 0.2274\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2492 - acc: 0.2253\n",
      ". theta fit =  [1.0615301 1.4733164]\n",
      "Iteration:  14\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2489 - acc: 0.1967 - val_loss: 0.2488 - val_acc: 0.1833\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1916 - val_loss: 0.2488 - val_acc: 0.1832\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1958 - val_loss: 0.2489 - val_acc: 0.1863\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1965 - val_loss: 0.2488 - val_acc: 0.1887\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.1894 - val_loss: 0.2489 - val_acc: 0.1889\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1914 - val_loss: 0.2489 - val_acc: 0.1910\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1881 - val_loss: 0.2488 - val_acc: 0.1881\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1920 - val_loss: 0.2491 - val_acc: 0.2119\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2488 - acc: 0.1887\n",
      ". theta fit =  [1.0372562 1.4975249]\n",
      "Iteration:  15\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 22s 22us/step - loss: 0.2490 - acc: 0.1976 - val_loss: 0.2489 - val_acc: 0.1853\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1938 - val_loss: 0.2489 - val_acc: 0.1852\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2488 - acc: 0.2041 - val_loss: 0.2488 - val_acc: 0.1780\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1991 - val_loss: 0.2489 - val_acc: 0.1858\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.1997 - val_loss: 0.2488 - val_acc: 0.1893\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2025 - val_loss: 0.2489 - val_acc: 0.1839\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2488 - acc: 0.1777\n",
      ". theta fit =  [1.0128206 1.4730942]\n",
      "Iteration:  16\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2490 - acc: 0.2121 - val_loss: 0.2490 - val_acc: 0.2162\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2243 - val_loss: 0.2491 - val_acc: 0.2209\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2297 - val_loss: 0.2489 - val_acc: 0.2567\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2238 - val_loss: 0.2489 - val_acc: 0.2147\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2331 - val_loss: 0.2489 - val_acc: 0.1924\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2490 - acc: 0.2206\n",
      ". theta fit =  [0.9883249 1.4976867]\n",
      "Iteration:  17\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2490 - acc: 0.2586 - val_loss: 0.2492 - val_acc: 0.2514\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2528 - val_loss: 0.2490 - val_acc: 0.2697\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2763 - val_loss: 0.2491 - val_acc: 0.2098\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2636 - val_loss: 0.2491 - val_acc: 0.3124\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2737 - val_loss: 0.2489 - val_acc: 0.2343\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2752 - val_loss: 0.2490 - val_acc: 0.2450\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2704 - val_loss: 0.2490 - val_acc: 0.3162\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2744 - val_loss: 0.2491 - val_acc: 0.3226\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2705 - val_loss: 0.2491 - val_acc: 0.2850\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2710 - val_loss: 0.2490 - val_acc: 0.2969\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2489 - acc: 0.3163\n",
      ". theta fit =  [1.01309  1.472889]\n",
      "Iteration:  18\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2489 - acc: 0.2493 - val_loss: 0.2489 - val_acc: 0.2212\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2358 - val_loss: 0.2490 - val_acc: 0.2170\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2191 - val_loss: 0.2491 - val_acc: 0.2236\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2304 - val_loss: 0.2489 - val_acc: 0.2190\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2191 - val_loss: 0.2490 - val_acc: 0.2413\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2244 - val_loss: 0.2492 - val_acc: 0.2395\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2362 - val_loss: 0.2490 - val_acc: 0.2249\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2489 - acc: 0.2187\n",
      ". theta fit =  [0.988048  1.4478376]\n",
      "Refining learning rate\n",
      "Iteration:  19\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2490 - acc: 0.2334 - val_loss: 0.2489 - val_acc: 0.1952\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2467 - val_loss: 0.2491 - val_acc: 0.3036\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2420 - val_loss: 0.2490 - val_acc: 0.2596\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2483 - val_loss: 0.2490 - val_acc: 0.2554\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2371 - val_loss: 0.2490 - val_acc: 0.2633\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2447 - val_loss: 0.2490 - val_acc: 0.2950\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2505 - val_loss: 0.2490 - val_acc: 0.2622\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2489 - acc: 0.2552\n",
      ". theta fit =  [0.9980546 1.4703851]\n",
      "Iteration:  20\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2490 - acc: 0.2459 - val_loss: 0.2493 - val_acc: 0.2938\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2527 - val_loss: 0.2490 - val_acc: 0.2211\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2498 - val_loss: 0.2490 - val_acc: 0.2087\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2349 - val_loss: 0.2490 - val_acc: 0.2391\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2431 - val_loss: 0.2494 - val_acc: 0.2705\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2491 - acc: 0.2565 - val_loss: 0.2491 - val_acc: 0.2549\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2560 - val_loss: 0.2489 - val_acc: 0.2157\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2426 - val_loss: 0.2490 - val_acc: 0.2143\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 15s 8us/step - loss: -0.2493 - acc: 0.2705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  [1.0006119 1.4729421]\n",
      "Iteration:  21\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2490 - acc: 0.2562 - val_loss: 0.2491 - val_acc: 0.2885\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2405 - val_loss: 0.2490 - val_acc: 0.2390\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2422 - val_loss: 0.2489 - val_acc: 0.2518\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2433 - val_loss: 0.2490 - val_acc: 0.2473\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2484 - val_loss: 0.2491 - val_acc: 0.2514\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2447 - val_loss: 0.2490 - val_acc: 0.2643\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2487 - val_loss: 0.2489 - val_acc: 0.2308\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2388 - val_loss: 0.2491 - val_acc: 0.2604\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2490 - acc: 0.2516\n",
      ". theta fit =  [1.0031611 1.4755195]\n",
      "Iteration:  22\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 23s 23us/step - loss: 0.2490 - acc: 0.2374 - val_loss: 0.2490 - val_acc: 0.2253\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2436 - val_loss: 0.2490 - val_acc: 0.2302\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2351 - val_loss: 0.2492 - val_acc: 0.2304\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2344 - val_loss: 0.2489 - val_acc: 0.2496\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2388 - val_loss: 0.2490 - val_acc: 0.2359\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2490 - acc: 0.2299\n",
      ". theta fit =  [1.000552  1.4781076]\n",
      "Iteration:  23\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2490 - acc: 0.2374 - val_loss: 0.2492 - val_acc: 0.2852\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2493 - val_loss: 0.2490 - val_acc: 0.2342\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2413 - val_loss: 0.2491 - val_acc: 0.2611\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2429 - val_loss: 0.2490 - val_acc: 0.2254\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2381 - val_loss: 0.2490 - val_acc: 0.2271\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2538 - val_loss: 0.2490 - val_acc: 0.2153\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2459 - val_loss: 0.2490 - val_acc: 0.2291\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2397 - val_loss: 0.2491 - val_acc: 0.2713\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2403 - val_loss: 0.2491 - val_acc: 0.3017\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2490 - acc: 0.2149\n",
      ". theta fit =  [0.9979086 1.4754667]\n",
      "Iteration:  24\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2490 - acc: 0.2403 - val_loss: 0.2491 - val_acc: 0.2310\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2482 - val_loss: 0.2489 - val_acc: 0.2798\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2509 - val_loss: 0.2489 - val_acc: 0.2245\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2540 - val_loss: 0.2491 - val_acc: 0.2376\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2411 - val_loss: 0.2490 - val_acc: 0.2680\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2565 - val_loss: 0.2490 - val_acc: 0.2343\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2419 - val_loss: 0.2491 - val_acc: 0.2769\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2490 - acc: 0.2374\n",
      ". theta fit =  [0.9952988 1.4781357]\n",
      "Iteration:  25\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2490 - acc: 0.2488 - val_loss: 0.2491 - val_acc: 0.2665\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2640 - val_loss: 0.2492 - val_acc: 0.2597\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2461 - val_loss: 0.2490 - val_acc: 0.2156\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2467 - val_loss: 0.2490 - val_acc: 0.2830\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2641 - val_loss: 0.2490 - val_acc: 0.2428\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2490 - acc: 0.2596\n",
      ". theta fit =  [0.99792075 1.4808314 ]\n",
      "Iteration:  26\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2490 - acc: 0.2346 - val_loss: 0.2490 - val_acc: 0.2341\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2481 - val_loss: 0.2490 - val_acc: 0.2350\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2574 - val_loss: 0.2491 - val_acc: 0.2484\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2495 - val_loss: 0.2490 - val_acc: 0.2403\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2413 - val_loss: 0.2493 - val_acc: 0.2495\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2623 - val_loss: 0.2490 - val_acc: 0.2906\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2513 - val_loss: 0.2491 - val_acc: 0.2304\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2451 - val_loss: 0.2490 - val_acc: 0.2285\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2491 - acc: 0.2496\n",
      ". theta fit =  [1.000657 1.483565]\n",
      "Iteration:  27\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 24s 24us/step - loss: 0.2490 - acc: 0.2474 - val_loss: 0.2490 - val_acc: 0.2783\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2553 - val_loss: 0.2490 - val_acc: 0.2356\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2518 - val_loss: 0.2490 - val_acc: 0.2195\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2385 - val_loss: 0.2492 - val_acc: 0.2636\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2452 - val_loss: 0.2491 - val_acc: 0.2662\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2454 - val_loss: 0.2492 - val_acc: 0.2844\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2455 - val_loss: 0.2491 - val_acc: 0.2574\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2494 - val_loss: 0.2490 - val_acc: 0.2109\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2490 - acc: 0.2662\n",
      ". theta fit =  [1.003415  1.4863212]\n",
      "Iteration:  28\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2490 - acc: 0.2481 - val_loss: 0.2490 - val_acc: 0.2322\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2286 - val_loss: 0.2492 - val_acc: 0.2661\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2492 - val_loss: 0.2490 - val_acc: 0.2185\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2349 - val_loss: 0.2492 - val_acc: 0.2269\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2414 - val_loss: 0.2493 - val_acc: 0.2458\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2379 - val_loss: 0.2491 - val_acc: 0.2783\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2437 - val_loss: 0.2491 - val_acc: 0.2751\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2378 - val_loss: 0.2490 - val_acc: 0.2374\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2491 - acc: 0.2462\n",
      ". theta fit =  [1.0062151 1.4891139]\n",
      "Iteration:  29\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2490 - acc: 0.2376 - val_loss: 0.2489 - val_acc: 0.2344\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2366 - val_loss: 0.2490 - val_acc: 0.2516\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2405 - val_loss: 0.2491 - val_acc: 0.2580\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2490 - acc: 0.2314 - val_loss: 0.2490 - val_acc: 0.2018\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2316 - val_loss: 0.2490 - val_acc: 0.1892\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2489 - acc: 0.2514\n",
      ". theta fit =  [1.0035248 1.486303 ]\n",
      "Iteration:  30\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2490 - acc: 0.2328 - val_loss: 0.2491 - val_acc: 0.2441\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2516 - val_loss: 0.2491 - val_acc: 0.2589\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2392 - val_loss: 0.2490 - val_acc: 0.2233\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2346 - val_loss: 0.2490 - val_acc: 0.2285\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2424 - val_loss: 0.2491 - val_acc: 0.1993\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2412 - val_loss: 0.2490 - val_acc: 0.1842\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2490 - acc: 0.2230\n",
      ". theta fit =  [1.0006557 1.4834329]\n",
      "Iteration:  31\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2489 - acc: 0.2422 - val_loss: 0.2490 - val_acc: 0.2156\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2363 - val_loss: 0.2489 - val_acc: 0.2312\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2559 - val_loss: 0.2490 - val_acc: 0.2203\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2441 - val_loss: 0.2491 - val_acc: 0.2428\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2453 - val_loss: 0.2490 - val_acc: 0.2600\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2444 - val_loss: 0.2491 - val_acc: 0.2357\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2453 - val_loss: 0.2491 - val_acc: 0.2404\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2540 - val_loss: 0.2490 - val_acc: 0.1813\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2415 - val_loss: 0.2490 - val_acc: 0.2674\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2459 - val_loss: 0.2490 - val_acc: 0.2595\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2489 - acc: 0.2405\n",
      ". theta fit =  [0.9977733 1.4861838]\n",
      "Iteration:  32\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2490 - acc: 0.2625 - val_loss: 0.2490 - val_acc: 0.2493\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2550 - val_loss: 0.2489 - val_acc: 0.2194\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2544 - val_loss: 0.2491 - val_acc: 0.2915\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2559 - val_loss: 0.2490 - val_acc: 0.2368\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2523 - val_loss: 0.2489 - val_acc: 0.2338\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2485 - val_loss: 0.2491 - val_acc: 0.2623\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2490 - acc: 0.2916\n",
      ". theta fit =  [1.0007051 1.4834093]\n",
      "Iteration:  33\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2490 - acc: 0.2448 - val_loss: 0.2490 - val_acc: 0.2404\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2504 - val_loss: 0.2490 - val_acc: 0.2197\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2438 - val_loss: 0.2490 - val_acc: 0.2770\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2497 - val_loss: 0.2490 - val_acc: 0.2549\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2469 - val_loss: 0.2491 - val_acc: 0.2560\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2383 - val_loss: 0.2490 - val_acc: 0.2354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2405 - val_loss: 0.2490 - val_acc: 0.2834\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 16s 8us/step - loss: -0.2489 - acc: 0.2550\n",
      ". theta fit =  [0.9977426 1.480443 ]\n",
      "Iteration:  34\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2490 - acc: 0.2524 - val_loss: 0.2490 - val_acc: 0.2461\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2522 - val_loss: 0.2491 - val_acc: 0.2347\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2572 - val_loss: 0.2492 - val_acc: 0.2232\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2428 - val_loss: 0.2489 - val_acc: 0.2539\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2548 - val_loss: 0.2489 - val_acc: 0.2902\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2584 - val_loss: 0.2490 - val_acc: 0.2592\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 9s 9us/step - loss: 0.2489 - acc: 0.2480 - val_loss: 0.2489 - val_acc: 0.2371\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.2489 - acc: 0.2539\n",
      ". theta fit =  [0.99474096 1.4774386 ]\n",
      "Iteration:  35\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2489 - acc: 0.2530 - val_loss: 0.2491 - val_acc: 0.2848\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2626 - val_loss: 0.2490 - val_acc: 0.2526\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2521 - val_loss: 0.2490 - val_acc: 0.2816\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2481 - val_loss: 0.2492 - val_acc: 0.3011\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.2490 - acc: 0.2849\n",
      ". theta fit =  [0.9977711 1.4804603]\n",
      "Refining learning rate\n",
      "Iteration:  36\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 25s 25us/step - loss: 0.2490 - acc: 0.2581 - val_loss: 0.2490 - val_acc: 0.2392\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2536 - val_loss: 0.2490 - val_acc: 0.2407\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2564 - val_loss: 0.2490 - val_acc: 0.2359\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2471 - val_loss: 0.2490 - val_acc: 0.2395\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2471 - val_loss: 0.2490 - val_acc: 0.2432\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2489 - acc: 0.2409\n",
      ". theta fit =  [0.99743396 1.480131  ]\n",
      "Iteration:  37\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2489 - acc: 0.2578 - val_loss: 0.2490 - val_acc: 0.2483\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2472 - val_loss: 0.2490 - val_acc: 0.2298\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2471 - val_loss: 0.2490 - val_acc: 0.2935\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2580 - val_loss: 0.2492 - val_acc: 0.2594\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2488 - acc: 0.2628 - val_loss: 0.2491 - val_acc: 0.2555\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2451 - val_loss: 0.2491 - val_acc: 0.2330\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2487 - acc: 0.2525 - val_loss: 0.2489 - val_acc: 0.2590\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2444 - val_loss: 0.2492 - val_acc: 0.2387\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2524 - val_loss: 0.2490 - val_acc: 0.2305\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2495 - val_loss: 0.2492 - val_acc: 0.2272\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 9us/step - loss: -0.2490 - acc: 0.2589\n",
      ". theta fit =  [0.9971235 1.4798203]\n",
      "Iteration:  38\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2491 - acc: 0.2525 - val_loss: 0.2491 - val_acc: 0.2267\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2326 - val_loss: 0.2491 - val_acc: 0.2691\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2540 - val_loss: 0.2491 - val_acc: 0.2643\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2572 - val_loss: 0.2490 - val_acc: 0.2629\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2476 - val_loss: 0.2490 - val_acc: 0.2838\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2550 - val_loss: 0.2491 - val_acc: 0.2501\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2490 - acc: 0.2643\n",
      ". theta fit =  [0.99743533 1.4801335 ]\n",
      "Iteration:  39\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 26s 26us/step - loss: 0.2489 - acc: 0.2659 - val_loss: 0.2492 - val_acc: 0.2481\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2478 - val_loss: 0.2491 - val_acc: 0.2360\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2538 - val_loss: 0.2491 - val_acc: 0.2559\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2513 - val_loss: 0.2490 - val_acc: 0.2312\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2490 - acc: 0.2480\n",
      ". theta fit =  [0.99774885 1.4804506 ]\n",
      "Iteration:  40\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2490 - acc: 0.2483 - val_loss: 0.2490 - val_acc: 0.2796\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2619 - val_loss: 0.2491 - val_acc: 0.2333\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2482 - val_loss: 0.2491 - val_acc: 0.2677\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2542 - val_loss: 0.2491 - val_acc: 0.2288\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2483 - val_loss: 0.2491 - val_acc: 0.2926\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2490 - acc: 0.2329\n",
      ". theta fit =  [0.9974327 1.4807689]\n",
      "Iteration:  41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2490 - acc: 0.2428 - val_loss: 0.2491 - val_acc: 0.2683\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2516 - val_loss: 0.2491 - val_acc: 0.2730\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2592 - val_loss: 0.2490 - val_acc: 0.2418\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2566 - val_loss: 0.2490 - val_acc: 0.2507\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2530 - val_loss: 0.2490 - val_acc: 0.2569\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2554 - val_loss: 0.2490 - val_acc: 0.2603\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2481 - val_loss: 0.2491 - val_acc: 0.2416\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2622 - val_loss: 0.2490 - val_acc: 0.2301\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2574 - val_loss: 0.2490 - val_acc: 0.2635\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2389 - val_loss: 0.2490 - val_acc: 0.2669\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2490 - acc: 0.2418\n",
      ". theta fit =  [0.99775624 1.4804474 ]\n",
      "Iteration:  42\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2490 - acc: 0.2536 - val_loss: 0.2491 - val_acc: 0.2977\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2570 - val_loss: 0.2491 - val_acc: 0.2572\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2571 - val_loss: 0.2491 - val_acc: 0.2278\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2477 - val_loss: 0.2492 - val_acc: 0.2576\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2529 - val_loss: 0.2489 - val_acc: 0.2878\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2551 - val_loss: 0.2490 - val_acc: 0.2253\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2535 - val_loss: 0.2490 - val_acc: 0.2425\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 17s 8us/step - loss: -0.2491 - acc: 0.2576\n",
      ". theta fit =  [0.99808294 1.4807745 ]\n",
      "Iteration:  43\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2491 - acc: 0.2607 - val_loss: 0.2491 - val_acc: 0.2546\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2532 - val_loss: 0.2491 - val_acc: 0.2643\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2569 - val_loss: 0.2491 - val_acc: 0.2676\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2431 - val_loss: 0.2493 - val_acc: 0.2833\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2585 - val_loss: 0.2490 - val_acc: 0.2581\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2400 - val_loss: 0.2491 - val_acc: 0.2878\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2547 - val_loss: 0.2491 - val_acc: 0.2597\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2488 - acc: 0.2501 - val_loss: 0.2490 - val_acc: 0.2337\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2528 - val_loss: 0.2491 - val_acc: 0.2648\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2564 - val_loss: 0.2491 - val_acc: 0.2696\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2548 - val_loss: 0.2490 - val_acc: 0.2515\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2489 - acc: 0.2339\n",
      ". theta fit =  [0.9977526 1.4804438]\n",
      "Iteration:  44\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2489 - acc: 0.2527 - val_loss: 0.2491 - val_acc: 0.2505\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2533 - val_loss: 0.2492 - val_acc: 0.2334\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2471 - val_loss: 0.2491 - val_acc: 0.2841\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2477 - val_loss: 0.2490 - val_acc: 0.2313\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2500 - val_loss: 0.2490 - val_acc: 0.3026\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2525 - val_loss: 0.2490 - val_acc: 0.2375\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2467 - val_loss: 0.2491 - val_acc: 0.2370\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2501 - val_loss: 0.2491 - val_acc: 0.2813\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2589 - val_loss: 0.2491 - val_acc: 0.2606\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2489 - acc: 0.2377\n",
      ". theta fit =  [0.9974193 1.48011  ]\n",
      "Iteration:  45\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2489 - acc: 0.2564 - val_loss: 0.2491 - val_acc: 0.2605\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2481 - val_loss: 0.2490 - val_acc: 0.2660\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2520 - val_loss: 0.2491 - val_acc: 0.2355\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2558 - val_loss: 0.2493 - val_acc: 0.2651\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2607 - val_loss: 0.2490 - val_acc: 0.2592\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2506 - val_loss: 0.2490 - val_acc: 0.2460\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2507 - val_loss: 0.2490 - val_acc: 0.2353\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2491 - acc: 0.2650\n",
      ". theta fit =  [0.99775636 1.4804474 ]\n",
      "Iteration:  46\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 28s 28us/step - loss: 0.2489 - acc: 0.2582 - val_loss: 0.2490 - val_acc: 0.2382\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2528 - val_loss: 0.2490 - val_acc: 0.2549\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2480 - val_loss: 0.2490 - val_acc: 0.2457\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2466 - val_loss: 0.2490 - val_acc: 0.2490\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2491 - val_loss: 0.2492 - val_acc: 0.2641\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2488 - acc: 0.2639 - val_loss: 0.2490 - val_acc: 0.2508\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2490 - acc: 0.2440 - val_loss: 0.2490 - val_acc: 0.2464\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2581 - val_loss: 0.2490 - val_acc: 0.2431\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2490 - acc: 0.2415 - val_loss: 0.2490 - val_acc: 0.2469\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2490 - acc: 0.2507\n",
      ". theta fit =  [0.9974157 1.4801066]\n",
      "Refining learning rate\n",
      "Iteration:  47\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 28s 28us/step - loss: 0.2489 - acc: 0.2408 - val_loss: 0.2490 - val_acc: 0.2639\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2533 - val_loss: 0.2491 - val_acc: 0.2498\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2440 - val_loss: 0.2491 - val_acc: 0.2497\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2543 - val_loss: 0.2491 - val_acc: 0.2333\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2527 - val_loss: 0.2490 - val_acc: 0.2300\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2617 - val_loss: 0.2490 - val_acc: 0.2535\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2400 - val_loss: 0.2492 - val_acc: 0.2613\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2621 - val_loss: 0.2489 - val_acc: 0.2467\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2463 - val_loss: 0.2489 - val_acc: 0.2271\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2525 - val_loss: 0.2490 - val_acc: 0.2462\n",
      "Epoch 11/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2449 - val_loss: 0.2490 - val_acc: 0.2630\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2489 - acc: 0.2466\n",
      ". theta fit =  [0.9975516 1.4802426]\n",
      "Iteration:  48\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2489 - acc: 0.2569 - val_loss: 0.2489 - val_acc: 0.2518\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2453 - val_loss: 0.2490 - val_acc: 0.2691\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2407 - val_loss: 0.2491 - val_acc: 0.2375\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2625 - val_loss: 0.2491 - val_acc: 0.2519\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2563 - val_loss: 0.2490 - val_acc: 0.2556\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2553 - val_loss: 0.2490 - val_acc: 0.2464\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: -0.2490 - acc: 0.2377\n",
      ". theta fit =  [0.99758625 1.4802765 ]\n",
      "Iteration:  49\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2489 - acc: 0.2678 - val_loss: 0.2490 - val_acc: 0.2358\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2434 - val_loss: 0.2490 - val_acc: 0.2614\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2593 - val_loss: 0.2490 - val_acc: 0.2900\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2641 - val_loss: 0.2490 - val_acc: 0.2527\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2484 - val_loss: 0.2490 - val_acc: 0.2628\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2538 - val_loss: 0.2489 - val_acc: 0.2692\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: -0.2489 - acc: 0.2902\n",
      ". theta fit =  [0.99762094 1.4802415 ]\n",
      "Iteration:  50\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 28s 28us/step - loss: 0.2489 - acc: 0.2580 - val_loss: 0.2490 - val_acc: 0.2513\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2506 - val_loss: 0.2491 - val_acc: 0.2727\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2475 - val_loss: 0.2491 - val_acc: 0.2627\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2555 - val_loss: 0.2490 - val_acc: 0.2806\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2585 - val_loss: 0.2490 - val_acc: 0.2460\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2490 - acc: 0.2727\n",
      ". theta fit =  [0.997656  1.4802762]\n",
      "Iteration:  51\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2489 - acc: 0.2586 - val_loss: 0.2490 - val_acc: 0.2236\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2443 - val_loss: 0.2490 - val_acc: 0.2353\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2402 - val_loss: 0.2492 - val_acc: 0.2405\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2649 - val_loss: 0.2490 - val_acc: 0.2498\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2630 - val_loss: 0.2490 - val_acc: 0.2105\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2335 - val_loss: 0.2490 - val_acc: 0.2852\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2547 - val_loss: 0.2491 - val_acc: 0.2710\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2522 - val_loss: 0.2491 - val_acc: 0.2917\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2489 - acc: 0.2102\n",
      ". theta fit =  [0.9976203 1.4802406]\n",
      "Iteration:  52\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2489 - acc: 0.2424 - val_loss: 0.2493 - val_acc: 0.2663\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2481 - val_loss: 0.2490 - val_acc: 0.2603\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2513 - val_loss: 0.2492 - val_acc: 0.2637\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2487 - val_loss: 0.2491 - val_acc: 0.2397\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 18s 9us/step - loss: -0.2492 - acc: 0.2664\n",
      ". theta fit =  [0.9976563 1.4802766]\n",
      "Iteration:  53\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2490 - acc: 0.2488 - val_loss: 0.2491 - val_acc: 0.2347\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2439 - val_loss: 0.2490 - val_acc: 0.2389\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2599 - val_loss: 0.2490 - val_acc: 0.2424\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2541 - val_loss: 0.2490 - val_acc: 0.2453\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2446 - val_loss: 0.2490 - val_acc: 0.2307\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2596 - val_loss: 0.2490 - val_acc: 0.2329\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: -0.2489 - acc: 0.2421\n",
      ". theta fit =  [0.99762   1.4802402]\n",
      "Iteration:  54\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2489 - acc: 0.2454 - val_loss: 0.2490 - val_acc: 0.2574\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2533 - val_loss: 0.2490 - val_acc: 0.2197\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2482 - val_loss: 0.2491 - val_acc: 0.2551\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2490 - acc: 0.2462 - val_loss: 0.2490 - val_acc: 0.2727\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2597 - val_loss: 0.2491 - val_acc: 0.2664\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2619 - val_loss: 0.2490 - val_acc: 0.2655\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2571 - val_loss: 0.2490 - val_acc: 0.2555\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2470 - val_loss: 0.2490 - val_acc: 0.2448\n",
      "Epoch 9/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2602 - val_loss: 0.2490 - val_acc: 0.2344\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.2489 - acc: 0.2654\n",
      ". theta fit =  [0.99758345 1.4802036 ]\n",
      "Iteration:  55\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2489 - acc: 0.2564 - val_loss: 0.2492 - val_acc: 0.2540\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2569 - val_loss: 0.2490 - val_acc: 0.2489\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2509 - val_loss: 0.2491 - val_acc: 0.2677\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2514 - val_loss: 0.2493 - val_acc: 0.2485\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2491 - acc: 0.2539\n",
      ". theta fit =  [0.99762034 1.4802406 ]\n",
      "Iteration:  56\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 27s 27us/step - loss: 0.2490 - acc: 0.2530 - val_loss: 0.2490 - val_acc: 0.2667\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2670 - val_loss: 0.2490 - val_acc: 0.2380\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2464 - val_loss: 0.2491 - val_acc: 0.2520\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2509 - val_loss: 0.2491 - val_acc: 0.2738\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2569 - val_loss: 0.2490 - val_acc: 0.2565\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2522 - val_loss: 0.2491 - val_acc: 0.2552\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.2489 - acc: 0.2520\n",
      ". theta fit =  [0.99758655 1.4802072 ]\n",
      "Iteration:  57\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: 0.2489 - acc: 0.2558 - val_loss: 0.2491 - val_acc: 0.2332\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2372 - val_loss: 0.2489 - val_acc: 0.2695\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2490 - acc: 0.2593 - val_loss: 0.2490 - val_acc: 0.2515\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2408 - val_loss: 0.2492 - val_acc: 0.2814\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.2490 - acc: 0.2330\n",
      ". theta fit =  [0.99754906 1.4801705 ]\n",
      "Iteration:  58\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2490 - acc: 0.2353 - val_loss: 0.2490 - val_acc: 0.2603\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2572 - val_loss: 0.2490 - val_acc: 0.2619\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2562 - val_loss: 0.2489 - val_acc: 0.2352\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2504 - val_loss: 0.2491 - val_acc: 0.2404\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2539 - val_loss: 0.2492 - val_acc: 0.2508\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2574 - val_loss: 0.2491 - val_acc: 0.2552\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2489 - acc: 0.2353\n",
      ". theta fit =  [0.99751127 1.4801326 ]\n",
      "Iteration:  59\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 28s 28us/step - loss: 0.2490 - acc: 0.2609 - val_loss: 0.2490 - val_acc: 0.2449\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2446 - val_loss: 0.2490 - val_acc: 0.2309\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2531 - val_loss: 0.2490 - val_acc: 0.2300\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2453 - val_loss: 0.2490 - val_acc: 0.2737\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2570 - val_loss: 0.2490 - val_acc: 0.2580\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2515 - val_loss: 0.2490 - val_acc: 0.2590\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2539 - val_loss: 0.2490 - val_acc: 0.2607\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2612 - val_loss: 0.2490 - val_acc: 0.2419\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2489 - acc: 0.2579\n",
      ". theta fit =  [0.9974732 1.4800944]\n",
      "Iteration:  60\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: 0.2489 - acc: 0.2496 - val_loss: 0.2491 - val_acc: 0.2655\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2533 - val_loss: 0.2490 - val_acc: 0.2400\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2499 - val_loss: 0.2491 - val_acc: 0.2710\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2663 - val_loss: 0.2490 - val_acc: 0.2493\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2490 - acc: 0.2655\n",
      ". theta fit =  [0.99751145 1.4801327 ]\n",
      "Refining learning rate\n",
      "Iteration:  61\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: 0.2489 - acc: 0.2564 - val_loss: 0.2491 - val_acc: 0.2688\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2651 - val_loss: 0.2492 - val_acc: 0.2938\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2626 - val_loss: 0.2491 - val_acc: 0.2425\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2412 - val_loss: 0.2490 - val_acc: 0.2719\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2606 - val_loss: 0.2492 - val_acc: 0.2353\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2556 - val_loss: 0.2492 - val_acc: 0.2564\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2572 - val_loss: 0.2490 - val_acc: 0.2493\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2536 - val_loss: 0.2490 - val_acc: 0.2577\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2491 - acc: 0.2355\n",
      ". theta fit =  [0.99751514 1.4801364 ]\n",
      "Iteration:  62\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: 0.2489 - acc: 0.2515 - val_loss: 0.2490 - val_acc: 0.2373\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2653 - val_loss: 0.2490 - val_acc: 0.2334\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2492 - val_loss: 0.2490 - val_acc: 0.2320\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2434 - val_loss: 0.2490 - val_acc: 0.2679\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2580 - val_loss: 0.2490 - val_acc: 0.2397\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2489 - acc: 0.2331\n",
      ". theta fit =  [0.99751127 1.4801325 ]\n",
      "Iteration:  63\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2489 - acc: 0.2495 - val_loss: 0.2490 - val_acc: 0.2580\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2588 - val_loss: 0.2491 - val_acc: 0.2552\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2490 - acc: 0.2545 - val_loss: 0.2490 - val_acc: 0.2300\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2464 - val_loss: 0.2491 - val_acc: 0.2486\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2489 - acc: 0.2580\n",
      ". theta fit =  [0.99750733 1.4801285 ]\n",
      "Iteration:  64\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: 0.2490 - acc: 0.2664 - val_loss: 0.2490 - val_acc: 0.2387\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2530 - val_loss: 0.2490 - val_acc: 0.2576\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2478 - val_loss: 0.2490 - val_acc: 0.2586\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2531 - val_loss: 0.2490 - val_acc: 0.2473\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2596 - val_loss: 0.2489 - val_acc: 0.2470\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2541 - val_loss: 0.2490 - val_acc: 0.2630\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2589 - val_loss: 0.2490 - val_acc: 0.2493\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2504 - val_loss: 0.2490 - val_acc: 0.2533\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2489 - acc: 0.2470\n",
      ". theta fit =  [0.9975034 1.4801246]\n",
      "Iteration:  65\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 31s 31us/step - loss: 0.2490 - acc: 0.2549 - val_loss: 0.2490 - val_acc: 0.2370\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2577 - val_loss: 0.2490 - val_acc: 0.2423\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2518 - val_loss: 0.2490 - val_acc: 0.2738\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2535 - val_loss: 0.2491 - val_acc: 0.2444\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2527 - val_loss: 0.2491 - val_acc: 0.2675\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: -0.2489 - acc: 0.2420\n",
      ". theta fit =  [0.9974994 1.4801207]\n",
      "Iteration:  66\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 29s 29us/step - loss: 0.2489 - acc: 0.2543 - val_loss: 0.2491 - val_acc: 0.2517\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2481 - val_loss: 0.2490 - val_acc: 0.2483\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2515 - val_loss: 0.2490 - val_acc: 0.2631\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2566 - val_loss: 0.2490 - val_acc: 0.2446\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2450 - val_loss: 0.2491 - val_acc: 0.2876\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2589 - val_loss: 0.2490 - val_acc: 0.2449\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2453 - val_loss: 0.2492 - val_acc: 0.2577\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2490 - acc: 0.2574 - val_loss: 0.2490 - val_acc: 0.2535\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2643 - val_loss: 0.2490 - val_acc: 0.2451\n",
      "Epoch 10/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2438 - val_loss: 0.2489 - val_acc: 0.2697\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 9us/step - loss: -0.2490 - acc: 0.2577\n",
      ". theta fit =  [0.9975034 1.4801247]\n",
      "Iteration:  67\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 31s 31us/step - loss: 0.2490 - acc: 0.2627 - val_loss: 0.2491 - val_acc: 0.2573\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2623 - val_loss: 0.2490 - val_acc: 0.2542\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2470 - val_loss: 0.2491 - val_acc: 0.2428\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2533 - val_loss: 0.2490 - val_acc: 0.2693\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2687 - val_loss: 0.2490 - val_acc: 0.2265\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2435 - val_loss: 0.2490 - val_acc: 0.2376\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 19s 10us/step - loss: -0.2490 - acc: 0.2430\n",
      ". theta fit =  [0.99750704 1.4801288 ]\n",
      "Iteration:  68\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 31s 31us/step - loss: 0.2489 - acc: 0.2595 - val_loss: 0.2491 - val_acc: 0.2656\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2535 - val_loss: 0.2490 - val_acc: 0.2458\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2587 - val_loss: 0.2490 - val_acc: 0.2340\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2521 - val_loss: 0.2489 - val_acc: 0.2658\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2463 - val_loss: 0.2490 - val_acc: 0.2623\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 10us/step - loss: -0.2489 - acc: 0.2458\n",
      ". theta fit =  [0.997503  1.4801247]\n",
      "Iteration:  69\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 30s 30us/step - loss: 0.2489 - acc: 0.2552 - val_loss: 0.2490 - val_acc: 0.2523\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2584 - val_loss: 0.2490 - val_acc: 0.2564\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2554 - val_loss: 0.2490 - val_acc: 0.2362\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2592 - val_loss: 0.2491 - val_acc: 0.2566\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2459 - val_loss: 0.2491 - val_acc: 0.2569\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2590 - val_loss: 0.2490 - val_acc: 0.2394\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2460 - val_loss: 0.2490 - val_acc: 0.2597\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.2490 - acc: 0.2566\n",
      ". theta fit =  [0.99749887 1.4801207 ]\n",
      "Iteration:  70\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 32s 32us/step - loss: 0.2489 - acc: 0.2521 - val_loss: 0.2490 - val_acc: 0.2354\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2430 - val_loss: 0.2490 - val_acc: 0.2494\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2447 - val_loss: 0.2491 - val_acc: 0.2688\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2590 - val_loss: 0.2490 - val_acc: 0.2528\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.2489 - acc: 0.2351\n",
      ". theta fit =  [0.99749476 1.4801165 ]\n",
      "Iteration:  71\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 32s 32us/step - loss: 0.2489 - acc: 0.2453 - val_loss: 0.2492 - val_acc: 0.2616\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2548 - val_loss: 0.2490 - val_acc: 0.2444\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2498 - val_loss: 0.2490 - val_acc: 0.2432\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2588 - val_loss: 0.2491 - val_acc: 0.2518\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2448 - val_loss: 0.2490 - val_acc: 0.2541\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.2489 - acc: 0.2444\n",
      ". theta fit =  [0.9974906 1.4801123]\n",
      "Iteration:  72\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 32s 32us/step - loss: 0.2490 - acc: 0.2510 - val_loss: 0.2490 - val_acc: 0.2497\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2475 - val_loss: 0.2490 - val_acc: 0.2472\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2559 - val_loss: 0.2490 - val_acc: 0.2485\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2498 - val_loss: 0.2490 - val_acc: 0.2670\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2600 - val_loss: 0.2490 - val_acc: 0.2570\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2576 - val_loss: 0.2490 - val_acc: 0.2346\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2543 - val_loss: 0.2491 - val_acc: 0.2779\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.2489 - acc: 0.2671\n",
      ". theta fit =  [0.9974947 1.4801081]\n",
      "Iteration:  73\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 32s 32us/step - loss: 0.2489 - acc: 0.2626 - val_loss: 0.2490 - val_acc: 0.2372\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2429 - val_loss: 0.2491 - val_acc: 0.2739\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2592 - val_loss: 0.2490 - val_acc: 0.2401\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2549 - val_loss: 0.2490 - val_acc: 0.2509\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2488 - acc: 0.2417 - val_loss: 0.2494 - val_acc: 0.2487\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2554 - val_loss: 0.2490 - val_acc: 0.2814\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2464 - val_loss: 0.2491 - val_acc: 0.2376\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 10s 10us/step - loss: 0.2489 - acc: 0.2685 - val_loss: 0.2490 - val_acc: 0.2805\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 21s 11us/step - loss: -0.2492 - acc: 0.2487\n",
      ". theta fit =  [0.9974989 1.4801124]\n",
      "Iteration:  74\n",
      "Training g\n",
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      "1000000/1000000 [==============================] - 32s 32us/step - loss: 0.2490 - acc: 0.2574 - val_loss: 0.2490 - val_acc: 0.2531\n",
      "Epoch 2/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2582 - val_loss: 0.2490 - val_acc: 0.2570\n",
      "Epoch 3/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2573 - val_loss: 0.2490 - val_acc: 0.2575\n",
      "Epoch 4/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2625 - val_loss: 0.2490 - val_acc: 0.2499\n",
      "Epoch 5/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2435 - val_loss: 0.2491 - val_acc: 0.2504\n",
      "Epoch 6/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2553 - val_loss: 0.2490 - val_acc: 0.2595\n",
      "Epoch 7/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2601 - val_loss: 0.2490 - val_acc: 0.2366\n",
      "Epoch 8/20\n",
      "1000000/1000000 [==============================] - 11s 11us/step - loss: 0.2489 - acc: 0.2525 - val_loss: 0.2490 - val_acc: 0.2648\n",
      "Training theta\n",
      "Train on 2000000 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      "2000000/2000000 [==============================] - 20s 10us/step - loss: -0.2490 - acc: 0.2505\n",
      ". theta fit =  [0.99750316 1.4801166 ]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):    \n",
    "    print(\"Iteration: \", iteration )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=my_loss_wrapper_fit(1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=20, batch_size=1000,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=[earlystopping])\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer=optimizer, loss=my_loss_wrapper_fit(-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train_theta), y_train_theta, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test_theta), y_test_theta),verbose=1,callbacks=callbacks)\n",
    "    \n",
    "    # Detecting oscillatory behavior (oscillations around truth values)\n",
    "    # Then refine fit by decreasing learning rate /10\n",
    "    \n",
    "    fit_vals_mu = np.array(fit_vals)[(index_refine[-1]):,0]\n",
    "    fit_vals_sigma = np.array(fit_vals)[(index_refine[-1]):,1]\n",
    "    \n",
    "    # Get RECENT relative extrema, if it alternates --> oscillatory behavior\n",
    "    extrema_mu = np.concatenate((argrelmin(fit_vals_mu)[0], argrelmax(fit_vals_mu)[0]))\n",
    "    extrema_mu = extrema_mu[extrema_mu>= iteration - index_refine[-1] -20]\n",
    "            \n",
    "    extrema_sigma = np.concatenate((argrelmin(fit_vals_sigma)[0], argrelmax(fit_vals_sigma)[0]))\n",
    "    extrema_sigma = extrema_sigma[extrema_sigma >= iteration - index_refine[-1] - 20]\n",
    "    '''\n",
    "    print(\"index_refine\", index_refine)\n",
    "    print(\"extrema_mu\", extrema_mu)\n",
    "    print(\"extrema_sigma\", extrema_sigma)\n",
    "    '''\n",
    "    \n",
    "    if (len(extrema_mu) == 0) or (len(extrema_sigma) == 0): # If none are found, keep fitting (catching index error)\n",
    "        pass\n",
    "    elif (len(extrema_mu) >= 6) and (len(extrema_sigma) >= 6): #If enough are found, refine fit\n",
    "        index_refine = np.append(index_refine, iteration + 1)\n",
    "        print(\"Refining learning rate\")\n",
    "        optimizer.lr = optimizer.lr/10\n",
    "        \n",
    "        mean_fit = np.array([[np.mean(fit_vals_mu[len(fit_vals_mu)-4:len(fit_vals_mu)]),\n",
    "                              np.mean(fit_vals_sigma[len(fit_vals_sigma)-4:len(fit_vals_sigma)])]])\n",
    "\n",
    "        model_fit.layers[-1].set_weights(mean_fit)\n",
    "    pass\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:30:59.995617Z",
     "start_time": "2020-06-01T04:30:59.490123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex= True,  constrained_layout=True)\n",
    "fig.suptitle(\"GaussianAltFit-2D-Detector Effects (Analytical Reweight):\\n N = {:.0e}, Iterations = {:.0f}\".format(N, len(fit_vals)))\n",
    "axs[0].plot(fit_vals[:,0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], 0, len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(fit_vals[:,1], label='Model $\\sigma$ Fit', color ='orange')\n",
    "axs[1].hlines(theta1_param[1], 0, len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.savefig(\"GaussianAltFit-2D-Detector Effects (Analytical Reweight):\\n N = {:.0e}, Iterations = {:.0f}.png\".format(N, len(fit_vals)))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T04:31:00.509188Z",
     "start_time": "2020-06-01T04:30:59.999605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAElCAYAAACWMvcuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsnXl4VeW1/z8ryQkJSQiQAYUwiSIik4Agk2KtXgfEsVWUOl+l2qvtrf5sb1unavVeqdcOWrWtU1X01qrVOrRqpQIqShQQARlkCggJAUIgCSQ56/fHu09ycnKmDCcT6/M8+znn7P3u9117OPu71zstUVUMwzAMo6uR1N4GGIZhGEYiMIEzDMMwuiQmcIZhGEaXxATOMAzD6JKYwBmGYRhdEhM4wzAMo0tyyAiciLwpIpe3UVkqIkc2Y78BIrJPRJITYZcRHyLSR0TeF5FyEfmlOJ4Qkd0i8nF729eZEJErRGRhjDSLROS4NrBlkPffTGnm/l+IyPQW2nCHiDzTkjwSiYhcKiL/iDNtzGsbkr6biKwWkbzmW9g0EiZwInKxiCwWkf0iUux9v15EJFFlRkNVz1DVp1orPxEZLCJ+EfldjHRPisjdIes2ikilJ2aBpa+qblbVTFWt9dLNF5FrYuR/i4is8B7GG0TklpDt6l2DfSJSKiLvishFMfK8QkRqg2zb4D3gh0Y/Kw3yiGl7nPm06KEUIc/p3rXbF7JM8pJcC+wEeqjqD4GpwKlAgapOaEG5TXoghOz7RRh7D4iIv7n2dARE5GygXFU/C1l/hXfdo96rCbSr0f9WVY9V1fkJLDP4viwXkS9F5MpElRcOVX1WVU9rjbxCnwGqegB4HPhRa+QfDwkROBH5IfAr4H7gMKAPMAeYAqQmosx24DJgN3CRiHRrxv5ne2IWWLY10w7xbOkFnA58T0QuDkkzWlUzgaOBJ4HfisjtMfL90NsnG/gmUAkUisiIZtrZLkQRxm0h5z9TVT/0tg0EVmr9LAgDgY2quj/hBkfAe7jW2Yr7X30F/Ly9bGol5gB/CrP+cmAX7t4+lNjmXd8ewA+A34vI0e1sU2vyHHB5M5+ZTUdVW3XBPRD3AxfESHcW8BmwF9gC3BG0bTpQFJJ+I/BN7/sEYIm37w7gAW99GvAMUArsAT4B+njb5gPXeN+HAP/00u0EngV6hpR1M7AcKANeANKCtguwHviuV/6FIbYqcCTOE6gGDgL7gNdCjyVkv0HevinAPUAtUOXt+9s4z/+vgd+E2hKS5kIv35wIeVwBLAyz/m/Ai0G/TwA+8M71MmC6tz6s7cAw4G3cg+tL4NtBeaUDvwQ2eed8obdus3cM+7xlEu7F7Kde2mLgaSA75Bxe7e37fpjjaHR/BW17MuSaXecdR633+04v3QxgqXfsHwCjgvLoD7wElHj32G+BY0Ly2eOlPRNYCZQDW4Gb47zOzwP/AJKC1v07sM47v68CfYO2Tcb9H8q8z8lB2+YDd3vHsQ94DcjB/S/2eukHBaWPdh1zvLL3Ah/jBLjRveSlTcW9OBWErB8I+IELgBrgsNBrB/zQu/ZfA1fG+VwJ3BspwLeAwpBy/xP4K3H8b4Fk4L9wz4FyoBDo7237lVf2Xm/9tKAy7gCeiXA+ptP4uVcMfCvWuQcG4+7FJO/374HioP3+BHw/6Bn9R+/cbfWufXK4/z5wmldOGfAw8C/qn6NX4P6nc3Ev+xuAM6I9A7xta4GT4rnPW7okQuBO927KlBjppgMjcQ+rUTihODfKhQ6+uT4EvuN9zwRO8L5fh/tzdvduwHG4aiZoKHBH4qqcugF5wPvAgyFlfQz0BXoDq4A5QdunAQdwXtNv8P4AQdvrRAX3wLw70rGErB/k7ZsSanOc515wf+454WwJWufzrtEZEfJpcJMHrb8K2OF974d7eJ/pXcNTvd954WwHMnB/+itxD5jjcC8Xw73tD3n79POu3WTv+jQ4J0F2rAOO8K7/S8CfQs7h016Z6RHuvbACF+6ahZ4Pz/ZiYKJn6+XeNe3m/V4G/K9XfhowNdJ5xT1kpnnfewFj47jON+LEOzdo3Te88znWs+M3eOKOu4d3A9/xzv0s73dO0LVah3vxy8YJ7hqc557incsn4ryOzwP/56UbgXuARhK4Y4H9Ydb/DPjY+/458MOQa1cD3IW7j88EKoBecTxX6u4l7xztAo4JyvszvBfz0HsgzDPoFs+2o3H/u9FB53M2TuhTcEK8He8FmTgFzrN/Jk7oj4vz3G8Gxnnfv8R5+McEbQvk8zLwqJdfPu5Zd13oPQrk4kT6fK+8m3DCHyxw1bgXq2TcC/82QKI9v3AvQDd63wfghHlAvM+5piyJqKLMBXaqak1ghYh8ICJ7vHanEwFUdb6qfq6qflVdDswDToqzjGrgSBHJVdV9qvpR0Poc3AO9VlULVXVv6M6quk5V31bVA6paAjwQpuxfq+o2Vd2FE80xQdsuB95U1d04l/t0EcmP0/YAr3jnZI+IvNLEfSNxB+6P8US0RKpajftj9G5i/tuC9pkNvKGqb3jX8G2cV31mhH1n4Kr5nlDVGnVtLn8BviUiSTjRuklVt3rX7gN1dfbhuBTntX+lqvuAHwMXh1RH3qGq+1W1MkIefYPOf2DJiPM8XAs8qqqLPVufwr3wnICrXegL3OKVX6Wq0drdqoHhItJDVXer6qfRChaRE4Bf4N7cdwZtuhR4XFU/9c7bj4FJIjII59WsVdU/eed+HrAaODto/ydUdb2qlgFvAutV9R3vf/xn3MMUol/HZJzXdZt37CuAaO3ePXHeTyiX4f5XeJ+h1ZTVwF2qWq2qb+A8hKMh/ueKd45ewN3HiMixOAH8WxR7g7kG+KmqfqmOZapa6uX9jKqWeufnlzgxjbeasa+I7MF5ti8D/6n17ZMRz723/V/ASSJymPf7Re/3YFyV5zIR6YP7j37fu0bFuJex0GYNvHRfqOpL3n3wa5xYB7NJVX+vrt/AU8DhuCapaJTjrj3q+h30VNXNMc9MM0iEwJUCucEPG1WdrKo9vW1JACIyUUTeE5ESESnD1cXnxlnG1cBQYLWIfCIiM7z1fwL+DjwvIttE5H9ExBe6s7hecs+LyFYR2Yur1gwtO/hCVuA8BUQkHXdDPesd24e4t6NL4rQ9wLnehe2pqufGs4OI/FdQB4NHQrZ9D/cgOCuKMATS+nCe6y4RmRaU5xcxTOiHe+sFV430rWCBwHXGODzCvgOBiSHpL8W1JeXiPJ31McoP0BdXPRlgE+4NM/iPtSVGHtuCzn9gibeNbSDww5Bj6e/Z1R/3p6+JmkM9F+AeJJtE5F9BHV0aISK5OLH5cdBLXYAG58QT/lLcNQs9X3i/+wX93hH0vTLM70zve7TrmIe7DsHnPrTcYHYDWSHHOAVX3fa8t+o5YKSIBL9gloac3+D/Z1OeK08Bl4iI4Lzb/4v13wmiPxHuVxG5WURWiUiZd36yo9gQyjbvWdkDJyjfCNoW7dyDE7jpwIm4Wqn5OHE/CVigqn4vDx/wdVAej+I8uVD6EnQtVVVx1cPBbA/aXuF9zSQ6WTivLeEkQuA+xL3NnhMj3XM4V7W/qmYDj+BcfXBteN0DCb03w7qupaq6VlVn4S7KfwMvikiG90Z3p6oOx1VxzSB8I/UvcFUVI1W1B+4tLt7enefhbr6HRWS7iGzHPSguj5Be48w35r6q+gut72gwJ7BeRK7C9Uw6RVVDb8BwnIOr5vlYVRcE5XlsjP3OAxZ437fgqgWDBSJDVe8LZ7uX/l8h6TNV9bs4b7IKV0UW9Rx4bMP9UQMM8I4n+KHckvMeiy3APSHH0t3zjLYAAyJ0bmlkk6p+oqrn4O7lV3DVe43wvNzngEWq+pswSRqcE88bzcFVEYaeL3DnbGuM4wxHtOtYgrsO/UPKicQ6Z6oEC+3luP/iUu+/tThofTxEe640wHtJOIhrcriEhp1dYt0/Wwhzv4rINOD/Ad/GVZv2xLVfNan3uCe0t+LEPfACHO3cgxO4aTiR+xeufWwKTuD+FZTHAVz1diCPHhH++18DBUHHJsG/4zmMCOuPwVXjJ5xWFzhV3QPciROAC0UkS0SSvDew4CqgLGCXqlaJyAQaekBrgDQROcvzNn6Kc/MBEJHZIpLnvZEE3gT8InKyiIz0BHEvriojXDfqLFy1Rpn357olTJpIXI7r6joSV205BncTjRaRkWHS78C1FTWHmPuKyKU4wT5VVb+Kkba3l/4h4L8DVSox9kkWNyTiN7g/zp3epmeAs0Xk37w0aeK6OQf+AKG2/w0YKiLfERGftxwvIsd41/Fx4AER6evlN0lcT6sS3DUMzmse8APPrkzv+F9ogtfUUn4PzPG8BRGRDO9ezcK1Z3wN3OetT/O8EnDnpEBEUgFEJFXcuKNsddXGewl/v4Krfu6PqxoLxzzgShEZ4523XwCLVXUj8Abu3F8iIiniut4PJ/7quGCiXcdaXHvoHSLSXUSGE0WYVPUg8A5eFaKIpOGE4Vrq/1tjgP/AeVrxDBWJ9lwJx9O4TkDV2rAqOdZ/7w/Az0XkKO8eGCUiOV75Nbj7NkVEbsO9EDcZ7/z8ErjNWxXx3Hvp1+K87dk4IQx0wrsAT+BU9Wtc56RfikgP79k8RETCNQ+9jiew3rm/gXpvMR4anUPvedsbCK2BSAyagIY958lyKe7PXoG72ItxN26qt/1CXPVFOe7C/ZagxldcA+bXuMb8m2nYwPuMt34f8AX1jcizcI2r+3En99eE6bCBa9wu9PZfimsILgoqu64sDWoYxnlqNTjPL/R43wDmet+DO5kcRX1vu1fC5R+UxyAadjKZhBP73bg2wXDneQNOyPcFLY8EbVfvfOzDVS++B1wS49pdQX1vv/3edXqKoAZ5L91E3B9nl3eNX8drLA5nO64d4nXqexf+ExjjbUsHHsR5FWW4KpZ0b9td3j57cO1cSbg//RZv/TPUdzJocA4jHN90nJDsC1nCdjAgfOeQ03G9C/fg7tM/A1la33D+CvW9dAPHn+od/y5vfSrwlneOAr0Vp0aw2Y978w61eV/QOZ+DqzbbhftPFQTtPxV3z5d5n1ODts2nYYegu4Eng35/E1gX9Dvadczzyo7Zi9JLfxauPRtcO9DXgC8kTbpXzgxid0CL+FwJd29418qP1zs2aH3U/y2uU8VPcf+/cu/aFXjrH/eO/2ucNxe83x00rRdld+9eOTvWufe2zwM2BP2e69mXHLQuG/gdrrqxDNe55uJw9zruPl9DfS/K4A5+DdKGefaFewbcgtfrPej8193Drb0EersYhmG0CyKyCPiehgz2bqOy03Evy2PVeUBGBLxq8iLgUlV9rxn7d8NVTZ6ornNLwmm12SEMwzCag6pOiZ0qYXwX+MTELTwi8m+42rdKnPclNLN6UV274rDWsy42JnCGYRySiMhG3AM7rl7MhyiTcB13UnHjI8/VyENvOhxWRWkYhmF0SQ6ZaAKGYRjGoYUJnGF0cqQNQ0EZRmfCBM4Ii7hQJZ97PacC6+4WkSdbuZzDReRVcTPPqLippVodbwzYJnGhg14Rkd4h2y8WN/vEfhFZ7w3YjSffurAqkoDQPmHKaxRPTFs5FFScdlwjIuvEzYDzloj0jZK2RaGTpHGYoFpvXGZge3cReVhEdoqbPeT95pZldC1M4Ixo9CX8HHWtiR83FuyClmYkImEblMXNM/gobjqmPrixmQ8HbT8VNyPOlbiBuifiJqptUxIpjK2JuKCfv8DNiNMbNxZsXqLK08Zhgipx4w4DPObZcYz3+YNE2WJ0MhIxuM6Wzr/gBmzeigttERh43mAAcCuXl+KVOShkfcTQHuFsjrD+F8BzQb+H4KZoCgzM/gC4upl2P4k3KJwwoX289VfhIlLsxs2VOjDkPN/gnecN3rqw4VZwg24PUj+wf5m3fj71kxjEE0rocs/WncBPgmwJG4YqzDHPBR4K+t3Xy3dImLSRQidFDN8T43xfjnv5CHSQG+bZ26O9/zO2dLzFPDgjGi/hHh5XxEooIgOk8ez8wUtTJ6MO8CRu9pgjcTPan0bk6aoicSxBc9+p6nqcUAwVN63beCDPq3IrEpHfegOAm8qJ3mdgjsAPReQcXNyw83GzfCygsbdzLm5WmOHe709wU1T1xnXR/rOIpKnqW9RPS5apqqPD2HCFt5xMfTih34akmYqbEeMU4LbAVE84Yf2VuvlZhxBhXkwPCfO9UTBcVf0J7pi/59n8Pa96+HXcTEM5uGger3tTXcXicuBpVQ146xNwYn6nV0X5uYi0uDbA6BqYwBnRUFxsrp8F5k+MmLA+7EWk5blo+4dDmhbaIxqZOE8hmDJcdWQf3OzqF+Imqh2DE9KfNtXeCMwB7lXVVermyvwFMEZEgic/vldVd6k3vkhbFm4lnlBCd6pqpaouwwl/QCgjhaEK5S3g2978i+m4adOUoAnSYxBP+J5GeOfsJBqG4CnACWsZzpP8HvBUkGgbhzAmcEZU1MXbKsIFk21rBhIltIeITJWGoUMI8Rqnevnso/GEtz1wc/QFBq3+RlW/Vhdj7QEix7VrzjH8KsjGXTiPJ3gG/QahfaRl4VbiCSUUNhQUkcNQNUBV3wFux8Ui2+gt5TQOpRKvjQE7+4VJG8x3cHMfbghaV4kT5rtV9aCq/gs33+ppcdpidGFM4Ix4+Amumi3iG7pXRRna2y14ubQZ5UYN7aGqC4O9RG9dsNcYmB3+C+q9FETkCJxXtEZd0NoiGob2aO7sB+H224KLlhxsV7qqfhBuP4kdbiWWbfGEEgpvfIQwVBHSPqSqR6lqH5zQpQArImUdw8aAnbHC91xG4wCqy+MozzhEMYEzYqKq83EPr4hjrbwqyswoy7OR9hUXJiUQDqmb9xttWmiPaDyLC+0zzXtg3wW8pKqBaNJPAP8hIvki0gvXC68ulIzX9X96HOWEC+3zCPBjrycnIpItIt8Kt7NHrHArO4BBwcM3Qmh2KCGJEIYqTLo0ERkhjgG4Xoy/8l4WwhEaNqXJ4XtEZDLOw/tzyKb3cR1mfuzlNQXX/vj3WMdrdH1M4Ix4+Smu00MiqMRVI4Jriwme6+4y6ufB2w28SOSo4WFR1S9wbWHP4noWZgHXByX5Oa5jxxpcb8fPcL3/EJH+uOq3z+Mop8Lbb5FXJXmCqr6M84aeFxc9fgVwRpRs/o5r41qDq7aromEVZuABXyoin4bZ/3Fc4M73cd33q3Dx1OLhdOALEdmH63BysYafdzAN1/llHy4kzoe4ttpI/Aq4UER2i8iv1cUhnIELU1WK81hneNXDkbichi8lAKiLo3cOrkq5DBer7zJVXR3zaI0uj81FaRhREJHZwLGq+uP2tsUwjKZhAmcYhmF0SayK0jAMw+iSmMAZhmEYXRITOMMwDKNLYgJnGIZhdElM4AzDMIwuiQmcYRiG0SUxgTMMwzC6JCZwhmEYRpfEBM4wDMPokpjAGYZhGF2SlNhJOh+5ubk6aNCg9jbDMAzDSACFhYU7VTUvVrouKXCDBg1iyZIl7W2GYRiGkQBEJDRgblisitIwDMPokpjAhaG61s+Bmtr2NsMwDMNoASZwIew7UMNRP3mTpz7Y2N6mGIZhGC2gS7bBtYSM1GSSk4Syyur2NsUwjA5GdXU1RUVFVFVVtbcphwRpaWkUFBTg8/matb8JXAgiQna6jz0VJnCGYTSkqKiIrKwsBg0ahIi0tzldGlWltLSUoqIiBg8e3Kw8El5FKSKPi0ixiKyIsH2YiHwoIgdE5OaQbRtF5HMRWSoibdYtsme6zzw4wzAaUVVVRU5OjolbGyAi5OTktMhbbos2uCeB06Ns3wXcCMyNsP1kVR2jquNb27BI9DCBMwwjAiZubUdLz3XCBU5V38eJWKTtxar6CdBhFCXbBM4wDKPT09F7USrwDxEpFJFroyUUkWtFZImILCkpKWlRoT27m8AZhmF0djq6wE1V1bHAGcANInJipISq+piqjlfV8Xl5MWdwiYp5cIZhGJ2fDi1wqrrV+ywGXgYmtEW5AYHz+7UtijMMw2gSIsLs2bPrftfU1JCXl8eMGTPizuOOO+5g7txIXR/qyczMbJaNAMnJyYwZM6Zu2bhxIwCTJ08GYM+ePTz88MPNzj8WHVbgRCRDRLIC34HTgLA9MVub7HQfqlB+oKYtijMMw2gSGRkZrFixgsrKSgDefvtt+vXr185WNSY9PZ2lS5fWLYFJ8D/44AOgCwiciMwDPgSOFpEiEblaROaIyBxv+2EiUgT8J/BTL00PoA+wUESWAR8Dr6vqW4m2F5zAAZTZWDjDMDooZ555Jq+//joA8+bNY9asWXXbHnjgAUaMGMGIESN48MEH69bfc889DB06lKlTp/Lll182yO+ZZ55hwoQJjBkzhuuuu47a2ujTFU6fPp3Vq1cDUFpayogRI+K2PeAV/uhHP2L9+vWMGTOGW265Je794yXhA71VdVaM7duBgjCb9gKjE2JUDHp2TwWwdjjDMCJy52tfsHLb3lbNc3jfHtx+9rFxpb344ou56667mDFjBsuXL+eqq65iwYIFFBYW8sQTT7B48WJUlYkTJ3LSSSfh9/t5/vnnWbp0KTU1NYwdO5Zx48YBsGrVKl544QUWLVqEz+fj+uuv59lnn+Wyyy6LWP66desYOnQoAMuXL2fkyJGN0lRWVjJmzBgABg8ezMsvv9xg+3333ceKFStYunRpXMfcVGwmkzDUeXAmcIZhdFBGjRrFxo0bmTdvHmeeeWbd+oULF3LeeeeRkZEBwPnnn8+CBQvw+/2cd955dO/eHYCZM2fW7fPuu+9SWFjI8ccfDzhhys/Pj1j2pk2b6NevH0lJrhJw+fLljBo1qlG6QBVle2ECF4aAwO2pPNjOlhiG0VGJ19NKJDNnzuTmm29m/vz5lJaWNjsfVeXyyy/n3nvvjSv9smXLGghaYWEhF110UbPLTxQdtpNJe9Kzu3lwhmF0fK666ipuv/32BtWD06ZN45VXXqGiooL9+/fz8ssvM23aNE488UReeeUVKisrKS8v57XXXqvb55RTTuHFF1+kuLgYgF27drFpU+SYokuXLq2bQmvt2rX89a9/DVtFGYusrCzKy8ubvF+8mAcXhjoPzjqZGIbRgSkoKODGG29ssG7s2LFcccUVTJjgRlVdc801HHfccQBcdNFFjB49mvz8/LrqSIDhw4dz9913c9ppp+H3+/H5fDz00EMMHDgwbLnLli0jLS2N0aNHM2rUKIYPH85TTz3Fz372sybZn5OTw5QpUxgxYgRnnHEG999/f5P2j4Wodr2xXuPHj9clS1o2N/PQn77JlZMH8eMzj2klqwzD6OysWrWKY46xZ8JRRx3Fp59+SlZWVsLLCnfORaQwnvmJrYoyAhZRwDAMozHl5eWISJuIW0sxgYuAxYQzDMNoTFZWFmvWrGlvM+LCBC4CNuGyYRhG58YELgI24bJhGEbnxgQuAhb01DAMo3NjAheBnumpJnCGYRidGBO4CGSn+9h3oIbqWn97m2IYhmE0AxO4CGSnuzHwe82LMwzD6JSYwEXAIgoYhmF0bkzgIlA/4bIJnGEYRmfEBC4C2TbhsmEYHZSWBBs9lEj4ZMsi8jgwAyhW1UZXQUSGAU8AY4GfqOrcoG2nA78CkoE/qOp9ibY3gEX1NgwjFtOnT2/V/ObPnx9XuniCjcbD7t276dWrV7P27Qy0hQf3JHB6lO27gBuBucErRSQZeAg4AxgOzBKR4QmysREW9NQwjI5IpGCjTzzxBHPmzGHw4MHMmTOHRx99tG6fSJPq/+AHPwBcxIGuSMI9OFV9X0QGRdleDBSLyFkhmyYA61T1KwAReR44B1iZIFMbYAJnGEYs4vW4WpNIwUbPOusszjnnHKqrq3nkkUfYvn07kyZN4txzz2Xy5MksXryYm2++mRtuuIH777+f999/n9WrV3PnnXeybt06fvKTn7By5UpefvnlNj+mRNGR2+D6AVuCfhd569oEX3ISGanJNuGyYRgdimjBRgsLCxk3blxdulmzZnHrrbeyYcMGRo8eDcC+ffvo3r07ubm5zJ49m1NOOYULLriAe+65h4yMjPY5qATRkQWuSYjItSKyRESWlJSUtEqeNh+lYRgdjWXLluH3+xk9ejR33XVXXbBRaCxwp556KgCff/45o0aNYu/evYgI4Ko2R48ezSeffMIpp5wCQHJycjscUeLoyBG9twL9g34XeOvCoqqPAY+BC3jaGgZkd7fpugzD6FgsX748YrDRZcuWcdNNNwHOuzv66KMBGDZsGHPnziUlJYVhw4YBkJubyx/+8Ae2bdvGTTfdxM6dO8nLy2u7A2kDOrLAfQIcJSKDccJ2MXBJWxqQnZ5CWeXBtizSMAwjIrGCjc6bN6/u+x//+Me671dffXWjtDNnzmTmzJl1v3Nzc5k7d26jdJ2ZthgmMA+YDuSKSBFwO+ADUNVHROQwYAnQA/CLyPeB4aq6V0S+B/wdN0zgcVX9ItH2BtMzPZWvdu5ryyINwzAi0pmCjXYE2qIX5awY27fjqh/DbXsDeCMRdsWDRfU2DMPovHSZTiaJINuiehuGYXRaTOCikJ3u40CNn6rq2vY2xTCMDkKkQdNG69PSc20CFwUb7G0YRjBpaWmUlpaayLUBqkppaSlpaWnNzqMj96Jsd4IFrk+P5p9kwzC6BgUFBRQVFdFaY22N6KSlpVFQELaLRlyYwEWhpxdRwDqaGIYB4PP5GDx4cHubYcSJVVFGwaooDcMwOi8mcFHomW5RvQ3DMDorMasoReRzYHnQ8jlwuarek2Db2p26qN4VNpuJYRhGZyMeD+4k4PdAJW66rBXAmYk0qqOQlZaCCOw1D84wDKPTEdODU9VdwHxvQUSOAn6aUKs6CElJQo80G+xtGIbRGYnpwYnI0ODfqroWGBUheZcjO93HHhM4wzCMTkc8wwQeFZEhuBn9lwNpwAoR6a6qFQm1rgNgMeEMwzA6J/FUUZ4MICIDgNHAGO9zqYj4VXVYYk1sX3p2twmXDcMwOiNxD/RW1c3AZuC1wDoRyUyEUR2JHuk+tu6ujDv9jr1VVB6sZVBu9NDvlQdrWbOjnNH9e7bURMMwDCNGogbvAAAgAElEQVQMLRoHp6pdPlhazyZWUc55ppCLHvuQ6lp/1HT/+84azn14EetLuvwpNAzDaBdsoHcMAp1M4plc9fOiMj7bvIcdew/w9sodEdNVVdfywidbUIVnP9rcmuYahmEYHiZwMchO91HrV/YfjB0y508fbSTdl8zh2Wk889GmiOleW7aNsspqjsjL4MXCLVTGkbdhGIbRNBIucCLyuIgUi8iKCNtFRH4tIutEZLmIjA3aVisiS73l1UTbGo76CZejz2ZSVlHNX5du49zj+jH7hIF8sL6UdcXlYdM+89EmjszP5J5zR7K3qobXlm9rdbsNwzAOddrCg3sSOD3K9jOAo7zlWuB3QdsqVXWMt8xMnImRiXfC5T8XbuFAjZ/vnDCQi47vjy9ZeCZM9ePyoj0sKypj9sQBnHBEb47Kz+TZKN6eYRiG0TwSLnCq+j6wK0qSc4Cn1fER0FNEDk+0XfHSIw6B8/uVZz7axPiBvRjetwe5md04c+Th/KWwiIqDNQ3SPvPRJtJ9yZw/rgAR4dKJA1hWVMbnRWUJPQ7DMIxDjY7QBtcP2BL0u8hbB5AmIktE5CMROTdaJiJyrZd2SWsGI6yLKBBlLNzCdTvZWFrBdyYNrFv3nRMGUn6ghleX1lc/Bldj9khzwnn+uALSfclR2+wMwzCMptMRBC4aA1V1PHAJ8KA3o0pYVPUxVR2vquPz8vJazYDs7rE9uD99tImcjFROH3FY3bpxA3sx7LAsnv5wU10PzEA15uwTBtSl65Hm45wxffnrsq02Y4phGEYr0hEEbivQP+h3gbcOVQ18foWb7Pm4tjauZ4wqyq17Knl31Q4uOr4/3VKS69aLCN+ZNJCVX+/lsy178PuVZxdvZtzAXhzbN7tBHrNPGEhVtZ+XPi1K3IEYhmEcYnQEgXsVuMzrTXkCUKaqX4tILxHpBiAiucAUYGVbG9c9NZmUJIk44fJzi13V4iUTBzTadu6YfmR2S+GZDzexaP1ONuzc38B7CzCiXzaj+/fk2cWb4xpvZxiGYcQm7qm6mouIzAOmA7kiUgTcDvgAVPUR4A1cfLl1QAVwpbfrMbiJnv04Ib5PVdtc4EQk4oTLB2rcgO1vDOtDQa/ujbZndEvh/LH9eP7jLWwrq6R3RipnjAjff2b2xAHc8uJyPvpqF5OG5LT6cRiGYRxqJFzgVHVWjO0K3BBm/QfAyETZ1RSyu/vCdjJ5a8V2du472KBzSSizTxjI0x9u4qOvdjHnpCGk+ZLDpjt7dF/ufn0VzyzeZAJnGIbRCnSEKsoOTyQP7k8fbmJQTnemHZkbcd+hfbKYOLg3InBpmGrMAGm+ZC4cV8DfV2ynuLyqVew2DMM4lDGBi4NwEy6v3LaXJZt2c+nEgSQlSdT9f37uCB68aAz9ezeuxgzm0okDqPErf15inU0MwzBaiglcHLgJlxtO1fXM4k10S0niW+MLYu4/tE8W54zpFzPdEXmZTB6Sw3OLN1Prt84mhmEYLcEELg6y0xu2wZVXVfPKZ1uZObovPbuntmpZs08YyNY9lby/pvUGqxuGYRyKmMDFQXb3VPZW1dR5VS9/tpWKg7XMPiFy55LmcurwPuRldbOZTQzDMFqICVwcBCZcLq9yceH+9OEmRhVkJyQaty85iYvG9+efXxZTtLui1fM3DMM4VDCBi4PgiAKLN+xibfG+hHhvAS6e4CZ2eeGTLTFSGoZhGJEwgYuDwHRdeyqq+dNHm8hO93H2qL4JK6+gV3e+cXQ+z3+yhepaf8LKMQzD6MqYwMVBYMLldcX7+PuK7XxrXAHpqeEHbLcWl54wgJLyA7y9ckdCyzEMw+iqmMDFQcCD+/2Cr6jxK5cmsHoywElD8+nXM51nF1tnE8MwjOZgAhcHgTa41dvLmXZULoNzMxJeZnKSMGtCfxatK+Wrkn0JL88wDKOrYQIXB4Go3kBCO5eE8u3j+5OSJDy3eHOblWkYhtFVMIGLgzRfMmm+JA7PTuOUYfltVm5+Vhr/duxhvPhpEVXVtTHTL1hbQvFem8fSMAwDTODi5twx/bj5tKNJSW7bU3bpxAHsqajm9eVfR023ZVcFlz3+MXf9rc0jChmGYXRITODi5L4LRnHBuNjzTrY2k4bkcERuBs99HL2a8vlPNqMK//hiB7v2H4ya1jAM41DABK6DIyJcMnEAhZt2s+rrvWHTVNf6+b8lRQztk8nBWj8vfWrRCAzDMEzgOgEXjC0gNSUpYmeTd1buoKT8ALeePoyxA3ry3MebcXFkOz+1fuWzzbt58J01nP/wImb8ZgG/eGMV768piatd0jCMQ5eER/QGEJHHgRlAsaqOCLNdgF8BZwIVwBWq+qm37XLgp17Su1X1qbawuSPRKyOVGSMP5+XPtvKjM4aR0a3hZXvu4830zU5j+tH57Np/kFteXM7HG3Yx8YjmRwZXVdbs2EdxeRXHDehFZrc2uVUAKN5bxftrd/KvNSUsWFvCnopqRGB0QU8yu6XwxKINPPb+V6SmJDFhUG+mHZXL9KPzGdonE3crGYZhtJHAAU8CvwWejrD9DOAob5kI/A6YKCK9gduB8YAChSLyqqruTrjFHYxLTxjAS59t5dVl25g1oT4y+ObSChas3ckPvjmU5CRhxqi+3PW3lTz/yZYmCZyqsrG0gg/W7+TD9aV89FUpO/e5tryUJGF0/55MOTKXKUNyOG5AL1JTWs/5r6quZcnG3by/toT315Swens5ALmZ3ThlWB9OOjqPaUfm0ivDhSaqOFjD4g27WLh2JwvX7uTeN1dz75urncgPy+fko/OZPCSn0YtAOA7W+Fv1WAzD6DhIW1Vlicgg4G8RPLhHgfmqOs/7/SUwPbCo6nXh0kVi/PjxumTJkhbZO3369Bbt39oosG3kFQh+Dv/8aQJ+yu7+0yjrO4GCTx8lpdoNCC8d9E3K80fSv/B3JNdGHjZQk9qDyuz+VPUYSFWP/tR26wFA8oFy0vZuJm3vZlIO7qOqR38qswdwMOMwkCSk9iBp5UWklW0ivWwTvooSmuI3KVCdnkNl9iCqeg6iKqs/muwDfy1p5UWkl20kbc9GUiuK48q3JjWTyp5HUNlzMJXZg9DkVPDXkLa3iPQ9X9F9z1ekVO1GvLIPZh5ORc8jqOw1hIMZffDtL6b77vWk71lPt33bEbpG9a5hdETmz5/f4jxEpFBVx8dK13b1TtHpBwRPnV/krYu0vhEici1wLcCAAQPCJenUCJBVvJRdg0/lYMZhdNu/HZUkyvNGkr57fZ24AWQVL6P8sOPYnzecHts/bZCPAnsPH095n+OoSXPhfpKq95O2dwtpWz8ife8mUqr2NBCW9LKN9NoCtcndONCjP5XZA6nKHsjugSez29s/vWwzaWWbSNu7Gd+Bskb21/q6U9ljIFXZA6jMHlQnpimVu8gsXu5Ebe8WkvzVjfaNRcrBfWQVLyereDkqSVRlFdQJ3u5B32A33yClajep+4upyirAn5oB6qdb+TZ6bFvMgczDKes3kbKCSe5Y9mzwBG9DWHsUqO6eR0XPI/CnpCP+asRfU79oDeKvDfpdi6iiSSloUjIqKfXfk1Ia/pbkxnloLSpe2tDFW+9PSoGkZFC/V25tA5uS6vJ0Cwj+Oht8aJJ7FDQ4Dn8Non4QQSUJ8D4l4PFK3YcG/0a8r/W/tcFvdxZFA2eTJqTToI/gdaFlEzadhNs36Hfdy02DF/+GNtZ/bWJ1eBdpF28KGaWrSa6pbLfyO4rAtRhVfQx4DJwH19L8WuMto7Upr6pm4i/eZdqVP+Z/LhzN68u/5obnPuXh/7yEk4d9v0Hacx5aRGWfc/j7939Z1y6lqjzw9hp+8891TDoih9OO7cPkIbnNbrv6uqySRetK+WDdThas601J+TEA9O+dzpQhuRw3oCdfbt/HonU7+XKHq3bskZbCaUfmcuLQPKYemUv/3t1beFais2VXBfO/LOa9L0tY/fXhHD+4N98Yls9JQ/MaRGMvq6hm/ppi/rm6mPlf9qQkbwSpyUmcMCSHbx6Tz/Sh+WzZXcHbK3fw9sodbNvj/rRpviQO1Phb5dmVmpxEcpJwoKYWf5T8UpKEbilJpPmS6z5TU5LolpJEjV85UOOnqrqWqmo/B6prqarxczBGVIpuKUmIQFW1Ra8wWo+nH7iDEf2y2618q6LsZPz4peW8/NlWFv/XN7n+2UI27qzg/f93MslJDQXqhU82c+tfPucv353EuIG9UVX+9+01/Pqf67j4+P784ryRJCW1XocMVWVdsROzRV4bXnlVDd1Skjh+UG8mH5nD1CNzObZvdiNbOxo1tX6WbNrNu6t28O6qYr7aub9uW5ovialH5nHq8HxOHpZPflYaqsrBWn+doByo8XOgxhMY77uq27dbihOlbinJdPMlkeZ9piYnNbgeNbV+qmpcftW1Widg3VKSmjXZgD9Y+GpqSRKpK9uJW/1LkLPZK9uvpCQJSSLuM0lIThIE58CI59EE3o8C68RzpESC0ga9RKkq6jlPqhp3Ogh8B0UbvFiElh0uXfDvQJ6BdETZpmjE44qHeB+zTXkax1N0vPk1VQfifSHukZaSkMkx4q2i7CgCdxbwPVwvyonAr1V1gtfJpBAY6yX9FBinqruildWVBe7zojLO/u1Crpg8iCc/2MgPTx3Kf5xyVKN0+w/UMPEX7/Jvxx7GL789mv99ew2/enctF43vz73nt664haOm1s/G0goKeqWT5ktsaKFEs75kHwvX7qRvz3SmHpmb8FBJhmFEp0O1wYnIPJw3lisiRbiekT4AVX0EeAMnbutwwwSu9LbtEpGfA594Wd0VS9y6OiMLshldkM2TH2wkOUn49vH9w6bL6JbCzDF9eenTInIzU3n0/a/49viCNhE3gJTkJI7Mz0x4OW3BkLxMhuR1jWMxjEOJNhE4VZ0VY7sCN0TY9jjweCLs6qxcOnEgy4qW881j8unTIy1iuksmDOC5xZt59P2vuHBcAfedP6pNxM0wDKMj0GU6mRxKnD26L+99Wcz104+Mmm5Ev2xmjDqcnIxUbjv7WBM3wzAOKdqsDa4t6cptcIZhGIc68bbB2RQOhmEYRpfEBM4wDMPokpjAGYZhGF2SLtkGJyIlwKYEZJ0L7ExAvu2BHUvHpCsdC3St47Fj6TgMVNW8WIm6pMAlChFZEk/DZmfAjqVj0pWOBbrW8dixdD6sitIwDMPokpjAGYZhGF0SE7im8Vh7G9CK2LF0TLrSsUDXOh47lk6GtcEZhmEYXRLz4AzDMIwuiQmcYRiG0SUxgTMMwzC6JCZwhmEYRpfEBM4wDMPokpjAGYZhGF0SEzjDMAyjS2ICZxiGYXRJTOAMwzCMLklKexuQCHJzc3XQoEHtbYZhGIaRAAoLC3fGEy6nSwrcoEGDWLJkSXubYRiGYSQAEYkr3qdVURqGYRhdEhM4wzAMo0tiAhdKzX54fQSs+0N7W2IYhmG0gC7ZBtciktKg7AuoKGpvSwzD6GBUV1dTVFREVVVVe5tySJCWlkZBQQE+n69Z+5vAhZKUDClZUF3W3pYYhtHBKCoqIisri0GDBiEi7W1Ol0ZVKS0tpaioiMGDBzcrD6uiDEdqtgmcYRiNqKqqIicnx8StDRARcnJyWuQtm8CFw2cCZxhGeEzc2o6WnmsTuHD4esBBEzjDMIzOjAlcOHzZUL23va0wDMMwWoAJXDisitIwDKPTYwIXDutkYhhGB0ZEmD17dt3vmpoa8vLymDFjRtx53HHHHcydOzdmuszMzGbZCJCcnMyYMWPqlo0bNwIwefJkAPbs2cPDDz/c7PxjYcMEwmEenGEYHZiMjAxWrFhBZWUl6enpvP322/Tr16+9zWpEeno6S5cubbT+gw8+AOoF7vrrr09I+ebBhcOXDbVVUHuwvS0xDMMIy5lnnsnrr78OwLx585g1a1bdtgceeIARI0YwYsQIHnzwwbr199xzD0OHDmXq1Kl8+eWXDfJ75plnmDBhAmPGjOG6666jtrY2avnLli3jxBNPZPjw4SQlJSEi3HbbbXHZHvAKf/SjH7F+/XrGjBnDLbfcEte+TcE8uHD4erjP6r2QnNu+thiG0TEp/D7sbuydtIheY2Dcg7HTARdffDF33XUXM2bMYPny5Vx11VUsWLCAwsJCnnjiCRYvXoyqMnHiRE466ST8fj/PP/88S5cupaamhrFjxzJu3DgAVq1axQsvvMCiRYvw+Xxcf/31PPvss1x22WVhy66qquKiiy7i6aefZsKECfzsZz+jqqqKO++8s0G6yspKxowZA8DgwYN5+eWXG2y/7777WLFiRVgvrzUwgQuHL9t9VpdBmgmcYRgdj1GjRrFx40bmzZvHmWeeWbd+4cKFnHfeeWRkZABw/vnns2DBAvx+P+eddx7du3cHYObMmXX7vPvuuxQWFnL88ccDTpjy8/Mjlv3OO+8wduxYJkyYUGfLW2+91WjcWqQqyrbCBC4cqUECZxiGEY44Pa1EMnPmTG6++Wbmz59PaWlps/NRVS6//HLuvffeuNKvWLGCkSNH1v3+9NNPGTt2bLPLTxTWBhcOnwmcYRgdn6uuuorbb7+9gdhMmzaNV155hYqKCvbv38/LL7/MtGnTOPHEE3nllVeorKykvLyc1157rW6fU045hRdffJHi4mIAdu3axaZNkWOK5uTksHz5cgDWrFnDSy+9xMUXX9xk+7OysigvL2/yfvFiHlw4AgJns5kYhtGBKSgo4MYbb2ywbuzYsVxxxRV11YfXXHMNxx13HAAXXXQRo0ePJj8/v646EmD48OHcfffdnHbaafj9fnw+Hw899BADBw4MW+6sWbN49dVXGTFiBLm5ucybN4+cnJwm25+Tk8OUKVMYMWIEZ5xxBvfff3+T84iGqGqrZtgRGD9+vC5ZsqT5GZSvh9eOhBOegiPCN7IahnHosWrVKo455pj2NuOQItw5F5FCVR0fa1+rogyHVVEahmF0ekzgwmGdTAzDMDo9CRc4EXlcRIpFZEWE7dNFpExElnrLbUHbTheRL0VknYj8KNG21pHkg+R0EzjDMIxOTFt4cE8Cp8dIs0BVx3jLXQAikgw8BJwBDAdmicjwhFoajC/bOpkYhmF0YhLei1JV3xeRQc3YdQKwTlW/AhCR54FzgJWtZ11kNm8vY90Xr3DXrWvaojjDMDoBt99+O0lJ1rLTEo4++ug2K6ujXKlJIrJMRN4UkWO9df2ALUFpirx1YRGRa0VkiYgsKSkpabFB+w+mkOGraXE+hmEYRvvQEcbBfQoMVNV9InIm8ApwVFMzUdXHgMfADRNoqVHHjJoINfuZ/6P5Lc0qfmoPwNa/Qf/zIVqodvXD6v+FQZdA+uFtZ59hHOKsWrWqTT0Qo2W0uwenqntVdZ/3/Q3AJyK5wFagf1DSAm9d29AeIXM2PAULL4SdH0RPt/sz+OxmWPto29hlGIbRCWl3gRORw8SboVNEJuBsKgU+AY4SkcEikgpcDLzaZoa1RyeT4gXus2Rh9HQli9znzkWJtccwjA5JS0LVHEokvIpSROYB04FcESkCbgd8AKr6CHAh8F0RqQEqgYvVTa9SIyLfA/4OJAOPq+oXiba3Dl8PFy6nLQkIW/FCGH5r7HQ7PwJ/LSQlJ942wzAaMX369FbNb/78+THTxBuqJha7d++mV69ezbS0c5BwD05VZ6nq4arqU9UCVf2jqj7iiRuq+ltVPVZVR6vqCar6QdC+b6jqUFUdoqr3JNrWBviyoabcCUhbUFEE+zdCcprzzNQfPp2q8+BSsqBmH5R93jb2GYbRIQgXqmbXrl08+eSTzJkzh8GDBzNnzhwefbS+CSPclIw/+MEP6r5fc801iTe8HegInUw6JoHZTGrKIbVn4ssr9ryyI66GtQ/B3tWQHWbY3/5NULkNjrkFVt3vxK7XmMTbZxhGI+LxuFqbSKFqrrzySs455xyqq6t55JFH2L59O5MmTeLcc89l8uTJLF68mJtvvpkbbriBs846i9WrV3P//fdzww03sG7dOn7yk5+wcuXKRkFJOzPt3gbXYWnr+ShLFkJKBgy9of53pHQAgy6F9L717XGGYRwSRAtVU1hYWBele+nSpcyaNYtbb72VDRs2MHr0aAD27dtHfn4+s2fP5pZbbuHTTz/lggsu4J577qkLktpVMIGLRFuHzClZCLmToMcwSMuv9+gapVvk2gezR0DeFBM4wzjEmDVrFvv27WPEiBFce+21DULVhArcqaeeCsDnn3/OqFGj2Lt3LyLC8uXL6wTvk08+4ZRTTgEgOblrtedbFWUkfD3cZ1t4cAf3wJ7lMPION/4tb2pkD27nIieEScmQOwU2/9m133UvSLyd7cnOj2DlfVBTCcNvgT6nRB8raBhdlMzMzAbBSoNZtmwZN910EwBr166tG7M3bNgw5s6dS0pKCsOGDSM3N5c//OEP5ObmsnLlSm666SZ27txJXl5emx1HW2ACF4m6Kso26Em580NAnbCBE64tL0HFNujetz7dwT2wZwX0/5b7nTfFfZZ8AAO/nXg724PiBbDiLtj+DnTLgaQ0+OepTuRH3AaH/5sJnWF4zJs3r+77H//4x7rvV199daO0M2fOBOD3v/89ALm5ucydOzfBFrYtVkUZibYMmVOyECQZcie63wGhCx3nVieEnrD1Gg3J3bteNaUqbP8nvHMyvHOi826Pux9mboSZ6+H430HFVph/BvzjBNj6utsnGrUHYc8XsdMZhtFlMIGLRFt2MilZCL3Guk4mAL2Pc+F6QtvhShY1FMIkH+RM6DoDvlVh6xvw9hT45ylQ/iWMfRBmboBjbgZfJiR3g6PmwNlrYcJjUFUM/5oBf5/o9g0VsJpK+PI3LkL7GyPgzeNg84uRh2EYhtFlMIGLRFt1Mqk9ADsXQ/60+nVJPsiZ2LgdrmQR9DquXgjBeXO7l0L1vsTamUjU76pk3xoH/zrLeWfHPwwzv4JhN0FK98b7JKfCkf8OZ6+BiX+AAyVu339Mgm1vQXU5rPwfeHUwFN4IGQOdF1hbCQu/Ba+PgA3Pgt8m1DaMrooJXCSS00BSEu/B7SoE/4H6askAeVNhz1L3oAbwV0Pp4vrqybp0U0BrofTjxNqZCPw1sOEZeGMkLLjAHevEx2HmOjjqu+4axCLJB0OuhhlfOo+u8mtXdfmXPFh6K/QcBd/8F5y6wHmBZ62EyfNAkuDD2fC3Y5wNbTWg3+j0hBs0bSSGlp5rE7hIiLh2uER3Mgl4aY2Ea6rzbEoXu9+7PnPeR2i63EmAxJ6guSNRewDWPQZ/Oxo+/A4gMPlZmLEahlzpRKup1Hl0a+H4R2DwZXDaYvjGPyD/xPp0Sckw6GI4czlMe8l5wx9+B94c7bxIe3gZUUhLS6O0tNRErg1QVUpLS0lLi+NFNwLWizIabRFRoGQhZA11Y9+CyZvkvIzihXDYN+vb2XJDBC61J2Qf2zk6mtTsh3W/h1VzoXIr9D4eTnwA+p3tjrU1SE6Fo66LnU6SoP95UHCOa5P7/DbnRfYeD6PuhsNPi907s2a/W3w94vM2o+GvcbPmVO911eLVe72lDGorICXTlePr4e7LlAz3wlOXtsztn9TNbU/Ndp+B7yk9Is9Z6q/2quL94OvpzqERloKCAoqKimiNmJNGbNLS0igoaP4QKBO4aCQ6ooD6ncD1Pz9M2T0ge2S9h1eyCDIGNxw2ECBvMmx6weXXWkLRmhzYBWt+C2t+DQdKIX86THqyY4xlkyQ3xKL/+bDhT7DiTph/uvP6Rt/rzm0o1eUuHt+quU5UAJJSPUHpEfQZtEhykGh5S83eekGrrUj8saZk1gue1nriWOaEMpjkNC9dTzcsw38wZKn2Oulo/ScKiHf/BX0Gp6lLK2HSquc9+4PSASR590iSS1f3PfhTQsoIdCAKLSPS/oG8g9LWpW9oo0/9DG50PPES5l4Pe//H859oYblx2xInkTzaKc9D9rDm59tCTOCikWgPrmwVHNzduP0tQN5U2PCke7svWQiHnRo+Xe4UV+VX9gX0HBk+TXtQsQ1WPwDrHnUTQ/c7G4b/2HmnHY2kFFc9OugS52V+cbfrzdl3Boy+B3qN8qpWH4UVd7tOLf3Phz7fcPdInecV5H3t31QvZP7aerFLzYZuuZA5OEQMs8GX1VgoU9Kdpxjs1VXvc51vGqTNdDZWB3l1dTaFfJekIA+vp9epSoLS7vHEr8r1XE1KDVp8hBWZwEM/WAAaiUYY0VN/Q6FB6h+2daKnTpQDYtpIZEJEyu0cff/gbQFhDQhk6LrAsYQKeNwvlGEEIKwoRFrXmkIYry3R8oxTINu5NsAELhqp2bBvQ+Lyr2t/iyJwax9ybUNVO6KkCwz4XpQ4gVN19q6a68bjDboUhv0nZPRvnHbvWjcR9IanQGtgwMVw7I86lvhGIrkbHP09J3Zf/sr1xHxzjKvO3FXoRKvPyTD6Psid0N7WGoYRhQ5Yn9WBSElwTLiShZDWBzKHhN8eEK6V/93wdyiZR7h8ShLQ0cRf66YD+8cJbtD1zkWQewKs+Q28egR8eLkbQA1OABZ+23Ue2fA0HHEVzFgDU57tHOIWTEoGHPtfcM4GF5tv25vO6zr5H/CNd03cDKMTELcHJyJ3qOodCbSl45Ga4CrKkgWQNy1y3XdGf+g+AHZ/6hr/w4XPAW/+yimtO+C7Zj+sf8JVMe7fAJlHuhlEBl/mqsb2b4JVD8D6Pzgxyx4OZStdddnwH8HRN0F6n9azp71I7QVj7nXVlMFVZ4ZhdHiaUkV5m4ikA72BT4HnVXV3YszqIPi8YQKqrf9g27/FicTRP4ieLm8qbHrOdXaIVt8fmL+ycjukH9Z8uyq/dh1C1v7OtQ/mToKxv4R+Mxv2wssYCON/BSNvgzUPwbY3YMx/w5HX1U9z1pXoiJ13DMOISlMEToEq4O/AWOADEblYVZclxLKOgM/rbVaz3zrtoskAAA5uSURBVDXgx8P2f8LBUhjwrejpAt368yO0qwXIDwhchOrJAIHefiWLYMAF8dkazJ4vnLe28RnXS67/eTDsh+F7EQbTLceJ3Mjbml6mYRhGAmmKwK1W1du97y+KyJPAI8A3Wt2qjkLwhMvxCtzSW12VXv8Lo3t9JQtdt+2eo6Pnd/jp0L2/64EYjV5jXffupgicKux4F1b9Er5+y81/OeTfYdj3IevI+PIwDMPooDRF4HaKyDhVLQRQ1TUi0rWCB4WSEogJtxfoFzt99T7Y/Znz+vauhuxjIqctWeDFdYtxCTIHw7mbY5ednOoGTsczo0ntQdj0PKz+pZupP60PjPo5HDkH0nJj728YhtEJaIrA3Qg8LyKFwOfAKCCBfeg7AE0NmVP6kTfWBidgkQTu4B7Y8zmMbEZVYjTyJrtqxppKN3YqEh9eBptfcFHBJz7uxn4ld2tdWwzDMNqZuFvOvba2MUAgot57wKxEGNVhaGpEgeKFrjNCaq/GoW6CCQ1w2lrkTnbtZ7sKI6epPQhbX3Nd+M9c7sZ7mbgZhtEFadJAb1U9ALzuLV2fpsaEK1ng2tQyB7vvkShe4CIVBOK6tRa5XoeQnR9E7ryy6xM3LVS/Gdbl3TCMLo31fY5GU6oo/dWw8yM3ri1vGuzfCBVF4dOWLITeYxvGdWsN0nLdxM3R2uF2vAcI5J/UumUbhmF0MEzgouEL7mQSg12fOc8of1p98NJw1ZS1B1zsttaungyQN9nNaBJpbrkd70Gv0dCtd2LKNwzD6CCYwEUjJZO6CWhjEaiSzJvqqilTMsNXU+5a4gU4ndZ4W2uQO9lNBFy+rvG22gNe9eX0xJRtGIbRgTCBi4YkOS8unk4mJQvcdFbph7mu/7mT6idTbpAuQoDT1iK4HS6U0sVudvg+JyembMMwjA6ECVws4gmZE5hpPz/IK8ub5oYCHNzTMG3xQuhxNKQlaAhh9jFu3spwAlfX/nZi422GYRhdDBO4WMQz4fLe1S6QZ3C7Wv5UQBtG2la/mxA5UdWT4LzO3EnhI3zveA96HefifxmGYXRxTOBi4YsjZE5d+1uQcOVMdIEhg6spy1ZGD3DaWuRNdsFPg73H2irXy9OqJw3DOEQwgYtFPFWUxV5ct+D5G1O6Q69xDTuaxApw2lrUtcN9VL9u54euc4sJnGEYhwgmcLHwZcfuZFKywIlW6MDp/KlQ+onznsAN8E4/3AUoTSQ5E0CSG7bD7XjPVV8mWlwNwzA6CCZwsYjlwVUUuUHd4drV8qaB/6ATOXAeXDghbG18XpSC4Ha4HfOdR9kVY7UZhmGEwQQuFrE6mQQGc+eHEzhvKEDJAti/GSo2J7aDSYOyJ7thAf4aqKlwE0Fb9aRhGIcQJnCx8PVwXligmjGUkgVeXLdRjbd1y4Hs4U4E26r9LUDuZBeodc/nrqrSXw19prdN2YZhGB2AJk22fEhSN+HyXhdQNJSShU5MIsV1y5sGm+ZBRn9IyQovhIkgL2jAd+XXrk3O2t8MwziESLgHJyKPi0ixiKyIke54EakRkQuD1tWKyFJveTXRtoYlWsicQFy3cNWTAfKmOnHcOM+JTlJyYuwMpfsASO/n2uF2vOeCofqy2qZswzCMDkBbVFE+CZweLYGIJAP/DfwjZFOlqo7xlpkJsi860ULmlCwiZly3gPjVlLetByXiBHXHe25yZ6ueNAzjECPhAqeq7wO7YiT7D+AvQHGi7Wky0ULmlCx0g7lzosR1yxgI3fu7721dRZg7Gaq2g9ZYBxPDMA452r2TiYj0A84Dfhdmc5qILBGRj0Tk3Bj5XOulXVJSUtJ6Bkb14Ba4rvcp6dHzyJvmCeGE1rMrHgIDviUlcZM7G4ZhdFDaXeCAB4FbVdUfZttAVR0PXAI8KCJDImWiqo+p6nhVHZ+X14oTGUeKCVdb5ca3RWt/CzDqLpj2spvdpC3pfRwkpzthbe3gqoZhGB2cjtCLcjzwvLjBz7nAmSJSo6qvqOpWAFX9SkTmA8cB69vUukidTEo/ccMH4hnXljXELW1Nkg+OmwuZ7VC2YRhGO9PuAqeqgwPfReRJ4G+q+oqI9AIqVPWAiOQCU4D/aXMD6zy4EIGrm2B5ctva01SGXt/eFhiGYbQLCRc4EZkHTAdyRaQIuB3wAajqI1F2PQZ4VET8uKrU+1R1ZYLNbUxSyv9v705j7arKMI7/HzpibVpLK6kWaYlAU9QOyFCpBFFJSRqixIioEYfEIQ6AGoJ8MZqY1GhUPpkQUPxQq1gsECQFgqhtiZ0HCqURLWqp7a0pFAQltn39sNahmxOu3edye/dZO88vOTl7urvr7dm5711rn73eNLzXneAG1sCkc9LD3GZm1ndOeIKLiKt7OPaTleVHgLefiDb1rHs+yqNHUl2302uHZmZmI6wfvmTS/7prwh3akdZHal5JMzPrmRNcHd0lcwby/bc3euorM7N+5QRXR/cQ5YE16eHtCac31yYzM/u/nODqqJbMiThW4NTMzPqWE1wd1R7cC7vh33vrPeBtZmaNcYKro/olk4ERrutmZmZD4gRXx5hJqXjo0cNpeHLM5PQMnJmZ9S0nuDqqRU8PrEkTF8v/dWZm/cy/pevolMx5/kl47gnffzMzK4ATXB2dHtze+9K7H/A2M+t7TnB1dCZc3vsbOGkcTDm32faYmdlxOcHV0enBHdwIUy+AUeOabY+ZmR2XE1wdnQQHfjzAzKwQTnB1jK0mON9/MzMrgRNcHS/34ARTFzbaFDMzq6fxit5FGDUOThoLk+a8sjdnZmZ9yz24uiaeBW9a0nQrzMysJvfg6lq8CTSq6VaYmVlNTnB1jRrbdAvMzKwHHqI0M7NWcoIzM7NWUkQ03YZhJ+kA8NcTcOqpwD9PwHmb4Fj6U5tigXbF41j6x+kRMe14B7UywZ0okjZGxDubbsdwcCz9qU2xQLvicSzl8RClmZm1khOcmZm1khNcb25pugHDyLH0pzbFAu2Kx7EUxvfgzMysldyDMzOzVnKCG4Skn0gakLSjsm2KpAcl/Sm/v6HJNtYh6TRJD0t6XNJjkq7N24uLBUDSeEnrJW3L8Xwrb58laZ2kJyX9UlIxU89IGiVpi6R783qRsUh6StKjkrZK2pi3lXqdTZa0QtITknZKWlhiLJLOzp9H5/WcpOtKjGUonOAGdzuwuGvbjcBDEXEm8FBe73eHga9FxBzgQuCLkuZQZiwALwGXRsRcYB6wWNKFwHeBH0bEW4FngM802MZeXQvsrKyXHMt7ImJe5SvopV5nNwOrImI2MJf0+RQXS0Tsyp/HPOBc4EVgJQXGMiQR4dcgL2AmsKOyvguYnpenA7uabuMQYrobeH9LYnkdsBm4gPTQ6ui8fSFwf9PtqxnDDNIvmEuBewEVHMtTwNSubcVdZ8AkYDf5Owolx9LV/suAtW2Ipe7LPbjenBoR/8jL+4BTm2xMryTNBOYD6yg4ljyktxUYAB4E/gw8GxGH8yF7gDc31b4e/Qi4ATia10+h3FgCeEDSJkmfzdtKvM5mAQeAn+ah41slTaDMWKo+AizPy6XHUosT3BBF+tOnmK+gSno9cCdwXUQ8V91XWiwRcSTSkMsM4HxgdsNNGhJJS4CBiNjUdFuGyaKIWABcThoKv7i6s6DrbDSwAPhxRMwHXqBrCK+gWADI93GvAH7Vva+0WHrhBNeb/ZKmA+T3gYbbU4ukMaTktiwifp03FxlLVUQ8CzxMGsabLKlT/mkG8HRjDavvIuAKSU8BvyANU95MmbEQEU/n9wHSfZ7zKfM62wPsiYh1eX0FKeGVGEvH5cDmiNif10uOpTYnuN7cA1yTl68h3c/qa5IE3AbsjIgfVHYVFwuApGmSJuflk0n3E3eSEt2H8mFFxBMR34iIGRExkzR89NuI+BgFxiJpgqSJnWXS/Z4dFHidRcQ+4O+Szs6b3gs8ToGxVFzNseFJKDuW2vyg9yAkLQcuIc26vR/4JnAXcAfwFlK1gg9HxMGm2liHpEXAauBRjt3nuYl0H66oWAAkvQP4GTCK9AfaHRHxbUlnkHpBU4AtwMcj4qXmWtobSZcAX4+IJSXGktu8Mq+OBn4eEd+RdAplXmfzgFuBscBfgE+RrzfKi2UC8DfgjIg4lLcV+bn0ygnOzMxayUOUZmbWSk5wZmbWSk5wZmbWSk5wZmbWSk5wZmbWSk5wZiNA0r/y+0xJHx3mc9/Utf7IcJ7frFROcGYjaybQU4KrzGoymFckuIh4V49tMmslJzizkbUUeHeuzXV9njj6e5I2SNou6XOQHvyWtFrSPaRZNJB0V57I+LHOZMaSlgIn5/Mty9s6vUXlc+/Iddquqpz7d5V6Z8vyjDdIWqpUO3C7pO+P+P+O2TA63l+GZja8biTPWAKQE9WhiDhP0jhgraQH8rELgLdFxO68/umIOJinKNsg6c6IuFHSl/Lk092uJNXMm0uakWeDpD/kffOBc4C9wFrgIkk7gQ8CsyMiOlOimZXKPTizZl0GfCKX/1lHKpdzZt63vpLcAL4iaRvwR+C0ynGDWQQsz9UX9gO/B86rnHtPRBwFtpKGTg8B/wFuk3QlqTimWbGc4MyaJeDLkasuR8SsiOj04F54+aA0V+X7gIWRqplvAca/hn+3OrflEVKB1cOkCgArgCXAqtdwfrPGOcGZjazngYmV9fuBL+SSRkg6K0+O220S8ExEvChpNnBhZd9/Oz/fZTVwVb7PNw24GFg/WMNyzcBJEXEfcD1paNOsWL4HZzaytgNH8lDj7aT6bzOBzfmLHgeAD7zKz60CPp/vk+0iDVN23AJsl7Q5l9vpWEmqlbeNVNDyhojYlxPkq5kI3C1pPKln+dWhhWjWH1xNwMzMWslDlGZm1kpOcGZm1kpOcGZm1kpOcGZm1kpOcGZm1kpOcGZm1kpOcGZm1kpOcGZm1kr/AwPIsBFVm7nlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zoom into later iterations (finer fit)\n",
    "\n",
    "fit_vals = np.array(fit_vals)\n",
    "fig, axs = plt.subplots(2, sharex= True,  constrained_layout=True)\n",
    "fig.suptitle(\"GaussianAltFit-2D-Detector Effects Zoomed (Analytical Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}\".format(N,index_refine[1], len(fit_vals)))\n",
    "axs[0].plot(np.arange(index_refine[1], len(fit_vals[:,0])), fit_vals[index_refine[1]:,0], label='Model $\\mu$ Fit')\n",
    "axs[0].hlines(theta1_param[0], index_refine[1], len(fit_vals), label = '$\\mu_{Truth}$')\n",
    "axs[0].set_ylabel(r'$\\mu$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.arange(index_refine[1], len(fit_vals[:,1])), fit_vals[index_refine[1]:,1], label='Model $\\sigma$ Fit', color = 'orange')\n",
    "axs[1].hlines(theta1_param[1], index_refine[1], len(fit_vals), label = '$\\sigma_{Truth}$')\n",
    "axs[1].set_ylabel(r'$\\sigma$')\n",
    "axs[1].legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.savefig(\"GaussianAltFit-2D-Detector Effects Zoomed (Analytical Reweight):\\n N = {:.0e}, Iterations {:.0f} to {:.0f}.png\".format(N,index_refine[1], len(fit_vals)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
