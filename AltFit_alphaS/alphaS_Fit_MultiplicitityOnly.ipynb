{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras import Sequential\n",
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping\n",
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.utils import remap_pids\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(data_dir + '1D_alphaS_train.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['X']\n",
    "Y = dataset['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_data(X)\n",
    "Y = to_categorical(Y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mult = []\n",
    "#More preprocessing: zipping data points with no particle with parameters\n",
    "\n",
    "index = 0\n",
    "for jet in X:\n",
    "    pTs = jet[:,0]\n",
    "    alphaS = jet[0][4]\n",
    "    multiplicity = np.sum(pTs!=0)\n",
    "    X_mult.append([multiplicity, alphaS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mult = np.array(X_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_mult, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DCTR Model\n",
    "First, we need to train a DCTR model to provide us with a reweighting function to be used during fitting.\n",
    "This is taken directly from the first Gaussian Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((2,))\n",
    "hidden_layer_1 = Dense(50, activation='relu')(inputs)\n",
    "hidden_layer_2 = Dense(50, activation='relu')(hidden_layer_1)\n",
    "hidden_layer_3 = Dense(50, activation='relu')(hidden_layer_2)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(hidden_layer_3)\n",
    "\n",
    "dctr_model = Model(inputs = inputs, outputs = outputs)\n",
    "dctr_model.compile(loss='categorical_crossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DCTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1440000/1440000 [==============================] - 6s 4us/step - loss: 0.7300 - val_loss: 0.6932\n",
      "Epoch 2/200\n",
      "1440000/1440000 [==============================] - 6s 4us/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 3/200\n",
      "1440000/1440000 [==============================] - 5s 3us/step - loss: 0.6926 - val_loss: 0.6927\n",
      "Epoch 4/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6926 - val_loss: 0.6925\n",
      "Epoch 5/200\n",
      "1440000/1440000 [==============================] - 6s 4us/step - loss: 0.6925 - val_loss: 0.6926\n",
      "Epoch 6/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6925 - val_loss: 0.6925\n",
      "Epoch 7/200\n",
      "1440000/1440000 [==============================] - 6s 4us/step - loss: 0.6924 - val_loss: 0.6923\n",
      "Epoch 8/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6923 - val_loss: 0.6924\n",
      "Epoch 9/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6922 - val_loss: 0.6920\n",
      "Epoch 10/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6919 - val_loss: 0.6918\n",
      "Epoch 11/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6917 - val_loss: 0.6911\n",
      "Epoch 12/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6914 - val_loss: 0.6911\n",
      "Epoch 13/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6909 - val_loss: 0.6906\n",
      "Epoch 14/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6907 - val_loss: 0.6908\n",
      "Epoch 15/200\n",
      "1440000/1440000 [==============================] - 2s 1us/step - loss: 0.6900 - val_loss: 0.6899\n",
      "Epoch 16/200\n",
      "1440000/1440000 [==============================] - 2s 2us/step - loss: 0.6892 - val_loss: 0.6885\n",
      "Epoch 17/200\n",
      "1440000/1440000 [==============================] - 5s 3us/step - loss: 0.6890 - val_loss: 0.6884\n",
      "Epoch 18/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6886 - val_loss: 0.6880\n",
      "Epoch 19/200\n",
      "1440000/1440000 [==============================] - 3s 2us/step - loss: 0.6880 - val_loss: 0.6882\n",
      "Epoch 20/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6876 - val_loss: 0.6877\n",
      "Epoch 21/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6872 - val_loss: 0.6862\n",
      "Epoch 22/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6865 - val_loss: 0.6857\n",
      "Epoch 23/200\n",
      "1440000/1440000 [==============================] - 3s 2us/step - loss: 0.6862 - val_loss: 0.6852\n",
      "Epoch 24/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6857 - val_loss: 0.6851\n",
      "Epoch 25/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6856 - val_loss: 0.6853\n",
      "Epoch 26/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6855 - val_loss: 0.6846\n",
      "Epoch 27/200\n",
      "1440000/1440000 [==============================] - 3s 2us/step - loss: 0.6853 - val_loss: 0.6850\n",
      "Epoch 28/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6850 - val_loss: 0.6849\n",
      "Epoch 29/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6850 - val_loss: 0.6846\n",
      "Epoch 30/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6852 - val_loss: 0.6842\n",
      "Epoch 31/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6852 - val_loss: 0.6845\n",
      "Epoch 32/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6848 - val_loss: 0.6844\n",
      "Epoch 33/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6848 - val_loss: 0.6844\n",
      "Epoch 34/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6845 - val_loss: 0.6844\n",
      "Epoch 35/200\n",
      "1440000/1440000 [==============================] - 4s 2us/step - loss: 0.6849 - val_loss: 0.6847\n",
      "Epoch 36/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6848 - val_loss: 0.6842\n",
      "Epoch 37/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6844 - val_loss: 0.6846\n",
      "Epoch 38/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6846 - val_loss: 0.6841\n",
      "Epoch 39/200\n",
      "1440000/1440000 [==============================] - 3s 2us/step - loss: 0.6845 - val_loss: 0.6839\n",
      "Epoch 40/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6845 - val_loss: 0.6841\n",
      "Epoch 41/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6845 - val_loss: 0.6840\n",
      "Epoch 42/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6844 - val_loss: 0.6844\n",
      "Epoch 43/200\n",
      "1440000/1440000 [==============================] - 3s 2us/step - loss: 0.6845 - val_loss: 0.6843\n",
      "Epoch 44/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6846 - val_loss: 0.6845\n",
      "Epoch 45/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6845 - val_loss: 0.6843\n",
      "Epoch 46/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6844 - val_loss: 0.6842\n",
      "Epoch 47/200\n",
      "1440000/1440000 [==============================] - 3s 2us/step - loss: 0.6847 - val_loss: 0.6844\n",
      "Epoch 48/200\n",
      "1440000/1440000 [==============================] - 5s 3us/step - loss: 0.6846 - val_loss: 0.6840\n",
      "Epoch 49/200\n",
      "1440000/1440000 [==============================] - 4s 3us/step - loss: 0.6846 - val_loss: 0.6840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8415569090>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience = 10,\n",
    "                              restore_best_weights=True)\n",
    "dctr_model.fit(X_train, Y_train, \n",
    "               epochs=200, batch_size = 10000, \n",
    "               validation_data = (X_val, Y_val), \n",
    "               verbose = 1, \n",
    "               callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_alphaS.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = default_dataset['multiplicity']\n",
    "X_1 = unknown_dataset['multiplicity']\n",
    "\n",
    "labels0 = np.zeros(len(X_0))\n",
    "labels1 = np.ones(len(X_1))\n",
    "\n",
    "xvals = np.concatenate([X_0,X_1])\n",
    "yvals = np.concatenate([labels0,labels1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xvals, yvals, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr_model(d)\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,897\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(1,), dtype = tf.float32)\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x2 = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x2)\n",
    "          \n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0):\n",
    "    x  = inputs\n",
    "    x = K.squeeze(x, axis = 1)\n",
    "    x = K.gather(x, np.arange(500))\n",
    "\n",
    "    theta = 0. #starting value\n",
    "    #theta0 = tf.constant(val, dtype= tf.float32)#target value\n",
    "    \n",
    "    #creating tensor with same shape as inputs, with val in every entry\n",
    "    theta0_stack = K.constant(val, dtype=tf.float32, shape = x.shape)\n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.stack((x, theta0_stack), axis=-1) \n",
    "    \n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing theta = : 0.1\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 18s 20us/step - loss: 0.2326 - acc: 0.5600 - val_loss: 0.2308 - val_acc: 0.5632\n",
      "testing theta = : 0.10250000000000001\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 20s 22us/step - loss: 0.2301 - acc: 0.5634 - val_loss: 0.2304 - val_acc: 0.5612\n",
      "testing theta = : 0.10500000000000001\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 19s 21us/step - loss: 0.2300 - acc: 0.5633 - val_loss: 0.2303 - val_acc: 0.5612\n",
      "testing theta = : 0.1075\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 19s 21us/step - loss: 0.2304 - acc: 0.5633 - val_loss: 0.2305 - val_acc: 0.5637\n",
      "testing theta = : 0.11\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 19s 22us/step - loss: 0.2313 - acc: 0.5639 - val_loss: 0.2317 - val_acc: 0.5637\n",
      "testing theta = : 0.1125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 16s 18us/step - loss: 0.2325 - acc: 0.5640 - val_loss: 0.2327 - val_acc: 0.5612\n",
      "testing theta = : 0.115\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 13s 14us/step - loss: 0.2339 - acc: 0.5641 - val_loss: 0.2341 - val_acc: 0.5637\n",
      "testing theta = : 0.11750000000000001\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 17s 19us/step - loss: 0.2353 - acc: 0.5642 - val_loss: 0.2354 - val_acc: 0.5637\n",
      "testing theta = : 0.12\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 17s 19us/step - loss: 0.2367 - acc: 0.5644 - val_loss: 0.2368 - val_acc: 0.5637\n",
      "testing theta = : 0.1225\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 17s 19us/step - loss: 0.2381 - acc: 0.5646 - val_loss: 0.2383 - val_acc: 0.5637\n",
      "testing theta = : 0.125\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 17s 19us/step - loss: 0.2394 - acc: 0.5648 - val_loss: 0.2398 - val_acc: 0.5637\n",
      "testing theta = : 0.1275\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 17s 19us/step - loss: 0.2407 - acc: 0.5646 - val_loss: 0.2408 - val_acc: 0.5637\n",
      "testing theta = : 0.13\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 18s 19us/step - loss: 0.2419 - acc: 0.5647 - val_loss: 0.2420 - val_acc: 0.5637\n",
      "testing theta = : 0.1325\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 18s 20us/step - loss: 0.2431 - acc: 0.5645 - val_loss: 0.2433 - val_acc: 0.5637\n",
      "testing theta = : 0.135\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 18s 20us/step - loss: 0.2440 - acc: 0.5646 - val_loss: 0.2442 - val_acc: 0.5632\n",
      "testing theta = : 0.1375\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 18s 20us/step - loss: 0.2449 - acc: 0.5645 - val_loss: 0.2450 - val_acc: 0.5637\n",
      "testing theta = : 0.14\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 17s 19us/step - loss: 0.2457 - acc: 0.5643 - val_loss: 0.2458 - val_acc: 0.5637\n",
      "testing theta = : 0.14250000000000002\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 19s 21us/step - loss: 0.2464 - acc: 0.5639 - val_loss: 0.2465 - val_acc: 0.5637\n",
      "testing theta = : 0.145\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 15s 17us/step - loss: 0.2470 - acc: 0.5637 - val_loss: 0.2470 - val_acc: 0.5612\n",
      "testing theta = : 0.1475\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 24s 26us/step - loss: 0.2474 - acc: 0.5629 - val_loss: 0.2475 - val_acc: 0.5637\n",
      "testing theta = : 0.15\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 24s 26us/step - loss: 0.2480 - acc: 0.5627 - val_loss: 0.2480 - val_acc: 0.5612\n",
      "testing theta = : 0.1525\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 23s 26us/step - loss: 0.2482 - acc: 0.5600 - val_loss: 0.2482 - val_acc: 0.5612\n",
      "testing theta = : 0.155\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 23s 25us/step - loss: 0.2485 - acc: 0.5441 - val_loss: 0.2484 - val_acc: 0.5381\n",
      "testing theta = : 0.1575\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 24s 27us/step - loss: 0.2486 - acc: 0.5112 - val_loss: 0.2486 - val_acc: 0.4875\n",
      "testing theta = : 0.16\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 23s 25us/step - loss: 0.2487 - acc: 0.4667 - val_loss: 0.2487 - val_acc: 0.4795\n",
      "testing theta = : 0.1625\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 21s 23us/step - loss: 0.2487 - acc: 0.4478 - val_loss: 0.2487 - val_acc: 0.4438\n",
      "testing theta = : 0.16499999999999998\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 17s 19us/step - loss: 0.2487 - acc: 0.4417 - val_loss: 0.2486 - val_acc: 0.4395\n",
      "testing theta = : 0.16749999999999998\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 20s 22us/step - loss: 0.2485 - acc: 0.4399 - val_loss: 0.2484 - val_acc: 0.4387\n",
      "testing theta = : 0.16999999999999998\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 21s 23us/step - loss: 0.2481 - acc: 0.4395 - val_loss: 0.2481 - val_acc: 0.4369\n",
      "testing theta = : 0.1725\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 20s 23us/step - loss: 0.2476 - acc: 0.4389 - val_loss: 0.2475 - val_acc: 0.4398\n",
      "testing theta = : 0.175\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 21s 23us/step - loss: 0.2470 - acc: 0.4394 - val_loss: 0.2470 - val_acc: 0.4363\n",
      "testing theta = : 0.1775\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 21s 23us/step - loss: 0.2462 - acc: 0.4399 - val_loss: 0.2462 - val_acc: 0.4448\n",
      "testing theta = : 0.18\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 21s 23us/step - loss: 0.2453 - acc: 0.4415 - val_loss: 0.2453 - val_acc: 0.4398\n",
      "[[0.23077724283768072], [0.2303547474907504], [0.23033516515460278], [0.23051007977790303], [0.23170842331316735], [0.23266669512622887], [0.23414526851640807], [0.2353526222705841], [0.23681919410824775], [0.23831932039724457], [0.2398496327549219], [0.2408289631621705], [0.24195981964468957], [0.24326023645699024], [0.24417766390575302], [0.244984420388937], [0.24579806183775266], [0.2464651229315334], [0.2470395772986942], [0.2474789791388644], [0.2479649240937498], [0.24819231863651012], [0.2484465545001957], [0.2485782779753208], [0.2486919972880019], [0.24866360650294356], [0.24857006101972526], [0.2483917409264379], [0.24805585919982856], [0.24754688248038292], [0.2470116942955388], [0.2461661132093933], [0.24526541135377355]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.18, 33)\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"testing theta = :\", theta)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    model.fit(np.array(X_train), y_train, epochs=1, batch_size=500,validation_data=(np.array(X_test), y_test),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvals_orig= [[0.2205248251867791], \n",
    " [0.21686763239817486], \n",
    " [0.21641605897910066], \n",
    " [0.2165923680799703], \n",
    " [0.21757270126707023], \n",
    " [0.21946977500079407], \n",
    " [0.22129104937323266], \n",
    " [0.2230205901588003], \n",
    " [0.22444402735887303], \n",
    " [0.22604122912097308], \n",
    " [0.22767174139411914], \n",
    " [0.2295547942423986], \n",
    " [0.2314727822939555], \n",
    " [0.2333153494116333], \n",
    " [0.23476429952101574], \n",
    " [0.23622041649909484], \n",
    " [0.23832544412256942], \n",
    " [0.24102286402550008], \n",
    " [0.24376624565985466], \n",
    " [0.24562504440546035], \n",
    " [0.24651510941071642], \n",
    " [0.24727022836191787], \n",
    " [0.24861559607088565], \n",
    " [0.2502629839401278], \n",
    " [0.2513116162063347], \n",
    " [0.2514477237645123], \n",
    " [0.251843279817452], \n",
    " [0.2516317962668836], \n",
    " [0.2509480234235525], \n",
    " [0.2501125036428372], \n",
    " [0.24921633639476365], \n",
    " [0.24822285794135596],\n",
    " [0.2471035583048231]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals, label = 'multiplicity only')\n",
    "plt.plot(thetas, lvals_orig, label = 'full phase space')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Fitting alphaS with only multiplicity vs. full phase space\")\n",
    "plt.vlines(0.160, ymin = np.min(lvals_orig), ymax = np.max(lvals_orig), label = 'Truth')\n",
    "plt.legend()\n",
    "plt.savefig(\"Fitting alphaS with only multiplicity vs full phase space.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the $\\theta$ and $g$ optimization together with a minimax setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.12\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               256       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 1)                 1         \n",
      "=================================================================\n",
      "Total params: 16,898\n",
      "Trainable params: 16,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch:  0\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.2482 - acc: 0.5224 - val_loss: 0.2443 - val_acc: 0.5381\n",
      ". theta fit =  0.12\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: -0.2486 - acc: 0.5391 - val_loss: -0.2542 - val_acc: 0.5381\n",
      ". theta fit =  0.1697952\n",
      "Epoch:  1\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.2510 - acc: 0.5334 - val_loss: 0.2493 - val_acc: 0.5361\n",
      ". theta fit =  0.1697952\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: -0.2495 - acc: 0.5358 - val_loss: -0.2494 - val_acc: 0.5361\n",
      ". theta fit =  0.16746299\n",
      "Epoch:  2\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: 0.2510 - acc: 0.5174 - val_loss: 0.2491 - val_acc: 0.5558\n",
      ". theta fit =  0.16746299\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 8s 9us/step - loss: -0.2491 - acc: 0.5572 - val_loss: -0.2491 - val_acc: 0.5558\n",
      ". theta fit =  0.16714235\n",
      "Epoch:  3\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.2503 - acc: 0.5008 - val_loss: 0.2488 - val_acc: 0.4729\n",
      ". theta fit =  0.16714235\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 6s 6us/step - loss: -0.2489 - acc: 0.4717 - val_loss: -0.2489 - val_acc: 0.4729\n",
      ". theta fit =  0.16344452\n",
      "Epoch:  4\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.2504 - acc: 0.4881 - val_loss: 0.2489 - val_acc: 0.4999\n",
      ". theta fit =  0.16344452\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 9us/step - loss: -0.2489 - acc: 0.5000 - val_loss: -0.2489 - val_acc: 0.4999\n",
      ". theta fit =  0.16620716\n",
      "Epoch:  5\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: 0.2501 - acc: 0.4827 - val_loss: 0.2487 - val_acc: 0.4363\n",
      ". theta fit =  0.16620716\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: -0.2488 - acc: 0.4350 - val_loss: -0.2488 - val_acc: 0.4363\n",
      ". theta fit =  0.15963574\n",
      "Epoch:  6\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.2502 - acc: 0.4804 - val_loss: 0.2488 - val_acc: 0.4442\n",
      ". theta fit =  0.15963574\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: -0.2489 - acc: 0.4428 - val_loss: -0.2488 - val_acc: 0.4442\n",
      ". theta fit =  0.15891221\n",
      "Epoch:  7\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.2500 - acc: 0.4819 - val_loss: 0.2487 - val_acc: 0.4398\n",
      ". theta fit =  0.15891221\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: -0.2488 - acc: 0.4397 - val_loss: -0.2488 - val_acc: 0.4398\n",
      ". theta fit =  0.16440424\n",
      "Epoch:  8\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.2500 - acc: 0.4787 - val_loss: 0.2490 - val_acc: 0.4999\n",
      ". theta fit =  0.16440424\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 9s 10us/step - loss: -0.2490 - acc: 0.5000 - val_loss: -0.2490 - val_acc: 0.4999\n",
      ". theta fit =  0.16409923\n",
      "Epoch:  9\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 12us/step - loss: 0.2495 - acc: 0.4765 - val_loss: 0.2488 - val_acc: 0.4442\n",
      ". theta fit =  0.16409923\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: -0.2489 - acc: 0.4428 - val_loss: -0.2489 - val_acc: 0.4442\n",
      ". theta fit =  0.16073635\n",
      "Epoch:  10\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.2497 - acc: 0.4786 - val_loss: 0.2488 - val_acc: 0.4574\n",
      ". theta fit =  0.16073635\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: -0.2488 - acc: 0.4579 - val_loss: -0.2488 - val_acc: 0.4574\n",
      ". theta fit =  0.16407213\n",
      "Epoch:  11\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 11s 12us/step - loss: 0.2497 - acc: 0.4747 - val_loss: 0.2488 - val_acc: 0.4368\n",
      ". theta fit =  0.16407213\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: -0.2489 - acc: 0.4360 - val_loss: -0.2488 - val_acc: 0.4368\n",
      ". theta fit =  0.16029963\n",
      "Epoch:  12\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: 0.2495 - acc: 0.4777 - val_loss: 0.2488 - val_acc: 0.4363\n",
      ". theta fit =  0.16029963\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 10s 11us/step - loss: -0.2488 - acc: 0.4350 - val_loss: -0.2488 - val_acc: 0.4363\n",
      ". theta fit =  0.16308464\n",
      "Epoch:  13\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 11s 12us/step - loss: 0.2495 - acc: 0.4776 - val_loss: 0.2489 - val_acc: 0.4522\n",
      ". theta fit =  0.16308464\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 6s 7us/step - loss: -0.2490 - acc: 0.4509 - val_loss: -0.2490 - val_acc: 0.4522\n",
      ". theta fit =  0.15667962\n",
      "Epoch:  14\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 14s 15us/step - loss: 0.2491 - acc: 0.4785 - val_loss: 0.2486 - val_acc: 0.4878\n",
      ". theta fit =  0.15667962\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 12s 14us/step - loss: -0.2489 - acc: 0.4879 - val_loss: -0.2489 - val_acc: 0.4878\n",
      ". theta fit =  0.16545841\n",
      "Epoch:  15\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 13s 14us/step - loss: 0.2492 - acc: 0.4719 - val_loss: 0.2487 - val_acc: 0.4368\n",
      ". theta fit =  0.16545841\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 12s 13us/step - loss: -0.2488 - acc: 0.4360 - val_loss: -0.2488 - val_acc: 0.4368\n",
      ". theta fit =  0.16007978\n",
      "Epoch:  16\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 12s 14us/step - loss: 0.2490 - acc: 0.4628 - val_loss: 0.2488 - val_acc: 0.4574\n",
      ". theta fit =  0.16007978\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 12s 13us/step - loss: -0.2489 - acc: 0.4579 - val_loss: -0.2488 - val_acc: 0.4574\n",
      ". theta fit =  0.16371326\n",
      "Epoch:  17\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 13s 14us/step - loss: 0.2493 - acc: 0.4738 - val_loss: 0.2488 - val_acc: 0.4442\n",
      ". theta fit =  0.16371326\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 13s 14us/step - loss: -0.2489 - acc: 0.4428 - val_loss: -0.2489 - val_acc: 0.4442\n",
      ". theta fit =  0.16011083\n",
      "Epoch:  18\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 14s 15us/step - loss: 0.2490 - acc: 0.4687 - val_loss: 0.2489 - val_acc: 0.4999\n",
      ". theta fit =  0.16011083\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 12s 14us/step - loss: -0.2490 - acc: 0.5001 - val_loss: -0.2490 - val_acc: 0.4999\n",
      ". theta fit =  0.16377968\n",
      "Epoch:  19\n",
      "Training g\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 12s 14us/step - loss: 0.2490 - acc: 0.4646 - val_loss: 0.2488 - val_acc: 0.4388\n",
      ". theta fit =  0.16377968\n",
      "Training theta\n",
      "Train on 900000 samples, validate on 900000 samples\n",
      "Epoch 1/1\n",
      "900000/900000 [==============================] - 11s 13us/step - loss: -0.2489 - acc: 0.4376 - val_loss: -0.2489 - val_acc: 0.4388\n",
      ". theta fit =  0.16006933\n"
     ]
    }
   ],
   "source": [
    "myinputs_fit = Input(shape=(1,))\n",
    "x_fit = Dense(128, activation='relu')(myinputs_fit)\n",
    "x2_fit = Dense(128, activation='relu')(x_fit)\n",
    "predictions_fit = Dense(1, activation='sigmoid')(x2_fit)\n",
    "identity = Lambda(lambda x: x + 0)(predictions_fit)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),initializer = keras.initializers.Constant(value = theta_fit_init),trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = int(len(X_0)/50) #larger batch_size leads to better precision\n",
    "epochs = 20 #but requires more epochs to train\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs\n",
    "    x = K.squeeze(x, axis = 1)\n",
    "    x = K.gather(x, np.arange(batch_size))\n",
    "    theta = 0. #starting value\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when trainingn theta, fetch as tf.Variable\n",
    "        \n",
    "    #creating tensor with same shape as inputs, with val in every entry \n",
    "    theta0_stack = K.ones_like(x,dtype=tf.float32)*theta0 \n",
    "    \n",
    "    #combining and reshaping into correct format:\n",
    "    data = K.stack((x, theta0_stack), axis=-1) \n",
    "   \n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    \n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(np.array(X_train), y_train, epochs=1, batch_size=batch_size,validation_data=(np.array(X_test), y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.16, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"N = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}\".format(len(X_0), batch_size, epochs*2))\n",
    "#plt.savefig(\":N = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}\".format(N, batch_size, epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
