{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_alphaS.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small = X_train[0:int(0.8*10**5)]\n",
    "Y_train_small = Y_train[0:int(0.8*10**5)]\n",
    "X_test_small = X_test[0:int(0.2*10**5)]\n",
    "Y_test_small = Y_test[0:int(0.2*10**5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_alphaS_modified.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    #Creating theta_prime\n",
    "    alphaS = K.ones(shape =x.shape[0:2])*val # Fitting parameter\n",
    "    aLund = K.ones(shape =x.shape[0:2])*0.68 # Fixed at default\n",
    "    probStoUD = K.ones(shape =x.shape[0:2])*0.217 # Fixed at default\n",
    "    theta_prime = K.stack((alphaS, aLund, probStoUD), axis = 2)\n",
    "\n",
    "\n",
    "    data = K.concatenate((x, theta_prime), axis =2)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 159s 110us/step - loss: 0.2214 - acc: 0.5779 - val_loss: 0.2174 - val_acc: 0.5806\n",
      "trainnig theta = : 0.10250000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 152s 106us/step - loss: 0.2170 - acc: 0.5824 - val_loss: 0.2170 - val_acc: 0.5825\n",
      "trainnig theta = : 0.10500000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 155s 108us/step - loss: 0.2165 - acc: 0.5833 - val_loss: 0.2165 - val_acc: 0.5807\n",
      "trainnig theta = : 0.1075\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2166 - acc: 0.5837 - val_loss: 0.2167 - val_acc: 0.5815\n",
      "trainnig theta = : 0.11\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2176 - acc: 0.5840 - val_loss: 0.2179 - val_acc: 0.5814\n",
      "trainnig theta = : 0.1125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2194 - acc: 0.5848 - val_loss: 0.2198 - val_acc: 0.5824\n",
      "trainnig theta = : 0.115\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2213 - acc: 0.5849 - val_loss: 0.2223 - val_acc: 0.5797\n",
      "trainnig theta = : 0.11750000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 102us/step - loss: 0.2229 - acc: 0.5854 - val_loss: 0.2236 - val_acc: 0.5842\n",
      "trainnig theta = : 0.12\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2244 - acc: 0.5851 - val_loss: 0.2248 - val_acc: 0.5840\n",
      "trainnig theta = : 0.1225\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2260 - acc: 0.5859 - val_loss: 0.2271 - val_acc: 0.5811\n",
      "trainnig theta = : 0.125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2277 - acc: 0.5855 - val_loss: 0.2283 - val_acc: 0.5848\n",
      "trainnig theta = : 0.1275\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2295 - acc: 0.5861 - val_loss: 0.2302 - val_acc: 0.5844\n",
      "trainnig theta = : 0.13\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2314 - acc: 0.5865 - val_loss: 0.2321 - val_acc: 0.5841\n",
      "trainnig theta = : 0.1325\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2333 - acc: 0.5865 - val_loss: 0.2342 - val_acc: 0.5831\n",
      "trainnig theta = : 0.135\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2348 - acc: 0.5867 - val_loss: 0.2358 - val_acc: 0.5815\n",
      "trainnig theta = : 0.1375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2362 - acc: 0.5865 - val_loss: 0.2369 - val_acc: 0.5847\n",
      "trainnig theta = : 0.14\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2383 - acc: 0.5870 - val_loss: 0.2389 - val_acc: 0.5847\n",
      "trainnig theta = : 0.14250000000000002\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2410 - acc: 0.5864 - val_loss: 0.2416 - val_acc: 0.5844\n",
      "trainnig theta = : 0.145\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2438 - acc: 0.5860 - val_loss: 0.2443 - val_acc: 0.5821\n",
      "trainnig theta = : 0.1475\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2456 - acc: 0.5849 - val_loss: 0.2463 - val_acc: 0.5776\n",
      "trainnig theta = : 0.15\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2465 - acc: 0.5839 - val_loss: 0.2470 - val_acc: 0.5802\n",
      "trainnig theta = : 0.1525\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2473 - acc: 0.5827 - val_loss: 0.2478 - val_acc: 0.5791\n",
      "trainnig theta = : 0.155\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2486 - acc: 0.5786 - val_loss: 0.2491 - val_acc: 0.5772\n",
      "trainnig theta = : 0.1575\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2503 - acc: 0.5647 - val_loss: 0.2507 - val_acc: 0.5643\n",
      "trainnig theta = : 0.16\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2514 - acc: 0.5319 - val_loss: 0.2517 - val_acc: 0.5169\n",
      "trainnig theta = : 0.1625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 147s 102us/step - loss: 0.2515 - acc: 0.5030 - val_loss: 0.2519 - val_acc: 0.4980\n",
      "trainnig theta = : 0.16499999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2519 - acc: 0.4759 - val_loss: 0.2524 - val_acc: 0.4799\n",
      "trainnig theta = : 0.16749999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2517 - acc: 0.4575 - val_loss: 0.2522 - val_acc: 0.4548\n",
      "trainnig theta = : 0.16999999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 148s 103us/step - loss: 0.2510 - acc: 0.4518 - val_loss: 0.2514 - val_acc: 0.4385\n",
      "trainnig theta = : 0.1725\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 103us/step - loss: 0.2502 - acc: 0.4484 - val_loss: 0.2507 - val_acc: 0.4424\n",
      "trainnig theta = : 0.175\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 149s 104us/step - loss: 0.2492 - acc: 0.4448 - val_loss: 0.2499 - val_acc: 0.4343\n",
      "trainnig theta = : 0.1775\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 150s 104us/step - loss: 0.2482 - acc: 0.4444 - val_loss: 0.2491 - val_acc: 0.4464\n",
      "trainnig theta = : 0.18\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 151s 105us/step - loss: 0.2471 - acc: 0.4435 - val_loss: 0.2479 - val_acc: 0.4380\n",
      "[[0.22137477812874648], [0.21703989108403524], [0.21648895731195808], [0.21661257067074377], [0.21762132816430596], [0.21944715375494625], [0.22126773997313445], [0.2229303162234525], [0.22444050016088618], [0.22599984856529368], [0.22770258308284813], [0.22953290323623352], [0.23142603193927141], [0.2333012963231239], [0.23475367535526553], [0.23622168957566222], [0.23828660022053455], [0.24102714501528277], [0.24377428261149262], [0.24562977058812976], [0.2465265239795877], [0.24727935270509785], [0.2486292091715667], [0.2502777778957453], [0.2513585097880827], [0.25148530460687146], [0.2518951222817931], [0.2516558851115406], [0.2509761217671136], [0.25015956332079237], [0.24922220586902566], [0.24818791372494564], [0.2470780098810792]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.18, 33) #iterating across possible alphaS values\n",
    "vlvals = []\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals, label = \"lvals\")\n",
    "plt.plot(thetas,vlvals, label = \"vlvals\")\n",
    "plt.vlines(0.160, ymin = np.min(lvals), ymax = np.max(lvals), label = 'Truth')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"MSE for alphaS altFit SUCCESS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx  = X_test\\nprint(x.shape)\\n#x = K.squeeze(x, axis = 1)\\nx = tf.gather(x, np.arange(2))\\nx = tf.gather(x, np.arange(51), axis = 1)\\nprint(x.shape)\\n\\npar1 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.16\\npar2 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.68\\npar3 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.217\\npar = K.stack((par1, par2, par3), axis = 2)\\n\\n#combining and reshaping into correct format:\\n#data = K.stack((x, theta0_stack), axis=-1) \\ndata = K.concatenate((x, par), axis =2)\\nprint(data.shape)\\nw = reweight(data)\\nprint(w.shape)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check to ensure that \"data\" in loss wrapper is being constructed properly\n",
    "\n",
    "'''\n",
    "x  = X_test\n",
    "print(x.shape)\n",
    "#x = K.squeeze(x, axis = 1)\n",
    "x = tf.gather(x, np.arange(2))\n",
    "x = tf.gather(x, np.arange(51), axis = 1)\n",
    "print(x.shape)\n",
    "\n",
    "par1 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.16\n",
    "par2 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.68\n",
    "par3 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.217\n",
    "par = K.stack((par1, par2, par3), axis = 2)\n",
    "\n",
    "#combining and reshaping into correct format:\n",
    "#data = K.stack((x, theta0_stack), axis=-1) \n",
    "data = K.concatenate((x, par), axis =2)\n",
    "print(data.shape)\n",
    "w = reweight(data)\n",
    "print(w.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.Session() as sess:\\n    print(K.eval(data))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with tf.Session() as sess:\n",
    "    print(K.eval(data))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.12\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            1           activation_14[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch:  0\n",
      "Training g\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 162s 113us/step - loss: 0.2510 - acc: 0.5507 - val_loss: 0.2325 - val_acc: 0.5710\n",
      ". theta fit =  0.12\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2432 - acc: 0.5719 - val_loss: -0.2607 - val_acc: 0.5710\n",
      ". theta fit =  0.15416707\n",
      "Epoch:  1\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 161s 112us/step - loss: 0.2646 - acc: 0.5270 - val_loss: 0.2495 - val_acc: 0.5443\n",
      ". theta fit =  0.15416707\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 129us/step - loss: -0.2533 - acc: 0.5462 - val_loss: -0.2543 - val_acc: 0.5443\n",
      ". theta fit =  0.17169042\n",
      "Epoch:  2\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 160s 111us/step - loss: 0.2589 - acc: 0.4754 - val_loss: 0.2523 - val_acc: 0.4384\n",
      ". theta fit =  0.17169042\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 185s 128us/step - loss: -0.2529 - acc: 0.4378 - val_loss: -0.2532 - val_acc: 0.4384\n",
      ". theta fit =  0.16544263\n",
      "Epoch:  3\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 161s 112us/step - loss: 0.2537 - acc: 0.4632 - val_loss: 0.2527 - val_acc: 0.4432\n",
      ". theta fit =  0.16544263\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 186s 129us/step - loss: -0.2526 - acc: 0.4431 - val_loss: -0.2528 - val_acc: 0.4432\n",
      ". theta fit =  0.16664709\n",
      "Epoch:  4\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 162s 112us/step - loss: 0.2539 - acc: 0.4560 - val_loss: 0.2528 - val_acc: 0.4399\n",
      ". theta fit =  0.16664709\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2526 - acc: 0.4401 - val_loss: -0.2527 - val_acc: 0.4399\n",
      ". theta fit =  0.16615531\n",
      "Epoch:  5\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 161s 112us/step - loss: 0.2536 - acc: 0.4556 - val_loss: 0.2527 - val_acc: 0.4404\n",
      ". theta fit =  0.16615531\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2526 - acc: 0.4399 - val_loss: -0.2527 - val_acc: 0.4404\n",
      ". theta fit =  0.1665804\n",
      "Epoch:  6\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 162s 112us/step - loss: 0.2530 - acc: 0.4456 - val_loss: 0.2526 - val_acc: 0.4329\n",
      ". theta fit =  0.1665804\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 130us/step - loss: -0.2525 - acc: 0.4328 - val_loss: -0.2526 - val_acc: 0.4329\n",
      ". theta fit =  0.16604024\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 162s 113us/step - loss: 0.2527 - acc: 0.4371 - val_loss: 0.2526 - val_acc: 0.4370\n",
      ". theta fit =  0.16604024\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2525 - acc: 0.4366 - val_loss: -0.2526 - val_acc: 0.4370\n",
      ". theta fit =  0.1666991\n",
      "Epoch:  8\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 113us/step - loss: 0.2527 - acc: 0.4374 - val_loss: 0.2525 - val_acc: 0.4300\n",
      ". theta fit =  0.1666991\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2524 - acc: 0.4306 - val_loss: -0.2525 - val_acc: 0.4300\n",
      ". theta fit =  0.16599078\n",
      "Epoch:  9\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 114us/step - loss: 0.2526 - acc: 0.4392 - val_loss: 0.2527 - val_acc: 0.4452\n",
      ". theta fit =  0.16599078\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 190s 132us/step - loss: -0.2525 - acc: 0.4448 - val_loss: -0.2527 - val_acc: 0.4452\n",
      ". theta fit =  0.16653258\n",
      "Epoch:  10\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2525 - acc: 0.4359 - val_loss: 0.2525 - val_acc: 0.4337\n",
      ". theta fit =  0.16653258\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 191s 133us/step - loss: -0.2524 - acc: 0.4333 - val_loss: -0.2525 - val_acc: 0.4337\n",
      ". theta fit =  0.16562065\n",
      "Epoch:  11\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 165s 115us/step - loss: 0.2526 - acc: 0.4397 - val_loss: 0.2525 - val_acc: 0.4359\n",
      ". theta fit =  0.16562065\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 193s 134us/step - loss: -0.2524 - acc: 0.4358 - val_loss: -0.2525 - val_acc: 0.4359\n",
      ". theta fit =  0.1667716\n",
      "Epoch:  12\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 165s 115us/step - loss: 0.2525 - acc: 0.4351 - val_loss: 0.2525 - val_acc: 0.4352\n",
      ". theta fit =  0.1667716\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 193s 134us/step - loss: -0.2524 - acc: 0.4350 - val_loss: -0.2525 - val_acc: 0.4352\n",
      ". theta fit =  0.16533259\n",
      "Epoch:  13\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 165s 115us/step - loss: 0.2523 - acc: 0.4337 - val_loss: 0.2524 - val_acc: 0.4326\n",
      ". theta fit =  0.16533259\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 193s 134us/step - loss: -0.2523 - acc: 0.4325 - val_loss: -0.2524 - val_acc: 0.4326\n",
      ". theta fit =  0.16598596\n",
      "Epoch:  14\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 166s 115us/step - loss: 0.2524 - acc: 0.4345 - val_loss: 0.2525 - val_acc: 0.4316\n",
      ". theta fit =  0.16598596\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1260000/1440000 [=========================>....] - ETA: 21s - loss: -0.2523 - acc: 0.4311"
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = int(len(X_default)/20) #larger batch_size leads to better precision (at least for Guassian case)\n",
    "epochs = 15 #but requires more epochs to train\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "        \n",
    "    #Creating theta_prime\n",
    "    alphaS = K.ones(shape =x.shape[0:2])*theta0 # Fitting parameter\n",
    "    aLund = K.ones(shape =x.shape[0:2])*0.68 # Fixed at default\n",
    "    probStoUD = K.ones(shape =x.shape[0:2])*0.217 # Fixed at default\n",
    "    \n",
    "    theta_prime = K.stack((alphaS, aLund, probStoUD), axis = 2)\n",
    "    \n",
    "    data = K.concatenate((x, theta_prime), axis =2)\n",
    "    # print(data.shape) # = (batch_size, 51, 7); correct format to pass to DCTR\n",
    "   \n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.16, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"(Starting at fit = 0.12) \\nN = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}\".format(len(X_default), batch_size, epochs*2))\n",
    "plt.savefig(\"(Starting at fit = 0.12) N = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}.png\".format(len(X_default), batch_size, epochs))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
