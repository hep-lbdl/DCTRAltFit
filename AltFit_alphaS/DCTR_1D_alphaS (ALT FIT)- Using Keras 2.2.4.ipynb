{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__) #2.2.4\n",
    "print(tf.__version__) #1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to downloaded data from Zenodo\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = np.load(data_dir + 'test1D_default.npz')\n",
    "unknown_dataset = np.load(data_dir + 'test1D_alphaS.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(default_dataset['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(unknown_dataset['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data_split(X_fit, Y_fit, test=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small = X_train[0:int(0.8*10**5)]\n",
    "Y_train_small = Y_train[0:int(0.8*10**5)]\n",
    "X_test_small = X_test[0:int(0.2*10**5)]\n",
    "Y_test_small = Y_test[0:int(0.2*10**5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)\n",
    "\n",
    "# load model from saved file\n",
    "# model trained in original alphaS notebook\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_alphaS.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "$w(x_{T,i},\\theta)=((f(x_{T,i},\\theta)/(1-f(x_{T,i},\\theta)))$\n",
    "\n",
    "Takes observable from simulation ${\\bf \\theta_0}$ and weights it to observable from data (target) ${\\bf \\theta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining reweighting functions\n",
    "\n",
    "def reweight(d): #from NN (DCTR)\n",
    "    f = dctr.model(d) # Use dctr.model.predict_on_batch(d) when using outside training\n",
    "    weights = (f[:,1])/(f[:,0])\n",
    "    weights = K.expand_dims(weights, axis = 1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs = model.inputs[0]\n",
    "batch_size = 1000\n",
    "\n",
    "def my_loss_wrapper(inputs,val=0):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    #Creating theta_prime\n",
    "    alphaS = K.ones(shape =x.shape[0:2])*val # Fitting parameter\n",
    "    aLund = K.ones(shape =x.shape[0:2])*0.68 # Fixed at default\n",
    "    probStoUD = K.ones(shape =x.shape[0:2])*0.217 # Fixed at default\n",
    "    theta_prime = K.stack((alphaS, aLund, probStoUD), axis = 2)\n",
    "\n",
    "\n",
    "    data = K.concatenate((x, theta_prime), axis =2)\n",
    "    # print(data.shape) # = (batch_size, 51, 7), correct format to pass to DCTR\n",
    "    w = reweight(data) # NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean-Squared Loss:\n",
    "        t_loss = (y_true)*(y_true - y_pred)**2 +(w)*(1-y_true)*(y_true - y_pred)**2\n",
    "        \n",
    "        # Categorical Cross-Entropy Loss\n",
    "        '''\n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1    0.1025 0.105  0.1075 0.11   0.1125 0.115  0.1175 0.12   0.1225\n",
      " 0.125  0.1275 0.13   0.1325 0.135  0.1375 0.14   0.1425 0.145  0.1475\n",
      " 0.15   0.1525 0.155  0.1575 0.16   0.1625 0.165  0.1675 0.17   0.1725\n",
      " 0.175  0.1775 0.18  ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnig theta = : 0.1\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 57s 39us/step - loss: 0.1903 - acc: 0.5797 - val_loss: 0.1904 - val_acc: 0.5810\n",
      "trainnig theta = : 0.10250000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 58s 40us/step - loss: 0.1920 - acc: 0.5802 - val_loss: 0.1922 - val_acc: 0.5769\n",
      "trainnig theta = : 0.10500000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 58s 40us/step - loss: 0.1949 - acc: 0.5802 - val_loss: 0.1954 - val_acc: 0.5756\n",
      "trainnig theta = : 0.1075\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 58s 40us/step - loss: 0.1993 - acc: 0.5813 - val_loss: 0.1998 - val_acc: 0.5826\n",
      "trainnig theta = : 0.11\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 58s 40us/step - loss: 0.2050 - acc: 0.5821 - val_loss: 0.2054 - val_acc: 0.5820\n",
      "trainnig theta = : 0.1125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 58s 40us/step - loss: 0.2118 - acc: 0.5831 - val_loss: 0.2128 - val_acc: 0.5840\n",
      "trainnig theta = : 0.115\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 59s 41us/step - loss: 0.2164 - acc: 0.5842 - val_loss: 0.2168 - val_acc: 0.5830\n",
      "trainnig theta = : 0.11750000000000001\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 56s 39us/step - loss: 0.2197 - acc: 0.5844 - val_loss: 0.2199 - val_acc: 0.5830\n",
      "trainnig theta = : 0.12\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 59s 41us/step - loss: 0.2229 - acc: 0.5853 - val_loss: 0.2234 - val_acc: 0.5830\n",
      "trainnig theta = : 0.1225\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 58s 41us/step - loss: 0.2255 - acc: 0.5860 - val_loss: 0.2262 - val_acc: 0.5834\n",
      "trainnig theta = : 0.125\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 59s 41us/step - loss: 0.2302 - acc: 0.5862 - val_loss: 0.2308 - val_acc: 0.5849\n",
      "trainnig theta = : 0.1275\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 59s 41us/step - loss: 0.2355 - acc: 0.5868 - val_loss: 0.2362 - val_acc: 0.5836\n",
      "trainnig theta = : 0.13\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 60s 41us/step - loss: 0.2415 - acc: 0.5854 - val_loss: 0.2420 - val_acc: 0.5826\n",
      "trainnig theta = : 0.1325\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 60s 42us/step - loss: 0.2476 - acc: 0.5826 - val_loss: 0.2479 - val_acc: 0.5810\n",
      "trainnig theta = : 0.135\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 60s 42us/step - loss: 0.2518 - acc: 0.5777 - val_loss: 0.2523 - val_acc: 0.5734\n",
      "trainnig theta = : 0.1375\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 60s 42us/step - loss: 0.2542 - acc: 0.5706 - val_loss: 0.2545 - val_acc: 0.5672\n",
      "trainnig theta = : 0.14\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 60s 42us/step - loss: 0.2548 - acc: 0.5614 - val_loss: 0.2550 - val_acc: 0.5640\n",
      "trainnig theta = : 0.14250000000000002\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 60s 42us/step - loss: 0.2562 - acc: 0.5511 - val_loss: 0.2563 - val_acc: 0.5535\n",
      "trainnig theta = : 0.145\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 61s 42us/step - loss: 0.2562 - acc: 0.5408 - val_loss: 0.2563 - val_acc: 0.5323\n",
      "trainnig theta = : 0.1475\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 61s 42us/step - loss: 0.2549 - acc: 0.5334 - val_loss: 0.2551 - val_acc: 0.5244\n",
      "trainnig theta = : 0.15\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 61s 42us/step - loss: 0.2536 - acc: 0.5244 - val_loss: 0.2539 - val_acc: 0.5373\n",
      "trainnig theta = : 0.1525\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 58s 40us/step - loss: 0.2519 - acc: 0.5185 - val_loss: 0.2522 - val_acc: 0.5058\n",
      "trainnig theta = : 0.155\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 61s 42us/step - loss: 0.2499 - acc: 0.5131 - val_loss: 0.2501 - val_acc: 0.5125\n",
      "trainnig theta = : 0.1575\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 61s 42us/step - loss: 0.2479 - acc: 0.5080 - val_loss: 0.2481 - val_acc: 0.5101\n",
      "trainnig theta = : 0.16\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 61s 43us/step - loss: 0.2464 - acc: 0.5046 - val_loss: 0.2467 - val_acc: 0.5118\n",
      "trainnig theta = : 0.1625\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 62s 43us/step - loss: 0.2449 - acc: 0.5028 - val_loss: 0.2453 - val_acc: 0.5108\n",
      "trainnig theta = : 0.16499999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 62s 43us/step - loss: 0.2433 - acc: 0.5008 - val_loss: 0.2439 - val_acc: 0.5075\n",
      "trainnig theta = : 0.16749999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 62s 43us/step - loss: 0.2417 - acc: 0.5020 - val_loss: 0.2423 - val_acc: 0.4957\n",
      "trainnig theta = : 0.16999999999999998\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 62s 43us/step - loss: 0.2400 - acc: 0.5005 - val_loss: 0.2407 - val_acc: 0.4917\n",
      "trainnig theta = : 0.1725\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 63s 43us/step - loss: 0.2384 - acc: 0.5005 - val_loss: 0.2395 - val_acc: 0.4988\n",
      "trainnig theta = : 0.175\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 63s 44us/step - loss: 0.2368 - acc: 0.5008 - val_loss: 0.2377 - val_acc: 0.4935\n",
      "trainnig theta = : 0.1775\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 63s 44us/step - loss: 0.2354 - acc: 0.5008 - val_loss: 0.2364 - val_acc: 0.4982\n",
      "trainnig theta = : 0.18\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 63s 44us/step - loss: 0.2331 - acc: 0.5024 - val_loss: 0.2342 - val_acc: 0.5029\n",
      "[[0.1903308656687538], [0.19204508746042848], [0.19491921071377066], [0.19932102270217406], [0.2050324663416379], [0.21180334996639028], [0.21641631762807567], [0.21969873398128484], [0.22289305718408692], [0.2254909612238407], [0.2301976298706399], [0.23552407997970778], [0.24151804325067336], [0.2475613575014803], [0.2518305274657905], [0.2542412154169546], [0.25480665294453503], [0.2561565833890604], [0.25616011476765077], [0.25493173128407864], [0.25364350528559754], [0.251894533354789], [0.2499365414906707], [0.24788056754817564], [0.24637446583559117], [0.24488399008082018], [0.2432920131728881], [0.24169098179166515], [0.24003212341210908], [0.23836636658137042], [0.23681943538702196], [0.23535373205732968], [0.2330690062397884]]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.10, 0.18, 33) #iterating across possible alphaS values\n",
    "vlvals = []\n",
    "lvals = []\n",
    "\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2778513290>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thetas,lvals, label = \"lvals\")\n",
    "plt.plot(thetas,vlvals, label = \"vlvals\")\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.savefig(\"MSE for alphaS altFit2.png\")\n",
    "\n",
    "#incorrect maxima value? should be at alphaS = theta = 0.160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check to ensure that \"data\" in loss wrapper is being constructed properly\n",
    "\n",
    "'''\n",
    "x  = X_test\n",
    "print(x.shape)\n",
    "#x = K.squeeze(x, axis = 1)\n",
    "x = tf.gather(x, np.arange(2))\n",
    "x = tf.gather(x, np.arange(51), axis = 1)\n",
    "print(x.shape)\n",
    "\n",
    "par1 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.16\n",
    "par2 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.68\n",
    "par3 = K.ones(shape =x.shape[0:2], dtype= tf.float64)*0.217\n",
    "par = K.stack((par1, par2, par3), axis = 2)\n",
    "\n",
    "#combining and reshaping into correct format:\n",
    "#data = K.stack((x, theta0_stack), axis=-1) \n",
    "data = K.concatenate((x, par), axis =2)\n",
    "print(data.shape)\n",
    "w = reweight(data)\n",
    "print(w.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "with tf.Session() as sess:\n",
    "    print(K.eval(data))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.18\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    500         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 1)            0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            1           activation_21[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 56,730\n",
      "Trainable params: 56,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch:  0\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 113us/step - loss: 0.2610 - acc: 0.4867 - val_loss: 0.2379 - val_acc: 0.4853\n",
      ". theta fit =  0.18\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2575 - acc: 0.4869 - val_loss: -0.2630 - val_acc: 0.4853\n",
      ". theta fit =  0.13767204\n",
      "Epoch:  1\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 113us/step - loss: 0.2587 - acc: 0.5452 - val_loss: 0.2560 - val_acc: 0.5378\n",
      ". theta fit =  0.13767204\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2578 - acc: 0.5382 - val_loss: -0.2582 - val_acc: 0.5378\n",
      ". theta fit =  0.14448263\n",
      "Epoch:  2\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 113us/step - loss: 0.2590 - acc: 0.5327 - val_loss: 0.2573 - val_acc: 0.5381\n",
      ". theta fit =  0.14448263\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 187s 130us/step - loss: -0.2571 - acc: 0.5388 - val_loss: -0.2573 - val_acc: 0.5381\n",
      ". theta fit =  0.14456691\n",
      "Epoch:  3\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 113us/step - loss: 0.2578 - acc: 0.5344 - val_loss: 0.2572 - val_acc: 0.5290\n",
      ". theta fit =  0.14456691\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2571 - acc: 0.5285 - val_loss: -0.2573 - val_acc: 0.5290\n",
      ". theta fit =  0.14396328\n",
      "Epoch:  4\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 163s 113us/step - loss: 0.2574 - acc: 0.5337 - val_loss: 0.2573 - val_acc: 0.5203\n",
      ". theta fit =  0.14396328\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 188s 131us/step - loss: -0.2571 - acc: 0.5195 - val_loss: -0.2573 - val_acc: 0.5203\n",
      ". theta fit =  0.14416194\n",
      "Epoch:  5\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2572 - acc: 0.5356 - val_loss: 0.2571 - val_acc: 0.5482\n",
      ". theta fit =  0.14416194\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2569 - acc: 0.5493 - val_loss: -0.2571 - val_acc: 0.5482\n",
      ". theta fit =  0.14363432\n",
      "Epoch:  6\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2571 - acc: 0.5388 - val_loss: 0.2570 - val_acc: 0.5260\n",
      ". theta fit =  0.14363432\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2568 - acc: 0.5258 - val_loss: -0.2570 - val_acc: 0.5260\n",
      ". theta fit =  0.14401065\n",
      "Epoch:  7\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2570 - acc: 0.5366 - val_loss: 0.2570 - val_acc: 0.5389\n",
      ". theta fit =  0.14401065\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2567 - acc: 0.5393 - val_loss: -0.2570 - val_acc: 0.5389\n",
      ". theta fit =  0.14359935\n",
      "Epoch:  8\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2568 - acc: 0.5404 - val_loss: 0.2569 - val_acc: 0.5311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". theta fit =  0.14359935\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2567 - acc: 0.5310 - val_loss: -0.2569 - val_acc: 0.5311\n",
      ". theta fit =  0.14323358\n",
      "Epoch:  9\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 164s 114us/step - loss: 0.2568 - acc: 0.5419 - val_loss: 0.2570 - val_acc: 0.5555\n",
      ". theta fit =  0.14323358\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 189s 131us/step - loss: -0.2568 - acc: 0.5570 - val_loss: -0.2571 - val_acc: 0.5555\n",
      ". theta fit =  0.14406689\n",
      "Epoch:  10\n",
      "Training g\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1440000/1440000 [==============================] - 165s 114us/step - loss: 0.2567 - acc: 0.5394 - val_loss: 0.2569 - val_acc: 0.5238\n",
      ". theta fit =  0.14406689\n",
      "Training theta\n",
      "Train on 1440000 samples, validate on 360000 samples\n",
      "Epoch 1/1\n",
      "1425600/1440000 [============================>.] - ETA: 1s - loss: -0.2567 - acc: 0.5233"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b8e8ba4e082b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m#model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training theta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asuresh/.local/lib/python2.7/site-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PFN_model = PFN(input_dim=4, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "batch_size = int(1440000/100) #larger batch_size leads to better precision (at least for Guassian case)\n",
    "epochs = 10 #but requires more epochs to train\n",
    "\n",
    "def my_loss_wrapper_fit(inputs,mysign = 1):\n",
    "    x  = inputs #x.shape = (?,?,4)\n",
    "    # Reshaping to correct format\n",
    "    x = tf.gather(x, np.arange(batch_size))\n",
    "    x = tf.gather(x, np.arange(51), axis = 1) # Axis corressponds to (max) number of particles in each event\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "        \n",
    "    #Creating theta_prime\n",
    "    alphaS = K.ones(shape =x.shape[0:2])*theta0 # Fitting parameter\n",
    "    aLund = K.ones(shape =x.shape[0:2])*0.68 # Fixed at default\n",
    "    probStoUD = K.ones(shape =x.shape[0:2])*0.217 # Fixed at default\n",
    "    \n",
    "    theta_prime = K.stack((alphaS, aLund, probStoUD), axis = 2)\n",
    "    \n",
    "    data = K.concatenate((x, theta_prime), axis =2)\n",
    "    # print(data.shape) # = (batch_size, 51, 7); correct format to pass to DCTR\n",
    "   \n",
    "    w = reweight(data) #NN reweight\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        # Mean Squared Loss\n",
    "        t_loss = mysign*(y_true*(y_true - y_pred)**2+(w)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        # Categorical Cross-Entropy Loss\n",
    "        \n",
    "        #Clip the prediction value to prevent NaN's and Inf's\n",
    "        '''\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        t_loss = -mysign*((y_true)*K.log(y_pred) +w*(1-y_true)*K.log(1-y_pred))\n",
    "        '''\n",
    "        return K.mean(t_loss)\n",
    "    return my_loss\n",
    "    \n",
    "for k in range(epochs):    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        train_theta = False\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    #model.summary()    \n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,1),metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)\n",
    "\n",
    "    #Now, fix g and train \\theta.\n",
    "\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    \n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit,-1),metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1,callbacks=callbacks)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.16, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "#plt.title(\"N = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}\".format(N, batch_size, epochs*2))\n",
    "#plt.savefig(\":N = {:.0e}, batch_size = {:.0f}, Epochs = {:.0f}\".format(N, batch_size, epochs))\n",
    "plt.title(\"alpha S Fit\")\n",
    "plt.savefig(\"alphaSFit.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
